

# 三、拟合数据的代码

在本章中，我们将通过使用上一章中讨论的 MLOps 工作流来解决业务问题，从动手 **MLOps** 实现开始。我们还将讨论用于**机器学习** ( **ML** )的源代码管理的有效方法，探索数据质量特征，并为 ML 解决方案分析和形成数据。

本章开始时，我们对业务问题进行分类，以便为 it 策划最合适的 MLOps 解决方案。接下来，我们将设置实现解决方案所需的资源和工具。讨论了 ML 源代码管理的 10 个指导原则，以应用干净的代码实践。我们将讨论什么构成了 ML 的高质量数据等等，然后处理与业务问题相关的数据集，并将其接收和版本化到 ML 工作空间。本章的大部分内容都是实践操作，旨在让您对 MLOps 有一个很好的理解和体验。为此，我们将在本章中讨论以下主要话题:

*   业务问题分析和问题分类
*   设置资源和工具
*   机器学习的 10 个源代码管理原则
*   机器学习的好数据
*   数据预处理
*   数据注册和版本控制
*   走向 ML 管道

事不宜迟，让我们开始揭开业务问题的神秘面纱，并使用 MLOps 方法实现解决方案。

# 业务问题分析和问题分类

在前一章中，我们研究了下面的业务问题陈述。在本节中，我们将通过对问题陈述进行分类来揭开问题陈述的神秘面纱使用原则来策划实现路线图。我们将浏览为解决业务问题而提供给我们的数据集，并决定哪种类型的 ML 模型将有效地解决业务问题。最后，我们将对实现健壮和可扩展的 ML 操作的 MLOps 方法进行分类，并决定实现的工具。

以下是问题陈述:

您是一名数据科学家，带领一个小型数据科学家团队为一家总部位于芬兰的货运公司工作。90%的货物通过货运进口到芬兰。你的任务是在芬兰的图尔库港节省 20%的货运成本。这可以通过开发提前 4 小时预测港口天气状况的 ML 解决方案来实现。您需要监控可能的降雨条件，这可能会扭曲港口人力资源和运输的运营，进而影响港口的供应链运营。您的 ML 解决方案将帮助港务局提前 4 小时预测可能的降雨；这将节省 20%的成本，并使港口的供应链运作顺畅。

解决问题的第一步是使用适当的方法对其进行简化和分类。在前一章中，我们讨论了如何对业务问题进行分类，以使用 ML 来解决它。让我们应用这些原则来绘制一个清晰的实现路线图。

首先，我们将看看我们将训练什么类型的模型来产生最大的商业价值。其次，我们将确定实现 MLOps 的正确方法。

为了决定训练模型的类型，我们可以从 GitHub 上的数据集开始:[https://github.com/PacktPublishing/EngineeringMLOps](https://github.com/PacktPublishing/EngineeringMLOps)。

这是`weather_dataset_raw.csv`的快照，在*图 3.1* 中。文件大小为 10.7 MB，行数为 96，453，文件为 CSV 格式:

![Figure 3.1 – Dataset snapshot
](img/B16572_03_01.jpg)

图 3.1–数据集快照

通过评估数据，我们可以将业务问题分类如下:

*   `Weather condition`列描述了一个事件是否记录了雨、雪或晴朗的条件。这可以被框定或重新标记为`rain`或`no rain`，并用于执行二元分类。因此，用有监督的学习方法来解决业务问题是很简单的。
*   **MLOps approach**: By observing the problem statement and data, here are the facts:

    (a)数据:训练数据是 10.7 MB。数据大小相当小(不能视为大数据)。

    (b)业务:我们需要训练、测试、部署和监测一个 ML 模型，以便在记录新数据时每小时(提前 4 小时)预报图尔库港的天气。

    (c)团队规模:一个中小型数据科学家团队，没有 DevOps 工程师。

基于上述事实，我们可以将操作分为**小团队操作**；没有需要大数据处理，团队小而敏捷。现在，我们将研究一些合适的工具来实现解决当前业务问题所需的操作。

为了全面了解 MLOps 实现，我们将同时使用两种不同的工具来实现业务问题:

*   **Azure 机器学习**(微软 Azure)
*   **MLflow (** 一个开源云和平台无关的工具 **)**

我们使用这两个工具来了解纯基于云的方法和开源/云不可知方法的工作原理。所有代码和 CI/CD 操作都将使用 Azure DevOps 进行管理和编排，如图 3.2 所示:

![Figure 3.2 – MLOps tools for the solution 
](img/B16572_03_02.jpg)

图 3.2–解决方案的 MLOps 工具

现在，我们将设置实现业务问题解决方案所需的工具和资源。因为我们将使用 Python 作为主要编程语言，所以在您的 Mac、Linux 或 Windows 操作系统中安装 **Python 3** 是一个先决条件。

# 设置资源和工具

如果你已经在你的电脑上安装并设置了这些工具，请随意跳过这一部分；否则，请按照详细说明启动并运行它们。

## 安装 MLflow

我们从安装 MLflow 开始，这是一个用于管理 ML 生命周期的开源平台，包括实验、可复制性、部署和中央模型注册。

要安装 MLflow，请转到您的终端并执行以下命令:

```py
pip3 install mlflow
```

成功安装后，通过执行以下命令启动`mlflow`跟踪 UI 来测试安装:

```py
mlflow ui
```

在运行`mlflow`跟踪 UI 时，您将在您的机器上运行一个监听端口`5000`的服务器，它输出如下消息:

```py
[2021-03-11 14:34:23 +0200] [43819] [INFO] Starting gunicorn 20.0.4
[2021-03-11 14:34:23 +0200] [43819] [INFO] Listening at: http://127.0.0.1:5000 (43819)
[2021-03-11 14:34:23 +0200] [43819] [INFO] Using worker: sync
[2021-03-11 14:34:23 +0200] [43821] [INFO] Booting worker with pid: 43821
```

您可以在`http://localhost:5000`访问并查看`mlflow`界面。当您成功安装`mlflow`并运行跟踪 UI 后，您就可以安装下一个工具了。

## Azure 机器学习

Azure 机器学习为训练、部署和管理 ML 模型提供了一个基于云的 ML 平台。这项服务在微软 Azure 上提供，所以的先决条件是免费订阅微软 Azure。请创建一个大约 170 美元的信用免费帐户，这足以实现解决方案，这里:[https://azure.microsoft.com/](https://azure.microsoft.com/)。

当你拥有 Azure 的访问/订阅权限时，继续下一部分，让 Azure 机器学习开始运行。

### 创建资源组

一个**资源组**是一个 Azure 解决方案的相关资源集合。它是一个容器，绑定了与服务或解决方案相关的所有资源。创建资源组可以轻松访问和管理解决方案。让我们从创建您自己的资源组开始:

1.  打开 Azure 门户。
2.  Access the portal menu (go to the portal's home page if you are not there by default) and hover over the resource group icon in the navigation section. A **Create** button will appear; click on it to create a new resource group:![Figure 3.3 – Creating a resource group
    ](img/B16572_03_03.jpg)

    图 3.3–创建资源组

3.  使用您选择的名称创建一个资源组(建议使用`Learn_MLOps`)，如图*图 3.3* 所示。
4.  选择一个离你近的地区，以获得最佳的性能和价格。例如，在*图 3.3* 中，名为`Learn MLOps`且区域为**(欧洲)北欧**的资源组已经准备好创建。在你点击 **Review + Create** 按钮并且 Azure 验证了请求之后，最终的 **Create** 按钮将会出现。应该按下最后一个**创建**按钮来创建新的资源组。

当检查和创建资源组时，您可以设置和管理该资源组中与 ML 解决方案相关的所有服务。新创建的资源组将在资源组列表中列出。

### 创建 Azure 机器学习工作区

ML 工作空间是一个中心枢纽，用于跟踪和管理您的 ML 培训、部署和监控实验。要创建 Azure 机器学习工作区，进入 Azure 门户菜单，点击`Machine Learning`并选择它。您将看到以下屏幕:

![Figure 3.4 – Creating an Azure Machine Learning workspace
](img/B16572_03_04.jpg)

图 3.4–创建 Azure 机器学习工作区

用您选择的名称命名工作区(例如，我们在*图 3.4* 中将其命名为 **MLOps_WS** )。选择您之前创建的资源组，将这个 ML 服务绑定到它(在*图 3.4* 中选择 **Learn_MLOps** )。最后，点击**查看+创建**按钮，您将被带到最后一个**创建**按钮的新屏幕。按下最后的**创建**按钮来创建你的 Azure 机器学习工作空间。

在创建了 Azure 机器学习工作空间(`Learn_MLOps`)之后，Azure 平台将部署这个服务需要的所有资源。使用 Azure 机器学习实例(`Learn_MLOps`)部署的资源，如 Blob 存储、密钥库和应用洞察，被提供并绑定到工作区。这些资源将通过工作区和 SDK 来消耗或使用。

你可以在这里找到关于创建 Azure 机器学习实例的详细说明:[https://docs . Microsoft . com/en-us/Azure/Machine-Learning/how-to-management-workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace)。

### 正在安装 Azure 机器学习 SDK

在你的电脑上打开终端或命令行，安装 Azure Machine Learning SDK，它将在代码中被广泛用于编排实验。要安装它，请运行以下命令:

```py
pip3 install --upgrade azureml-sdk
```

可以在这里找到详细说明:[https://docs . Microsoft . com/en-us/python/API/overview/azure/ml/install？view=azure-ml-py](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py) 。

## 天蓝色的 DevOps

所有的源代码和 CI/CD 相关的操作都将使用 Azure DevOps 进行管理和编排。我们在 Azure DevOps 的存储库中管理的代码将用于训练、部署和监控由 CI/CD 管道支持的 ML 模型。让我们从创建 Azure DevOps 订阅开始:

1.  在 dev.azure.com 创建一个免费账户。可以使用现有的 Microsoft 或 GitHub 帐户创建免费帐户。
2.  创建一个名为`Learn_MLOps`的项目(根据您的喜好将其设为公共或私有)。
3.  转到**回购**部分。在**导入库**部分，按下**导入**按钮。
4.  从这个库导入一个公共 GitHub 项目的库:[https://github.com/PacktPublishing/EngineeringMLOps](https://github.com/PacktPublishing/EngineeringMLOps )(如图*图 3.5* ):

![Figure 3.5 – Import the GitHub repository into the Azure DevOps project
](img/B16572_03_05.jpg)

图 3.5–将 GitHub 存储库导入 Azure DevOps 项目

导入 GitHub 存储库后，将显示导入的存储库中的文件。

## 木星枢纽

最后，我们将需要一个交互式数据分析和可视化工具来使用我们的代码处理数据。为此，我们使用 **JupyterHub** 。这是一种常见的数据科学工具，被数据科学家广泛用于处理数据、可视化数据和训练 ML 模型。要安装它，请遵循两个简单的步骤:

1.  Install JupyterHub via the command line on your PC:

    ```py
    python3 -m pip install jupyterhub
    ```

    你可以在这里找到详细的说明:[https://jupyterhub.readthedocs.io/en/stable/quickstart.html](https://jupyterhub.readthedocs.io/en/stable/quickstart.html)。

2.  Install Anaconda.

    Anaconda 是必需的，因为它安装依赖项、设置环境和服务来支持 JupyterHub。下载 Anaconda，按照这里的详细说明安装:[https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/)。

    既然我们已经为实际操作的实现做好了准备，那么让我们看看如何管理好的代码和数据。

# ML 源代码管理的 10 个原则

这里有 10 条原则可以应用到你的代码中，以确保代码的质量、健壮性和可伸缩性:

*   **Modularity:** It is better to have modular code than to have one big chunk. Modularity encourages reusability and facilitates upgrading by replacing the required components. To avoid needless complexity and repetition, follow this golden rule:

    只有当其中一个组件使用另一个组件时，两个或多个 ML 组件才应该配对。如果它们都不互相使用，那么就应该避免配对。

    与其环境不紧密配对的 ML 组件比紧密配对的组件更容易修改或替换。

*   **单任务专用函数:**函数是管道和系统的重要构建模块，是用于执行特定任务的小段代码。函数的目的是避免命令的重复，并支持可重用的代码。它们很容易变成一组复杂的命令来简化任务。对于可读和可重用的代码，将单个函数专用于单个任务比多个任务更有效。拥有多个函数比拥有一个又长又复杂的函数要好。
*   `Error 300`。结构化代码块并尝试限制函数和类的最大缩进量可以增强代码的可读性。
*   **Clean code:** If you have to explain the code, it's not that good. Clean code is self-explanatory. It focuses on high readability, optimal modularity, reusability, non-repeatability, and optimal performance. Clean code reduces the cost of maintaining and upgrading your ML pipelines. It enables a team to perform efficiently and can be extended to other developers.

    要深入理解这一点，请阅读罗伯特·C·马丁**的*干净代码:敏捷软件工艺手册*。**

*   **测试:**确保系统的健壮性至关重要，测试在其中扮演着重要的角色。一般来说，测试延伸到单元测试和验收测试。单元测试是一种使用强制数据和使用方法测试源代码组件健壮性的方法，以确定组件是否适合生产系统。验收测试是对整个系统进行测试，以确保系统实现用户需求；端到端业务流在实时场景中得到验证。测试对于确保代码的有效运行是至关重要的:“如果它没有被测试，它就是坏的。”

要了解更多关于单元测试的实现，请阅读这个文档:[https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html)。

*   **Version control (code, data and models):** Git is used for version control of code in ML systems. The purpose of version control is to ensure that all the team members working on the system have access to up-to-date code and that code is not lost when there is a hardware failure. One rule of working with Git should be to not break the master (branch). This means when you have working code in the repository and you add new features or make improvements, you do this in a feature branch, which is merged to the master branch when the code is working and reviewed. Branches should be given a short descriptive name, such as feature/label-encoder. Branch naming and approval guidelines should be properly communicated and agreed upon with the team to avoid any complexity and unnecessary conflicts. Code review is done with pull requests to the repository of the code. Usually, it is best to review code in small sets, less than 400 lines. In practice, it often means one module or a submodule at a time.

    数据的版本化对于 ML 系统是必不可少的，因为它帮助我们跟踪哪些数据被用于特定版本的代码来生成模型。对数据进行版本控制可以复制模型，并符合业务需求和法律。我们总是可以回溯并看到 ML 系统采取某些行动的原因。类似地，模型(工件)的版本化对于跟踪哪个版本的模型已经为 ML 系统产生了某些结果或者动作是很重要的。我们还可以跟踪或记录用于训练模型的某个版本的参数。这样，我们可以为模型工件、数据和代码启用端到端的可追溯性。对代码、数据和模型的版本控制可以增强 ML 系统，对于开发和维护它的人来说具有很大的透明度和效率。

*   `print`语句适合测试和调试，但不适合生产。记录器包含的信息，尤其是系统信息、警告和错误，在监视生产系统时非常有用。
*   **错误处理:**错误处理对于处理边缘情况至关重要，尤其是那些难以预料的情况。建议即使您认为不需要，也要捕捉并处理异常，因为预防胜于治疗。日志记录结合异常处理是处理边缘情况的有效方法。
*   **Readability:** Code readability enables information transfer, code efficiency, and code maintainability. It can be achieved by following principles such as following industry-standard coding practices such as PEP-8 ([https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/)) or the JavaScript standard style (depending on the language you are using). Readability is also increased by using docstrings. A docstring is a text that is written at the beginning of, for example, a function, describing what it does and possibly what it takes as input. In some cases, it is enough to have a one-liner explanation, such as this:

    ```py
    def swap(a,b):
    """Swaps the variables a and b. Returns the swapped variables""" 
    return b, a
    ```

    更复杂的功能需要更长的 docstring。解释参数和回报是一个好主意:

    ```py
    def function_with_types_in_docstring(param1, param2): """Example function with types documented in the docstring.
    `PEP 484`_ type annotations are supported. If attribute, parameter, and
    return types are annotated according to `PEP 484`_, they do not need to be
    included in the docstring:
    Args:
        param1 (int): The first parameter. 
        param2 (str): The second parameter.
    Returns:
         bool: The return value. True for success, False otherwise.
                                  """
    ```

**注释和文档化:**注释和文档化对于维护可持续的代码至关重要。T21 并不总是能够解释清楚代码。在这种情况下，注释有助于防止混淆和解释代码。注释可以传达诸如版权信息、意图、代码说明、可能的警告和代码详细说明等信息。系统和模块的详细文档可以使团队高效地执行，并且代码和资产可以扩展到其他开发人员。对于文档，可以使用开源工具来记录 API，如 Swagger([https://Swagger . io](https://swagger.io))和 ReadDocs([https://readthedocs.org](https://readthedocs.org))。使用正确的文档工具可以提高效率，并使开发人员的知识标准化。

# 什么是 ML 的好数据？

好的 ML 模型是对高质量数据进行训练的结果。在进行 ML 培训之前，一个先决条件是要有高质量的数据。因此，我们需要处理数据以提高其质量。因此，确定数据的质量至关重要。以下五个特征将使我们能够辨别数据的质量:

*   **准确性**:准确性是数据质量的一个至关重要的特征，因为不准确的数据会导致较差的 ML 模型性能以及现实生活中的后果。要检查数据的准确性，请确认该信息是否代表真实情况。
*   **完整性**:在大多数情况下，不完整的信息是不可用的，如果对一个 ML 模型进行训练，可能会导致不正确的结果。检查数据的全面性至关重要。
*   **可靠性**:数据中的矛盾或重复会导致数据的不可靠性。可靠性是一个至关重要的特性；信任数据是必不可少的，主要是在使用 ML 进行实际决策时。在某种程度上，我们可以通过检查偏倚和分布来评估数据的可靠性。在任何极端情况下，数据对于 ML 训练可能是不可靠的，或者可能带有偏差。
*   **相关性**:如果正在收集不相关的信息，数据的相关性在上下文和确定中起着至关重要的作用。拥有相关数据可以在现实生活中使用 ML 做出适当的决策。
*   **及时性**:陈旧或过时的信息耗费企业的时间和金钱；在某些情况下，拥有最新的信息是至关重要的，可以提高数据的质量。ML 使用不合时宜的数据做出的决策可能代价高昂，并可能导致错误的决策。

当这五个特征最大化时，它确保了最高的数据质量。记住这些原则，让我们深入研究代码与数据相遇的实现。

首先，让我们对数据进行评估和处理，为 ML 训练做好准备。首先，克隆您导入到您的 Azure DevOps 项目的存储库(从 GitHub):

```py
git clone https://xxxxxxxxx@dev.azure.com/xxxxx/Learn_MLOps/_git/Learn_MLOps
```

接下来，打开您的终端，访问克隆存储库的文件夹，启动 JupyterLab 服务器进行数据处理。为此，请在终端中键入以下命令:

```py
jupyter lab
```

这将在浏览器的`http://localhost:8888`处自动打开一个窗口，您可以在这里编写并执行 JupyterLab 界面上的代码。在`Code_meets_data_c3`文件夹里，有一个 Python 脚本(`dataprocessing.py`)和一个`.ipynb`笔记本(`dataprocessing.ipynb`)；请随意运行这些文件或创建一个新的笔记本，然后按照下面的步骤操作。

我们将按照*图 3.6* 中描述的任务进行计算。数据处理将在您的 PC 上本地完成，然后在云中的计算目标上进行 ML 培训、部署和监控。这是为了获得在各种设置中实现模型的经验。在本章的其余部分，我们将(在本地)进行数据处理，以使数据达到最佳质量，从而进行 ML 训练(在云中，这将在下一章中描述)。

![Figure 3.6 – Computation locations for data and ML tasks
](img/B16572_03_06.jpg)

图 3.6-数据和 ML 任务的计算位置

为了处理原始数据并为 ML 做好准备，您将在本地 PC 上进行计算和数据处理。我们从安装和导入所需的包以及导入原始数据集开始(如`dataprocessing.ipynb`和`.py`脚本所示)。笔记本中的 Python 指令必须在现有笔记本中执行:

```py
%matplotlib inline
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns
from azureml.core import Workspace, Dataset
#import dataset
df = pd.read_csv('Dataset/weather_dataset_raw.csv')
```

这样，您已经将数据集导入到 pandas 数据帧`df`中，以进行进一步处理。

# 数据预处理

原始数据不能直接传递给 ML 模型用于训练目的。在训练 ML 模型之前，我们必须细化或预处理数据。为了进一步分析导入的数据，我们将执行一系列步骤来将数据预处理成适合 ML 训练的形状。我们首先评估数据的质量，以检查准确性、完整性、可靠性、相关性和及时性。在这之后，我们校准所需的数据，并将文本编码成数字数据，这对于 ML 训练来说是非常理想的。最后，我们将分析相关性和时间序列，并过滤掉不相关的数据来训练最大似然模型。

## 数据质量评估

为了评估数据的质量，我们寻求准确性、完整性、可靠性、相关性和及时性。首先，让我们通过评估格式、累积统计数据和异常(如缺失数据)来检查数据是否完整可靠。我们使用熊猫的功能如下:

```py
df.describe()
```

通过使用`describe`函数，我们可以观察到如下输出中的描述性统计数据:

![Figure 3.7 – Descriptive statistics of the DataFrame
](img/B16572_03_07.jpg)

图 3.7-数据帧的描述性统计

一些观察可以得出结论，数据是连贯的，相关的，因为它描述了真实生活的统计数据，如平均温度约为 11°C，风速约为 10 公里/小时。芬兰的最低气温往往达到零下 21 摄氏度左右，平均能见度为 10 公里。诸如此类的事实描述了相关性和数据来源条件。现在，让我们观察列格式:

```py
df.dtypes
```

以下是每列的格式:

*   `S_No` `int64`
*   `Timestamp`
*   `Location` `object`
*   `Temperature_C` `float64`
*   `Apparent_Temperature_C`
*   `Humidity`
*   `Wind_speed_kmph`
*   `Wind_bearing_degrees`
*   `Visibility_km` `float64`
*   `Pressure_millibars`
*   `Weather_conditions` `object`
*   `dtype:` `object`

正如所料，大多数列都是数字(`float`和`int`)。`Timestamp`栏为`object`格式，需改为`DateTime`格式:

```py
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
```

使用 pandas 的`to_datetime`函数，我们将`Timestamp`转换为`DateTime`格式。接下来，让我们看看是否有空值。我们使用 pandas 的`isnull`函数来检查这一点:

```py
df.isnull().values.any()
```

在检查任何空值时，如果发现空值，作为下一步，缺失数据的校准是必要的。

## 校准缺失数据

数据中有缺失值是不理想的，因为这是数据质量差的标志。可以使用各种技术替换缺失的数据或值，而不会损害数据的正确性和可靠性。在检查了我们一直在处理的数据之后，发现了一些缺失的值。我们使用`Forward fill`方法处理缺失数据:

```py
df['Weather_conditions'].fillna(method='ffill', inplace=True, axis=0)
```

`NaN`或空值仅出现在`Weather_conditions`栏中。我们通过使用 pandas 的`fillna()`方法和向前填充(`ffill`)方法来替换`NaN`值。由于天气是渐进的，它可能会复制数据中的前一个事件。因此，我们使用向前填充方法，该方法复制最后观察到的非空值，直到遇到另一个非空值。

## 标签编码

由于机器不理解人类语言或文本，所有文本都必须转换成数字。在此之前，让我们处理文本。我们在文本中有一个`Weather_conditons`列，带有值或标签，如`rain`、`snow`和`clear`。使用 pandas 的`value_counts()`函数找到这些值，如下所示:

```py
df['Weather_conditions'].value_counts()
```

`Weather_conditions`可以通过将列标签分为两个标签`rain`或`no_rain`来简化。这两个类别的预测将使我们能够为货运公司解决业务问题:

```py
df['Weather_conditions'].replace({"snow": "no_rain",  "clear": "no_rain"}, inplace=True)
```

这将用`no_rain`替换`snow`和`clear`值，因为这两种情况都意味着港口没有降雨。既然已经处理了标签，我们可以使用`rain`和`no_rain`将`Weather_conditions`列转换成机器可读的形式或数字，标签编码会很有效，因为它会将这些值转换成 0 和 1。如果有两个以上的值，**一键编码**是一个很好的选择，因为给分类变量分配增量数字可以在训练期间给变量更高的优先级或数值偏差。一键编码防止对任何变量的偏见或更高的偏好，确保对分类变量的每个值的中立特权。在我们的例子中，由于我们只有两个分类变量，我们使用 scikit-learn 执行标签编码，如下所示:

```py
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = df['Weather_conditions']
y = le.fit_transform(y)
```

这里，我们导入了`LabelEncoder()`函数，它将使用`fit_transform()`方法将`Weather_conditions`列编码为 0 和 1。我们可以这样做，用标签编码或机器可读的形式替换之前的文本列到`Weather_condition`列，如下所示:

```py
y = pd.DataFrame(data=y, columns=["Weather_condition"])
df = pd.concat([df,y], axis=1)
df.drop(['Weather_conditions'], axis=1, inplace=True)
```

这里，我们将新的标签编码或机器可读的`Weather_condition`列连接到数据帧，并删除先前的非机器可读或文本的`Weather_conditions`列。数据现在是机器可读的形式，并准备进一步处理。您可以通过执行笔记本中的`df.head()`来检查转换后的数据(可选)。

## 新功能–未来天气状况

由于我们的任务是预测未来 4 小时的天气状况，我们通过将`Current_weather_condition`移动四行来创建一个名为`Future_weather_condition`的新功能，因为每行记录有一个小时的时间间隔。`Future_weather_condition`是未来 4 小时天气状况的标签。我们将使用这一新功能作为因变量，使用 ML 进行预测:

```py
df['Future_weather_condition'] = df.Current_weather_condition.shift(4, axis = 0) 
df.dropna(inplace=True)
```

我们将在 DataFrame 上使用 pandas 的`dropna()`函数来丢弃或删除空值，因为由于转移到新列，一些行将具有空值。

## 数据关联和过滤

现在数据完全是机器可读的，我们可以使用**皮尔逊相关系数**来观察每一列如何与其他列相关。数据和特征关联是 ML 模型训练的特征选择之前的重要步骤，特别是当特征是连续的时，就像我们的情况。皮尔逊相关系数是每个变量( *X* 和 *y* )之间的统计线性相关，产生一个在 *+1* 和 *-1* 之间的值。值 *+1* 为正线性相关， *-1* 为负线性相关， *0* 为无线性相关。它可以用来理解连续变量之间的关系，尽管值得注意的是，皮尔逊相关并不意味着因果关系。我们可以使用 pandas 观察我们数据的皮尔逊相关系数，如下所示:

```py
df.corr(method="pearson")
# Visualizing using heatmap
corrMatrix = df.corr()
sn.heatmap(corrMatrix, annot=True)
plt.show()
```

下面是`Pearson`关联结果的热图:

![Figure 3.8 – Heatmap of correlation scores
](img/B16572_03_08.jpg)

图 3.8–关联分数热图

从*图 3.8* 中的热图可以看出`Temperature`和`Apparent_Temperature_C`系数为`0.99`。`S_No`(序列号)是一个连续值，它或多或少类似于数据帧的增量索引，可以被丢弃或过滤掉，因为它没有提供很大的价值。因此`Apparent_Temperature`和`S_No`都被丢弃或过滤。现在让我们观察我们的因变量`Future_weather_condition`，以及它与其他自变量的相关性:

![Figure 3.9 – Pearson correlation for Future_weather_condition
](img/B16572_03_09.jpg)

图 3.9–未来天气状况的皮尔逊相关

介于 0.5 和 1.0 之间的任何值都具有正相关性，介于-0.5 和-1.0 之间的任何值都具有负相关性。从图形来看，与`Current_weather_condition`呈正相关，`Temperature_C`也与`Future_weather_c`呈正相关。

## 时间序列分析

由于温度是一个连续的变量，因此观察其随时间的变化是值得的。我们可以使用 matplotlib 可视化时间序列图，如下所示:

```py
time = df['Timestamp]
temp = df['Temperature_C']
# plot graph
plt.plot(time, temp)
plt.show()
```

这是结果图:

![ Figure 3.10 – Time series progression of Temperature in C
](img/B16572_03_10.jpg)

图 3.10–温度的时间序列变化

在评估了*图 3.10* 中的温度时间序列之后，我们可以看到，它描绘了一个稳定模式，因为均值、方差和协方差被观察到是随时间稳定的。静态行为可以是趋势、周期、随机漫步或三者的组合。这是有道理的，因为温度会随着季节变化，遵循季节性模式。这给我们带来了数据分析和处理的结束；我们现在准备在继续训练 ML 模型之前在工作空间中注册已处理的数据。

# 数据注册和版本控制

在开始 ML 训练之前，在工作空间中注册和版本化数据是至关重要的，因为它使我们能够回溯我们的实验或 ML 模型到用于训练模型的数据源。对数据进行版本控制的目的是在任何点进行回溯，复制模型的训练，或者根据用于解释 ML 模型的推断或测试数据来解释模型的工作方式。出于这些原因，我们将注册已处理的数据并对其进行版本化，以将其用于我们的 ML 管道。我们将使用 Azure Machine Learning SDK 将处理后的数据注册到 Azure Machine Learning workspace 并发布版本，如下所示:

```py
subscription_id = '---insert your subscription ID here----'
resource_group = 'Learn_MLOps'
workspace_name = 'MLOps_WS' 
workspace = Workspace(subscription_id, resource_group, workspace_name)
```

从 Azure 机器学习门户获取你的`subscription ID`、`resource_group`和`workspace_name`，如图*图 3.11* 所示:

![Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and Workspace name)
](img/B16572_03_11.jpg)

图 3.11–工作区凭证(资源组、订阅 ID 和工作区名称)

通过请求工作区凭证，获得工作区对象。运行`Workspace()`功能时，你的笔记本会连接到 Azure 平台。系统会提示您点击一个身份验证链接，并提供一个随机代码和 Azure 帐户的详细信息。之后，脚本将确认身份验证。使用 workspace 对象，我们可以访问默认的数据存储，并将所需的数据文件上传到连接到工作区的 Azure Blob 存储上的数据存储:

```py
# get the default datastore linked to upload prepared data
datastore = workspace.get_default_datastore()
#upload the local file from src_dir to target_path in datastore
datastore.upload(src_dir='Dataset', target_path='data')
dataset =  /
Dataset.Tablular.from_delimited_files(datastore.path('data/weather_dataset_processed.csv'))
```

`Tabular.from_delimited_files()`可能会导致未安装的 Linux 或 MacOS 机器出现故障。安装了 NET Core 2.1。为了正确安装该附件，请遵循这些说明:[https://docs.microsoft.com/en-us/dotnet/core/install/linux](https://docs.microsoft.com/en-us/dotnet/core/install/linux)。成功执行前面的命令后，您将把数据文件上传到数据存储器，并看到如图*图 3.12* 所示的结果。您可以从数据存储中预览数据集，如下所示:

```py
# preview the first 3 rows of the dataset from the datastore
dataset.take(3).to_pandas_dataframe()
```

当数据上传到数据存储时，我们会将数据集注册到工作区，并按如下方式对其进行版本化:

```py
weather_ds = dataset.register(workspace=workspace, name=weather_ds_portofTurku, description='processed weather data')
```

`register(...)`函数将数据集注册到工作区，如图*图 3.12* 所示。有关详细文档，请访问 https://docs . Microsoft . com/en-us/azure/machine-learning/how-to-create-register-datasets # register-datasets:

![Figure 3.12 – Processed dataset registered in the ML workspace
](img/B16572_03_12.jpg)

图 3.12–在 ML 工作空间中注册的已处理数据集

# 走向 ML 管道

到目前为止，我们已经通过处理缺失数据等不规则性来处理数据，通过观察相关性来选择特征，创建新特征，并最终将处理后的数据摄取并版本化到机器学习工作空间。有两种方式来为 ML 管道中的 ML 模型训练的数据摄取提供燃料。一种方法是从中央存储(存储所有原始数据的地方)获取，另一种方法是使用要素存储。因为知识就是力量，所以在我们进入 ML 管道之前，让我们先了解一下特性库的用途。

## 特色店

特征存储通过存储重要的特征并使它们可用于训练或推断来补充中央存储。要素存储是将原始数据转换为有用的要素的存储，ML 模型可以直接使用这些要素进行训练和推断以进行预测。原始数据通常来自各种数据源，包括结构化数据源、非结构化数据源、流式数据源、批处理数据源和实时数据源。所有这些都需要被提取、转换(使用特性管道)，并存储在某个地方，而这个地方可以是特性存储。然后，功能存储获取数据并使其可供消费。数据科学家倾向于重复工作(尤其是数据处理)。如果我们有一个集中的特性库，这是可以避免的。要素存储允许数据科学家与其他团队有效地共享和重用要素，从而提高他们的工作效率，因为他们不必从头开始预处理要素。

![Figure 3.13: Feature store workflow
](img/B16572_03_13.jpg)

图 3.13:功能存储工作流

正如我们在*图 3.13* 中所看到的，一个**特征库**正在使用一个**特征管道**连接到一个**中央存储器**(它存储来自多个来源的数据)来将原始数据转换并存储为对 ML 训练有用的特征。存储在特征存储中的特征可被检索用于训练、服务或发现见解或趋势。以下是使用功能库的一些好处:

*   高效的**特征工程**用于**训练数据**
*   避免训练前不必要的数据预处理
*   避免重复的特征工程
*   可用于快速推理(测试)的特性
*   系统支持功能服务
*   按特征存储进行探索性数据分析
*   重用模型功能的机会
*   对功能的快速查询
*   训练数据集的再现性
*   在生产中监控特征漂移(我们将在第十二章 、*模型服务和监控*中了解特征漂移)
*   可用于数据漂移监控的特性

了解特征库的优势是很好的，因为它可能有助于推动 ML 管道(尤其是数据摄取步骤)，但并不适用于所有情况。这取决于您的用例。对于我们的用例实现，我们将不会使用功能存储，而是直接从中央存储中连接数据，在中央存储中，我们已经预处理并注册了训练和测试所需的数据集。有了摄取和版本化的数据，您就可以着手构建您的 ML 管道了。ML 管道将实现进一步的特征工程、特征缩放、管理训练和测试数据集，这些数据集将用于训练 ML 模型和调整机器学习训练的超参数。ML 管道和功能将在云计算资源上执行，不像我们在本章中所做的那样在本地计算机上执行。它将完全基于云。

# 总结

在这一章中，我们学习了如何为业务问题确定一个合适的 ML 解决方案，并对操作进行分类以实现合适的 MLOps。我们建立了我们的工具、资源和开发环境。讨论了源代码管理的 10 个原则，以及数据质量特征。恭喜你。到目前为止，您已经实现了 MLOps 工作流的一个关键构建模块——数据处理和将处理后的数据注册到工作空间。最后，我们对 ML 管道的本质有所了解。

在下一章中，您将完成 MLOps 中最激动人心的部分:构建 ML 管道。让我们继续前进！