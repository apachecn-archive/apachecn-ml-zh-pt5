<html><head/><body>

  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">自动算法选择</h1>
                
            
            
                
<p class="mce-root">本章提供了对<strong class="calibre7">机器学习</strong> ( <strong class="calibre7"> ML </strong>)算法广阔前景的一瞥。鸟瞰图将向你展示你可以用 ML 解决的那种学习问题，你已经学会了。让我们简单回顾一下。</p>
<p class="mce-root">如果数据集中的示例/观察值具有关联的标签，则这些标签可以在模型训练期间为算法提供指导。有了这种指导或监督，你将使用监督或半监督学习算法。如果你没有标签，你会使用无监督学习算法。</p>
<p class="mce-root">还有其他情况需要不同的方法，例如强化学习，但是，在这一章中，主要的焦点将放在监督和非监督算法上。</p>
<p class="mce-root">ML 管道的下一个前沿是自动化。当您第一次考虑自动化 ML 管道时，核心元素是特征转换、模型选择和超参数优化。但是，对于您的具体问题，您还需要考虑其他一些要点，您将在本章中研究以下要点:</p>
<ul class="calibre13">
<li class="calibre14">计算的复杂性</li>
<li class="calibre14">训练和得分时间的差异</li>
<li class="calibre14">线性与非线性</li>
<li class="calibre14">算法特定的特征变换</li>
</ul>
<p class="mce-root">理解这些将有助于您理解哪些算法可能适合您对给定问题的需求。本章结束时:</p>
<ul class="calibre13">
<li class="calibre14">你将学会自动监督学习和无监督学习的基础</li>
<li class="calibre14">您将了解到使用 ML 管道时要考虑的主要方面</li>
<li class="calibre14">您将在各种用例中练习您的技能，并构建监督和非监督 ML 管道</li>
</ul>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">技术要求</h1>
                
            
            
                
<p class="mce-root">检查<kbd class="calibre16">requirements.txt</kbd>文件中要安装的库，以在 GitHub 中运行本章的代码示例。</p>
<p class="mce-root">所有的代码示例都可以在 GitHub 的<kbd class="calibre16">Chapter 04</kbd>文件夹中找到。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">计算的复杂性</h1>
                
            
            
                
<p class="mce-root">计算效率和复杂性是选择 ML 算法的重要方面，因为它们将决定模型训练和评分在时间和内存需求方面所需的资源。</p>
<p class="mce-root">例如，计算密集型算法将需要更长的时间来训练和优化其超参数。您通常会在可用的 CPU 或 GPU 之间分配工作负载，以将花费的时间减少到可接受的水平。</p>
<p class="mce-root">在这一节中，我们将根据这些约束条件来研究一些算法，但是在深入研究 ML 算法的细节之前，您需要了解算法复杂性的基础知识。</p>
<p>算法的复杂性将基于其输入大小。对于 ML 算法，这可以是元素和特征的数量。你通常会计算在最坏的情况下完成任务所需的运算次数，这就是你的算法的复杂度。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">大 O 符号</h1>
                
            
            
                
<p class="mce-root">你可能听说过大 O 符号。它有不同的类别来表示复杂性，如线性— <kbd class="calibre16">O(n)</kbd>、对数— <kbd class="calibre16">O(log n)</kbd>、二次— <kbd class="calibre16">O(n2)</kbd>、三次— <kbd class="calibre16">O(n3)</kbd>以及类似的类别。您使用 big O 的原因是因为算法的运行时间高度依赖于硬件，并且您需要一种基于其输入大小来测量算法性能的系统方法。Big O 查看算法的步骤，并计算出上述最坏的情况。</p>
<p class="mce-root">例如，如果<kbd class="calibre16">n</kbd>是您想要添加到列表中的元素数量，那么它的复杂度就是<kbd class="calibre16">O(n)</kbd>，因为添加的操作数量取决于<kbd class="calibre16">n</kbd>。下面的代码块将帮助您绘制不同的复杂性如何作为其输入大小的函数而增长:</p>
<pre class="calibre21"># Importing necessary libraries<br class="calibre2"/>import pandas as pd<br class="calibre2"/>import numpy as np<br class="calibre2"/>import matplotlib.pyplot as plt<br class="calibre2"/>import seaborn as sns<br class="calibre2"/><br class="calibre2"/># Setting the style of the plot<br class="calibre2"/>plt.style.use('seaborn-whitegrid')<br class="calibre2"/><br class="calibre2"/># Creating an array of input sizes<br class="calibre2"/>n = 10<br class="calibre2"/>x = np.arange(1, n)<br class="calibre2"/><br class="calibre2"/># Creating a pandas data frame for popular complexity classes<br class="calibre2"/>df = pd.DataFrame({'x': x,<br class="calibre2"/>                   'O(1)': 0,<br class="calibre2"/>                   'O(n)': x,<br class="calibre2"/>                   'O(log_n)': np.log(x),<br class="calibre2"/>                   'O(n_log_n)': n * np.log(x),<br class="calibre2"/>                   'O(n2)': np.power(x, 2), # Quadratic<br class="calibre2"/>                   'O(n3)': np.power(x, 3)}) # Cubic<br class="calibre2"/><br class="calibre2"/># Creating labels<br class="calibre2"/>labels = ['$O(1) - Constant$',<br class="calibre2"/>          '$O(\log{}n) - Logarithmic$',<br class="calibre2"/>          '$O(n) - Linear$',<br class="calibre2"/>          '$O(n^2) - Quadratic$',<br class="calibre2"/>          '$O(n^3) - Cubic$',<br class="calibre2"/>          '$O(n\log{}n) - N log n$']<br class="calibre2"/><br class="calibre2"/># Plotting every column in dataframe except 'x'<br class="calibre2"/>for i, col in enumerate(df.columns.drop('x')):<br class="calibre2"/>    print(labels[i], col)<br class="calibre2"/>    plt.plot(df[col], label=labels[i])<br class="calibre2"/><br class="calibre2"/># Adding a legend<br class="calibre2"/>plt.legend()<br class="calibre2"/><br class="calibre2"/># Limiting the y-axis<br class="calibre2"/>plt.ylim(0,50)<br class="calibre2"/><br class="calibre2"/>plt.show()</pre>
<p class="mce-root">我们得到下面的图作为前面代码的输出:</p>
<div><img src="img/4604f644-a706-4b19-8226-c6cdf65c08ef.png" width="1490" height="1034" class="calibre130"/></div>
<p>不同的复杂性随着它们的输入大小而增长</p>
<p class="mce-root">这里需要注意的一点是，不同级别的复杂性之间有一些交叉点。由此可见数据大小的作用。简单例子的复杂性很容易理解，但是 ML 算法的复杂性呢？如果到目前为止的介绍已经激起了你的兴趣，请继续阅读下一部分。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">训练和得分时间的差异</h1>
                
            
            
                
<p class="mce-root">花在训练和评分上的时间可以成就或破坏一个 ML 项目。如果在当前可用的硬件上训练一个算法需要太长时间，用新数据和超参数优化更新模型将是痛苦的，这可能会迫使你将该算法从候选列表中划掉。如果一个算法需要太长的时间来评分，那么这可能是生产环境中的一个问题，因为您的应用程序可能需要快速的推理时间，如毫秒或微秒来获得预测。这就是为什么学习最大似然算法的内部工作是重要的，至少开始时是常见的，以检测算法的适用性。</p>
<p class="mce-root">例如，监督学习算法在训练过程中学习样本集及其相关标签之间的关系，其中每个样本由一组特征组成。训练作业将在成功完成后输出 ML 模型，该模型可用于进行新的预测。当模型被输入没有标签的新示例时，在训练期间在特征和标签之间映射的关系被用于预测标签。用于预测的时间通常很短，因为模型的学习权重将应用于新数据。</p>
<p class="mce-root">然而，一些监督算法跳过了这个训练阶段，它们基于训练数据集中所有可用的例子来评分。这样的算法被称为<strong class="calibre7">基于实例的</strong>或<strong class="calibre7">懒惰学习者</strong>。对于基于实例的算法，训练简单地意味着将所有特征向量及其相关联的标签存储在存储器中，这是整个训练数据集。这实际上意味着，随着数据集大小的增加，模型将需要更多的计算和内存资源。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">简单测量训练和得分时间</h1>
                
            
            
                
<p class="mce-root">让我们来看一个简单的例子:k-最近邻 ( <strong class="calibre7"> k-NN </strong>)算法，它适用于分类和回归问题。当一个算法对一个新的特征向量进行评分时，它会检查<em class="calibre17"> k </em>个最近邻居并输出一个结果。如果是分类问题，则使用多数票进行预测；如果是回归问题，则使用平均值作为预测值。</p>
<p class="mce-root">让我们通过一个分类问题的例子来更好地理解这一点。首先，您将创建一个样本数据集，并根据训练和评分所花费的时间来检查 k-NN 算法。</p>
<p class="mce-root">为了方便起见，下面的函数将用于测量在给定行上花费的时间:</p>
<pre class="calibre21">from contextlib import contextmanager<br class="calibre2"/>from time import time<br class="calibre2"/><br class="calibre2"/>@contextmanager<br class="calibre2"/>def timer():<br class="calibre2"/>    s = time()<br class="calibre2"/>    yield<br class="calibre2"/>    e = time() - s<br class="calibre2"/>    print("{0}: {1} ms".format('Elapsed time', e))</pre>
<p class="mce-root">您可以通过以下方式使用该功能:</p>
<pre class="calibre21">import numpy as np<br class="calibre2"/><br class="calibre2"/>with timer():<br class="calibre2"/>    X = np.random.rand(1000)</pre>
<p class="mce-root">它输出执行该行所花费的时间:</p>
<pre class="calibre21">Elapsed time: 0.0001399517059326172 ms</pre>
<p class="mce-root">现在，您可以使用 scikit-learn 库的<kbd class="calibre16">KNeighborsClassifier</kbd>来测量训练和评分所花费的时间:</p>
<pre class="calibre21">from sklearn.neighbors import KNeighborsClassifier<br class="calibre2"/><br class="calibre2"/># Defining properties of dataset<br class="calibre2"/>nT = 100000000 # Total number of values in our dataset<br class="calibre2"/>nF = 10 # Number of features<br class="calibre2"/>nE = int(nT / nF) # Number of examples<br class="calibre2"/><br class="calibre2"/># Creating n x m matrix where n=100 and m=10<br class="calibre2"/>X = np.random.rand(nT).reshape(nE, nF)<br class="calibre2"/><br class="calibre2"/># This will be a binary classification with labels 0 and 1<br class="calibre2"/>y = np.random.randint(2, size=nE)<br class="calibre2"/><br class="calibre2"/># Data that we are going to score<br class="calibre2"/>scoring_data = np.random.rand(nF).reshape(1,-1)<br class="calibre2"/><br class="calibre2"/># Create KNN classifier<br class="calibre2"/>knn = KNeighborsClassifier(11, algorithm='brute')<br class="calibre2"/><br class="calibre2"/># Measure training time<br class="calibre2"/>with timer():<br class="calibre2"/>    knn.fit(X, y)<br class="calibre2"/><br class="calibre2"/># Measure scoring time<br class="calibre2"/>with timer():<br class="calibre2"/>    knn.predict(scoring_data)</pre>
<p class="mce-root">让我们看看输出:</p>
<pre class="calibre21">Elapsed time: 1.0800271034240723 ms<br class="calibre2"/>Elapsed time: 0.43231201171875 ms</pre>
<p class="mce-root">为了了解这种算法与其他算法相比有何不同，您可以再尝试一种分类器，例如逻辑回归:</p>
<pre class="calibre21">from sklearn.linear_model import LogisticRegression<br class="calibre2"/>log_res = LogisticRegression(C=1e5)<br class="calibre2"/><br class="calibre2"/>with timer():<br class="calibre2"/>    log_res.fit(X, y)<br class="calibre2"/><br class="calibre2"/>with timer():<br class="calibre2"/>    prediction = log_res.predict(scoring_data)</pre>
<p class="mce-root">逻辑回归的输出如下:</p>
<pre class="calibre21">Elapsed time: 12.989803075790405 ms<br class="calibre2"/>Elapsed time: 0.00024318695068359375 ms</pre>
<p class="mce-root">看着挺不一样的！逻辑回归在训练中慢一些，在评分中快得多。这是为什么呢？</p>
<p class="mce-root">您将会了解到这个问题的答案，但是，在详细了解前面的结果之前，我们先来谈谈 Python 中的代码剖析。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">Python 中的代码剖析</h1>
                
            
            
                
<p class="mce-root">一些应用程序将要求您的机器学习模型在训练和评分时间方面表现良好。例如，推荐引擎可能要求您在不到一秒钟的时间内生成推荐，如果您有超过一秒钟的延迟，分析是理解密集型操作的一种方式。代码剖析将非常有助于你理解程序的不同部分是如何执行的。分析统计数据将给出度量，例如调用次数、执行函数调用(包括/不包括对其子函数的调用)所花费的总时间，以及增量和总内存使用量。</p>
<p class="mce-root">Python 中的<kbd class="calibre16">cProfile</kbd>模块将帮助您查看每个函数的时间统计。这里有一个小例子:</p>
<pre class="calibre21"># cProfile<br class="calibre2"/>import cProfile<br class="calibre2"/><br class="calibre2"/>cProfile.run('np.std(np.random.rand(1000000))')</pre>
<p class="mce-root">在前一行中，计算了从均匀分布中抽取的 1，000，000 个随机样本的标准差。输出将显示执行给定行的所有函数调用的时间统计信息:</p>
<pre class="calibre21">23 function calls in 0.025 seconds<br class="calibre2"/>   Ordered by: standard name<br class="calibre2"/>   ncalls tottime percall cumtime percall filename:lineno(function)<br class="calibre2"/>        1 0.001 0.001 0.025 0.025 &lt;string&gt;:1(&lt;module&gt;)<br class="calibre2"/>        1 0.000 0.000 0.007 0.007 _methods.py:133(_std)<br class="calibre2"/>        1 0.000 0.000 0.000 0.000 _methods.py:43(_count_reduce_items)<br class="calibre2"/>        1 0.006 0.006 0.007 0.007 _methods.py:86(_var)<br class="calibre2"/>        1 0.001 0.001 0.008 0.008 fromnumeric.py:2912(std)<br class="calibre2"/>        2 0.000 0.000 0.000 0.000 numeric.py:534(asanyarray)<br class="calibre2"/>        1 0.000 0.000 0.025 0.025 {built-in method builtins.exec}<br class="calibre2"/>        2 0.000 0.000 0.000 0.000 {built-in method builtins.hasattr}<br class="calibre2"/>        4 0.000 0.000 0.000 0.000 {built-in method builtins.isinstance}<br class="calibre2"/>        2 0.000 0.000 0.000 0.000 {built-in method builtins.issubclass}<br class="calibre2"/>        1 0.000 0.000 0.000 0.000 {built-in method builtins.max}<br class="calibre2"/>        2 0.000 0.000 0.000 0.000 {built-in method numpy.core.multiarray.array}<br class="calibre2"/>        1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}<br class="calibre2"/>        1 0.017 0.017 0.017 0.017 {method 'rand' of 'mtrand.RandomState' objects}<br class="calibre2"/>        2 0.001 0.001 0.001 0.001 {method 'reduce' of 'numpy.ufunc' objects}</pre>
<p class="mce-root"><kbd class="calibre16">23</kbd>函数调用在<kbd class="calibre16">0.025</kbd>秒内执行，正如你所料，大部分时间都花在生成随机数和计算标准差上。</p>
<p class="mce-root">有一个很棒的库叫做<kbd class="calibre16">snakeviz</kbd>，可以用来可视化<kbd class="calibre16">cProfile</kbd>的输出。创建一个名为<kbd class="calibre16">profiler_example_1.py</kbd>的文件，并添加以下代码:</p>
<pre class="calibre21">import numpy as np<br class="calibre2"/><br class="calibre2"/>np.std(np.random.rand(1000000))</pre>
<p class="mce-root">在您的终端中，导航到您的<kbd class="calibre16">profiler_example_1.py</kbd>所在的文件夹，并运行以下命令:</p>
<pre class="calibre21"><strong class="calibre3">python -m cProfile -o profiler_output -s cumulative profiler_example_1.py</strong></pre>
<p class="mce-root">这将创建一个名为<kbd class="calibre16">profiler_output</kbd>的文件，现在您可以使用<kbd class="calibre16">snakeviz</kbd>来创建一个可视化</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">可视化性能统计</h1>
                
            
            
                
<p class="mce-root">Snakeviz 是基于浏览器的，它允许你与性能指标进行交互。<kbd class="calibre16">snakeviz</kbd>将使用由名为<kbd class="calibre16">profiler_output</kbd>的分析器生成的文件来创建可视化效果:</p>
<pre class="calibre21">snakeviz profiler_output</pre>
<p class="mce-root">这个命令将在<kbd class="calibre16">127.0.0.1:8080</kbd>上运行一个小型 web 服务器，它将为您提供可以找到可视化的地址，比如<kbd class="calibre16">http://127.0.0.1:8080/snakeviz/…/2FAutomated_Machine_Learning%2FCh4_Automated_Algorithm_Selection%2Fprofiler_output</kbd>。</p>
<p class="mce-root">在这里，您可以看到带有各种设置的旭日风格图表，例如深度和截止值:</p>
<div><img src="img/8ea97012-2972-436d-833c-65acef75a006.png" width="944" height="622" class="calibre131"/></div>
<p class="mce-root">你可以将鼠标悬停在它上面，它会显示函数名称、累计时间、文件、行和目录。您可以深入到特定区域并查看详细信息。</p>
<p class="mce-root">如果选择冰柱样式，将会有不同的视觉效果:</p>
<div><img src="img/8da347d2-0606-49ed-81b5-09dc8684d7cf.png" width="944" height="504" class="calibre132"/></div>
<p class="mce-root">您可以尝试样式、深度和截点，看看哪些设置最适合您。</p>
<p class="mce-root">如果向下滚动到底部，会出现一个类似下面截图的表格:</p>
<div><img src="img/3124f894-e731-4a0a-88a8-297fe613f736.png" width="765" height="429" class="calibre133"/></div>
<p class="mce-root">如果您根据<kbd class="calibre16">percall</kbd>列对这些值进行排序，您会看到<kbd class="calibre16">mtrand.RandomState</kbd>对象的<kbd class="calibre16">rand</kbd>方法和<kbd class="calibre16">_var</kbd>方法是最耗时的调用。</p>
<p class="mce-root">您可以检查以这种方式运行的任何东西，这是更好地理解和诊断代码的良好开端。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">从头开始实现 k-NN</h1>
                
            
            
                
<p class="mce-root">您已经看到了运行中的 k-NN 算法；让我们看一个非常简单的实现。将以下代码块另存为<kbd class="calibre16">knn_prediction.py</kbd>:</p>
<pre class="calibre21">import numpy as np<br class="calibre2"/>import operator<br class="calibre2"/><br class="calibre2"/># distance module includes various distance functions<br class="calibre2"/># You will use euclidean distance function to calculate distances between scoring input and training dataset.<br class="calibre2"/>from scipy.spatial import distance<br class="calibre2"/><br class="calibre2"/># Decorating function with @profile to get run statistics<br class="calibre2"/>@profile<br class="calibre2"/>def nearest_neighbors_prediction(x, data, labels, k):<br class="calibre2"/><br class="calibre2"/>    # Euclidean distance will be calculated between example to be predicted and examples in data<br class="calibre2"/>    distances = np.array([distance.euclidean(x, i) for i in data])<br class="calibre2"/><br class="calibre2"/>    label_count = {}<br class="calibre2"/>    for i in range(k):<br class="calibre2"/>        # Sorting distances starting from closest to our example<br class="calibre2"/>        label = labels[distances.argsort()[i]]<br class="calibre2"/>        label_count[label] = label_count.get(label, 0) + 1<br class="calibre2"/>    votes = sorted(label_count.items(), key=operator.itemgetter(1), reverse=True)<br class="calibre2"/><br class="calibre2"/>    # Return the majority vote<br class="calibre2"/>    return votes[0][0]<br class="calibre2"/><br class="calibre2"/># Setting seed to make results reproducible<br class="calibre2"/>np.random.seed(23)<br class="calibre2"/><br class="calibre2"/># Creating dataset, 20 x 5 matrix which means 20 examples with 5 features for each<br class="calibre2"/>data = np.random.rand(100).reshape(20,5)<br class="calibre2"/><br class="calibre2"/># Creating labels<br class="calibre2"/>labels = np.random.choice(2, 20)<br class="calibre2"/><br class="calibre2"/># Scoring input<br class="calibre2"/>x = np.random.rand(5)<br class="calibre2"/><br class="calibre2"/># Predicting class for scoring input with k=2<br class="calibre2"/>pred = nearest_neighbors_prediction(x, data, labels, k=2)<br class="calibre2"/># Output is ‘0’ in my case</pre>
<p class="mce-root">您将分析这个函数，看看每一行执行需要多长时间。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">逐行分析您的 Python 脚本</h1>
                
            
            
                
<p class="mce-root">转到您的终端，运行以下命令:</p>
<pre class="calibre21"><strong class="calibre3">$ pip install line_profiler</strong>
knn_prediction.py.</pre>
<p class="mce-root">正如你已经注意到的，<kbd class="calibre16">nearest_neighbors_prediction</kbd>布置如下:</p>
<pre class="calibre21"><strong class="calibre3">@profile</strong><br class="calibre2"/><strong class="calibre3">def nearest_neighbors_prediction(x, data, labels, k):</strong><br class="calibre2"/><strong class="calibre3"> …</strong></pre>
<p class="mce-root">它允许<kbd class="calibre16">line_profiler</kbd>知道要分析哪个函数。运行以下命令保存分析结果:</p>
<pre class="calibre21"><strong class="calibre3">$ kernprof -l knn_prediction.py</strong></pre>
<p class="mce-root">输出如下所示:</p>
<pre class="calibre21"><strong class="calibre3">Start</strong><br class="calibre2"/><strong class="calibre3">Wrote profile results to knn_prediction.py.lprof</strong></pre>
<p class="mce-root">您可以按如下方式查看探查器结果:</p>
<pre class="calibre21"><strong class="calibre3">$ python -m line_profiler knn_prediction.py.lprof</strong><br class="calibre2"/><strong class="calibre3">Timer unit: 1e-06 s</strong><br class="calibre2"/><br class="calibre2"/><strong class="calibre3">Total time: 0.001079 s</strong><br class="calibre2"/><strong class="calibre3">File: knn_prediction.py</strong><br class="calibre2"/><strong class="calibre3">Function: nearest_neighbors_prediction at line 24</strong><br class="calibre2"/><br class="calibre2"/><strong class="calibre3">Line # Hits Time Per Hit % Time Line Contents</strong><br class="calibre2"/><strong class="calibre3">==============================================================</strong><br class="calibre2"/><strong class="calibre3">    24 @profile</strong><br class="calibre2"/><strong class="calibre3">    25 def nearest_neighbors_prediction(x, data, labels, k):</strong><br class="calibre2"/><strong class="calibre3">    26 </strong><br class="calibre2"/><strong class="calibre3">    27 # Euclidean distance will be calculated between example to be predicted and examples in data</strong><br class="calibre2"/><strong class="calibre3">    28 1 1043.0 1043.0 96.7 distances = np.array([distance.euclidean(x, i) for i in data])</strong><br class="calibre2"/><strong class="calibre3">    29 </strong><br class="calibre2"/><strong class="calibre3">    30 1 2.0 2.0 0.2 label_count = {}</strong><br class="calibre2"/><strong class="calibre3">    31 3 4.0 1.3 0.4 for i in range(k):</strong><br class="calibre2"/><strong class="calibre3">    32 # Sorting distances starting from closest to our example</strong><br class="calibre2"/><strong class="calibre3">    33 2 19.0 9.5 1.8 label = labels[distances.argsort()[i]]</strong><br class="calibre2"/><strong class="calibre3">    34 2 3.0 1.5 0.3 label_count[label] = label_count.get(label, 0) + 1</strong><br class="calibre2"/><strong class="calibre3">    35 1 8.0 8.0 0.7 votes = sorted(label_count.items(), key=operator.itemgetter(1), reverse=True)</strong><br class="calibre2"/><strong class="calibre3">    36 </strong><br class="calibre2"/><strong class="calibre3">    37 # Return the majority vote</strong><br class="calibre2"/><strong class="calibre3">    38 1 0.0 0.0 0.0 return votes[0][0]</strong></pre>
<p class="mce-root">正如您所料，最耗时的部分是计算距离。</p>
<p>用大 O 记法来说，k-NN 算法的复杂度是<kbd class="calibre72">O(nm + kn)</kbd>，其中<kbd class="calibre72">n</kbd>是样本数，<kbd class="calibre72">m</kbd>是特征数，<kbd class="calibre72">k</kbd>是算法的超参数。你现在可以把原因当作一个练习来思考。</p>
<p class="mce-root">每个算法都有相似的属性，你应该知道这些属性会影响算法的训练和评分时间。这些限制对于生产用例变得尤为重要。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">线性与非线性</h1>
                
            
            
                
<p class="mce-root">另一个考虑是决策界限。一些算法，如逻辑回归或<strong class="calibre7">支持向量机</strong> ( <strong class="calibre7"> SVM </strong>)，可以学习线性决策边界，而其他算法，如基于树的算法，可以学习非线性决策边界。虽然线性决策边界相对容易计算和解释，但您应该意识到在存在非线性关系的情况下，线性算法会产生错误。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">绘制决策边界</h1>
                
            
            
                
<p class="mce-root">以下代码片段将允许您检查不同类型算法的决策界限:</p>
<pre class="calibre21">import matplotlib.cm as cm<br class="calibre2"/><br class="calibre2"/># This function will scale training datatset and train given classifier.<br class="calibre2"/># Based on predictions it will draw decision boundaries.<br class="calibre2"/><br class="calibre2"/>def draw_decision_boundary(clf, X, y, h = .01, figsize=(9,9), boundary_cmap=cm.winter, points_cmap=cm.cool):<br class="calibre2"/><br class="calibre2"/>    # After you apply StandardScaler, feature means will be removed and all features will have unit variance.<br class="calibre2"/>    from sklearn.preprocessing import StandardScaler<br class="calibre2"/>    X = StandardScaler().fit_transform(X)<br class="calibre2"/><br class="calibre2"/>    # Splitting dataset to train and test sets.<br class="calibre2"/>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)<br class="calibre2"/><br class="calibre2"/>    # Training given estimator on training dataset by invoking fit function.<br class="calibre2"/>    clf.fit(X_train, y_train)<br class="calibre2"/><br class="calibre2"/>    # Each estimator has a score function.<br class="calibre2"/>    # Score will show you estimator's performance by showing metric suitable to given classifier.<br class="calibre2"/>    # For example, for linear regression, it will output coefficient of determination R^2 of the prediction.<br class="calibre2"/>    # For logistic regression, it will output mean accuracy.<br class="calibre2"/><br class="calibre2"/>    score = clf.score(X_test, y_test)<br class="calibre2"/>    print("Score: %0.3f" % score)<br class="calibre2"/><br class="calibre2"/>    # Predict function of an estimator, will predict using trained model<br class="calibre2"/>    pred = clf.predict(X_test)<br class="calibre2"/><br class="calibre2"/>    # Figure is a high-level container that contains plot elements<br class="calibre2"/>    figure = plt.figure(figsize=figsize)<br class="calibre2"/><br class="calibre2"/>    # In current figure, subplot will create Axes based on given arguments (nrows, ncols, index)<br class="calibre2"/>    ax = plt.subplot(1, 1, 1)<br class="calibre2"/><br class="calibre2"/>    # Calculating min/max of axes<br class="calibre2"/>    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1<br class="calibre2"/>    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1<br class="calibre2"/><br class="calibre2"/>    # Meshgrid is usually used to evaluate function on grid.<br class="calibre2"/>    # It will allow you to create points to represent the space you operate<br class="calibre2"/>    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))<br class="calibre2"/><br class="calibre2"/>    # Generate predictions for all the point-pair created by meshgrid<br class="calibre2"/>    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br class="calibre2"/>    Z = Z.reshape(xx.shape)<br class="calibre2"/><br class="calibre2"/>    # This will draw boundary<br class="calibre2"/>    ax.contourf(xx, yy, Z, cmap=boundary_cmap)<br class="calibre2"/><br class="calibre2"/>    # Plotting training data<br class="calibre2"/>    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=points_cmap, edgecolors='k')<br class="calibre2"/><br class="calibre2"/>    # Potting testing data<br class="calibre2"/>    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=points_cmap, alpha=0.6, edgecolors='k')<br class="calibre2"/><br class="calibre2"/>    # Showing your masterpiece<br class="calibre2"/>    figure.show()</pre>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">逻辑回归的决策边界</h1>
                
            
            
                
<p class="mce-root">您可以从逻辑回归开始测试这个函数:</p>
<pre class="calibre21">import numpy as np<br class="calibre2"/>import matplotlib.pyplot as plt<br class="calibre2"/>from matplotlib import cm<br class="calibre2"/><br class="calibre2"/># sklearn.linear_model includes regression models where target variable is a linear combination of input variables<br class="calibre2"/>from sklearn.linear_model import LogisticRegression<br class="calibre2"/><br class="calibre2"/># make_moons is another useful function to generate sample data<br class="calibre2"/>from sklearn.datasets import make_moons<br class="calibre2"/>from sklearn.model_selection import train_test_split<br class="calibre2"/><br class="calibre2"/>X, y = make_moons(n_samples=1000, noise=0.1, random_state=0)<br class="calibre2"/><br class="calibre2"/># Plot sample data<br class="calibre2"/>plt.scatter(X[:,0], X[:,1], c=y, cmap=cm.cool)<br class="calibre2"/>plt.show()</pre>
<p class="mce-root">我们得到如下的情节:</p>
<div><img src="img/633a0332-76b4-4a83-9bdd-6ce44db4267f.png" width="944" height="648" class="calibre134"/></div>
<p class="mce-root">现在，您可以使用<kbd class="calibre16">draw_decision_boundary</kbd>函数来可视化<kbd class="calibre16">LogisticRegression</kbd>的决策边界:</p>
<pre class="calibre21">draw_decision_boundary(LogisticRegression(), X, y)</pre>
<p class="mce-root">它将输出以下图形:</p>
<div><img src="img/7dbca387-4be3-495d-9610-5195af978b4f.png" width="944" height="923" class="calibre135"/></div>
<p class="mce-root">逻辑回归是广义线性模型的一员，它产生一个线性决策边界。线性决策边界无法将此类数据集的类分开。逻辑回归的输出是根据其输入的加权和计算的。由于输出不依赖于其参数的乘积或商，因此会产生一个线性决策边界。有一些方法可以解决这个问题，例如正则化和要素映射，但是在这种情况下，您可以使用能够处理非线性数据的其他算法。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">随机森林的决策边界</h1>
                
            
            
                
<p class="mce-root">随机森林是一个元估计器，它将建立许多不同的模型，并聚合它们的预测以得出最终预测。随机森林能够产生非线性决策边界，因为输入和输出之间没有线性关系。它有许多可以使用的超参数，但是为了简单起见，您将使用默认配置:</p>
<pre class="calibre21">from sklearn.ensemble import RandomForestClassifier<br class="calibre2"/><br class="calibre2"/>draw_decision_boundary(RandomForestClassifier(), X, y)</pre>
<p class="mce-root">从前面的代码中我们得到了下面的图:</p>
<div><img src="img/f5a7d1f2-9b00-43d6-9a9a-ea89cd1ac805.png" width="944" height="942" class="calibre136"/></div>
<p class="mce-root">看起来一点也不差！每种算法都会根据其内部工作原理为您提供不同的决策界限，您应该尝试使用不同的评估工具来更好地理解它们的行为。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">常用的机器学习算法</h1>
                
            
            
                
<p class="mce-root">作为练习，下面列出了常用的监督和非监督算法；scikit-learn 拥有其中的大部分:</p>
<ul class="calibre13">
<li class="calibre14">监督算法:<ul class="calibre13">
<li class="calibre14">线性回归</li>
<li class="calibre14">逻辑回归</li>
<li class="calibre14">k-神经网络</li>
<li class="calibre14">随机森林</li>
<li class="calibre14">升压算法(GBM、XGBoost 和 LightGBM)</li>
<li class="calibre14">SVM</li>
<li class="calibre14">神经网络</li>
</ul>
</li>
</ul>
<ul class="calibre13">
<li class="calibre14">无监督算法:<ul class="calibre13">
<li class="calibre14">k 均值</li>
<li class="calibre14">分层聚类</li>
<li class="calibre14">主成分分析</li>
<li class="calibre14">混合模型</li>
<li class="calibre14">自动编码器</li>
</ul>
</li>
</ul>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">必要的功能转换</h1>
                
            
            
                
<p class="mce-root">您可能已经注意到，在训练机器学习算法之前，在前面的部分中对特征进行了缩放。特征变换通常是 ML 算法正常工作所必需的。例如，根据经验，对于使用正则化的 ML 算法，通常将归一化应用于特征。</p>
<p class="mce-root">以下是您应该变换要素以使数据集为 ML 算法做好准备的用例列表:</p>
<ul class="calibre13">
<li class="calibre14">SVM 希望其输入在标准范围内。在将变量输入算法之前，你应该将它们规范化。</li>
<li class="calibre14"><strong class="calibre3">主成分分析</strong> ( <strong class="calibre3"> PCA </strong>)帮助你基于方差最大化将你的特征投射到另一个空间。然后，您可以选择覆盖数据集中大多数差异的组件，将其余组件排除在外以降低维数。使用 PCA 时，您可以应用归一化，因为某些要素似乎可以解释由于比例差异而产生的几乎所有差异。您可以通过归一化要素来消除比例差异，正如您将在下一节的一些示例中看到的那样。</li>
<li class="calibre14">如果您正在使用正则化回归(这通常是高维数据集的情况)，您将对变量进行归一化以控制比例，因为正则化不是比例不变的。</li>
<li class="calibre14">要使用朴素贝叶斯算法，其中要素和标注列应是分类的，应通过应用宁滨变换连续变量使其离散化。</li>
<li class="calibre14">在时间序列中，通常应用对数变换来处理指数增长趋势，以便获得线性趋势和恒定方差。</li>
<li class="calibre14">当处理非数值变量(如分类数据)时，您将通过应用转换(如一键编码、虚拟编码或要素哈希)将其编码为数值要素。</li>
</ul>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">监督 ML</h1>
                
            
            
                
<p class="mce-root">除了前面提到的特征变换，每个 ML 算法都有自己的超参数空间需要优化。您可以将搜索最佳 ML 管道看作是遍历您的配置空间，并以一种聪明的方式尝试您的选项，以找到性能最佳的 ML 管道。</p>
<p class="mce-root">Auto-sklearn 非常有助于实现这一目标，您在介绍性章节中看到的示例向您展示了该库的易用性。这一节将解释为了使这个实现成功，在幕后发生了什么。</p>
<p class="mce-root">Auto-sklearn 使用<em class="calibre17">元学习</em>来基于给定数据集的属性选择有前途的数据/特征处理器和 ML 算法。有关预处理方法、分类器和回归器的列表，请参考以下链接:</p>
<ul class="calibre13">
<li class="calibre14">分类器(<a href="https://github.com/automl/auto-sklearn/tree/master/autosklearn/pipeline/components/classification" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre12 pcalibre2">https://github . com/automl/auto-sk learn/tree/master/autosklearn/pipeline/components/classification</a>)</li>
<li class="calibre14">回归器(<a href="https://github.com/automl/auto-sklearn/tree/master/autosklearn/pipeline/components/regression" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre12 pcalibre2">https://github . com/automl/auto-sk learn/tree/master/autosklearn/pipeline/components/regression</a>)</li>
<li class="calibre14">预处理器(<a href="https://github.com/automl/auto-sklearn/tree/master/autosklearn/pipeline/components/feature_preprocessing" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre12 pcalibre2">https://github . com/automl/auto-sk learn/tree/master/autosklearn/pipeline/components/feature _ 预处理</a></li>
</ul>
<p class="mce-root">元学习通过分析跨不同数据集的 ML 管道的性能来模仿数据科学家的经验，并将这些发现与新数据集进行匹配，以对初始配置提出建议。</p>
<p class="mce-root">一旦元学习创建了初始配置，贝叶斯优化将处理调整不同管道的超参数，并且顶级管道将被用于创建可能优于其任何成员并且也有助于避免过度拟合的集成。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">自动 sklearn 的默认配置</h1>
                
            
            
                
<p class="mce-root">当您创建一个<kbd class="calibre16">AutoSklearnClassifier</kbd>对象时，顺便说一下，您将很快完成，有一些默认配置您需要知道；您可以通过运行以下代码来查看它们:</p>
<pre class="calibre21">from autosklearn.classification import AutoSklearnClassifier<br class="calibre2"/>AutoSklearnClassifier?</pre>
<p class="mce-root">在 Python 中，在函数后添加<kbd class="calibre16">?</kbd>会输出非常有用的信息，比如签名、文档字符串、参数解释、属性和文件位置。</p>
<p class="mce-root">如果您查看签名，您会看到默认值:</p>
<pre class="calibre21">Init signature: AutoSklearnClassifier(time_left_for_this_task=3600, per_run_time_limit=360, initial_configurations_via_metalearning=25, ensemble_size=50, ensemble_nbest=50, seed=1, ml_memory_limit=3072, include_estimators=None, exclude_estimators=None, include_preprocessors=None, exclude_preprocessors=None, resampling_strategy='holdout', resampling_strategy_arguments=None, tmp_folder=None, output_folder=None, delete_tmp_folder_after_terminate=True, delete_output_folder_after_terminate=True, shared_mode=False, disable_evaluator_output=False, get_smac_object_callback=None, smac_scenario_args=None)</pre>
<p class="mce-root">例如，<kbd class="calibre16">time_left_for_this_task</kbd>设置为 60 分钟。如果您正在处理一个相当复杂的数据集，您应该将该参数设置为一个较高的值，以增加找到更好的 ML 管道的机会。</p>
<p class="mce-root">还有一个是<kbd class="calibre16">per_run_time_limit</kbd>，设置为六分钟。许多 ML 算法的训练时间与输入数据大小成比例，此外，训练时间还会受到算法复杂性的影响。您应该相应地设置该参数。</p>
<p class="mce-root"><kbd class="calibre16">ensemble_size</kbd>和<kbd class="calibre16">ensemble_nbest</kbd>是集合相关的参数，其设置要包括在集合中的最佳模型的大小和数量。</p>
<p class="mce-root"><kbd class="calibre16">ml_memory_limit</kbd>是一个重要参数，因为如果您的算法需要更多内存，训练将被取消。</p>
<p class="mce-root">通过使用以下参数提供列表，您可以在 ML 管道中包括/排除特定的数据预处理程序或估算程序:<kbd class="calibre16">include_estimators</kbd>、<kbd class="calibre16">exclude_estimators</kbd>、<kbd class="calibre16">include_preprocessors</kbd>和<kbd class="calibre16">exclude_preprocessors</kbd></p>
<p class="mce-root">将为您提供选项来决定如何处理过度拟合。</p>
<p class="mce-root">您可以检查签名中的其余参数，看看是否需要对您的环境进行任何特定的调整。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">为产品线预测寻找最佳 ML 管道</h1>
                
            
            
                
<p class="mce-root">让我们先编写一个小的包装器函数，通过对分类变量进行编码来准备数据集:</p>
<pre class="calibre21"># Importing necessary variables<br class="calibre2"/>import numpy as np<br class="calibre2"/>import pandas as pd<br class="calibre2"/>from autosklearn.classification import AutoSklearnClassifier<br class="calibre2"/>from autosklearn.regression import AutoSklearnRegressor<br class="calibre2"/>from sklearn.model_selection import train_test_split<br class="calibre2"/>from sklearn.metrics import accuracy_score<br class="calibre2"/>from sklearn.preprocessing import LabelEncoder<br class="calibre2"/>import wget<br class="calibre2"/>import pandas as pd<br class="calibre2"/><br class="calibre2"/># Machine learning algorithms work with numerical inputs and you need to transform all non-numerical inputs to numerical ones<br class="calibre2"/># Following snippet encode the categorical variables<br class="calibre2"/><br class="calibre2"/>link_to_data = 'https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600'<br class="calibre2"/>filename = wget.download(link_to_data)<br class="calibre2"/><br class="calibre2"/>print(filename)<br class="calibre2"/># GoSales_Tx_NaiveBayes.csv<br class="calibre2"/><br class="calibre2"/>df = pd.read_csv('GoSales_Tx_NaiveBayes.csv')<br class="calibre2"/>df.head()</pre>
<p class="mce-root">这将输出数据帧的前五条记录:</p>
<pre class="calibre21"># PRODUCT_LINE GENDER AGE MARITAL_STATUS PROFESSION<br class="calibre2"/># 0 Personal Accessories M 27 Single Professional<br class="calibre2"/># 1 Personal Accessories F 39 Married Other<br class="calibre2"/># 2 Mountaineering Equipment F 39 Married Other<br class="calibre2"/># 3 Personal Accessories F 56 Unspecified Hospitality<br class="calibre2"/># 4 Golf Equipment M 45 Married Retired</pre>
<p class="mce-root">该数据集中有四个特征(<kbd class="calibre16">GENDER</kbd>、<kbd class="calibre16">AGE</kbd>、<kbd class="calibre16">MARITAL_STATUS</kbd>和<kbd class="calibre16">PROFESSION</kbd>)和一个标签(<kbd class="calibre16">PRODUCT_LINE</kbd>)列。目标是预测客户感兴趣的产品线。</p>
<p class="mce-root">您需要对要素和标注的文本数据进行编码。您可以应用<kbd class="calibre16">LabelEncoder</kbd>:</p>
<pre class="calibre21">df = df.apply(LabelEncoder().fit_transform)<br class="calibre2"/>df.head()</pre>
<p class="mce-root">这将对<kbd class="calibre16">label</kbd>列进行编码:</p>
<pre class="calibre21">#   PRODUCT_LINE GENDER AGE MARITAL_STATUS PROFESSION<br class="calibre2"/># 0 4 1 27 1 3<br class="calibre2"/># 1 4 0 39 0 2<br class="calibre2"/># 2 2 0 39 0 2<br class="calibre2"/># 3 4 0 56 2 1<br class="calibre2"/># 4 1 1 45 0 5</pre>
<p class="mce-root">如您所见，所有分类列都被编码了。请记住，在 auto-sklearn 的 API 中，您有<kbd class="calibre16">feat_type</kbd>参数，它允许您将列指定为<kbd class="calibre16">Categorical</kbd>或<kbd class="calibre16">Numerical</kbd>:</p>
<pre class="calibre21">feat_type : list, optional (default=None)</pre>
<p class="mce-root">描述属性类型的<kbd class="calibre16">len(X.shape[1])</kbd>的<kbd class="calibre16">str</kbd>列表。可能的类型有<kbd class="calibre16">Categorical</kbd>和<kbd class="calibre16">Numerical</kbd>。分类属性将被自动一次性编码。用于分类属性的值必须是由<kbd class="calibre16">sklearn.preprocessing.LabelEncoder</kbd>获得的整数。</p>
<p class="mce-root">然而，在这个例子中，您也可以使用 pandas 数据帧的<kbd class="calibre16">apply</kbd>函数。</p>
<p class="mce-root">以下包装函数将使用 auto-sklearn 的自动分类或自动回归算法处理输入数据并运行实验:</p>
<pre class="calibre21"># Function below will encode the target variable if needed<br class="calibre2"/>def encode_target_variable(df=None, target_column=None, y=None):<br class="calibre2"/><br class="calibre2"/>    # Below section will encode target variable if given data is pandas dataframe<br class="calibre2"/>    if df is not None:<br class="calibre2"/>        df_type = isinstance(df, pd.core.frame.DataFrame)<br class="calibre2"/><br class="calibre2"/>        # Splitting dataset as train and test data sets<br class="calibre2"/>        if df_type:<br class="calibre2"/><br class="calibre2"/>            # If column data type is not numeric then labels are encoded<br class="calibre2"/>            if not np.issubdtype(df[target_column].dtype, np.number):<br class="calibre2"/>                le = preprocessing.LabelEncoder()<br class="calibre2"/>                df[target_column] = le.fit_transform(df[target_column])<br class="calibre2"/>                return df[target_column]<br class="calibre2"/><br class="calibre2"/>            return df[target_column]<br class="calibre2"/>    # Below section will encode numpy array.<br class="calibre2"/>    else:<br class="calibre2"/><br class="calibre2"/>        # numpy array's data type is not numeric then labels are encoded<br class="calibre2"/>        if not np.issubdtype(y.dtype, np.number):<br class="calibre2"/>            le = preprocessing.LabelEncoder()<br class="calibre2"/>            y = le.fit_transform(y)<br class="calibre2"/>            return y<br class="calibre2"/><br class="calibre2"/>        return y<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/># Create a wrapper function where you can specify the type of learning problem<br class="calibre2"/>def supervised_learner(type, X_train, y_train, X_test, y_test):<br class="calibre2"/><br class="calibre2"/>    if type == 'regression':<br class="calibre2"/>        # You can play with time related arguments for discovering more pipelines<br class="calibre2"/>        automl = AutoSklearnRegressor(time_left_for_this_task=7200, per_run_time_limit=720)<br class="calibre2"/>    else:<br class="calibre2"/>        automl = AutoSklearnClassifier(time_left_for_this_task=7200, per_run_time_limit=720)<br class="calibre2"/><br class="calibre2"/>    # Training estimator based on learner type<br class="calibre2"/>    automl.fit(X_train, y_train)<br class="calibre2"/><br class="calibre2"/>    # Predicting labels on test data<br class="calibre2"/>    y_hat = automl.predict(X_test)<br class="calibre2"/><br class="calibre2"/>    # Calculating accuracy_score<br class="calibre2"/>    metric = accuracy_score(y_test, y_hat)<br class="calibre2"/><br class="calibre2"/>    # Return model, labels and metric<br class="calibre2"/>    return automl, y_hat, metric<br class="calibre2"/><br class="calibre2"/># In function below, you need to provide numpy array or pandas dataframe together with the name of the target column as arguments<br class="calibre2"/>def supervised_automl(data, target_column=None, type=None, y=None):<br class="calibre2"/><br class="calibre2"/>    # First thing is to check whether data is pandas dataframe<br class="calibre2"/>    df_type = isinstance(data, pd.core.frame.DataFrame)<br class="calibre2"/><br class="calibre2"/>    # Based on data type, you will split dataset as train and test data sets<br class="calibre2"/>    if df_type:<br class="calibre2"/>        # This is where encode_target_variable function is used before data split<br class="calibre2"/>        data[target_column] = encode_target_variable(data, target_column)<br class="calibre2"/>        X_train, X_test, y_train, y_test = \<br class="calibre2"/>            train_test_split(data.loc[:, data.columns != target_column], data[target_column], random_state=1)<br class="calibre2"/>    else:<br class="calibre2"/>        y_encoded = encode_target_variable(y=y)<br class="calibre2"/>        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1)<br class="calibre2"/><br class="calibre2"/>    # If learner type is given, then you invoke supervied_learner<br class="calibre2"/>    if type != None:<br class="calibre2"/>        automl, y_hat, metric = supervised_learner(type, X_train, y_train, X_test, y_test)<br class="calibre2"/><br class="calibre2"/>    # If type of learning problem is not given, you need to infer it<br class="calibre2"/>    # If there are more than 10 unique numerical values, problem will be treated as regression problem,<br class="calibre2"/>    # Otherwise, classification problem<br class="calibre2"/><br class="calibre2"/>    elif len(df[target_column].unique()) &gt; 10:<br class="calibre2"/>            print("""There are more than 10 uniques numerical values in target column. <br class="calibre2"/>            Treating it as regression problem.""")<br class="calibre2"/>            automl, y_hat, metric = supervised_learner('regression', X_train, y_train, X_test, y_test)<br class="calibre2"/>    else:<br class="calibre2"/>        automl, y_hat, metric = supervised_learner('classification', X_train, y_train, X_test, y_test)<br class="calibre2"/><br class="calibre2"/>    # Return model, labels and metric<br class="calibre2"/>    return automl, y_hat, metric</pre>
<p class="mce-root">您现在可以运行它来查看结果:</p>
<pre class="calibre21">automl, y_hat, metric = supervised_automl(df, target_column='PRODUCT_LINE')</pre>
<p class="mce-root">以下输出显示了所选模型及其参数:</p>
<pre class="calibre21">automl.get_models_with_weights()<br class="calibre2"/> [(1.0,<br class="calibre2"/>  SimpleClassificationPipeline({'balancing:strategy': 'none', 'categorical_encoding:__choice__': 'no_encoding', 'classifier:__choice__': 'gradient_boosting', 'imputation:strategy': 'most_frequent', 'preprocessor:__choice__': 'feature_agglomeration', 'rescaling:__choice__': 'robust_scaler', 'classifier:gradient_boosting:criterion': 'friedman_mse', 'classifier:gradient_boosting:learning_rate': 0.6019977814828193, 'classifier:gradient_boosting:loss': 'deviance', 'classifier:gradient_boosting:max_depth': 5, 'classifier:gradient_boosting:max_features': 0.4884281825655421, 'classifier:gradient_boosting:max_leaf_nodes': 'None', 'classifier:gradient_boosting:min_impurity_decrease': 0.0, 'classifier:gradient_boosting:min_samples_leaf': 20, 'classifier:gradient_boosting:min_samples_split': 7, 'classifier:gradient_boosting:min_weight_fraction_leaf': 0.0, 'classifier:gradient_boosting:n_estimators': 313, 'classifier:gradient_boosting:subsample': 0.3242201709371377, 'preprocessor:feature_agglomeration:affinity': 'cosine', 'preprocessor:feature_agglomeration:linkage': 'complete', 'preprocessor:feature_agglomeration:n_clusters': 383, 'preprocessor:feature_agglomeration:pooling_func': 'mean', 'rescaling:robust_scaler:q_max': 0.75, 'rescaling:robust_scaler:q_min': 0.25},<br class="calibre2"/>  dataset_properties={<br class="calibre2"/>    'task': 1,<br class="calibre2"/>    'sparse': False,<br class="calibre2"/>    'multilabel': False,<br class="calibre2"/>    'multiclass': False,<br class="calibre2"/>    'target_type': 'classification',<br class="calibre2"/>    'signed': False}))]</pre>
<p class="mce-root">您可能会看到通常选择梯度增强算法，这是有充分理由的。目前，在 ML 社区中，基于 boosting 的算法是最先进的，最受欢迎的是<strong class="calibre7"> XGBoost </strong>、<strong class="calibre7"> LightGBM </strong>和<strong class="calibre7"> CatBoost </strong>。</p>
<p class="mce-root">Auto-sklearn 提供了对 sklearn 和 XGBoost 的<kbd class="calibre16">GradientBoostingClassifier</kbd>的支持，由于集成问题，XGBoost 目前被禁用，但预计它很快会被添加回来。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">为网络异常检测寻找最佳机器学习管道</h1>
                
            
            
                
<p class="mce-root">让我们在 ML 社区中流行的另一个数据集上运行这个管道。<kbd class="calibre16">KDDCUP 99</kbd>数据集是 1998 DARPA <kbd class="calibre16">Intrusion Detection System Evaluation</kbd>数据集的 tcpdump 部分，目标是检测网络入侵。它包括数字特征，因此将更容易设置我们的 AutoML 管道:</p>
<pre class="calibre21"># You can import this dataset directly from sklearn<br class="calibre2"/>from sklearn.datasets import fetch_kddcup99<br class="calibre2"/><br class="calibre2"/># Downloading subset of whole dataset<br class="calibre2"/>dataset = fetch_kddcup99(subset='http', shuffle=True, percent10=True)<br class="calibre2"/># Downloading https://ndownloader.figshare.com/files/5976042<br class="calibre2"/># [INFO] [17:43:19:sklearn.datasets.kddcup99] Downloading https://ndownloader.figshare.com/files/5976042<br class="calibre2"/><br class="calibre2"/>X = dataset.data<br class="calibre2"/>y = dataset.target<br class="calibre2"/><br class="calibre2"/># 58725 examples with 3 features<br class="calibre2"/>X.shape<br class="calibre2"/># (58725, 3)<br class="calibre2"/><br class="calibre2"/>y.shape<br class="calibre2"/>(58725,)<br class="calibre2"/><br class="calibre2"/># 5 different classes to represent network anomaly<br class="calibre2"/>from pprint import pprint<br class="calibre2"/>pprint(np.unique(y))<br class="calibre2"/># array([b'back.', b'ipsweep.', b'normal.', b'phf.', b'satan.'], dtype=object)<br class="calibre2"/><br class="calibre2"/>automl, y_hat, metric = supervised_automl(X, y=y, type='classification')</pre>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">无人监督的 AutoML</h1>
                
            
            
                
<p class="mce-root">当数据集没有目标变量时，可以基于不同的特征使用聚类算法来探索它。这些算法将示例分组在一起，以便每个组中的示例彼此尽可能相似，但与其他组中的示例不相似。</p>
<p class="mce-root">由于在执行此类分析时，您通常没有标注，因此可以使用一个性能指标来检查算法所发现的最终分离的质量。</p>
<p class="mce-root">称为<strong class="calibre7">轮廓系数</strong>。轮廓系数将帮助你理解两件事:</p>
<ul class="calibre13">
<li class="calibre14"><strong class="calibre3">内聚力</strong>:聚类内的相似性</li>
<li class="calibre14"><strong class="calibre3">分离</strong>:集群间的相异</li>
</ul>
<p class="mce-root">它会给出一个介于 1 和-1 之间的值，接近 1 的值表示结构良好的簇。</p>
<p class="mce-root">如果您的训练数据中有标签，您还可以使用其他度量标准，如同质性和完整性，您将在本章的后面看到。</p>
<p class="mce-root">聚类算法用于处理许多不同的任务，例如查找相似的用户、歌曲或图像，检测模式中的关键趋势和变化，理解社交网络中的社区结构。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">常用的聚类算法</h1>
                
            
            
                
<p class="mce-root">有两种常用的聚类算法:基于距离的和概率模型。例如，带有噪声的应用程序的 k-means 和<strong class="calibre7">基于密度的空间聚类</strong> ( <strong class="calibre7"> DBSCAN </strong>)是基于距离的算法，而高斯混合模型是概率性的。</p>
<p class="mce-root">基于距离的算法可以使用各种距离度量，其中通常使用欧几里德距离度量。</p>
<p class="mce-root">概率算法将假设存在一个具有未知参数的概率分布混合的生成过程，目标是从数据中计算这些参数。</p>
<p class="mce-root">因为有许多聚类算法，所以选择正确的算法取决于数据的特征。例如，k-means 将处理聚类的质心，这要求数据中的聚类大小均匀，形状凸起。这意味着 k-means 不能很好地处理拉长的簇或不规则形状的流形。当数据中的聚类大小不均匀或呈凸形时，您可能希望使用 DBSCAN 对任何形状的区域进行聚类。</p>
<p class="mce-root">对你的数据略知一二会让你离找到正确的算法更近一步，但是如果你对你的数据了解不多呢？很多时候，当您执行探索性分析时，您可能很难理解正在发生的事情。如果您发现自己处于这种情况，自动化的无监督 ML 管道可以帮助您更好地了解数据的特征。</p>
<p class="mce-root">但是，在执行这种分析时要小心；你将采取的行动将会被你将会看到的结果所驱使，如果你不小心的话，这将会很快把你送上错误的道路。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">使用 sklearn 创建样本数据集</h1>
                
            
            
                
<p class="mce-root">在<kbd class="calibre16">sklearn</kbd>中，有一些为测试算法创建样本数据集的有用方法:</p>
<pre class="calibre21"># Importing necessary libraries for visualization<br class="calibre2"/>import matplotlib.pyplot as plt<br class="calibre2"/>import seaborn as sns<br class="calibre2"/><br class="calibre2"/># Set context helps you to adjust things like label size, lines and various elements<br class="calibre2"/># Try "notebook", "talk" or "paper" instead of "poster" to see how it changes<br class="calibre2"/>sns.set_context('poster')<br class="calibre2"/><br class="calibre2"/># set_color_codes will affect how colors such as 'r', 'b', 'g' will be interpreted<br class="calibre2"/>sns.set_color_codes()<br class="calibre2"/><br class="calibre2"/># Plot keyword arguments will allow you to set things like size or line width to be used in charts.<br class="calibre2"/>plot_kwargs = {'s': 10, 'linewidths': 0.1}<br class="calibre2"/><br class="calibre2"/>import numpy as np<br class="calibre2"/>import pandas as pd<br class="calibre2"/><br class="calibre2"/># Pprint will better output your variables in console for readability<br class="calibre2"/>from pprint import pprint<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/># Creating sample dataset using sklearn samples_generator<br class="calibre2"/>from sklearn.datasets.samples_generator import make_blobs<br class="calibre2"/>from sklearn.preprocessing import StandardScaler<br class="calibre2"/><br class="calibre2"/># Make blobs will generate isotropic Gaussian blobs<br class="calibre2"/># You can play with arguments like center of blobs, cluster standard deviation<br class="calibre2"/>centers = [[2, 1], [-1.5, -1], [1, -1], [-2, 2]]<br class="calibre2"/>cluster_std = [0.1, 0.1, 0.1, 0.1]<br class="calibre2"/><br class="calibre2"/># Sample data will help you to see your algorithms behavior<br class="calibre2"/>X, y = make_blobs(n_samples=1000,<br class="calibre2"/>                  centers=centers,<br class="calibre2"/>                  cluster_std=cluster_std,<br class="calibre2"/>                  random_state=53)<br class="calibre2"/><br class="calibre2"/></pre>
<pre class="calibre21"># Plot generated sample data<br class="calibre2"/>plt.scatter(X[:, 0], X[:, 1], **plot_kwargs)<br class="calibre2"/>plt.show()</pre>
<p class="mce-root">从前面的代码中我们得到了下面的图:</p>
<div><img src="img/c732f54a-9bbc-4958-aeb3-f0d68099d2f2.png" width="944" height="653" class="calibre137"/></div>
<p class="mce-root"><kbd class="calibre16">cluster_std</kbd>会影响色散量。将其更改为<kbd class="calibre16">[0.4, 0.5, 0.6, 0.5]</kbd>并重试:</p>
<pre class="calibre21">cluster_std = [0.4, 0.5, 0.6, 0.5] <br class="calibre2"/><br class="calibre2"/>X, y = make_blobs(n_samples=1000,<br class="calibre2"/>                  centers=centers,<br class="calibre2"/>                  cluster_std=cluster_std,<br class="calibre2"/>                  random_state=53)<br class="calibre2"/><br class="calibre2"/>plt.scatter(X[:, 0], X[:, 1], **plot_kwargs)<br class="calibre2"/>plt.show()</pre>
<p class="mce-root">从前面的代码中我们得到了下面的图:</p>
<div><img src="img/c1af55f2-36f3-4e21-a9cc-f8b8269c8a84.png" width="944" height="661" class="calibre138"/></div>
<p class="mce-root">现在看起来更逼真了！</p>
<p class="mce-root">让我们用有帮助的方法写一个小类来创建无监督的实验。首先，您将使用<kbd class="calibre16">fit_predict</kbd>方法对样本数据集应用一个或多个聚类算法:</p>
<pre class="calibre21">class Unsupervised_AutoML:<br class="calibre2"/><br class="calibre2"/>    def __init__(self, estimators=None, transformers=None):<br class="calibre2"/>        self.estimators = estimators<br class="calibre2"/>        self.transformers = transformers<br class="calibre2"/>        pass</pre>
<p class="mce-root">类别将用一组估计器和转换器初始化。第二类方法将是<kbd class="calibre16">fit_predict</kbd>:</p>
<pre class="calibre21">def fit_predict(self, X, y=None):<br class="calibre2"/>    """<br class="calibre2"/>    fit_predict will train given estimator(s) and predict cluster membership for each sample<br class="calibre2"/>    """<br class="calibre2"/><br class="calibre2"/>    # This dictionary will hold predictions for each estimator<br class="calibre2"/>    predictions = []<br class="calibre2"/>    performance_metrics = {}<br class="calibre2"/><br class="calibre2"/>    for estimator in self.estimators:<br class="calibre2"/>        labels = estimator['estimator'](*estimator['args'], **estimator['kwargs']).fit_predict(X)<br class="calibre2"/>        estimator['estimator'].n_clusters_ = len(np.unique(labels))<br class="calibre2"/>        metrics = self._get_cluster_metrics(estimator['estimator'].__name__, estimator['estimator'].n_clusters_, X, labels, y)<br class="calibre2"/>        predictions.append({estimator['estimator'].__name__: labels})<br class="calibre2"/>        performance_metrics[estimator['estimator'].__name__] = metrics<br class="calibre2"/><br class="calibre2"/>    self.predictions = predictions<br class="calibre2"/>    self.performance_metrics = performance_metrics<br class="calibre2"/><br class="calibre2"/>    return predictions, performance_metrics</pre>
<p class="mce-root"><kbd class="calibre16">fit_predict</kbd>方法使用<kbd class="calibre16">_get_cluster_metrics</kbd>方法获取性能指标，这在下面的代码块中定义:</p>
<pre class="calibre21"># Printing cluster metrics for given arguments<br class="calibre2"/>def _get_cluster_metrics(self, name, n_clusters_, X, pred_labels, true_labels=None):<br class="calibre2"/>    from sklearn.metrics import homogeneity_score, \<br class="calibre2"/>        completeness_score, \<br class="calibre2"/>        v_measure_score, \<br class="calibre2"/>        adjusted_rand_score, \<br class="calibre2"/>        adjusted_mutual_info_score, \<br class="calibre2"/>        silhouette_score<br class="calibre2"/><br class="calibre2"/>    print("""################## %s metrics #####################""" % name)<br class="calibre2"/>    if len(np.unique(pred_labels)) &gt;= 2:<br class="calibre2"/><br class="calibre2"/>        silh_co = silhouette_score(X, pred_labels)<br class="calibre2"/><br class="calibre2"/>        if true_labels is not None:<br class="calibre2"/><br class="calibre2"/>            h_score = homogeneity_score(true_labels, pred_labels)<br class="calibre2"/>            c_score = completeness_score(true_labels, pred_labels)<br class="calibre2"/>            vm_score = v_measure_score(true_labels, pred_labels)<br class="calibre2"/>            adj_r_score = adjusted_rand_score(true_labels, pred_labels)<br class="calibre2"/>            adj_mut_info_score = adjusted_mutual_info_score(true_labels, pred_labels)<br class="calibre2"/><br class="calibre2"/>            metrics = {"Silhouette Coefficient": silh_co,<br class="calibre2"/>                       "Estimated number of clusters": n_clusters_,<br class="calibre2"/>                       "Homogeneity": h_score,<br class="calibre2"/>                       "Completeness": c_score,<br class="calibre2"/>                       "V-measure": vm_score,<br class="calibre2"/>                       "Adjusted Rand Index": adj_r_score,<br class="calibre2"/>                       "Adjusted Mutual Information": adj_mut_info_score}<br class="calibre2"/><br class="calibre2"/>            for k, v in metrics.items():<br class="calibre2"/>                print("\t%s: %0.3f" % (k, v))<br class="calibre2"/><br class="calibre2"/>            return metrics<br class="calibre2"/><br class="calibre2"/>        metrics = {"Silhouette Coefficient": silh_co,<br class="calibre2"/>                   "Estimated number of clusters": n_clusters_}<br class="calibre2"/><br class="calibre2"/>        for k, v in metrics.items():<br class="calibre2"/>            print("\t%s: %0.3f" % (k, v))<br class="calibre2"/><br class="calibre2"/>        return metrics<br class="calibre2"/><br class="calibre2"/>    else:<br class="calibre2"/>        print("\t# of predicted labels is {}, can not produce metrics. \n".format(np.unique(pred_labels)))</pre>
<p class="mce-root"><kbd class="calibre16">_get_cluster_metrics</kbd>方法计算度量，例如<kbd class="calibre16">homogeneity_score</kbd>、<kbd class="calibre16">completeness_score</kbd>、<kbd class="calibre16">v_measure_score</kbd>、<kbd class="calibre16">adjusted_rand_score</kbd>、<kbd class="calibre16">adjusted_mutual_info_score</kbd>和<kbd class="calibre16">silhouette_score</kbd>。这些指标将帮助您评估分类的分离程度，并测量分类内部和分类之间的相似性。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">运行中的 K-means 算法</h1>
                
            
            
                
<p class="mce-root">您现在可以应用<kbd class="calibre16">KMeans</kbd>算法来看看它是如何工作的:</p>
<pre class="calibre21">from sklearn.cluster import KMeans<br class="calibre2"/><br class="calibre2"/>estimators = [{'estimator': KMeans, 'args':(), 'kwargs':{'n_clusters': 4}}]<br class="calibre2"/><br class="calibre2"/>unsupervised_learner = Unsupervised_AutoML(estimators)</pre>
<p class="mce-root">你可以看到<kbd class="calibre16">estimators</kbd>:</p>
<pre class="calibre21">unsupervised_learner.estimators</pre>
<p class="mce-root">这些将输出以下内容:</p>
<pre class="calibre21">[{'args': (),<br class="calibre2"/> 'estimator': sklearn.cluster.k_means_.KMeans,<br class="calibre2"/> 'kwargs': {'n_clusters': 4}}]</pre>
<p class="mce-root">您现在可以调用<kbd class="calibre16">fit_predict</kbd>来获得<kbd class="calibre16">predictions</kbd>和<kbd class="calibre16">performance_metrics</kbd>:</p>
<pre class="calibre21">predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)</pre>
<p class="mce-root">指标将被写入控制台:</p>
<pre class="calibre21">################## KMeans metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.631<br class="calibre2"/>  Estimated number of clusters: 4.000<br class="calibre2"/>  Homogeneity: 0.951<br class="calibre2"/>  Completeness: 0.951<br class="calibre2"/>  V-measure: 0.951<br class="calibre2"/>  Adjusted Rand Index: 0.966<br class="calibre2"/>  Adjusted Mutual Information: 0.950</pre>
<p class="mce-root">您可以随时在以后打印指标:</p>
<pre class="calibre21">pprint(performance_metrics)</pre>
<p class="mce-root">这将输出估计器的名称及其度量:</p>
<pre class="calibre21">{'KMeans': {'Silhouette Coefficient': 0.9280431207593165, 'Estimated number of clusters': 4, 'Homogeneity': 1.0, 'Completeness': 1.0, 'V-measure': 1.0, 'Adjusted Rand Index': 1.0, 'Adjusted Mutual Information': 1.0}}</pre>
<p class="mce-root">让我们添加另一个类方法来绘制给定估计值和预测标签的聚类:</p>
<pre class="calibre21"># plot_clusters will visualize the clusters given predicted labels<br class="calibre2"/>def plot_clusters(self, estimator, X, labels, plot_kwargs):<br class="calibre2"/><br class="calibre2"/>    palette = sns.color_palette('deep', np.unique(labels).max() + 1)<br class="calibre2"/>    colors = [palette[x] if x &gt;= 0 else (0.0, 0.0, 0.0) for x in labels]<br class="calibre2"/><br class="calibre2"/>    plt.scatter(X[:, 0], X[:, 1], c=colors, **plot_kwargs)<br class="calibre2"/>    plt.title('{} Clusters'.format(str(estimator.__name__)), fontsize=14)<br class="calibre2"/>    plt.show()<br class="calibre2"/><br class="calibre2"/></pre>
<p class="mce-root">让我们来看看用法:</p>
<pre class="calibre21">plot_kwargs = {'s': 12, 'linewidths': 0.1}<br class="calibre2"/>unsupervised_learner.plot_clusters(KMeans,<br class="calibre2"/>                                   X,<br class="calibre2"/>                                   unsupervised_learner.predictions[0]['KMeans'],<br class="calibre2"/>                                   plot_kwargs)</pre>
<p class="mce-root">从前面的块中可以得到下面的图:</p>
<div><img src="img/3591f881-f418-4f3f-a586-9a4cd8dc535c.png" width="944" height="693" class="calibre139"/></div>
<p class="mce-root">在本例中，聚类大小均匀，彼此之间明显分离，但是，当您进行这种探索性分析时，您应该尝试不同的超参数并检查结果。</p>
<p class="mce-root">在本章的后面，您将编写一个包装函数来应用一系列聚类算法及其超参数来检查结果。现在，让我们再看一个 k-means 不好用的例子。</p>
<p class="mce-root">当数据集中的聚类具有不同的统计属性(如方差差异)时，k-means 将无法正确识别聚类:</p>
<pre class="calibre21">X, y = make_blobs(n_samples=2000, centers=5, cluster_std=[1.7, 0.6, 0.8, 1.0, 1.2], random_state=220)<br class="calibre2"/><br class="calibre2"/># Plot sample data<br class="calibre2"/>plt.scatter(X[:, 0], X[:, 1], **plot_kwargs)<br class="calibre2"/>plt.show()</pre>
<p class="mce-root">从前面的代码中我们得到了下面的图:</p>
<div><img src="img/ba3f5565-737a-4519-bea1-b1a7ab202a06.png" width="944" height="644" class="calibre140"/></div>
<p class="mce-root">虽然这个样本数据集是由五个中心生成的，但这并不明显，也可能有四个聚类:</p>
<pre class="calibre21">from sklearn.cluster import KMeans<br class="calibre2"/><br class="calibre2"/>estimators = [{'estimator': KMeans, 'args':(), 'kwargs':{'n_clusters': 4}}]<br class="calibre2"/><br class="calibre2"/>unsupervised_learner = Unsupervised_AutoML(estimators)<br class="calibre2"/><br class="calibre2"/>predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)<br class="calibre2"/><br class="calibre2"/></pre>
<p class="mce-root">控制台中的指标如下:</p>
<pre class="calibre21">################## KMeans metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.549<br class="calibre2"/>  Estimated number of clusters: 4.000<br class="calibre2"/>  Homogeneity: 0.729<br class="calibre2"/>  Completeness: 0.873<br class="calibre2"/>  V-measure: 0.795<br class="calibre2"/>  Adjusted Rand Index: 0.702<br class="calibre2"/>  Adjusted Mutual Information: 0.729</pre>
<p class="mce-root"><kbd class="calibre16">KMeans</kbd>集群绘制如下:</p>
<pre class="calibre21">plot_kwargs = {'s': 12, 'linewidths': 0.1}<br class="calibre2"/>unsupervised_learner.plot_clusters(KMeans,<br class="calibre2"/>                                   X,<br class="calibre2"/>                                   unsupervised_learner.predictions[0]['KMeans'],<br class="calibre2"/>                                   plot_kwargs)</pre>
<p class="mce-root">从前面的代码中我们得到了下面的图:</p>
<div><img src="img/0665e4c2-6eb2-44a9-aae4-b1aef376bb28.png" width="944" height="643" class="calibre141"/></div>
<p class="mce-root">在这个例子中，红色(深灰色)和底部绿色(浅灰色)之间的点似乎形成了一个大的集群。K-means 基于质心周围点的平均值计算质心。在这里，你需要有一个不同的方法。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">运行中的 DBSCAN 算法</h1>
                
            
            
                
<p class="mce-root">DBSCAN 是一种聚类算法，可以处理不平坦的几何形状和不均匀的聚类大小。让我们看看它能做什么:</p>
<pre class="calibre21">from sklearn.cluster import DBSCAN<br class="calibre2"/><br class="calibre2"/>estimators = [{'estimator': DBSCAN, 'args':(), 'kwargs':{'eps': 0.5}}]<br class="calibre2"/><br class="calibre2"/>unsupervised_learner = Unsupervised_AutoML(estimators)<br class="calibre2"/><br class="calibre2"/>predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)</pre>
<p class="mce-root">控制台中的指标如下:</p>
<pre class="calibre21">################## DBSCAN metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.231<br class="calibre2"/>  Estimated number of clusters: 12.000<br class="calibre2"/>  Homogeneity: 0.794<br class="calibre2"/>  Completeness: 0.800<br class="calibre2"/>  V-measure: 0.797<br class="calibre2"/>  Adjusted Rand Index: 0.737<br class="calibre2"/>  Adjusted Mutual Information: 0.792</pre>
<p class="mce-root"><kbd class="calibre16">DBSCAN</kbd>集群绘制如下:</p>
<pre class="calibre21">plot_kwargs = {'s': 12, 'linewidths': 0.1}<br class="calibre2"/>unsupervised_learner.plot_clusters(DBSCAN,<br class="calibre2"/>                                   X,<br class="calibre2"/>                                   unsupervised_learner.predictions[0]['DBSCAN'],<br class="calibre2"/>                                   plot_kwargs)</pre>
<p class="mce-root">从前面的代码中我们得到了下面的图:</p>
<div><img src="img/d0570eb3-7bae-4ff5-89de-a7867e55216f.png" width="944" height="659" class="calibre142"/></div>
<p class="mce-root">k-means 案例中红色(深灰色)和底部绿色(浅灰色)聚类之间的冲突似乎已经消失，但这里有趣的是，一些小聚类出现了，一些点没有根据它们的距离分配给任何聚类。</p>
<p class="mce-root">DBSCAN 有<kbd class="calibre16">eps(epsilon)</kbd>超参数，它与相同邻域中的点的接近度有关；您可以使用这些参数来观察算法的表现。</p>
<p class="mce-root">当您在对数据了解不多的情况下进行这种探索性分析时，视觉线索总是很重要，因为度量标准可能会误导您，因为不是每个聚类算法都可以使用类似的度量标准进行评估。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">凝聚聚类算法在实践中的应用</h1>
                
            
            
                
<p class="mce-root">我们最后一次尝试使用凝聚聚类算法:</p>
<pre class="calibre21">from sklearn.cluster import AgglomerativeClustering<br class="calibre2"/><br class="calibre2"/>estimators = [{'estimator': AgglomerativeClustering, 'args':(), 'kwargs':{'n_clusters': 4, 'linkage': 'ward'}}]<br class="calibre2"/><br class="calibre2"/>unsupervised_learner = Unsupervised_AutoML(estimators)<br class="calibre2"/><br class="calibre2"/>predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)</pre>
<p class="mce-root">控制台中的指标如下:</p>
<pre class="calibre21">################## AgglomerativeClustering metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.546<br class="calibre2"/>  Estimated number of clusters: 4.000<br class="calibre2"/>  Homogeneity: 0.751<br class="calibre2"/>  Completeness: 0.905<br class="calibre2"/>  V-measure: 0.820<br class="calibre2"/>  Adjusted Rand Index: 0.719<br class="calibre2"/>  Adjusted Mutual Information: 0.750</pre>
<p class="mce-root"><kbd class="calibre16">AgglomerativeClustering</kbd>集群绘制如下:</p>
<pre class="calibre21">plot_kwargs = {'s': 12, 'linewidths': 0.1}<br class="calibre2"/>unsupervised_learner.plot_clusters(AgglomerativeClustering,<br class="calibre2"/>                                   X,<br class="calibre2"/>                                   unsupervised_learner.predictions[0]['AgglomerativeClustering'],<br class="calibre2"/>                                   plot_kwargs)</pre>
<p class="mce-root">从前面的代码中我们得到了下面的图:</p>
<div><img src="img/042a646b-2906-493d-b800-a0950d583bbe.png" width="944" height="660" class="calibre143"/></div>
<p class="mce-root"><kbd class="calibre16">AgglomerativeClustering</kbd>在本例中的表现类似于 k-means，只是略有不同。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">无监督学习的简单自动化</h1>
                
            
            
                
<p class="mce-root">您应该自动化整个发现过程，以使用不同的超参数设置来尝试不同的聚类算法。下面的代码将向您展示一种简单的方法:</p>
<pre class="calibre21"># You will create a list of algorithms to test<br class="calibre2"/>from sklearn.cluster import MeanShift, estimate_bandwidth, SpectralClustering<br class="calibre2"/>from hdbscan import HDBSCAN<br class="calibre2"/><br class="calibre2"/># bandwidth estimate for MeanShift algorithm to work properly<br class="calibre2"/>bandwidth = estimate_bandwidth(X, quantile=0.3, n_samples=100)<br class="calibre2"/><br class="calibre2"/>estimators = [{'estimator': KMeans, 'args': (), 'kwargs': {'n_clusters': 5}},<br class="calibre2"/>                         {'estimator': DBSCAN, 'args': (), 'kwargs': {'eps': 0.5}},<br class="calibre2"/>                         {'estimator': AgglomerativeClustering, 'args': (), 'kwargs': {'n_clusters': 5, 'linkage': 'ward'}},<br class="calibre2"/>                         {'estimator': MeanShift, 'args': (), 'kwargs': {'cluster_all': False, "bandwidth": bandwidth, "bin_seeding": True}},<br class="calibre2"/>                         {'estimator': SpectralClustering, 'args': (), 'kwargs': {'n_clusters':5}},<br class="calibre2"/>                         {'estimator': HDBSCAN, 'args': (), 'kwargs': {'min_cluster_size':15}}]<br class="calibre2"/><br class="calibre2"/><br class="calibre2"/>unsupervised_learner = Unsupervised_AutoML(estimators)<br class="calibre2"/><br class="calibre2"/>predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)</pre>
<p class="mce-root">您将在控制台中看到以下指标:</p>
<pre class="calibre21">################## KMeans metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.592<br class="calibre2"/>  Estimated number of clusters: 5.000<br class="calibre2"/>  Homogeneity: 0.881<br class="calibre2"/>  Completeness: 0.882<br class="calibre2"/>  V-measure: 0.882<br class="calibre2"/>  Adjusted Rand Index: 0.886<br class="calibre2"/>  Adjusted Mutual Information: 0.881<br class="calibre2"/><br class="calibre2"/>################## DBSCAN metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.417<br class="calibre2"/>  Estimated number of clusters: 5.000<br class="calibre2"/>  ...<br class="calibre2"/>################## AgglomerativeClustering metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.581<br class="calibre2"/>  Estimated number of clusters: 5.000<br class="calibre2"/>  ...<br class="calibre2"/>################## MeanShift metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.472<br class="calibre2"/>  Estimated number of clusters: 3.000<br class="calibre2"/>  ...<br class="calibre2"/>################## SpectralClustering metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.420<br class="calibre2"/>  Estimated number of clusters: 5.000<br class="calibre2"/>  ...<br class="calibre2"/>################## HDBSCAN metrics #####################<br class="calibre2"/>  Silhouette Coefficient: 0.468<br class="calibre2"/>  Estimated number of clusters: 6.000<br class="calibre2"/>  ...</pre>
<p class="mce-root">您可以稍后打印标签和指标，因为每个算法都有一个标签和指标:</p>
<pre class="calibre21">pprint(predictions)<br class="calibre2"/>[{'KMeans': array([3, 1, 4, ..., 0, 1, 2], dtype=int32)},<br class="calibre2"/> {'DBSCAN': array([ 0, 0, 0, ..., 2, -1, 1])},<br class="calibre2"/> {'AgglomerativeClustering': array([2, 4, 0, ..., 3, 2, 1])},<br class="calibre2"/> {'MeanShift': array([0, 0, 0, ..., 1, 0, 1])},<br class="calibre2"/> {'SpectralClustering': array([4, 2, 1, ..., 0, 1, 3], dtype=int32)},<br class="calibre2"/> {'HDBSCAN': array([ 4, 2, 3, ..., 1, -1, 0])}]<br class="calibre2"/><br class="calibre2"/>pprint(performance_metrics)<br class="calibre2"/>{'AgglomerativeClustering': {'Adjusted Mutual Information': 0.8989601162598674,<br class="calibre2"/>                             'Adjusted Rand Index': 0.9074196173180163,<br class="calibre2"/>                             ...},<br class="calibre2"/> 'DBSCAN': {'Adjusted Mutual Information': 0.5694008711591612,<br class="calibre2"/>            'Adjusted Rand Index': 0.4685215791890368,<br class="calibre2"/>            ...},<br class="calibre2"/> 'HDBSCAN': {'Adjusted Mutual Information': 0.7857262723310214,<br class="calibre2"/>             'Adjusted Rand Index': 0.7907512089039799,<br class="calibre2"/>             ...},<br class="calibre2"/> 'KMeans': {'Adjusted Mutual Information': 0.8806038790635883,<br class="calibre2"/>            'Adjusted Rand Index': 0.8862210038915361,<br class="calibre2"/>            ...},<br class="calibre2"/> 'MeanShift': {'Adjusted Mutual Information': 0.45701704058584275,<br class="calibre2"/>               'Adjusted Rand Index': 0.4043364504640998,<br class="calibre2"/>               ...},<br class="calibre2"/> 'SpectralClustering': {'Adjusted Mutual Information': 0.7628653432724043,<br class="calibre2"/>                        'Adjusted Rand Index': 0.7111907598912597,<br class="calibre2"/>                        ...}}</pre>
<p class="mce-root">您可以使用<kbd class="calibre16">plot_clusters</kbd>方法以同样的方式可视化预测。让我们编写另一个类方法，它将为您在实验中使用的所有估计量绘制聚类图:</p>
<pre class="calibre21">def plot_all_clusters(self, estimators, labels, X, plot_kwargs):<br class="calibre2"/><br class="calibre2"/>    fig = plt.figure()<br class="calibre2"/><br class="calibre2"/>    for i, algorithm in enumerate(labels):<br class="calibre2"/><br class="calibre2"/>        quotinent = np.divide(len(estimators), 2)<br class="calibre2"/><br class="calibre2"/>        # Simple logic to decide row and column size of the figure<br class="calibre2"/>        if isinstance(quotinent, int):<br class="calibre2"/>            dim_1 = 2<br class="calibre2"/>            dim_2 = quotinent<br class="calibre2"/>        else:<br class="calibre2"/>            dim_1 = np.ceil(quotinent)<br class="calibre2"/>            dim_2 = 3<br class="calibre2"/><br class="calibre2"/>        palette = sns.color_palette('deep',<br class="calibre2"/>                                    np.unique(algorithm[estimators[i]['estimator'].__name__]).max() + 1)<br class="calibre2"/>        colors = [palette[x] if x &gt;= 0 else (0.0, 0.0, 0.0) for x in<br class="calibre2"/>                  algorithm[estimators[i]['estimator'].__name__]]<br class="calibre2"/><br class="calibre2"/>        plt.subplot(dim_1, dim_2, i + 1)<br class="calibre2"/>        plt.scatter(X[:, 0], X[:, 1], c=colors, **plot_kwargs)<br class="calibre2"/>        plt.title('{} Clusters'.format(str(estimators[i]['estimator'].__name__)), fontsize=8)<br class="calibre2"/><br class="calibre2"/>    plt.show()</pre>
<p class="mce-root">让我们来看看用法:</p>
<pre class="calibre21">plot_kwargs = {'s': 12, 'linewidths': 0.1}<br class="calibre2"/>unsupervised_learner.plot_all_clusters(estimators, unsupervised_learner.predictions, X, plot_kwargs)</pre>
<p class="mce-root">我们从前面的代码块中得到下面的图:</p>
<div><img src="img/91bb84a1-d4cd-4f61-bb24-477861ed7511.png" width="944" height="460" class="calibre144"/></div>
<p>第一行，从左开始:KMeans、DBSCAN、AgglomerativeClustering</p>
<p>底部一行，从左开始:MeanShift，SpectralClustering，HDBSCAN</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">可视化高维数据集</h1>
                
            
            
                
<p class="mce-root">视觉检查超过三维的数据集怎么样？为了直观地检查您的数据集，您最多需要三个维度；如果没有，就需要使用特定的方法来降维。这通常通过应用<strong class="calibre7">主成分分析</strong> ( <strong class="calibre7"> PCA </strong>)或 t-SNE 算法来实现。</p>
<p class="mce-root">下面的代码将加载在 ML 教程中常用的<kbd class="calibre16">Breast Cancer Wisconsin Diagnostic</kbd>数据集:</p>
<pre class="calibre21"># Wisconsin Breast Cancer Diagnostic Dataset<br class="calibre2"/>from sklearn.datasets import load_breast_cancer<br class="calibre2"/>import pandas as pd<br class="calibre2"/><br class="calibre2"/>data = load_breast_cancer()<br class="calibre2"/>X = data.data<br class="calibre2"/><br class="calibre2"/>df = pd.DataFrame(data.data, columns=data.feature_names)<br class="calibre2"/>df.head()</pre>
<p class="mce-root">控制台中的输出如下:</p>
<pre class="calibre21"><strong class="calibre3">   mean radius mean texture mean perimeter mean area mean smoothness \</strong><br class="calibre2"/><strong class="calibre3">0 17.99 10.38 122.80 1001.0 0.11840 </strong><br class="calibre2"/><strong class="calibre3">1 20.57 17.77 132.90 1326.0 0.08474 </strong><br class="calibre2"/><strong class="calibre3">2 19.69 21.25 130.00 1203.0 0.10960 </strong><br class="calibre2"/><strong class="calibre3">3 11.42 20.38 77.58 386.1 0.14250 </strong><br class="calibre2"/><strong class="calibre3">4 20.29 14.34 135.10 1297.0 0.10030 </strong><br class="calibre2"/><br class="calibre2"/><strong class="calibre3">...</strong><br class="calibre2"/><br class="calibre2"/><strong class="calibre3">   mean fractal dimension ... worst radius \</strong><br class="calibre2"/><strong class="calibre3">0 0.07871 ... 25.38 </strong><br class="calibre2"/><strong class="calibre3">1 0.05667 ... 24.99 </strong><br class="calibre2"/><strong class="calibre3">2 0.05999 ... 23.57 </strong><br class="calibre2"/><strong class="calibre3">3 0.09744 ... 14.91 </strong><br class="calibre2"/><strong class="calibre3">4 0.05883 ... 22.54 </strong><br class="calibre2"/><br class="calibre2"/><strong class="calibre3">...</strong><br class="calibre2"/><br class="calibre2"/><strong class="calibre3">   worst fractal dimension </strong><br class="calibre2"/><strong class="calibre3">0 0.11890 </strong><br class="calibre2"/><strong class="calibre3">1 0.08902 </strong><br class="calibre2"/><strong class="calibre3">2 0.08758 </strong><br class="calibre2"/><strong class="calibre3">3 0.17300 </strong><br class="calibre2"/><strong class="calibre3">4 0.07678</strong> </pre>
<p class="mce-root">你有 30 种不同的特征可以用来理解给定患者肿瘤的不同特征。</p>
<p class="mce-root"><kbd class="calibre16">df.describe()</kbd>将向您显示每个功能的描述性统计数据:</p>
<pre class="calibre21"><br class="calibre2"/>df.describe()<br class="calibre2"/><br class="calibre2"/>       mean radius mean texture mean perimeter mean area \<br class="calibre2"/>count 569.000000 569.000000 569.000000 569.000000 <br class="calibre2"/>mean 14.127292 19.289649 91.969033 654.889104 <br class="calibre2"/>std 3.524049 4.301036 24.298981 351.914129 <br class="calibre2"/>min 6.981000 9.710000 43.790000 143.500000 <br class="calibre2"/>25% 11.700000 16.170000 75.170000 420.300000 <br class="calibre2"/>50% 13.370000 18.840000 86.240000 551.100000 <br class="calibre2"/>75% 15.780000 21.800000 104.100000 782.700000 <br class="calibre2"/>max 28.110000 39.280000 188.500000 2501.000000 <br class="calibre2"/><br class="calibre2"/>...<br class="calibre2"/><br class="calibre2"/>       mean symmetry mean fractal dimension ... \<br class="calibre2"/>count 569.000000 569.000000 ... <br class="calibre2"/>mean 0.181162 0.062798 ... <br class="calibre2"/>std 0.027414 0.007060 ... <br class="calibre2"/>min 0.106000 0.049960 ... <br class="calibre2"/>25% 0.161900 0.057700 ... <br class="calibre2"/>50% 0.179200 0.061540 ... <br class="calibre2"/>75% 0.195700 0.066120 ... <br class="calibre2"/>max 0.304000 0.097440 ... <br class="calibre2"/><br class="calibre2"/>...<br class="calibre2"/><br class="calibre2"/>       worst concave points worst symmetry worst fractal dimension <br class="calibre2"/>count 569.000000 569.000000 569.000000 <br class="calibre2"/>mean 0.114606 0.290076 0.083946 <br class="calibre2"/>std 0.065732 0.061867 0.018061 <br class="calibre2"/>min 0.000000 0.156500 0.055040 <br class="calibre2"/>25% 0.064930 0.250400 0.071460 <br class="calibre2"/>50% 0.099930 0.282200 0.080040 <br class="calibre2"/>75% 0.161400 0.317900 0.092080 <br class="calibre2"/>max 0.291000 0.663800 0.207500 <br class="calibre2"/>[8 rows x 30 columns]</pre>
<p class="mce-root">让我们看看缩放前后的结果。下面的代码片段将使 PCA 适合原始数据。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">主成分分析在行动中</h1>
                
            
            
                
<p class="mce-root">以下代码块向您展示了如何对两个组件应用 PCA 并可视化结果:</p>
<pre class="calibre21"># PCA<br class="calibre2"/>from sklearn.decomposition import PCA<br class="calibre2"/><br class="calibre2"/>pca = PCA(n_components=2, whiten=True)<br class="calibre2"/>pca = pca.fit_transform(df)<br class="calibre2"/><br class="calibre2"/>plt.scatter(pca[:, 0], pca[:, 1], c=data.target, cmap="RdBu_r", edgecolor="Red", alpha=0.35)<br class="calibre2"/>plt.colorbar()<br class="calibre2"/>plt.title('PCA, n_components=2')<br class="calibre2"/>plt.show()</pre>
<p class="mce-root">从前面的代码中我们得到了下面的图:</p>
<div><img src="img/e35afdf5-6536-4a7f-be4b-0bbde1b7177c.png" width="944" height="727" class="calibre145"/></div>
<p>PCA 图，n_components=2</p>
<p class="mce-root">在这里，你可以看到红色等级(深灰色)非常集中在一个特定的区域，很难区分等级。方差的差异会扭曲我们的观点，缩放可以帮助我们:</p>
<pre class="calibre21"># Preprocess data.<br class="calibre2"/>scaler = StandardScaler()<br class="calibre2"/>scaler.fit(df)<br class="calibre2"/>preprocessed_data = scaler.transform(df)<br class="calibre2"/>scaled_features_df = pd.DataFrame(preprocessed_data, index=df.index, columns=df.columns)</pre>
<p class="mce-root">应用<kbd class="calibre16">StandardScaler</kbd>预处理数据后，数据集有单位方差:</p>
<pre class="calibre21">scaled_features_df.describe()<br class="calibre2"/><br class="calibre2"/>        mean radius mean texture mean perimeter mean area \<br class="calibre2"/>count 5.690000e+02 5.690000e+02 5.690000e+02 5.690000e+02 <br class="calibre2"/>mean -3.162867e-15 -6.530609e-15 -7.078891e-16 -8.799835e-16 <br class="calibre2"/>std 1.000880e+00 1.000880e+00 1.000880e+00 1.000880e+00 <br class="calibre2"/>min -2.029648e+00 -2.229249e+00 -1.984504e+00 -1.454443e+00 <br class="calibre2"/>25% -6.893853e-01 -7.259631e-01 -6.919555e-01 -6.671955e-01 <br class="calibre2"/>50% -2.150816e-01 -1.046362e-01 -2.359800e-01 -2.951869e-01 <br class="calibre2"/>75% 4.693926e-01 5.841756e-01 4.996769e-01 3.635073e-01 <br class="calibre2"/>max 3.971288e+00 4.651889e+00 3.976130e+00 5.250529e+00 <br class="calibre2"/><br class="calibre2"/>...<br class="calibre2"/> <br class="calibre2"/>       mean symmetry mean fractal dimension ... \<br class="calibre2"/>count 5.690000e+02 5.690000e+02 ... <br class="calibre2"/>mean -1.971670e-15 -1.453631e-15 ... <br class="calibre2"/>std 1.000880e+00 1.000880e+00 ... <br class="calibre2"/>min -2.744117e+00 -1.819865e+00 ... <br class="calibre2"/>25% -7.032397e-01 -7.226392e-01 ... <br class="calibre2"/>50% -7.162650e-02 -1.782793e-01 ... <br class="calibre2"/>75% 5.307792e-01 4.709834e-01 ... <br class="calibre2"/>max 4.484751e+00 4.910919e+00 ... <br class="calibre2"/><br class="calibre2"/>...<br class="calibre2"/><br class="calibre2"/>       worst concave points worst symmetry worst fractal dimension <br class="calibre2"/>count 5.690000e+02 5.690000e+02 5.690000e+02 <br class="calibre2"/>mean -1.412656e-16 -2.289567e-15 2.575171e-15 <br class="calibre2"/>std 1.000880e+00 1.000880e+00 1.000880e+00 <br class="calibre2"/>min -1.745063e+00 -2.160960e+00 -1.601839e+00 <br class="calibre2"/>25% -7.563999e-01 -6.418637e-01 -6.919118e-01 <br class="calibre2"/>50% -2.234689e-01 -1.274095e-01 -2.164441e-01 <br class="calibre2"/>75% 7.125100e-01 4.501382e-01 4.507624e-01 <br class="calibre2"/>max 2.685877e+00 6.046041e+00 6.846856e+00 <br class="calibre2"/>[8 rows x 30 columns]</pre>
<p class="mce-root">应用 PCA 来查看前两个主成分是否足以分离标签:</p>
<pre class="calibre21"># PCA<br class="calibre2"/>from sklearn.decomposition import PCA<br class="calibre2"/><br class="calibre2"/>pca = PCA(n_components=2, whiten=True)<br class="calibre2"/>pca = pca.fit_transform(scaled_features_df)<br class="calibre2"/><br class="calibre2"/>plt.scatter(pca[:, 0], pca[:, 1], c=data.target, cmap="RdBu_r", edgecolor="Red", alpha=0.35)<br class="calibre2"/>plt.colorbar()<br class="calibre2"/>plt.title('PCA, n_components=2')<br class="calibre2"/>plt.show()</pre>
<p class="mce-root">我们从前面的代码中得到以下输出:</p>
<div><img src="img/6d0e6efc-0ed8-432b-9e47-15cd3b0dab5d.png" width="944" height="713" class="calibre146"/></div>
<p>PCA，n_components=2，缩放后</p>
<p class="mce-root">这似乎很有趣，因为带有不同标签的例子大多使用前两个主成分来分离。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">t-SNE 在行动</h1>
                
            
            
                
<p class="mce-root">你也可以尝试 t-SNE 来可视化高维数据。首先，<kbd class="calibre16">TSNE</kbd>将应用于原始数据:</p>
<pre class="calibre21"># TSNE<br class="calibre2"/>from sklearn.manifold import TSNE<br class="calibre2"/><br class="calibre2"/>tsne = TSNE(verbose=1, perplexity=40, n_iter=4000)<br class="calibre2"/>tsne = tsne.fit_transform(df)</pre>
<p class="mce-root">控制台中的输出如下:</p>
<pre class="calibre21">[t-SNE] Computing 121 nearest neighbors...<br class="calibre2"/>[t-SNE] Indexed 569 samples in 0.000s...<br class="calibre2"/>[t-SNE] Computed neighbors for 569 samples in 0.010s...<br class="calibre2"/>[t-SNE] Computed conditional probabilities for sample 569 / 569<br class="calibre2"/>[t-SNE] Mean sigma: 33.679703<br class="calibre2"/>[t-SNE] KL divergence after 250 iterations with early exaggeration: 48.886528<br class="calibre2"/>[t-SNE] Error after 1600 iterations: 0.210506</pre>
<p class="mce-root">绘制结果如下:</p>
<pre class="calibre21">plt.scatter(tsne[:, 0], tsne[:, 1], c=data.target, cmap="winter", edgecolor="None", alpha=0.35)<br class="calibre2"/>plt.colorbar()<br class="calibre2"/>plt.title('t-SNE')<br class="calibre2"/>plt.show()</pre>
<p class="mce-root">我们从前面的代码中得到以下输出:</p>
<div><img src="img/fd0133e2-c27f-4964-b7ec-bea65b5c6552.png" width="944" height="754" class="calibre147"/></div>
<p>TSNE 的情节</p>
<p class="mce-root">将<kbd class="calibre16">TSNE</kbd>应用于缩放数据如下:</p>
<pre class="calibre21">tsne = TSNE(verbose=1, perplexity=40, n_iter=4000)<br class="calibre2"/>tsne = tsne.fit_transform(scaled_features_df)</pre>
<p class="mce-root">控制台中的输出如下:</p>
<pre class="calibre21">[t-SNE] Computing 121 nearest neighbors...<br class="calibre2"/>[t-SNE] Indexed 569 samples in 0.001s...<br class="calibre2"/>[t-SNE] Computed neighbors for 569 samples in 0.018s...<br class="calibre2"/>[t-SNE] Computed conditional probabilities for sample 569 / 569<br class="calibre2"/>[t-SNE] Mean sigma: 1.522404<br class="calibre2"/>[t-SNE] KL divergence after 250 iterations with early exaggeration: 66.959343<br class="calibre2"/>[t-SNE] Error after 1700 iterations: 0.875110</pre>
<p class="mce-root">绘制结果如下:</p>
<pre class="calibre21">plt.scatter(tsne[:, 0], tsne[:, 1], c=data.target, cmap="winter", edgecolor="None", alpha=0.35)<br class="calibre2"/>plt.colorbar()<br class="calibre2"/>plt.title('t-SNE')<br class="calibre2"/>plt.show()</pre>
<p class="mce-root">我们从前面的代码中得到以下输出:</p>
<div><img src="img/5bb644e3-ff59-46fc-8783-ccec1c62f0ba.png" width="944" height="736" class="calibre148"/></div>
<p>缩放后的 TSNE</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">将简单的组件添加到一起以改善管道</h1>
                
            
            
                
<p class="mce-root">让我们对<kbd class="calibre16">fit_predict</kbd>方法做一些调整，在您的管道中加入一个分解器，这样您就可以在必要时可视化高维数据:</p>
<pre class="calibre21">def fit_predict(self, X, y=None, scaler=True, decomposer={'name': PCA, 'args':[], 'kwargs': {'n_components': 2}}):<br class="calibre2"/>    """<br class="calibre2"/>    fit_predict will train given estimator(s) and predict cluster membership for each sample<br class="calibre2"/>    """<br class="calibre2"/><br class="calibre2"/>    shape = X.shape<br class="calibre2"/>    df_type = isinstance(X, pd.core.frame.DataFrame)<br class="calibre2"/><br class="calibre2"/>    if df_type:<br class="calibre2"/>        column_names = X.columns<br class="calibre2"/>        index = X.index<br class="calibre2"/><br class="calibre2"/>    if scaler == True:<br class="calibre2"/>        from sklearn.preprocessing import StandardScaler<br class="calibre2"/>        scaler = StandardScaler()<br class="calibre2"/>        X = scaler.fit_transform(X)<br class="calibre2"/><br class="calibre2"/>        if df_type:<br class="calibre2"/>            X = pd.DataFrame(X, index=index, columns=column_names)<br class="calibre2"/><br class="calibre2"/>    if decomposer is not None:<br class="calibre2"/>        X = decomposer['name'](*decomposer['args'], **decomposer['kwargs']).fit_transform(X)<br class="calibre2"/><br class="calibre2"/>        if df_type:<br class="calibre2"/>            if decomposer['name'].__name__ == 'PCA':<br class="calibre2"/>                X = pd.DataFrame(X, index=index, columns=['component_' + str(i + 1) for i in<br class="calibre2"/>                                                          range(decomposer['kwargs']['n_components'])])<br class="calibre2"/>            else:<br class="calibre2"/>                X = pd.DataFrame(X, index=index, columns=['component_1', 'component_2'])<br class="calibre2"/><br class="calibre2"/>        # if dimensionality reduction is applied, then n_components will be set accordingly in hyperparameter configuration<br class="calibre2"/>        for estimator in self.estimators:<br class="calibre2"/>            if 'n_clusters' in estimator['kwargs'].keys():<br class="calibre2"/>                if decomposer['name'].__name__ == 'PCA':<br class="calibre2"/>                    estimator['kwargs']['n_clusters'] = decomposer['kwargs']['n_components']<br class="calibre2"/>                else:<br class="calibre2"/>                    estimator['kwargs']['n_clusters'] = 2<br class="calibre2"/><br class="calibre2"/>    # This dictionary will hold predictions for each estimator<br class="calibre2"/>    predictions = []<br class="calibre2"/>    performance_metrics = {}<br class="calibre2"/><br class="calibre2"/>    for estimator in self.estimators:<br class="calibre2"/>        labels = estimator['estimator'](*estimator['args'], **estimator['kwargs']).fit_predict(X)<br class="calibre2"/>        estimator['estimator'].n_clusters_ = len(np.unique(labels))<br class="calibre2"/>        metrics = self._get_cluster_metrics(estimator['estimator'].__name__, estimator['estimator'].n_clusters_, X, labels, y)<br class="calibre2"/>        predictions.append({estimator['estimator'].__name__: labels})<br class="calibre2"/>        performance_metrics[estimator['estimator'].__name__] = metrics<br class="calibre2"/><br class="calibre2"/>    self.predictions = predictions<br class="calibre2"/>    self.performance_metrics = performance_metrics<br class="calibre2"/><br class="calibre2"/>    return predictions, performance_metrics</pre>
<p class="mce-root">现在，您可以将<kbd class="calibre16">fit_predict</kbd>应用到您的数据集。下面的代码块显示了一个用法示例:</p>
<pre class="calibre21">from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift, estimate_bandwidth, SpectralClustering<br class="calibre2"/>from hdbscan import HDBSCAN<br class="calibre2"/><br class="calibre2"/>from sklearn.datasets import load_breast_cancer<br class="calibre2"/><br class="calibre2"/>data = load_breast_cancer()<br class="calibre2"/>X = data.data<br class="calibre2"/>y = data.target<br class="calibre2"/><br class="calibre2"/># Necessary for bandwidth<br class="calibre2"/>bandwidth = estimate_bandwidth(X, quantile=0.1, n_samples=100)<br class="calibre2"/><br class="calibre2"/>estimators = [{'estimator': KMeans, 'args': (), 'kwargs': {'n_clusters': 5}},<br class="calibre2"/>                         {'estimator': DBSCAN, 'args': (), 'kwargs': {'eps': 0.3}},<br class="calibre2"/>                         {'estimator': AgglomerativeClustering, 'args': (), 'kwargs': {'n_clusters': 5, 'linkage': 'ward'}},<br class="calibre2"/>                         {'estimator': MeanShift, 'args': (), 'kwargs': {'cluster_all': False, "bandwidth": bandwidth, "bin_seeding": True}},<br class="calibre2"/>                         {'estimator': SpectralClustering, 'args': (), 'kwargs': {'n_clusters':5}},<br class="calibre2"/>                         {'estimator': HDBSCAN, 'args': (), 'kwargs': {'min_cluster_size':15}}]<br class="calibre2"/><br class="calibre2"/>unsupervised_learner = Unsupervised_AutoML(estimators)<br class="calibre2"/><br class="calibre2"/>predictions, performance_metrics = unsupervised_learner.fit_predict(X, y, decomposer=None)</pre>
<p class="mce-root">自动无监督学习是一个高度实验性的过程，尤其是如果你不太了解你的数据。作为练习，您可以扩展<kbd class="calibre16">Unsupervised_AutoML</kbd>类，尝试为每个算法设置多个超参数，并可视化结果。</p>


            

            
        
    </div>



  




  
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  <div><h1 class="header-title">摘要</h1>
                
            
            
                
<p class="mce-root">在本章中，您了解了为给定问题选择合适的 ML 管道的许多不同方面。</p>
<p class="mce-root">计算复杂性、训练和评分时间的差异、线性与非线性、算法、特定的特征变换都是有效的考虑因素，从这些角度来看您的数据非常有用。</p>
<p class="mce-root">通过实践各种用例，您对选择合适的模型以及机器学习管道如何工作有了更好的理解。你已经开始触及表面，这一章是扩展这些技能的良好起点。</p>
<p class="mce-root">在下一章中，您将学习优化超参数，并将了解更高级的概念，如基于贝叶斯的超参数优化。</p>


            

            
        
    </div>



  

</body></html>