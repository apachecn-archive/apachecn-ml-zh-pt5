<html><head/><body>

    

        <title>Cluster Analysis</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">聚类分析</h1>

                

            

            

                

<p>“快给我拿一大杯酒来，我好润润脑子，说些聪明的话。”</p>

<p>——阿里斯托芬，雅典剧作家</p>

<p>在前面的章节中，我们着重于尝试学习最佳算法来解决一个结果或响应，例如，客户满意度或房价。在所有这些情况下，我们有<em> y，</em>并且<em> y </em>是<em> x，</em>或<em> y = f(x) </em>的函数。在我们的数据中，我们有实际的<em> y </em>值，我们可以相应地训练<em> x </em>。这被称为<strong>监督学习</strong>。然而，在许多情况下，我们试图从我们的数据中学习一些东西，或者我们没有<em> y，</em>或者我们实际上选择忽略它。如果是这样，我们就进入了<strong>无监督学习</strong>的世界。在这个世界里，我们构建和选择我们的算法是基于它如何很好地满足我们的业务需求，以及它有多准确。</p>

<p>为什么我们要在没有监督的情况下学习呢？首先，无监督学习可以帮助你理解和识别数据中的模式，这些模式可能是有价值的。第二，你可以用它来转换你的数据，以提高你的监督学习技术。</p>

<p>本章将集中讨论前者，下一章将讨论后者。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p>所以，让我们从一个流行且强大的技术开始，这个技术叫做<strong>聚类分析</strong>。使用聚类分析的目的是将观察结果分成若干组(k 组)，其中组内的成员尽可能相似，而组间的成员尽可能不同。有很多例子可以说明这是如何帮助一个组织的；以下是几个例子:</p>

<ul>

<li>创建客户类型或细分市场</li>

<li>地理中高犯罪率区域的检测</li>

<li>图像和面部识别</li>

<li>基因测序和转录</li>

<li>石油和地质勘探</li>

</ul>

<p>聚类分析有很多用途，但也有很多技术。我们将重点介绍两种最常见的:<strong>等级式</strong>和<strong> k-means </strong>。它们都是有效的聚类方法，但并不总是适用于您可能需要分析的大量不同的数据集。因此，我们还将使用一个基于<strong>高尔的</strong>度量相异矩阵作为输入，来检查围绕 medoids  ( <strong> PAM </strong>)的<strong>划分。最后，我们将检查我最近学习和应用的一种新方法，使用<strong>随机森林</strong>来转换您的数据。然后，转换后的数据可以用作无监督学习的输入。</strong></p>

<p>继续之前的最后一个评论:你可能会被问到这些技术是否更像艺术而不是科学，因为学习是无人监督的。我认为明确的答案是，<em>这取决于</em>。2016 年初，我在印第安纳州印第安纳波利斯 R-User Group 的一次会议上介绍了这些方法。对于一个人来说，我们都同意，是分析师和业务用户的判断使无监督学习变得有意义，并决定了你在最终的算法中是有三个还是四个集群。这句话很好地总结了这一点:</p>

<p>“主要的障碍是在不考虑上下文的情况下很难评估聚类算法:用户首先为什么要对他的数据进行聚类，然后他想对聚类做什么？我们认为，聚类不应被视为一个与应用无关的数学问题，而应始终在其最终用途的背景下进行研究。”</p>

<p>- Luxburg 等人(2012) <br/> <br/></p><p> </p>



<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root">以下是我们将在本章中涉及的主题:</p>

<ul>

<li>分层聚类</li>

<li>k 均值聚类</li>

<li>高尔和帕姆</li>

<li>随机森林</li>

<li>数据集背景</li>

<li>数据理解和准备</li>

<li>建模</li>

</ul>





            



            

        

    






    

        <title>Hierarchical clustering</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">分层聚类</h1>

                

            

            

                

<p>分层聚类算法基于观测值之间的相异度。我们将使用的一个常用度量是<strong>欧几里德距离</strong>。其他距离测量也是可用的。</p>

<div><strong>Hierarchical clustering</strong> is an agglomerative or bottom-up technique. By this, we mean that all observations are their own cluster. From there, the algorithm proceeds iteratively by searching all the pairwise points and finding the two clusters that are the most similar. So, after the first iteration, there are <em>n-1</em> clusters, and after the second iteration, there are <em>n-2</em> clusters, and so forth.</div>

<p class="mce-root">随着迭代的继续，理解除了距离度量之外，我们需要指定观察组之间的联系是很重要的。不同类型的数据将要求您使用不同的集群链接。当您试验这些联系时，您可能会发现有些联系会在一个或多个集群中产生高度不平衡的观察值。例如，如果您有 30 个观察值，一种技术可能只创建一个观察值的分类，而不管您指定了多少个总分类。在这种情况下，可能需要您的判断来选择最合适的链接，因为它与数据和业务案例相关。</p>

<p>下表列出了常见的链接类型，但请注意还有其他类型:</p>

<table border="1" style="border-collapse: collapse;width: 100%">

<tbody>

<tr>

<td>

<p><strong>联动</strong></p>

</td>

<td>

<p><strong>描述</strong></p>

</td>

</tr>

<tr>

<td>

<p>病房</p>

</td>

<td>

<p>这将最小化总的聚类内方差，该方差由从聚类点到其质心的误差平方和来测量。</p>

</td>

</tr>

<tr>

<td>

<p>完成</p>

</td>

<td>

<p>两个聚类之间的距离是一个聚类中的观测值与另一个聚类中的观测值之间的最大距离。</p>

</td>

</tr>

<tr>

<td>

<p>单一的</p>

</td>

<td>

<p>两个聚类之间的距离是一个聚类中的观测值与另一个聚类中的观测值之间的最小距离。</p>

</td>

</tr>

<tr>

<td>

<p>平均的</p>

</td>

<td>

<p>两个聚类之间的距离是一个聚类中的观测值和另一个聚类中的观测值之间的平均距离。</p>

</td>

</tr>

<tr>

<td>

<p>图心</p>

</td>

<td>

<p class="mce-root">两个聚类之间的距离是聚类质心之间的距离。</p>

</td>

</tr>

</tbody>

</table>

<p> </p>

<p class="packt_figure">分层聚类的输出将是一个<strong>树状图</strong>，这是一个树状图表，显示了各种聚类的排列。</p>

<p>正如我们将会看到的，在选择群集数量时，通常很难确定一个明确的断点。同样，您的决策在本质上应该是迭代的，并且关注于业务决策的上下文。</p>





            



            

        

    






    

        <title>Distance calculations</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">距离计算</h1>

                

            

            

                

<p>如前所述，欧氏距离通常用于构建层次聚类的输入。让我们看一个简单的例子，如何用两个观察值和两个变量/特征来计算它。</p>

<p>假设一个观察值<em>一个</em>值 5.00 美元，重 3 磅。此外，观察<em> B </em>的价格为 3.00 美元，重量为 5 磅。我们可以将这些值放在距离公式中:<em>A 和 B 之间的距离等于平方差之和的平方根</em>，在我们的示例中如下所示:</p>

<div><em>d(A, B) = square root((5 - 3)<sup>2</sup> + (3 - 5)<sup>2</sup>)</em>, which is equal to <em>2.83</em></div>

<p><em> 2.83 </em>的值本身不是有意义的值，但是在其他成对距离的上下文中是重要的。该计算是<kbd>dist()</kbd>函数在 R 中的默认值。您可以在函数中指定其他距离计算(最大值、曼哈顿、堪培拉、二进制和闵可夫斯基)。我们将避免详细讨论为什么或在哪里选择这些而不是欧几里德距离。这可能变得相当特定于领域；例如，欧几里德距离可能不适用的情况是您的数据受到高维度的影响，例如在基因组研究中。这将需要您的领域知识和/或反复试验来确定合适的距离度量。</p>

<p>最后一点要注意的是，用平均值 0 和标准偏差 1 来缩放数据，以便距离计算具有可比性。否则，任何具有较大规模的变量都会对距离产生较大的影响。</p>

<p class="mce-root"/>

<p class="mce-root"/>





            



            

        

    






    

        <title>K-means clustering</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">k 均值聚类</h1>

                

            

            

                

<p>使用 k-means，我们将需要指定我们想要的确切的聚类数。然后，该算法将迭代，直到每个观察值仅属于 k 个聚类中的一个。该算法的目标是最小化由平方欧几里德距离定义的类内变化。因此，第 k 个聚类变化是所有成对观测值的平方欧几里得距离之和除以该聚类中的观测值数量。</p>

<p>由于所涉及的迭代过程，一个 k-means 结果可能与另一个结果相差很大，即使您指定了相同数量的分类。让我们看看这个算法是如何运行的:</p>

<ol>

<li><strong>指定</strong>您想要的集群的确切数量(k)</li>

<li><strong>初始化:</strong>随机选取 k 个观测值作为初始<em>均值</em></li>

<li><strong>迭代</strong>:<ul>

<li>通过将每个观察值分配到其最近的聚类中心(最小化类内平方和)来创建 k 个聚类</li>

<li>每个聚类的质心成为新的<em>均值</em></li>

<li>重复这一过程，直到收敛，也就是说，群集质心不变</li>

</ul>

</li>

</ol>

<p>如您所见，最终结果会因步骤 1 中的初始赋值而有所不同。因此，运行多次初始启动并让软件确定最佳解决方案非常重要。在 R 中，这可能是一个简单的过程，我们将会看到。</p>





            



            

        

    






    

        <title>Gower and PAM</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">高尔和帕姆</h1>

                

            

            

                

<p>当您在现实生活中进行聚类分析时，一个很快就会变得明显的事实是，无论是分层还是 k-means 都不是专门为处理混合数据集而设计的。我所说的混合数据是指定量和定性数据，或者更具体地说，是指名义数据、序数数据和区间/比率数据。</p>

<p>您将使用的大多数数据集的实际情况是，它们可能包含混合数据。有许多方法来处理这一点，例如首先进行<strong>主成分分析</strong> ( <strong> PCA </strong>)以创建潜在变量，然后将它们用作聚类的输入或使用不同的相异度计算。我们将在下一章讨论 PCA。</p>

<p class="mce-root"/>

<p>借助 R 的强大和简单，您可以使用<strong>高尔</strong>T2【相异系数】将混合数据转换到适当的特征空间。在这种方法中，您甚至可以将因子作为输入变量。另外，代替 k-means，我推荐使用<strong> PAM 聚类算法</strong>。</p>

<p>PAM 与 k-means 非常相似，但也有一些优点。它们列举如下:</p>

<ol>

<li>首先，PAM 接受一个相异矩阵，它允许包含混合数据</li>

<li>第二，它对异常值和偏斜数据更稳健，因为它最小化了相异度的总和，而不是平方欧几里得距离的总和(Reynolds，1992)</li>

</ol>

<p>这并不是说你必须同时使用高尔和帕姆。如果你愿意的话，你可以将高尔系数用于层次结构，我已经看到了在 k-means 的上下文中支持和反对使用它的争论。此外，PAM 可以接受其他链接。然而，当配对时，它们成为处理混合数据的有效方法。在继续之前，让我们快速浏览一下这两个概念。</p>





            



            

        

    






    

        <title>Gower</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">英国威尔士南部</h1>

                

            

            

                

<p>高尔系数成对比较病例，并计算它们之间的差异，本质上是每个变量贡献的加权平均值。对于称为<em> i </em>和<em> j </em>的两种情况定义如下:</p>

<div><img src="img/206fbe85-de2a-4190-ae17-c057976f9700.png" style="width:17.67em;height:2.33em;"/></div>

<p>这里，<em>S<sub>ijk</sub>T3】是第<em>k</em>T6】第个变量提供的贡献，如果第<em>k</em>T14】第个变量有效，则<em>W<sub>ijk</sub>T11】为 1，否则<em> 0 </em>。</em></em></p>

<p>对于顺序变量和连续变量，<em>S<sub>ijk</sub>= 1-(x<sub>ij</sub>的绝对值- x <sub> ik </sub> ) / r <sub> k </sub> </em>，其中<em> r <sub> k </sub> </em>为<em> k </em> <sub> th </sub>变量的取值范围。</p>

<p>对于名义变量，<em>S<sub>ijk</sub>= 1</em>if<em>x<sub>ij</sub>= x<sub>JK</sub>T9】，否则<em> 0 </em>。</em></p>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p>对于二元变量，<em>S<sub>ijk</sub>T3】是根据属性是存在(+)还是不存在(-)来计算的，如下表所示:</em></p>

<table border="1" style="border-collapse: collapse;width: 100%">

<tbody>

<tr>

<td><strong>变量</strong></td>

<td><strong>属性值<em>k</em>T3】</strong></td>

</tr>

<tr>

<td>案例<em>一</em></td>

<td>+</td>

<td>+</td>

<td>-</td>

<td>-</td>

</tr>

<tr>

<td>案例<em> j </em></td>

<td>+</td>

<td>-</td>

<td>+</td>

<td>-</td>

</tr>

<tr>

<td>好吧</td>

<td>one</td>

<td>Zero</td>

<td>Zero</td>

<td>Zero</td>

</tr>

<tr>

<td>这是一个小区</td>

<td>one</td>

<td>one</td>

<td>one</td>

<td>Zero</td>

</tr>

</tbody>

</table>

<div>






    

        <title>PAM</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">脉冲幅度调制 （Pulse Amplitude Modulation 的缩写）</h1>

                

            

            

                

<p>对于<strong> PAM </strong>，我们先定义一个<strong> medoid </strong>。</p>

<p>medoid 是一个聚类的观测值，它最小化该聚类中其他观测值之间的不相似性(在我们的示例中，使用高尔度量计算)。因此，类似于 k-means，如果您指定五个聚类，您将有五个数据分区。</p>

<p>PAM 算法的目标是最小化所有观测值与最接近的 medoid 的不相似性，它迭代以下步骤:</p>

<ol>

<li>随机选择 k 个观测值作为初始中间值</li>

<li>将每个观测值分配给最近的中位形</li>

<li>交换每个中值和非中值观测值，计算不同的代价</li>

<li>选择使总差异最小化的配置</li>

<li>重复步骤 2 到 4，直到水母没有变化</li>

</ol>

<p>Gower 和 PAM 都可以使用 r 中的<kbd>cluster</kbd>包来调用。对于 Gower，我们将使用<kbd>daisy()</kbd>函数来计算相异矩阵，并使用<kbd>pam()</kbd>函数来进行实际分区。至此，让我们开始测试这些方法。</p>

<p class="mce-root"/>

<p class="mce-root"/>





            



            

        

    






    

        <title>Random forest</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">随机森林</h1>

                

            

            

                

<p>就像我们在处理混合的，实际上是<em>杂乱的</em>数据时使用高尔度量的动机一样，我们可以以一种无人监管的方式应用随机森林。选择这种方法有许多优点:</p>

<ul>

<li>对异常值和高度偏斜变量具有鲁棒性</li>

<li>不需要转换或缩放数据</li>

<li>处理混合数据(数字和因子)</li>

<li>可以容纳丢失的数据</li>

<li>可用于具有大量变量的数据；事实上，它可以通过检查可变的重要性来消除无用的特征</li>

<li>产生的相异度矩阵用作前面讨论的其他技术(分层、k-均值和 PAM)的输入</li>

</ul>

<p class="mce-root">几句警告的话。可能需要一些试错法来根据在每个树分裂处采样的变量数量(函数中的<kbd>mtry = ?</kbd>)和生长的树的数量适当地调整随机森林。研究表明，在一定程度上，种植的树木越多，效果越好，一个好的起点是种植 2000 棵树(施，，s，2006)。</p>

<p>给定一个没有标签的数据集，算法是这样工作的:</p>

<ul>

<li>当前观察到的数据被标记为类别 1</li>

<li>创建与观测数据大小相同的第二组(综合)观测值；这是通过对观测数据中的每个要素进行随机采样而创建的，因此，如果有 20 个观测要素，则有 20 个合成要素</li>

<li>数据的合成部分被标记为类 2，这有助于使用随机森林作为人工分类问题</li>

<li>创建一个随机森林模型来区分这两个类别</li>

<li>将仅由观察数据(合成数据现在被丢弃)构成的模型的接近度度量转换为相异度矩阵</li>

<li>利用相异度矩阵作为聚类输入特征</li>

</ul>

<p>那么这些接近度到底是什么呢？</p>

<p>邻近度量是所有观察值之间的成对度量。如果两个观察结果出现在树的同一个终端节点，则它们的邻近分数等于 1，否则为零。</p>

<p class="mce-root"/>

<p>在随机森林运行结束时，通过除以树的总数来归一化观察数据的接近度分数。所得到的 NxN 矩阵包含 0 到 1 之间的分数，当然对角线值都是 1。这就是全部了。这是一种有效的技术，我认为它没有被充分利用，我希望我几年前就学会了。</p>





            



            

        

    






    

        <title>Dataset background</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">数据集背景</h1>

                

            

            

                

<p>直到一年前，我还不知道全世界只有不到 300 名注册品酒师。这项考试由高级品酒师法庭管理，因其要求和高失败率而臭名昭著。</p>

<p>备受好评的纪录片<em> Somm </em>详细描述了几个追求认证的个人的考验、磨难和奖励。因此，在这个练习中，我们将试着帮助一个努力成为品酒师大师的假想个体找到意大利葡萄酒的潜在结构。</p>





            



            

        

    






    

        <title>Data understanding and preparation</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">数据理解和准备</h1>

                

            

            

                

<p>让我们从安装本章所需的 R 包开始，如果您还没有这样做的话:</p>

<pre>&gt; library(magrittr)<br/><br/>&gt; install.packages("cluster")<br/><br/>&gt; install.packages("dendextend")<br/><br/>&gt; install.packages("ggthemes")<br/><br/>&gt; install.packages("HDclassif") <br/><br/>&gt; install.packages("NbClust") <br/><br/>&gt; install.packages("tidyverse")<br/><br/>&gt; options(scipen=999)</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p>数据集在<kbd>HDclassif</kbd>包中。加载数据并用<kbd>str()</kbd>功能检查结构:</p>

<pre>&gt; library(HDclassif)<br/><br/>&gt; data(wine)<br/><br/>&gt; str(wine)<br/>'data.frame': 178 obs. of 14 variables:<br/> $ class: int 1 1 1 1 1 1 1 1 1 1 ...<br/> $ V1 : num 14.2 13.2 13.2 14.4 13.2 ...<br/> $ V2 : num 1.71 1.78 2.36 1.95 2.59 1.76 1.87 2.15 1.64 1.35 ...<br/> $ V3 : num 2.43 2.14 2.67 2.5 2.87 2.45 2.45 2.61 2.17 2.27 ...<br/> $ V4 : num 15.6 11.2 18.6 16.8 21 15.2 14.6 17.6 14 16 ...<br/> $ V5 : int 127 100 101 113 118 112 96 121 97 98 ...<br/> $ V6 : num 2.8 2.65 2.8 3.85 2.8 3.27 2.5 2.6 2.8 2.98 ...<br/> $ V7 : num 3.06 2.76 3.24 3.49 2.69 3.39 2.52 2.51 2.98 3.15 ...<br/> $ V8 : num 0.28 0.26 0.3 0.24 0.39 0.34 0.3 0.31 0.29 0.22 ...<br/> $ V9 : num 2.29 1.28 2.81 2.18 1.82 1.97 1.98 1.25 1.98 1.85 ...<br/> $ V10 : num 5.64 4.38 5.68 7.8 4.32 6.75 5.25 5.05 5.2 7.22 ...<br/> $ V11 : num 1.04 1.05 1.03 0.86 1.04 1.05 1.02 1.06 1.08 1.01 ...<br/> $ V12 : num 3.92 3.4 3.17 3.45 2.93 2.85 3.58 3.58 2.85 3.55 ...<br/> $ V13 : int 1065 1050 1185 1480 735 1450 1290 1295 1045 1045 ...</pre>

<p class="mce-root">该数据由 178 种葡萄酒组成，具有 13 个化学成分变量和一个变量<kbd>Class</kbd>，即栽培品种或植物品种的标签。我们不会在聚类中使用它，而是作为模型性能的测试。变量<kbd>V1</kbd>到<kbd>V13</kbd>是化学成分的测量值，如下所示:</p>

<ul>

<li><kbd>V1</kbd>:酒精</li>

<li><kbd>V2</kbd>:苹果酸</li>

<li><kbd>V3</kbd>:灰烬</li>

<li><kbd>V4</kbd>:灰分的碱度</li>

<li><kbd>V5</kbd>:镁</li>

<li><kbd>V6</kbd>:总酚</li>

<li><kbd>V7</kbd>:黄酮类化合物</li>

<li><kbd>V8</kbd>:非类黄酮酚类</li>

<li><kbd>V9</kbd>:原花色素</li>

<li><kbd>V10</kbd>:色彩强度</li>

<li><kbd>V11</kbd>:色调</li>

<li><kbd>V12</kbd> : OD280/OD315</li>

<li><kbd>V13</kbd>:脯氨酸</li>

</ul>

<p class="mce-root"/>

<p>变量都是定量的。我们应该将它们重命名为对我们的分析有意义的名称。使用<kbd>colnames()</kbd>功能很容易做到这一点:</p>

<pre>&gt; colnames(wine) &lt;- c(<br/>    "Class",<br/>    "Alcohol",<br/>    "MalicAcid",<br/>    "Ash",<br/>    "Alk_ash",<br/>    "magnesium",<br/>    "T_phenols",<br/>    "Flavanoids",<br/>    "Non_flav",<br/>    "Proantho",<br/>    "C_Intensity",<br/>    "Hue",<br/>    "OD280_315",<br/>    "Proline"<br/> )</pre>

<p>由于变量没有缩放，我们需要使用<kbd>scale()</kbd>函数来完成。这将首先使数据居中，从列中的每个个体中减去列平均值。然后，居中的值将除以相应列的标准偏差。我们还可以使用这种转换来确保我们只包含第 2 到第 14 列，删除 class 并将其放入数据框中。这都可以通过一行代码来完成:</p>

<pre>  &gt; wine_df &lt;- as.data.frame(scale(wine[, -1]))</pre>

<p>在继续之前，出于好奇，让我们做一个快速表格来看看品种的分布或<kbd>Class</kbd>:</p>

<pre>&gt; table(wine$Class)<br/><br/> 1  2  3 <br/>59 71 48 </pre>

<p>我们现在可以转向无监督学习模型。</p>

<p class="mce-root"/>





            



            

        

    






    

        <title>Modeling </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">建模</h1>

                

            

            

                

<p>创建了我们的数据框架<kbd>df</kbd>，我们可以开始开发聚类算法。我们将从层次结构开始，然后尝试 k-means。在此之后，我们将需要操纵我们的数据一点点，以演示如何将混合数据与高尔和随机森林。</p>





            



            

        

    






    

        <title>Hierarchical clustering</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">分层聚类</h1>

                

            

            

                

<p>要在 R 中构建一个层次集群模型，您可以利用基本<kbd>stats</kbd>包中的<kbd>hclust()</kbd>函数。该函数需要的两个主要输入是距离矩阵和聚类方法。距离矩阵很容易用<kbd>dist()</kbd>函数完成。对于距离，我们将使用欧几里得距离。有多种聚类方法可用，默认情况下<kbd>hclust()</kbd>是完全链接。</p>

<p>我们会试试这个，但是我也推荐沃德的联动法。沃德的方法倾向于产生具有相似数量的观察值的聚类。</p>

<p>完全连锁法得出任意两个聚类之间的距离，即一个聚类中任意一个观测值与另一个聚类中任意一个观测值之间的最大距离。沃德的连锁法试图将观察值进行聚类，以使组内平方和最小化。</p>

<p>值得注意的是，R 方法<kbd>ward.D2</kbd>使用的是平方欧氏距离，这的确是沃德连锁法。在 R 中，<kbd>ward.D</kbd>是可用的，但要求距离矩阵是平方值。因为我们将建立一个非平方值的距离矩阵，我们将需要<kbd>ward.D2</kbd>。</p>

<p>现在，最大的问题是我们应该创建多少个集群？如引言中所述，简短的、可能不太令人满意的答案是视情况而定。尽管有集群有效性测量来帮助解决这一困境——我们将会看到——但它确实需要对业务环境、底层数据以及坦率地说，试错法的深入了解。由于我们的侍酒师伙伴是虚构的，我们将不得不依靠有效性措施。然而，这并不是选择聚类数的灵丹妙药，因为有几十种有效性度量。</p>

<p class="mce-root"/>

<p class="mce-root"/>

<p>由于探究大量聚类有效性度量的优缺点超出了本章的范围，我们可以求助于一些论文，甚至 R 本身来简化这个问题。Miligan 和 Cooper 在 1985 年发表的一篇论文中探讨了 30 种不同测量/指数对模拟数据的影响。表现最好的五家是 CH 指数、杜达指数、Cindex、Gamma 和 Beale 指数。另一个众所周知的确定集群数量的方法是<strong>间隙统计</strong> (Tibshirani，Walther，和 Hastie，2001)。如果你的好奇心战胜了你，这是两篇很好的论文供你研究。</p>

<p>使用 R，我们可以使用<kbd>NbClust</kbd>包中的<kbd>NbClust()</kbd>函数来提取 23 个指数的结果，包括来自 Miligan 和 Cooper 的前五个指数以及 gap 统计数据。您可以在软件包的帮助文件中看到所有可用索引的列表。有两种方法可以接近这个过程:一种是选择你最喜欢的一个或多个指数并用 R 调用它们；另一种方法是将它们都包含在分析中，并采用多数规则方法，函数会很好地为您总结这种方法。该函数也将生成几幅图。</p>

<p>有了舞台布景，让我们浏览一下使用完整链接方法的示例。使用函数时，除了关联之外，您还需要指定最小和最大聚类数、距离测量值和指数。正如你在下面的代码中看到的，我们将创建一个名为<kbd>numComplete</kbd>的对象。函数规范针对欧几里德距离、最小聚类数 2、最大聚类数 6、完全连锁和所有索引。当您运行该命令时，该函数将自动生成类似于您在此处看到的输出—关于图形方法和多数规则的讨论结论:</p>

<pre>&gt; numComplete &lt;- NbClust::NbClust(<br/>    wine_df,<br/>    distance = "euclidean",<br/>    min.nc = 2,<br/>    max.nc = 6,<br/>    method = "complete",<br/>    index = "all"<br/> )<br/>*** : The Hubert index is a graphical method of determining the number of clusters.<br/> In the plot of Hubert index, we seek a significant knee that corresponds to a <br/> significant increase of the value of the measure that is, the significant peak in Hubert<br/> index second differences plot. <br/> <br/>*** : The D index is a graphical method of determining the number of clusters. <br/> In the plot of D index, we seek a significant knee (the significant peak in Dindex<br/> second differences plot) that corresponds to a significant increase of the value of<br/> the measure. <br/>******************************************************************* <br/>* Among all indices: <br/>* 1 proposed 2 as the best number of clusters <br/>* 11 proposed 3 as the best number of clusters <br/>* 6 proposed 5 as the best number of clusters <br/>* 5 proposed 6 as the best number of clusters <br/><br/> ***** Conclusion ***** <br/>* According to the majority rule, the best number of clusters is 3 <br/>  <br/>*******************************************************************</pre>

<p>使用多数规则方法，我们将选择三个集群作为最佳解决方案，至少对于分层集群来说是这样。生成的两个图各包含两个图形。正如前面的输出所述，您正在图中寻找一个明显的拐点(图的左侧)和图的峰值(图的右侧)。这是<strong>休伯特指数</strong>图:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-713 image-border" src="img/d732bcf5-49fe-4925-bb20-aca3bb72e175.png" style="width:37.92em;height:20.92em;"/></p>

<p>您可以看到弯曲或膝盖位于左侧图表中的三个集群处。此外，右侧的图表在三个集群处达到峰值。下面的<strong> Dindex </strong>图提供了相同的信息:</p>

<div><img src="img/f722df26-ffca-4af2-b83f-1a5eb1385b4e.png" style="width:35.17em;height:22.75em;"/></div>

<p>你可以用这个函数调用很多值，我想展示其中一个。该输出是每个索引的最佳聚类数以及相应聚类数的索引值。这是用<kbd>$Best.nc</kbd>完成的。我将输出简化为前几个索引:</p>

<pre>&gt; numComplete$Best.nc<br/>                     KL      CH Hartigan   CCC    Scott<br/>Number_clusters  5.0000  3.0000   3.0000 5.000   3.0000 <br/>Value_Index     14.2227 48.9898  27.8971 1.148 340.9634</pre>

<p>您可以看到，第一个索引<kbd>KL</kbd>的最佳聚类数为 5，下一个索引<kbd>CH</kbd>的最佳聚类数为 3。</p>

<p>使用三个集群作为推荐选择，我们现在将计算距离矩阵并构建我们的分层集群对象。这将构建距离矩阵:</p>

<pre>&gt; euc_dist &lt;- dist(wine_df, method = "euclidean")</pre>

<p>然后，我们将使用这个矩阵作为使用<kbd>hclust()</kbd>进行实际聚类的输入:</p>

<pre>&gt; hc_complete &lt;- hclust(euc_dist, method = "complete")</pre>

<p>可视化层次聚类的常用方法是绘制一个<strong>树状图</strong>。我们将通过<kbd>dendextend</kbd>包提供的功能来实现这一点:</p>

<pre>&gt; dend1 &lt;- dendextend::color_branches(dend_complete, k = 3)<br/><br/>&gt; plot(dend1, main = "Complete-Linkage")</pre>

<p>上述代码的输出如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-714 image-border" src="img/9e7d05ac-cd5a-495d-9051-5979330bbe5e.png" style="width:40.92em;height:21.58em;"/></p>

<p>树状图是一个树形图，向您显示各个观察值是如何聚集在一起的。连接的排列(分支，如果你愿意的话)告诉我们哪些观察是相似的。分支的高度表示距离矩阵中的观测值彼此相似或不相似的程度。</p>

<p>以下是集群计数表:</p>

<pre>&gt; complete_clusters &lt;- cutree(hc_complete, 3)<br/><br/>&gt; table(complete_clusters)<br/>complete_clusters<br/> 1  2  3 <br/>69 58 51</pre>

<p>出于好奇，让我们比较一下这种聚类算法与<strong>品种</strong>标签的对比情况:</p>

<pre>&gt; table(complete_clusters, wine$Class)<br/>                 <br/>complete_clusters  1  2  3<br/>                1 51 18  0<br/>                2  8 50  0<br/>                3  0  3 48</pre>

<p>在该表中，行是簇，列是栽培品种。这种方法与品种标签的匹配率为 84%。请注意，我们并不试图使用聚类来预测栽培品种，在这个例子中，我们没有先验的理由来将聚类与栽培品种相匹配，但这仍然是有启发性的。</p>

<p>我们现在将尝试沃德的联系。这是和以前一样的代码；它首先试图确定集群的数量，这意味着我们需要将方法改为<kbd>Ward.D2</kbd>:</p>

<pre>&gt; numWard &lt;- NbClust::NbClust(<br/>    wine_df,<br/>    distance = "euclidean",<br/>    min.nc = 2,<br/>    max.nc = 6,<br/>    method = "ward.D2",<br/>    index = "all"<br/>)<br/># Output abbreviated to just show the algorithm's conclusion.  <br/>                     ***** Conclusion *****                            <br/>    <br/>    * According to the majority rule, the best number of clusters is 3</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root">同样，多数规则适用于三集群解决方案。我会让你自己看情节的。</p>

<p class="mce-root">让我们转到沃德连锁反应的实际聚类和生成树状图:</p>

<pre>&gt; hc_ward &lt;- hclust(euc_dist, method = "ward.D2")<br/><br/>&gt; dend_ward &lt;- as.dendrogram(hc_ward)<br/><br/>&gt; dend2 &lt;- dendextend::color_branches(dend_ward, k = 3)<br/><br/>&gt; plot(dend2, main = "Ward Method")</pre>

<p>这是输出:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-715 image-border" src="img/605967f5-fcf7-4883-94cc-0eedda8071aa.png" style="width:43.00em;height:22.17em;"/></p>

<p>该图显示了三个大小大致相等的截然不同的星团。让我们计算聚类大小，并显示它与品种标签的关系:</p>

<pre>&gt; ward_clusters &lt;- cutree(hc_ward, 3)<br/><br/>&gt; table(ward_clusters, wine$Class)<br/>             <br/>ward_clusters  1  2  3<br/>            1 59  5  0<br/>            2  0 58  0<br/>            3  0  8 48</pre>

<p>因此，分类 1 有 64 个观察值，分类 2 有 58 个，分类 3 有 56 个。这种方法比使用完全连锁更接近品种类别。</p>

<p>通过另一个表格，我们可以比较这两种方法如何匹配观测值:</p>

<pre>&gt; table(complete_clusters, ward_clusters)<br/>                  ward_clusters<br/>complete_clusters  1  2  3<br/>                1 53 11  5<br/>                2 11 47  0<br/>                3  0  0 51</pre>

<p class="mce-root">虽然每种方法的第三类都是精确的，但其他两类却不是。现在的问题是，我们如何识别解释上的差异？在许多示例中，数据集非常小，您可以查看每个聚类的标签。在现实世界中，这通常是不可能的。我喜欢按类汇总结果，并据此进行比较。</p>

<p>将按聚类汇总的结果放入交互式电子表格或商业智能工具中有助于您和您的业务合作伙伴的理解，有助于选择适当的聚类方法和聚类数。</p>

<p>我将通过查看 Ward 方法中按聚类分组的特征的平均值来演示这一点。首先，使用缩放后的数据(或者原始数据，如果您愿意)和结果创建一个单独的数据框:</p>

<pre>&gt; ward_df &lt;- wine_df %&gt;%<br/>    dplyr::mutate(cluster = ward_clusters)</pre>

<p>现在，进行汇总:</p>

<pre>&gt; ward_df %&gt;%<br/>    dplyr::group_by(cluster) %&gt;%<br/>    dplyr::summarise_all(dplyr::funs(mean)) -&gt; ward_results</pre>

<p>现在，您可以在 RStudio 中查看该数据框，或者将其导出到您喜欢的 BI 工具中。也许你对某个情节感兴趣？如果是这样，试试这个:</p>

<pre>&gt; ggplot2::ggplot(ward_results, ggplot2::aes(cluster, Alcohol)) +<br/>    ggplot2::geom_bar(stat = "identity") +<br/>    ggthemes::theme_stata()</pre>

<p class="mce-root"/>

<p>这是输出:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-716 image-border" src="img/6ae219f1-1d1e-43ea-8f39-e718353a1516.png" style="width:37.83em;height:23.92em;"/></p>

<p>在酒精含量的聚类之间存在明显的分离。话虽如此，让我们继续 k-means。</p>





            



            

        

    






    

        <title>K-means clustering</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">k 均值聚类</h1>

                

            

            

                

<p>正如我们对层次聚类所做的那样，我们也可以使用<kbd>NbClust()</kbd>来确定 k-means 的最佳聚类数。你需要做的就是在函数中指定<kbd>kmeans</kbd>作为方法。让我们把最大集群数放宽到<kbd>15</kbd>。我将下面的输出简化为结论:</p>

<pre>&gt; numKMeans &lt;- NbClust::NbClust(wine_df,<br/>    min.nc = 2,<br/>    max.nc = 15,<br/>    method = "kmeans")<br/>                   <br/>***** Conclusion ***** <br/> <br/>* According to the majority rule, the best number of clusters is 3</pre>

<p>同样，三个集群似乎是最佳解决方案。</p>

<p class="mce-root">在 R 中，我们可以使用<kbd>kmeans()</kbd>函数来做这个分析。除了输入数据之外，我们还必须指定我们正在求解的聚类数和随机赋值的值，即<kbd>nstart</kbd>参数。我们还需要指定一个随机种子:</p>

<pre>    &gt; set.seed(1234)<br/>    <br/>    &gt; km &lt;- kmeans(df, 3, nstart = 25)</pre>

<p>创建一个聚类表可以让我们了解它们之间的观察分布情况:</p>

<pre>    &gt; table(km$cluster)<br/>    <br/>     1  2  3 <br/>    62 65 51</pre>

<p>每个集群的观测值数量非常均衡。我在很多情况下看到，对于更大的数据集和更多的特征，任何数量的 k-means 都不能产生有希望和令人信服的结果。分析聚类的另一种方法是查看每个聚类中每个变量的聚类中心矩阵:</p>

<pre>    &gt; km$centers<br/>         Alcohol  MalicAcid        Ash    Alk_ash   magnesium   T_phenols<br/>    1  0.8328826 -0.3029551  0.3636801 -0.6084749  0.57596208  0.88274724<br/>    2 -0.9234669 -0.3929331 -0.4931257  0.1701220 -0.49032869 -0.07576891<br/>    3  0.1644436  0.8690954  0.1863726  0.5228924 -0.07526047 -0.97657548<br/>       Flavanoids    Non_flav    Proantho C_Intensity        Hue  OD280_315<br/>    1  0.97506900 -0.56050853  0.57865427   0.1705823  0.4726504  0.7770551<br/>    2  0.02075402 -0.03343924  0.05810161  -0.8993770  0.4605046  0.2700025<br/>    3 -1.21182921  0.72402116 -0.77751312   0.9388902 -1.1615122 -1.2887761<br/>         Proline<br/>    1  1.1220202<br/>    2 -0.7517257<br/>    3 -0.4059428</pre>

<p class="mce-root">请注意，集群一平均酒精含量较高。让我们制作一个方框图来观察酒精含量的分布，并与沃德的进行比较:</p>

<pre>&gt; par(mfrow = c(1, 2))<br/><br/>&gt; boxplot(wine$Alcohol ~ km$cluster, data = wine, <br/>    main = "Alcohol Content, K-Means")<br/><br/>&gt; boxplot(wine$Alcohol ~ ward_clusters, data = wine, <br/>    main = "Alcohol Content, Ward's")</pre>

<p>这是输出:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-717 image-border" src="img/606827f1-4d7f-46b7-bd03-694cebf84423.png" style="width:40.00em;height:23.33em;"/></p>

<p>每个集群的酒精含量几乎完全相同。从表面上看，这告诉我，三个聚类是葡萄酒的合适的潜在结构，使用 k-means 或层次聚类之间没有什么区别。最后，让我们对 k 均值聚类和品种进行比较:</p>

<pre>    &gt; table(km$cluster, wine$Class)<br/>    <br/>         1  2  3<br/>      1 59  3  0<br/>      2  0 65  0<br/>      3  0  3 48</pre>

<p>这与沃德的方法产生的分布非常相似，任何一种都可能被我们假设的侍酒师接受。</p>

<p class="mce-root"/>

<p>但是，为了演示如何对具有数值和非数值的数据进行聚类，我们来看一些例子。</p>





            



            

        

    






    

        <title>Gower and PAM</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">高尔和帕姆</h1>

                

            

            

                

<p>为了开始这一步，我们需要稍微整理一下我们的数据。由于这种方法可以获取作为因子的变量，我们将酒精转换为高或低含量。利用<kbd>ifelse()</kbd>函数将变量改为因子也只需要一行代码。这将实现的是，如果酒精大于零，它将是<kbd>High</kbd>，否则，它将是<kbd>Low</kbd>:</p>

<pre>&gt; wine_df$Alcohol &lt;- as.factor(ifelse(df$Alcohol &gt; 0, "High", "Low"))</pre>

<p>我们现在准备使用<kbd>cluster</kbd>包中的<kbd>daisy()</kbd>函数创建相异度矩阵，并将方法指定为<kbd>gower</kbd>:</p>

<pre>&gt; gower_dist &lt;- cluster::daisy(wine[, -1], metric = "gower")</pre>

<p>集群对象的创建是通过<kbd>pam()</kbd>函数完成的，它是<kbd>cluster</kbd>包的一部分。在本例中，我们将创建三个集群，并创建一个集群大小表:</p>

<pre>&gt; set.seed(123)<br/><br/>&gt; pam_cluster &lt;- cluster::pam(gower_dist, k = 3)<br/><br/>&gt; table(pam_cluster$clustering)<br/><br/> 1  2  3 <br/>62 71 45 </pre>

<p>现在，让我们看看它与品种标签相比表现如何:</p>

<pre>&gt; table(pam_cluster$clustering, wine$Class)<br/>   <br/>     1  2  3<br/>  1 57  5  0<br/>  2  2 64  5<br/>  3  0  2 43</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p>如前所述，您可以使用这种方法运行类似的聚合和探索练习。让我们看看酒精在三个集群中的分布情况:</p>

<pre>&gt; table(pam_cluster$clustering, wine$Alcohol)<br/>   <br/>    High Low<br/>  1   62  0<br/>  2    1 70<br/>  3   29 16</pre>

<p>此表显示了按分类列出的因子级别的比例。高尔度量对于带有标签、因子、字符、缺失值等的数据非常强大。我强烈推荐。任何距离矩阵的一个缺点是，它可能成为大型数据集的计算问题。一个有效的解决方案是运行 k 样本并比较结果。做得好的话，您就可以构建一个分类器来预测您的人群的聚类。</p>

<p>最后，我们将使用随机森林创建一个相异矩阵，并使用 PAM 创建三个集群。</p>





            



            

        

    






    

        <title>Random forest and PAM</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">随机森林和帕姆</h1>

                

            

            

                

<p>要在 R 中执行这个方法，可以使用<kbd>randomForest()</kbd>函数。设置随机种子后，只需创建模型对象。在下面的代码中，我将树的数量指定为<kbd>2000</kbd>，并将邻近度设置为<kbd>TRUE</kbd>。您不必对缩放后的数据运行此操作:</p>

<pre>&gt; set.seed(1918)<br/><br/>&gt; rf &lt;- randomForest::randomForest(x = wine[, -1], ntree = 2000, proximity = T)<br/><br/>&gt; rf<br/><br/>Call:<br/> randomForest(x = wine[, -1], ntree = 2000, proximity = T) <br/>               Type of random forest: unsupervised<br/>                     Number of trees: 2000<br/>No. of variables tried at each split: 3</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p>正如您所看到的，调用<kbd>rf</kbd>并没有提供任何有意义的输出，除了在每次分割时采样的变量(<kbd>mtry</kbd>)。让我们检查一下<em> N x N </em>矩阵的前五行和前五列:</p>

<pre>&gt; dim(rf$proximity)<br/>[1] 178 178<br/><br/>&gt; rf$proximity[1:5, 1:5]<br/>          1          2         3          4          5<br/>1 1.0000000 0.27868852 0.4049296 0.36200717 0.12969283<br/>2 0.2786885 1.00000000 0.2142857 0.12648221 0.04453441<br/>3 0.4049296 0.21428571 1.0000000 0.26865672 0.14942529<br/>4 0.3620072 0.12648221 0.2686567 1.00000000 0.07692308<br/>5 0.1296928 0.04453441 0.1494253 0.07692308 1.00000000</pre>

<p>考虑这些值的一种方式是，它们是这两个观察值出现在相同终端节点中的次数百分比！看看可变重要性，我们看到转换后的<kbd>Alcohol</kbd>输入可能会被丢弃。为了简单起见，我们将保留它:</p>

<pre>&gt; randomForest::importance(rf)<br/>            MeanDecreaseGini<br/>Alcohol             3.692748<br/>MalicAcid          12.650096<br/>Ash                10.842885<br/>Alk_ash            11.636227<br/>magnesium          10.672465<br/>T_phenols          17.733783<br/>Flavanoids         21.410838<br/>Non_flav           11.527873<br/>Proantho           14.494229<br/>C_Intensity        14.795900<br/>Hue                14.296274<br/>OD280_315          17.815508<br/>Proline            15.922621</pre>

<p>现在只是创建相异度矩阵的问题，该矩阵将邻近度值(<em>平方根(1 -邻近度)</em>)转换如下:</p>

<pre>&gt; rf_dist &lt;- sqrt(1 - rf$proximity)<br/><br/>&gt; rf_dist[1:2, 1:2]<br/>          1         2<br/>1 0.0000000 0.8493006<br/>2 0.8493006 0.0000000</pre>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mce-root"/>

<p class="mceNonEditable"/>

<p class="mce-root"/>

<p class="mceNonEditable"/>

<p class="mceNonEditable"/>

<p class="mceNonEditable"/>

<p>我们现在已经有了输入特性，所以让我们像前面一样运行一个 PAM 集群:</p>

<pre>&gt; set.seed(1776)<br/><br/>&gt; pam_rf &lt;- cluster::pam(rf_dist, k = 3)<br/><br/>&gt; table(pam_rf$clustering)<br/><br/> 1  2  3 <br/>52 82 44 <br/><br/>&gt; table(pam_rf$clustering, wine$Class)<br/>   <br/>     1  2  3<br/>  1 52  0  0<br/>  2  7 70  5<br/>  3  0  1 43</pre>

<p>这些结果与所应用的其他技术相当。学到了什么？如果对于聚类问题，您有杂乱的数据，可以考虑使用随机森林来创建距离矩阵，甚至从您的聚类算法中消除特征。</p>





            



            

        

    






    

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:edef0689-6ba8-4dfc-93a3-190fd7a9b9a4" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>在这一章，我们开始探索无监督学习技术。我们专注于聚类分析，以提供数据简化和对观察结果的数据理解。</p>

<p>介绍了四种方法:传统的分层和 k-means 聚类算法，以及 PAM，结合了两种不同的输入(Gower 和 random forest)。我们应用这四种方法来寻找来自三个不同品种的意大利葡萄酒的结构，并检查结果。</p>

<p>在下一章中，我们将继续探索无监督学习，但不是在观察值中寻找结构，而是集中在变量中寻找结构，以便创建可用于监督学习问题的新特征。</p>





            



            

        

    



</div></body></html>