<html><head/><body>


	
		<title>B18216_01_ePub</title>
		
	
	
		<div><h1 id="_idParaDest-15">第一章:可解释技术的基本概念</h1>
			<p>随着越来越多的组织开始将<strong class="bold">人工智能</strong> ( <strong class="bold"> AI </strong>)和<strong class="bold">机器学习</strong> ( <strong class="bold"> ML </strong>)用于其关键的商业决策过程，解读<strong class="bold">黑盒算法</strong>以提高其采用率成为一种迫切的期望。人工智能和人工智能正越来越多地用于确定我们在多个领域的日常体验，如银行、医疗保健、教育、招聘、运输和供应链。但人工智能和人工智能模型所发挥的不可或缺的作用导致商业利益相关者和消费者越来越担心缺乏透明度和可解释性，因为这些黑箱算法很容易受到人类偏见的影响；特别是对于高风险领域，如医疗保健、金融、法律和其他关键的工业运营，模型可解释性是一个先决条件。</p>
			<p>由于人工智能和人工智能的好处可能是显著的，问题是，尽管人们越来越担心，我们如何才能增加它的采用？我们甚至能解决这些问题并使人工智能和人工智能的使用民主化吗？对于黑盒模型不可信的关键工业应用，我们如何才能让人工智能更具解释力？贯穿本书，我们将努力学习这些问题的答案，并应用这些概念和思想解决实际问题！</p>
			<p>在这一章中，你将学习到<strong class="bold">可解释人工智能</strong> ( <strong class="bold"> XAI </strong>)的基本概念，这样在以后的章节中使用的术语和概念就清晰了，也更容易理解和实现本书后面讨论的一些高级可解释技术。这将为你提供理解和实现后面章节中讨论的实用技术所需的理论知识。该章侧重于以下主要议题:</p>
			<ul>
				<li>XAI 简介</li>
				<li>定义解释方法和途径</li>
				<li>评估可解释方法的质量</li>
			</ul>
			<p>现在，让我们开始吧！</p>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>XAI 简介</h1>
			<p><strong class="bold"> XAI </strong>是最有效的实践<a id="_idIndexMarker000"/>来确保人工智能和人工智能解决方案是透明的、可信赖的、负责任的和道德的，以便有效地解决所有关于算法透明度、风险缓解和后备计划的监管要求。AI 和 ML 可解释性技术提供了这些算法如何在其解决方案生命周期的每个阶段运行的必要可见性，允许最终用户理解<em class="italic">为什么</em>和<em class="italic">如何</em>查询与 AI 和 ML 模型的结果相关。</p>
			<h2 id="_idParaDest-17"><a id="_idTextAnchor016"/>理解关键术语</h2>
			<p>通常，对于 ML 模型，对于解决<em class="italic">如何</em>问题的<a id="_idIndexMarker001"/>，我们使用术语<em class="italic">可解释性</em>，对于解决<em class="italic">为什么</em>问题，我们使用术语<em class="italic">可解释性</em>。在本书中，术语<strong class="bold">模型可解释性</strong>和<strong class="bold">模型可解释性</strong>可以互换使用。然而，为了<a id="_idIndexMarker002"/>对 ML 模型的结果提供<em class="italic">人性化的</em>整体解释<a id="_idIndexMarker003"/>，我们将需要使 ML 算法既可解释又可说明，从而允许最终用户容易理解这些模型的决策过程。</p>
			<p>在大多数情况下，ML 模型被认为是黑盒，我们在其中输入任何训练数据，并期望它根据新的、看不见的数据进行预测。与我们编写特定指令的传统编程不同，ML 模型会自动尝试从数据中学习这些指令。如图<em class="italic">图 1.1 </em>所示，当我们试图找出模型预测的基本原理时，我们没有获得足够的信息！</p>
			<div><div><img src="img/B18216_01_01.jpg" alt="Figure 1.1 – Conventionally, black-box models do not provide any rationale behind predictions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.1-传统上，黑盒模型不提供任何预测背后的基本原理</p>
			<p>现在，让我们来理解不正确的预测和不准确的 ML 模型的影响。</p>
			<h2 id="_idParaDest-18">糟糕预测的后果</h2>
			<p>传统上，所有的 ML 模型<a id="_idIndexMarker004"/>都被认为是神奇的黑盒，可以自动从数据中解读有趣的模式和见解，并提供<em class="italic">银弹</em>结果！与受到程序员智能限制的传统的基于规则的计算机程序相比，训练有素的 ML 算法被认为即使在复杂的情况下也能提供丰富的见解和准确的预测。但事实是，所有的 ML 模型<a id="_idIndexMarker005"/>都存在<em class="italic">偏差</em>，这可能是由于算法本身的<strong class="bold">归纳偏差</strong>，也可能是由于用于训练<a id="_idIndexMarker006"/>模型的数据集中存在偏差。实际上，<a id="_idIndexMarker007"/>可能有其他<a id="_idIndexMarker008"/>原因，例如<strong class="bold">数据漂移</strong>、<strong class="bold">概念漂移</strong>，以及<strong class="bold">过拟合</strong>或<strong class="bold">欠拟合</strong>模型，对于这些模型<a id="_idIndexMarker009"/>预测可能出错。正如英国著名统计学家乔治 E.P 博克斯曾经说过的，<em class="italic">“所有的模型都是错的，但有些是有用的”</em>；如果这些方法的初始假设不一致，所有的统计、科学和 ML 模型都可能给出不正确的结果。因此，对我们来说，了解为什么 ML 模型预测了特定的结果，如果它是错误的，可以做什么，以及如何改进预测是很重要的。</p>
			<p><em class="italic">图 1.2 </em>展示了一组新闻标题，强调了人工智能算法在产生公平公正的结果方面的失败。</p>
			<div><div><img src="img/B18216_01_02.jpg" alt="Figure 1.2 – Growing concern of bias and lack of fairness of ML models being reported frequently&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.2–越来越多的人担心 ML 模型的偏见和缺乏公平性</p>
			<p>在完全同意我关于模型可解释性的必要性之前，让我试着给出一些低风险和高风险领域的实际例子来理解糟糕预测的后果。天气预报是一个经典的预测问题，非常具有挑战性(因为它取决于多个动态因素)，其中 ML 被广泛使用，并且 ML 算法考虑不同类型的多个参数的能力使其比标准统计模型更有效地预测天气。尽管有高度准确的预报模型，但有时天气预报算法可能会错过对降雨的预测，即使几分钟后就开始下雨了！但是如此糟糕的预测的后果可能不会如此严重，此外，大多数人不会盲目地依赖自动天气预测，从而使天气预测成为一个低风险的领域问题。</p>
			<p>类似地，对于另一个低风险<a id="_idIndexMarker011"/>领域，例如内容推荐系统，即使 ML 算法提供了不相关的推荐，最终用户最多可能花费更多的时间来明确地搜索相关内容。虽然最终用户的整体体验可能会受到影响，但不会造成严重的生命或生计损失。因此，对模型可解释性的需求对于低风险领域来说并不重要，但为模型预测提供可解释性确实使自动化智能系统对最终用户来说更值得信赖和可靠，从而通过增强最终用户体验来增加人工智能的采用。</p>
			<p>现在，让我举一个例子，糟糕的预测导致了公司声誉和价值的严重损失，影响了许多人的生活！2021 年 11 月，一家名为<em class="italic">Zillow</em>(<a href="https://www.zillow.com/">https://www.zillow.com/</a>)的美国在线房地产<a id="_idIndexMarker012"/>市场公司报告其股票价值损失超过 40%，而购房部门<em class="italic"> Offers </em>损失超过 3 亿美元，因为其未能检测到他们的房价预测算法的不可预测性(有关更多信息，请参考<em class="italic">参考文献</em>部分提到的来源)。为了弥补损失，Zillow 不得不采取大刀阔斧的裁员措施，数千个家庭受到影响。</p>
			<p>同样，多家科技公司被指控使用高度偏见的人工智能算法，这些算法可能因种族或性别歧视而导致社会动荡。2015 年就发生了这样一件事，谷歌照片犯了一个严重的种族主义错误，自动将一对非裔美国夫妇标记为<em class="italic">大猩猩</em>(请查看<em class="italic">参考文献</em>部分提到的来源以了解更多信息)。虽然这些失误是无意的，并且主要是由于有偏见的数据集或非广义最大似然模型，但这些事件的后果可能会造成巨大的社会、经济和政治浩劫。在其他高风险领域(如医疗保健、信用贷款和招聘)对 ML 模型的偏见不断提醒我们需要更透明的解决方案和最终用户可以依赖的 XAI 解决方案。</p>
			<p>如图<em class="italic">图 1.3 </em>所示，糟糕预测的后果凸显了 XAI 的重要性，它可以提供早期指标，以防止因 AI 算法失败而导致的声誉、金钱、生命或生计损失:</p>
			<div><div><img src="img/B18216_01_03.jpg" alt="Figure 1.3 – Common consequences of poor prediction of ML models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.3–ML 模型预测不佳的常见后果</p>
			<p>现在，让我们在下一节中总结一下对模型可解释性的需求。</p>
			<h2 id="_idParaDest-19">总结对模型可解释性的需求</h2>
			<p>在上一节中，我们了解到<a id="_idIndexMarker013"/>不良预测的后果会影响高风险领域中的许多人，甚至在低风险领域中，最终用户的体验也会受到影响。<em class="italic"> Samek 和 Binder 的</em>工作在<em class="italic">关于可解释机器学习的教程，MICCAI'18 </em>，强调了模型可解释性的主要必要性。让我来总结一下为什么模型可解释性是必不可少的关键原因:</p>
			<ul>
				<li><strong class="bold">验证和调试 ML 系统</strong>:正如我们已经看到的一些例子，错误的模型决策可能是昂贵和危险的，模型可解释性技术帮助我们验证和确认 ML 系统。对不正确的预测进行解释有助于我们调试根本原因，并提供解决问题的方向。我们将在<a href="B18216_10_ePub.xhtml#_idTextAnchor209"> <em class="italic">第十章</em> </a>、<em class="italic"> XAI 行业最佳实践</em>中更详细地讨论一个可解释的 ML 系统的不同阶段。</li>
				<li>使用以用户为中心的方法来改进人工智能模型 : XAI 提供了一种机制，将人类的经验和直觉纳入其中来改进人工智能系统。传统上，基于预测误差来评估 ML 模型<a id="_idIndexMarker014"/>。使用这样的评估方法来改进 ML 模型不会增加任何透明度，并且可能不是健壮和有效的。然而，使用可解释性方法，我们可以使用人类经验来验证预测，并了解是否需要以模型为中心或以数据为中心的方法来进一步改进 ML 模型。<em class="italic">图 1.4 </em>比较了经典的 ML 系统和可解释的 ML 系统<a id="_idTextAnchor019"/>:</li>
			</ul>
			<div><div><img src="img/B18216_01_04.jpg" alt="Figure 1.4 – Comparison between classical ML and explainable ML approach&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.4-经典 ML 和可解释 ML 方法之间的比较</p>
			<ul>
				<li><strong class="bold">学习新见解</strong> : ML 被认为是从人类不明显的数据中自动解开有趣的见解和模式。可解释的 ML 为我们提供了一种机制来理解模型自动获取的见解和模式背后的基本原理，并允许我们详细研究这些模式以做出新的发现。</li>
				<li><strong class="bold">遵守立法</strong>:许多监管机构，如<strong class="bold">通用数据保护条例</strong> ( <strong class="bold"> GDPR </strong>)和<strong class="bold">加州消费者隐私法案</strong> ( <strong class="bold"> CCPA </strong>)，都对人工智能缺乏可解释性表示严重关切<a id="_idIndexMarker015"/>。因此，不断增长的全球人工智能法规赋予了个人权利，要求对可能影响他们的自动决策系统做出解释。模型可解释性技术试图确保 ML 模型符合提议的监管法律，从而促进公平性、责任性和透明度。</li>
			</ul>
			<div><div><img src="img/B18216_01_05_.jpg" alt="Figure 1.5 – (Top) Screenshots of tweets from the European Commission, highlighting the right to demand explanations. (Bottom) Table showing some important regulatory laws established for making automated decision systems explainable, transparent, accountable, and fair&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.5-(上)欧盟委员会的推文截图，强调了要求解释的权利。(底部)表格显示了为使自动化决策系统变得可解释、透明、负责和公平而制定的一些重要法规</p>
			<p>对模型可解释性的需求<a id="_idIndexMarker018"/>可以在下面的可解释 ML 的<strong class="bold">胖模型图中形象化，如由<em class="italic"> Serg Masís </em>所著的《用 Python 进行的<em class="italic">可解释机器学习</em>一书中提供的<a id="_idTextAnchor020"/> d。</strong></p>
			<div><div><img src="img/B18216_01_06.jpg" alt="Figure 1.6 – FAT model of explainable ML (from Interpretable Machine Learning with Python by Serg Masís)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.6–可解释 ML 的胖模型(来自 Serg Masís 的 Python 可解释机器学习)</p>
			<p><em class="italic">图 1.6 </em>显示了金字塔<a id="_idIndexMarker019"/>，其形成了用于增加 AI 采用的可解释 ML <a id="_idIndexMarker020"/>系统的胖模型。让我们在下一节讨论定义解释方法和途径。</p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor021"/>定义解释方法和途径</h1>
			<p>在这一节中，让我们试着理解理解和应用各种可解释技术和方法所需的一些关键概念。</p>
			<h2 id="_idParaDest-21">可解释性的维度</h2>
			<p>除了在<em class="italic">MICCAI’18 上介绍的概念<a id="_idIndexMarker021"/>之外，来自<em class="italic"> Samek 和 Binder </em>的《可解释机器学习</em>教程》，当我们谈论去神秘化<a id="_idIndexMarker022"/>黑盒算法的问题时，我们可以通过四个不同的维度来解决这个问题，如下图所示:</p>
			<div><div><img src="img/B18216_01_07.jpg" alt="Figure 1.7 – Four dimensions of explainability&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.7——可解释性的四个维度</p>
			<p>现在，让我们详细了解这些维度:</p>
			<ul>
				<li><strong class="bold">数据</strong>:可解释性的维度<a id="_idIndexMarker023"/>围绕着被建模的底层数据。理解数据，识别其局限性和相关组成部分，并形成某些假设，对于建立正确的预期至关重要。稳健的数据监管过程、分析数据纯度以及敌对效应对数据的影响是获得可解释结果的其他关键练习。</li>
				<li><strong class="bold">模型</strong>:基于模型的可解释性技术<a id="_idIndexMarker024"/>经常帮助我们理解输入数据是如何映射到输出预测的，并且知道所使用的 ML 算法的一些限制和假设。例如，用于 ML 分类的朴素贝叶斯算法假设某个特征的存在是完全独立的，并且与任何其他特征的存在无关。因此，了解 ML 算法的这些<em class="italic">归纳偏差</em>有助于我们理解和预测任何预测误差或 ML 模型的局限性。</li>
				<li><strong class="bold">结果</strong>:可解释性的结果<a id="_idIndexMarker025"/>是关于理解<em class="italic">为什么</em>某个预测或决策是由 ML 模型做出的。尽管数据和模型的可解释性非常重要，但大多数 ML 专家和最终用户都致力于使最终的模型预测具有可解释性。</li>
				<li><strong class="bold">最终用户</strong>:可解释性的最后一个维度<a id="_idIndexMarker026"/>就是为 ML 模型的最终消费者创建正确的抽象层次，并包含适量的细节，这样结果对任何非技术最终用户来说都是可靠和值得信赖的，并使他们能够理解黑盒算法的决策过程。</li>
			</ul>
			<p>AI/ML 算法的可解释性是相对于可解释性的一个或多个维度来提供的。接下来，让我们讨论如何解决可解释性的关键问题。</p>
			<h2 id="_idParaDest-22">解决可解释性的关键问题</h2>
			<p>现在我们已经理解了可解释性的不同维度，让我们来讨论让 ML 模型可解释的必要条件。为了使最大似然算法可以解释，下面是我们应该努力解决的关键问题:</p>
			<ul>
				<li>从数据中我们了解了什么？</li>
			</ul>
			<p>第一步是关于数据的。在进行 AI 和 ML 建模之前，我们应该花足够的时间分析和探索数据。我们的目标始终是在对数据建模和生成预测时，寻找可能影响或产生挑战的差距、不一致、潜在偏差或假设。这有助于我们了解预期的情况，以及数据的某些方面如何有助于解决业务问题。</p>
			<ul>
				<li><em class="italic">模型是如何创建的？</em></li>
			</ul>
			<p>我们需要了解算法的透明程度，以及在建模过程中对数据进行建模时，算法能捕捉到什么样的关系。在这一步，我们试图理解算法的归纳偏差，然后试图将其与探索数据时获得的初始假设或观察结果联系起来。例如，如果数据具有使用基于可视化的数据<a id="_idIndexMarker028"/>探索方法观察到的一些二次或循环模式，则线性模型将不能有效地对数据建模。预测误差预计会更高。因此，如果不清楚算法如何建立训练数据的模型，这些算法就不太透明，因此不太容易解释。</p>
			<ul>
				<li>关于训练模型的全局可解释性，我们知道些什么？</li>
			</ul>
			<p>理解全局模型的可解释性总是具有挑战性。它是关于获得所使用的基础特性的整体视图，了解重要的特性，模型对关键特性值的变化有多敏感，以及模型内部发生了什么样的复杂交互。对于复杂的深度学习模型来说，这在实践中尤其难以实现，这些模型有数百万个要学习的参数和数百个层。</p>
			<ul>
				<li><em class="italic">模型的不同部分对最终预测有什么影响？</em></li>
			</ul>
			<p>ML 模型的不同部分可能以不同的方式影响最终预测。特别是对于深度神经网络模型，每一层都试图学习不同类型的特征。当模型预测不正确时，了解模型的不同部分如何影响或控制最终结果非常重要。因此，可解释性技术可以从模型的不同部分揭示见解，并帮助调试和观察算法对于不同数据点的健壮性。</p>
			<ul>
				<li><em class="italic">为什么模型会对单个记录和一批记录做出特定的预测？</em></li>
			</ul>
			<p>可解释性最重要的方面是理解为什么模型做出特定的预测，而不是别的。因此，应用了某些局部和全局解释技术，这些技术要么考虑单个特征的影响，要么甚至考虑多个特征对结果的集体影响。通常，这些可解释性技术应用于单个数据实例和一批数据实例，以了解观察结果是否一致。</p>
			<ul>
				<li><em class="italic">结果是否符合最终用户的期望？</em></li>
			</ul>
			<p>最后一步总是提供以用户为中心的解释。这意味着可解释性就是将结果与最终用户基于常识和人类直觉的预测进行比较。如果模型预测与用户的预测相匹配，则提供合理的解释包括证明特定结果的主导因素。但是假设模型预测与用户的预测不匹配。在这种情况下，一个好的解释是试图证明输入观察值中可能发生了什么变化，从而得到不同的结果。</p>
			<p>例如，考虑到平时的交通拥堵，我从办公室到家需要 30 分钟。但如果下雨，我会预计路上的车辆行驶缓慢，交通拥堵程度会更高，因此可能会预计到家需要更长时间。现在，如果一个人工智能应用程序预测回家的时间仍然是 30 分钟，我可能不会相信这个预测，因为这是违反直觉的。</p>
			<p>现在，让我们假设这个算法的预测是准确的。然而，向我提供的理由是关于我路线上车辆的移动，而 AI 应用程序只是提到我路线上的车辆以与其他日子相同的速度移动。这个解释真的有助于我理解模型预测吗？不，不是的。但是，假设应用程序提到该路线上的车辆比平时少。在这种情况下，我很容易理解，由于下雨，车辆数量减少，因此到达目的地的时间仍然与平日一样。</p>
			<p>我自己的建议是，在训练和验证一个 ML 模型之后，总是试图寻找这些问题的答案，作为解释黑盒模型工作的第一步。</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor024"/>理解不同类型的解释方法</h2>
			<p>在上一节中，我们讨论了在设计和使用健壮的可解释性方法时要解决的一些关键问题。在这一节中，我们将讨论各种类型的解释方法，考虑 ML 中使用的四个可解释性维度:</p>
			<ul>
				<li><strong class="bold">局部可解释性</strong>和<strong class="bold">全局可解释性</strong>:可以对数据的单个局部<a id="_idIndexMarker030"/>实例进行 ML 模型可解释性，以了解<a id="_idIndexMarker031"/>某个范围的值<a id="_idIndexMarker032"/>或特定类别值<a id="_idIndexMarker033"/>如何与最终预测相关。这叫做局部可解释性。全局模型可解释性用于解释整个模型的行为，或者作为一个整体对一组特定的模型结果有贡献的某些重要特征。</li>
				<li><strong class="bold">内在可解释性</strong>和<strong class="bold">外在可解释性</strong>:一些 ML 模型，如线性模型、简单<a id="_idIndexMarker034"/>决策树和启发式<a id="_idIndexMarker035"/>算法，是内在可解释的<a id="_idIndexMarker036"/>，因为我们清楚地知道算法应用的输入和输出的逻辑或数学<a id="_idIndexMarker037"/>映射，而外在或事后可解释性是关于首先在给定数据上训练 ML 模型，然后分别使用某些模型可解释性技术来理解和解释模型的结果。</li>
				<li><strong class="bold">模型特定的可解释性</strong>和<strong class="bold">模型不可知的可解释性</strong>:当我们使用适用于任何特定<a id="_idIndexMarker039"/>算法的某些可解释性<a id="_idIndexMarker038"/>方法时，那么这些就是模型特定的<a id="_idIndexMarker040"/>方法。例如，决策树模型中的树结构的可视化<a id="_idIndexMarker041"/>只特定于决策树算法，因此属于特定于模型的可解释性方法。模型不可知的方法被用于提供对任何 ML 模型的解释，而不考虑所使用的算法。大多数情况下，这些是事后分析方法，在获得训练后的 ML 模型后使用，并且通常，这些方法不知道内部模型结构<a id="_idIndexMarker042"/>和权重。在这本<a id="_idIndexMarker043"/>书中，我们将主要关注<a id="_idIndexMarker044"/>与模型无关的可解释性<a id="_idIndexMarker045"/>方法，它们不依赖于任何特定的算法。</li>
				<li><strong class="bold">以模型为中心的可解释性</strong>和<strong class="bold">以数据为中心的可解释性</strong>:按照惯例，大多数<a id="_idIndexMarker046"/>的解释方法<a id="_idIndexMarker047"/>都是以模型为中心的，因为这些方法试图解释输入特征和目标值是如何被算法建模的，以及如何获得特定的结果<a id="_idIndexMarker048"/>。但是随着以数据为中心的人工智能领域的最新进展，人工智能专家和研究人员也在研究围绕用于训练模型的数据的解释方法，这被称为以数据为中心的可解释性。以数据为中心的方法用于了解数据是否一致，是否经过良好的管理，是否适合解决潜在的问题。数据剖析、数据和概念漂移的检测以及对抗性健壮性是某些特定的以数据为中心的可解释方法，我们将在第 3 章 、<em class="italic">以数据为中心的方法</em>中更详细地讨论这些方法。</li>
			</ul>
			<p>我们将在本书后面的章节中讨论所有这些类型的可解释方法。</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor025"/>了解准确性、可解释性的权衡</h2>
			<p>对于一个理想的场景，我们会<a id="_idIndexMarker050"/>希望我们的 ML 模型是高度准确和高度可解释的，以便任何非技术业务利益相关者或最终用户能够理解模型预测背后的基本原理。但是在实践中，实现高度准确和可解释的模型是极其困难的，并且在准确性和可解释性之间总是有一个权衡。</p>
			<p>例如，为了执行射线照相图像分类，本质上可解释的 ML 算法，例如决策树，可能不能给出有效和概括的结果，而更复杂的深度卷积神经网络，例如 DenseNet，可能对于射线照相图像数据的建模更有效和鲁棒。但是 DenseNet 本质上是不可解释的，并且向任何非技术终端用户解释算法的工作可能相当复杂和具有挑战性。因此，高度精确的模型，如深度神经网络，是非线性的，更复杂，可以从数据中捕捉复杂的关系和模式，但这些模型很难实现可解释性。高度可解释的模型，如线性回归和决策树，主要是线性的，不太复杂，但这些模型仅限于从数据中学习线性或不太复杂的模式。</p>
			<p>现在的问题是，使用高度精确的模型还是高度可解释的模型更好？我会说正确答案是，<em class="italic">看情况！</em>这取决于正在解决的问题和这种模式的消费者。对于高风险领域，糟糕的预测会带来严重的后果，我建议采用更易解释的模型，即使牺牲了准确性。在这种情况下，任何高度可解释的基于规则的启发式模型都可能非常有效。但是如果问题得到了很好的研究，并且获得最小的预测误差是主要目标(例如在任何学术用例或任何 ML 竞赛中),使得不良预测的后果不会造成任何重大损害，那么可以优先选择高度精确的模型。在大多数工业问题中，保持模型准确性和可解释性的正确平衡以促进人工智能的采用是至关重要的。</p>
			<p><em class="italic">图 1.10 </em>说明了流行的 ML 算法的准确性和可解释性之间的权衡:</p>
			<div><div><img src="img/B18216_01_08.jpg" alt="Figure 1.8 – Accuracy-interpretability trade-off diagram&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.8-准确性-可解释性权衡图</p>
			<p>既然我们对准确性和可解释性的权衡有了一个公平的想法，让我们试着理解如何评估可解释性方法的质量。</p>
			<h1 id="_idParaDest-25">评估可解释方法的质量</h1>
			<p>可解释性是主观的，可能因人而异。关键问题<a id="_idIndexMarker054"/>是<em class="italic">我们如何确定一种方法是否比另一种更好？</em>因此，在这一部分，让我们讨论一下评估 ML 系统的可解释技术时需要考虑的某些标准<a id="_idTextAnchor027"/>。</p>
			<h2 id="_idParaDest-26">好的可解释 ML 系统的标准</h2>
			<p>由于 ML 系统的可解释性<a id="_idIndexMarker055"/>是一个非常主观的话题，首先让我们试着理解一些好的人性化解释的关键标准。在他的书<em class="italic">可解释的机器学习</em>，<em class="italic">中，作者 Christoph Molnar </em>也试图强调在彻底研究后良好的人性化解释的重要性，考虑到现代、工业、可解释的 ML 系统，我将尝试以浓缩的形式提及:</p>
			<ul>
				<li><strong class="bold">与先验知识的一致性</strong>:与终端用户先验信念的一致性是可解释 ML 系统的一个重要的<a id="_idIndexMarker056"/>标准。如果任何解释与人类的先验知识相矛盾，那么人类往往对这样的解释不太信任。然而，将人类的先验知识<a id="_idIndexMarker057"/>引入 ML 模型具有挑战性。但是对人类友好的可解释的 ML 系统应该试图提供围绕某些特征的解释，这些特征与结果具有直接的和不太复杂的关系，使得这种关系与终端用户先前的信念一致。</li>
			</ul>
			<p>例如，为了预测糖尿病的存在，血糖水平的测量具有直接关系，这与先前的人类信念一致。如果血糖水平高于平时，这可能表明患者患有糖尿病，尽管糖尿病也可能是由某些遗传因素或其他原因引起的。同样，高血糖水平也可能是暂时的，高血糖水平并不总是意味着患者患有糖尿病。但是由于解释与先验知识一致，终端用户将更信任这样的解释。</p>
			<ul>
				<li><strong class="bold">保真度</strong>:为<a id="_idIndexMarker058"/>提供整体解释的另一个关键因素是解释的真实性，也称为 ML 模型的保真度。高保真度的解释可以近似黑盒模型的整体预测，而低保真度的解释可以解释局部数据实例或数据的特定子集。例如，在进行销售预测时，仅根据历史数据的趋势提供解释并不能给出完整的情况，因为其他因素，如生产能力、市场竞争和客户需求，可能会影响模型的结果。保真起着关键作用，尤其是在进行详细的根本原因分析时，但是太多的细节可能对普通用户没有用处，除非被要求。</li>
				<li>抽象:好的人性化的解释总是以简洁和抽象的形式出现。过多的<a id="_idIndexMarker059"/>复杂的细节也会影响最终用户的体验。比如天气预报，如果模型预测大概率下雨，简洁抽象的解释可以是现在多云，当前位置 5 公里范围内下雨，那么有大概率<a id="_idIndexMarker060"/>下雨。</li>
			</ul>
			<p>但是，如果模型包括与降雨量、湿度和风速相关的细节，这对于降雨量的预测可能也很重要，那么这些额外的细节就很复杂，很难理解，因此对人类不友好。因此，好的、人性化的解释包括适当的抽象，以简化最终用户的理解。最终用户大多喜欢简洁的解释，但是在对模型预测进行根本原因分析时，可能需要详细的解释。</p>
			<ul>
				<li>对比解释:好的、人性化的解释<a id="_idIndexMarker061"/>不是关于理解模型的内部运作，而是关于比较<em class="italic">假设</em>场景。假设结果是连续的数字数据，就像回归问题一样。在这种情况下，对预测的一个好的解释包括与另一个明显高于或低于预测的实例进行比较。类似地，对分类问题的一个很好的解释是将当前预测与其他可能的结果进行比较。但是对比解释取决于应用，因为它需要一个比较点，尽管理解<em class="italic">假设</em>场景有助于我们理解某些关键特征的重要性以及这些特征如何与目标变量相关。</li>
			</ul>
			<p>例如，对于员工流失预测的用例，如果模型预测员工可能会离开组织，那么对比解释会通过将其与员工预计会留在组织中的实例预测进行比较，并比较用于对数据建模的关键特征值，来尝试证明模型的决策是正确的。因此，解释方法可能传达出，由于可能离开组织的员工的工资比可能留在组织中的员工的工资低得多，因此模型预测该员工预计会离开组织。</p>
			<ul>
				<li><strong class="bold">关注异常</strong>:这听起来可能有违直觉，但人类试图为意料之外、不明显的事件寻求解释<a id="_idIndexMarker063"/>。假设<a id="_idIndexMarker064"/>在数据中有一个异常的观察值，比如一个罕见的分类值或者一个异常的连续值，它们会影响结果。在这种情况下，应该包括在解释中。即使其他正常和一致的特征对模型结果具有相同的影响，仍然包括异常在对人类友好的解释方面具有更高的重要性。</li>
			</ul>
			<p>例如，假设我们根据汽车的配置来预测汽车的价格，假设运行模式是<em class="italic">电动</em>，这与<em class="italic">汽油</em>相比是一种罕见的观察。这两个类别可能对最终的模型预测有相同的影响。尽管如此，模型解释应该包括罕见的观察，因为最终用户对异常观察更感兴趣。</p>
			<ul>
				<li><strong class="bold">社会方面</strong>:模型可解释性的社会方面<a id="_idIndexMarker065"/>决定了抽象层次和解释的内容。社会方面取决于特定目标受众的理解水平，在模型可解释性方法中可能难以概括和介绍。例如，假设一个股票预测 ML 模型被设计来为用户规定行动，建议<em class="italic">做空</em>一只特定的股票。在这种情况下，财务领域之外的最终用户可能会觉得难以理解。但是，如果模型建议在不持有股票的情况下以当前价格卖出<em class="italic">股票，并在 1 个月后价格预计会下跌</em>时买回，任何非技术用户都可能很容易理解模型的建议。因此，好的解释会考虑社会方面，通常，以用户为中心的人机交互设计原则会被用来设计好的、可解释的、考虑社会方面的人工智能系统。</li>
			</ul>
			<p>现在我们已经对好的解释的关键标准有了一个公平的想法，在下一节，我们将讨论一些辅助标准<a id="_idIndexMarker068"/>,它们在构建可解释的 ML 系统时同样重要。</p>
			<h2 id="_idParaDest-27">XAI 对 ML 系统的辅助标准</h2>
			<p>好的解释并不局限于之前讨论的关键标准，而是有一些 XAI 的辅助标准，如<em class="italic">多希-维勒兹</em>和<em class="italic">金</em>在他们的工作<em class="italic">中所讨论的，以实现可解释机器学习的严格科学</em>:</p>
			<ul>
				<li><strong class="bold">无偏性</strong>:模型可解释性技术也应该寻找数据或模型中任何形式的偏差<a id="_idIndexMarker070"/>的存在。因此，XAI 的主要目标之一是使 ML 模型公正公平。例如，为了预测信用卡欺诈，可解释性方法应该调查与客户性别相关的人口统计信息对模型决策过程的重要性。如果性别信息的重要性很高，这意味着模型偏向于特定的性别。</li>
				<li><strong class="bold">隐私</strong>:解释方法<a id="_idIndexMarker071"/>应符合数据隐私措施，因此任何敏感信息不应用于模型解释。主要是为了提供个性化的解释，确保遵守数据隐私可能是非常重要的。</li>
				<li><strong class="bold">因果关系</strong>:模型可解释性方法<a id="_idIndexMarker072"/>应该尝试寻找任何因果关系，以便最终用户意识到，由于任何扰动，生产系统的模型预测可能会发生变化。</li>
				<li><strong class="bold">稳健性</strong>:敏感性<a id="_idIndexMarker073"/>分析等方法有助于理解模型预测相对于其特征值的稳健性和一致性。如果输入要素的微小变化导致模型预测的显著变化，则表明模型不稳健或不稳定。</li>
				<li><strong class="bold">信任</strong>:XAI 的关键目标之一是<a id="_idIndexMarker074"/>通过增加最终用户的信任来增加人工智能的采用。因此，所有可解释性方法都应该使黑盒 ML 模型更加透明和可解释，以便最终用户可以信任和依赖它们。如果解释方法不符合好的解释的标准，正如在<em class="italic">好的可解释的 ML 系统的标准</em>部分所讨论的，它可能无助于增加消费者的信任。</li>
				<li><strong class="bold">可用</strong> : XAI 的方法应该尽量让人工智能模型更可用。因此，它应该向用户提供信息<a id="_idIndexMarker075"/>来完成<a id="_idIndexMarker076"/>任务。例如，反事实解释可能会建议贷款申请人在接下来的两个月内按时支付信用卡账单，并在申请新的贷款之前还清以前的债务，以便他们的贷款申请不会被拒绝。</li>
			</ul>
			<p>接下来，我们需要理解评估可解释的 ML 系统的不同层次。</p>
			<h2 id="_idParaDest-28">可解释的 ML 系统的评价等级分类</h2>
			<p>既然我们已经讨论了设计和评估好的可解释的 ML 系统的关键标准<a id="_idIndexMarker077"/>,让我们讨论一下判断解释质量的评估方法的分类。在他们的工作<em class="italic">迈向可解释机器学习的严格科学</em>、<em class="italic">多希-维勒兹和</em>、T5】金中提到了三种主要的评估方法，我们将在本节中尝试理解它们。由于可解释的 ML 系统是用人机交互的以用户为中心的设计原则来设计的，人类评估真实任务在评估解释质量中起着核心作用。</p>
			<p>但是人工评估机制可能有其自身的挑战，例如不同类型的人工偏见，更耗费时间和其他资源，并且可能有其他复合因素导致不一致的评估。因此，人体评估实验应该设计良好，只在需要时使用，否则就不用。现在，让我们看看三种主要的评估方法:</p>
			<ul>
				<li><strong class="bold">基于应用的评估</strong>:这种评估方法包括在真实的产品或应用中包含<a id="_idIndexMarker078"/>可解释的技术<a id="_idIndexMarker079"/>，从而允许进行人体实验，其中真实的最终用户参与执行某些实验。尽管实验设置成本高且耗时，但构建一个几乎完成的产品，然后让领域专家进行测试还是有好处的。它将使研究者能够评估关于系统最终任务的解释的质量，从而提供快速识别可解释性方法的错误或限制的方法。这种评估原则与人机交互中使用的评估方法是一致的，可解释性被注入到整个系统中，负责解决用户的保持问题，并帮助用户达到其最终目标。</li>
			</ul>
			<p>例如，为了评估用于从图像中自动检测<a id="_idIndexMarker080"/>皮肤癌的人工智能软件的解释方法的质量，可以让皮肤科医生直接测试人工智能软件构建的目的。如果解释方法是成功的，那么这样的解决方案可以很容易地扩大规模。从工业的角度来看，由于获得一个完美的成品可能是耗时的，更好的方法<a id="_idIndexMarker081"/>是构建一个健壮的原型或者一个<strong class="bold">最小可行产品</strong> ( <strong class="bold"> MVP </strong>)，以便测试系统的领域专家更好地了解成品将会是什么样。</p>
			<ul>
				<li><strong class="bold">以人为基础的评估</strong>:这种评估方法包括进行人类主体<a id="_idIndexMarker082"/>实验，由非专家新手<a id="_idIndexMarker083"/>用户进行更直接的任务，而不是领域专家。聘请领域专家既费时又费钱，因此以人为基础的评估实验更容易进行，成本也更低。任务有时也会被简化，通常对于某些概括很重要的用例，这些方法非常有用。<strong class="bold"> A/B 测试</strong>、<strong class="bold">反事实模拟</strong>和<strong class="bold">正向模拟</strong>是用于<a id="_idIndexMarker085"/>人性化<a id="_idIndexMarker086"/>评估的某些流行评估<a id="_idIndexMarker084"/>方法。</li>
			</ul>
			<p>在 XAI，A/B 测试向用户提供<a id="_idIndexMarker087"/>种不同类型的解释<a id="_idIndexMarker088"/>，要求用户选择解释质量较高的最佳解释。然后，根据最终汇总的投票，并使用点击率、屏幕悬停时间和任务完成时间等其他指标，决定最佳方法。</p>
			<p>对于反事实模拟方法，向人类受试者呈现模型的输入和输出以及对一定数量的数据样本的模型解释，并要求人类受试者提供对输入特征的某些改变，以便将模型的最终结果改变为特定范围的值或特定类别。在正向模拟方法中，向人类受试者提供模型输入及其相应的解释方法，然后要求他们模拟模型预测，而不看地面真实值。然后，用于找出人类预测结果与基本事实标签之间的差异的误差度量可以用作评估解释质量的定量方法。</p>
			<ul>
				<li><strong class="bold">基于功能的评估</strong>:这种评估方法不涉及任何人类<a id="_idIndexMarker089"/>主体实验，使用代理<a id="_idIndexMarker090"/>任务来评估讲解的质量。这些实验<a id="_idIndexMarker091"/>比其他两个更可行，成本也更低，特别是对于人体实验受到限制和不道德的情况，这是一种替代方法。当这种类型的算法已经在人类水平的评估中测试过时，这种方法工作得很好。</li>
			</ul>
			<p>例如，线性回归模型易于解释，最终用户可以有效地理解模型的工作原理。因此，对销售预测等用例使用线性回归模型可以帮助我们了解历史数据的总体趋势以及预测值与趋势的关系。</p>
			<p><em class="italic">图 1.9 </em>总结了可解释 ML 系统评估级别的分类:</p>
			<div><div><img src="img/B18216_01_09.jpg" alt="Figure 1.9 – Taxonomy of evaluation level for the explainable ML system&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.9-可解释的 ML 系统的评估级别分类</p>
			<p>除了这里讨论的方法<a id="_idIndexMarker092"/>之外，为了确定解释的质量，还经常使用其他指标，如解释的描述长度、解释中使用的特征的复杂性以及理解所提供的解释所需的认知处理时间。我们将在<a href="B18216_11_ePub.xhtml#_idTextAnchor217"> <em class="italic">第十一章</em> </a>，<em class="italic">以最终用户为中心的人工智能</em>中更详细地讨论它们。</p>
			<p>在这一章中，到目前为止，你已经遇到了许多 ML 可解释性技术的新概念。图 1.10 中的思维导图很好地总结了本章中讨论的各种术语和概念:</p>
			<div><div><img src="img/B18216_01_10.jpg" alt="Figure 1.10 – Mind-map diagram of machine learning explainability techniques&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图 1.10-机器学习可解释技术的思维导图</p>
			<p>我强烈建议你们所有人熟悉这个思维导图中使用的术语，因为我们将在整本书中使用它！让我们总结一下我们在总结部分讨论的内容。</p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor031"/>总结</h1>
			<p>读完这一章，你现在应该明白什么是 XAI，为什么它如此重要。你已经学习了与可解释性技术相关的各种术语和概念，我们将在本书中经常用到。你还学习了人类友好的可解释 ML 系统的某些关键标准，以及评估可解释技术质量的不同方法。在下一章，我们将关注结构化和非结构化数据的各种模型解释方法。</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor032"/>参考文献</h1>
			<p>请参考以下资源以获取更多信息:</p>
			<ul>
				<li><em class="italic">价值 3 亿美元的触发器:房地产网站 Zillow 的旁门左道是如何严重出错的</em>、【theguardian.com】和:<a href="https://www.theguardian.com/business/2021/nov/04/zillow-homes-buying-selling-flip-flop">https://www . The guardian . com/business/2021/nov/04/Zillow-homes-buying-selling-flip-flop</a></li>
				<li>Zillow 表示将关闭房屋购买业务，裁员 25%;收益错过估计，<em class="italic">cnbc.com</em>:<a href="https://www.cnbc.com/2021/11/02/zillow-shares-plunge-after-announcing-it-will-close-home-buying-business.html">https://www . CNBC . com/2021/11/02/zillow-shares-pull-after-declaration-it-close-home-buying-business . html</a></li>
				<li><em class="italic"> Google Photos 通过面部识别软件将两名非裔美国人标记为大猩猩</em>、【forbes.com】和:<a href="https://www.forbes.com/sites/mzhang/2015/07/01/google-photos-tags-two-african-americans-as-gorillas-through-facial-recognition-software/">https://www . Forbes . com/sites/mzhang/2015/07/01/Google-Photos-Tags-Two-African-Americans-As-Gorillas-Through-face-Recognition-Software/</a></li>
				<li><em class="italic">谷歌为照片应用的种族主义错误道歉</em>，【bbc.com】T2:【https://www.bbc.com/news/technology-33347866】T4</li>
				<li><em class="italic">可解释机器学习教程，MICCAI'18，作者 Samek 和 Binder</em>:【http://www.heatmapping.org/slides/2018_MICCAI.pdf T2】</li>
				<li><a href="B18216_01_ePub.xhtml#_idTextAnchor014"> <em class="italic">第一章</em></a><em class="italic">–</em><em class="italic">解释、可解释性、可说明性以及为什么这一切都很重要？</em>作者<em class="italic"> Serg Masis，Packt 出版有限公司:</em><a href="https://www.amazon.com/Interpretable-Machine-Learning-Python-hands/dp/180020390X">https://www . Amazon . com/Interpretable-Machine-Learning-Python-hands/DP/180020390 x</a></li>
				<li><em class="italic">可解释机器学习</em>，<em class="italic">克里斯托夫·莫尔纳尔:</em>，【https://christophm.github.io/interpretable-ml-book/】T4</li>
				<li>多希-维勒兹和金:【https://arxiv.org/abs/1702.08608】T2</li>
			</ul>
		</div>
	

</body></html>