

# *第四章*:从渐变 Boosting 到 XGBoost

XGBoost 是一种独特的梯度增强形式，具有几个明显的优势，这将在第五章*中解释。为了理解 XGBoost 相对于传统梯度增强的优势，您必须首先了解传统梯度增强的工作原理。XGBoost 结合了传统梯度增强的一般结构和超参数。在这一章中，你将发现渐变增强背后的力量，这是 XGBoost 的核心。*

在本章中，您将从头构建梯度增强模型，然后将梯度增强模型和误差与以前的结果进行比较。特别是，您将关注于**学习率**超参数，以构建包含 XGBoost 的强大梯度增强模型。最后，您将预览一个关于系外行星的案例研究，强调对更快算法的需求，这是大数据世界的一个关键需求，XGBoost 满足了这一需求。

在本章中，我们将讨论以下主要话题:

*   从装袋到助推

*   梯度增强的工作原理

*   修改梯度增强超参数

*   接近大数据—梯度提升与 XGBoost

# 技术要求

本章的代码可从 https://github . com/packt publishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/chapter 04 获得。

# 从装袋到增压

在 [*第三章*](B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070) 、*用随机森林装袋*中，你了解了为什么随机森林等集成机器学习算法通过将许多机器学习模型组合成一个模型来做出更好的预测。随机森林被归类为 bagging 算法，因为它们采用自举样本(决策树)的集合。

相比之下，Boosting 从单个树的错误中学习。总的思想是基于先前树的误差来调整新的树。

在 boosting 中，纠正每个新树的错误是一种不同于 bagging 的方法。在套袋模型中，新的树不会注意以前的树。此外，新的树是使用 bootstrapping 从零开始构建的，最终的模型聚集了所有的单个树。然而，在 boosting 中，每个新树都是从以前的树构建的。树木不是孤立存在的；相反，它们是建立在彼此之上的。

## 【AdaBoost 简介

**AdaBoost** 是最早也是最受欢迎的助推模型之一。在 AdaBoost 中，每棵新树都会根据先前树的误差调整其权重。通过调整以更高百分比影响这些样本的权重，更多地关注出错的预测。通过从错误中学习，AdaBoost 可以将弱学习者转变为强学习者。弱学习者是一种机器学习算法，其表现几乎不比机会好。相比之下，更强的学习者从数据中学到了相当多的东西，并且表现得相当好。

boosting 算法背后的一般思想是将弱学习者转化为强学习者。学习能力差的人比随机猜测好不了多少。但在这个疲软的开端背后有一个目的。基于这一总体思路，boosting 通过关注迭代误差修正来工作，*而不是*通过建立一个强大的基线模型。如果基础模型过于强大，学习过程必然会受到限制，从而破坏增强模型背后的总体策略。

弱学习者通过数百次迭代转化为强学习者。从这个意义上说，一个小优势可以走很长的路。事实上，在过去几十年里，就产生最佳结果而言，boosting 一直是最好的通用机器学习策略之一。

对 AdaBoost 的详细研究超出了本书的范围。像许多 scikit-learn 模型一样，在实践中实现 AdaBoost 很简单。`AdaBoostRegressor`和`AdaBoostClassifier`算法可以从`sklearn.ensemble`库下载，适合任何训练集。最重要的 AdaBoost 超参数是`n_estimators`，创建一个强学习器所需的树(迭代)数。

注意

有关 AdaBoost 的更多信息，请查看官方文档:https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . adaboostclassifier . html 分类器和[https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . adaboostregressor . html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)

我们现在将转向梯度增强，它是 AdaBoost 的强大替代方案，在性能上略有优势。

## 区分梯度推进

梯度增强使用不同于 AdaBoost 的方法。虽然梯度提升也基于错误的预测进行调整，但它将这一想法向前推进了一步:梯度提升完全基于前一棵树的预测误差来适应每一棵新树。也就是说，对于每个新的树，梯度推进查看错误，然后完全围绕这些错误构建新的树。新树不关心已经正确的预测。

建立一个只关注误差的机器学习算法需要一个综合的方法，对误差求和，以做出准确的最终预测。这种方法利用残差，即模型预测值和实际值之间的差异。大致的想法是这样的:

*梯度增强计算每棵树预测的残差，并对所有残差求和以对模型进行评分。*

理解**计算**和**求和残差**是很重要的，因为这个想法是 XGBoost 的核心，是梯度增强的高级版本。当您构建自己版本的梯度增强时，计算和求和残差的过程将变得清晰。在下一节中，您将构建自己版本的渐变增强模型。首先，我们来详细了解一下梯度提升的工作原理。

# 渐变增强的工作原理

在这一节中，我们将看看梯度推进的内幕，并通过在先前树的错误上训练新树来从头构建梯度推进模型。这里的关键数学概念是残差。接下来，我们将使用 scikit-learn 的梯度增强算法获得相同的结果。

## 残差

残差是给定模型的误差和预测值之间的差。在统计学中，通常分析残差来确定给定的线性回归模型与数据的拟合程度。

考虑下面的例子:

1.  自行车租赁

    a) *预测* : 759

    b) *结果* : 799

    c) *残差* : 799 - 759 = 40

2.  收入

    a) *预测* : 10 万

    b) *结果*:88000

    c) *剩余*:88000–100000 =-12000

如您所见，残差告诉您模型的预测与现实有多远，它们可能是正的，也可能是负的。

以下是一个显示**线性回归**线残差的直观示例:

![Figure 4.1 – Residuals of a linear regression line](img/B15551_04_01.jpg)

图 4.1-线性回归线的残差

线性回归的目标是最小化残差的平方。如图所示，残差直观地显示了直线与数据的拟合程度。在统计中，通常通过绘制残差来进行线性回归分析，以便更深入地了解数据。

为了从头开始构建梯度推进算法，我们将计算每棵树的残差，并为残差拟合新的模型。我们现在就开始吧。

## 学习如何从头开始构建梯度推进模型

从头构建一个渐变增强模型会让你更深入地理解渐变增强在代码中是如何工作的。在建立模型之前，我们需要访问数据，并为机器学习做准备。

### 处理自行车租赁数据集

我们继续使用自行车租赁数据集来比较新款与旧款:

1.  我们将从导入`pandas`和`numpy`开始。我们还将添加一行来隐藏任何警告:

    ```
    import pandas as pd
    import numpy as np
    import warnings
    warnings.filterwarnings('ignore')
    ```

2.  现在，加载`bike_rentals_cleaned`数据集并查看前五行:

    ```
    df_bikes = pd.read_csv('bike_rentals_cleaned.csv')
    df_bikes.head()
    ```

    您的输出应该如下所示:

    ![Figure 4.2 – First five rows of Bike Rental Dataset](img/B15551_04_02.jpg)

    图 4.2-自行车租赁数据集的前五行

3.  现在，将数据分成`X`和`y`。然后，将`X`和`y`分成训练集和测试集:

    ```
    X_bikes = df_bikes.iloc[:,:-1]
    y_bikes = df_bikes.iloc[:,-1]
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)
    ```

是时候从零开始构建一个梯度提升模型了！

### 从头开始构建梯度增强模型

下面是从头开始构建梯度推进机器学习模型的步骤:

1.  将数据拟合到决策树:您可以使用一个决策树树桩，它的`max_depth`值为`1`，或者使用一个`max_depth`值为`2`或`3`的决策树。最初的决策树，称为`max_depth=2`，将其作为`tree_1`放在训练集上，因为它是我们集合中的第一棵树:

    ```
    from sklearn.tree import DecisionTreeRegressor
    tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)
    tree_1.fit(X_train, y_train)
    ```

2.  使用训练集进行预测:梯度增强中的预测最初是使用训练集进行的，而不是使用测试集进行预测。为什么？为了计算残差，我们需要在训练阶段比较预测。模型构建的测试阶段在所有的树构建完成之后结束。第一轮训练集的预测是通过将`predict`方法添加到`tree_1`并以`X_train`作为输入获得的:

    ```
    y_train_pred = tree_1.predict(X_train)
    ```

3.  计算残差:残差是预测和目标列之间的差。从目标列`y_train`中减去`X_train`的预测，这里定义为`y_train_pred`，以计算残差:

    ```
    y2_train = y_train - y_train_pred
    ```

    注意

    残差被定义为`y2_train`，因为它们是下一棵树的新目标列。

4.  在残差上拟合新树:在残差上拟合新树不同于在训练集上拟合模型。主要的区别在于预测。在自行车租赁数据集中，当在残差上拟合新树时，我们应该逐渐得到更小的数字。

    初始化一个新树，并将其拟合到`X_train`和残差`y2_train`:

    ```
    tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)
    tree_2.fit(X_train, y2_train)
    ```

5.  重复步骤 2-4:随着过程的继续，残差应该从正负方向逐渐接近`0`。针对估计器的数量`n_estimators`继续迭代。

    让我们对第三棵树重复这个过程，如下所示:

    ```
    y2_train_pred = tree_2.predict(X_train)
    y3_train = y2_train - y2_train_pred
    tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)
    tree_3.fit(X_train, y3_train)
    ```

    这个过程可能会持续几十、几百或几千棵树。在正常情况下，你肯定会坚持下去。要把一个弱学习者转变成强学习者，需要的不仅仅是几棵树。因为我们的目标是理解梯度增强是如何在幕后工作的，然而，我们现在将继续讨论这个基本概念。

6.  对结果求和:对结果求和需要使用测试集对每棵树进行预测，如下所示:

    ```
    y1_pred = tree_1.predict(X_test)
    y2_pred = tree_2.predict(X_test)
    y3_pred = tree_3.predict(X_test)
    ```

    由于预测是正负差，因此对预测求和应该会得到更接近目标列的预测，如下所示:

    ```
    y_pred = y1_pred + y2_pred + y3_pred
    ```

7.  最后，让我们计算**均方误差** ( **MSE** )以获得如下结果:

    ```
    from sklearn.metrics import mean_squared_error as MSE
    MSE(y_test, y_pred)**0.5
    ```

    以下是预期的输出:

    ```
    911.0479538776444
    ```

对于一个还不够强的弱学习者来说，这还不错！现在让我们尝试使用 scikit-learn 获得相同的结果。

## 在 scikit-learn 中构建梯度增强模型

让我们看看我们是否能使用 scikit-learn 的`GradientBoostingRegressor`获得与上一节相同的结果。这个可以通过一些超参数调整来完成。使用`GradientBoostingRegressor`的优点是构建更快，实现更容易:

1.  首先，从`sklearn.ensemble`库中导入回归量:

    ```
    from sklearn.ensemble import GradientBoostingRegressor
    ```

2.  初始化`GradientBoostingRegressor`时，有几个重要的超参数。为了获得相同的结果，必须匹配`max_depth=2`和`random_state=2`。再者，既然只有三棵树，就要有`n_estimators=3`。最后，我们必须设置`learning_rate=1.0`超参数。我们很快就会对`learning_rate`有更多的了解:

    ```
    gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=2, learning_rate=1.0)
    ```

3.  既然已经初始化了模型，那么就可以根据训练数据对其进行拟合，并根据测试数据对进行评分:

    ```
    gbr.fit(X_train, y_train)
    y_pred = gbr.predict(X_test)
    MSE(y_test, y_pred)**0.5
    ```

    结果如下:

    ```
    911.0479538776439
    ```

    结果都一样到小数点后 11 位！

    回想一下，梯度推进的要点是建立一个具有足够多的树的模型，以将弱学习者转化为强学习者。这很容易通过将迭代次数`n_estimators`改为一个更大的数来实现。

4.  让我们用 30 个估计量构建一个梯度推进回归方程并进行评分:

    ```
    gbr = GradientBoostingRegressor(max_depth=2, n_estimators=30, random_state=2, learning_rate=1.0)
    gbr.fit(X_train, y_train)
    y_pred = gbr.predict(X_test)
    MSE(y_test, y_pred)**0.5
    ```

    结果如下:

    ```
    857.1072323426944
    ```

    分数是一个进步。现在让我们来看看 300 个评估者:

    ```
    gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0)
    gbr.fit(X_train, y_train)
    y_pred = gbr.predict(X_test)
    MSE(y_test, y_pred)**0.5
    ```

    结果是这样的:

    ```
    936.3617413678853
    ```

这是一个惊喜！分数变差了！我们被误导了吗？梯度推进并不像它所宣传的那样好吗？

每当你得到一个惊喜的结果，值得仔细检查代码。现在，我们没有多说什么就改变了`learning_rate`。那么，如果我们删除`learning_rate=1.0`并使用 scikit-learn 默认值，会发生什么呢？

让我们来看看:

```
gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2)
gbr.fit(X_train, y_train)
y_pred = gbr.predict(X_test)
MSE(y_test, y_pred)**0.5
```

结果是这样的:

```
653.7456840231495
```

难以置信！通过对`learning_rate`超参数使用 scikit-learn 默认值，分数从`936`变为`654`。

在下一节中，我们将详细了解不同的梯度增强超参数，重点是`learning_rate`超参数。

# 修改梯度增强超参数

在本节中，我们将重点关注`learning_rate`，最重要的梯度增强超参数，除了`n_estimators`，模型中迭代或树的数量。我们还将调查一些树超参数和`subsample`，这导致了`RandomizedSearchCV`，并将结果与 XGBoost 进行比较。

## 学习 _ 速率

在最后的部分，将`GradientBoostingRegressor`的`learning_rate`值从`1.0`改为 scikit-learn 的默认值`0.1`，获得了巨大的收益。

`learning_rate`，也被称为*收缩*的，收缩个体树的贡献，以便在建立模型时没有树有太大的影响。如果整个集成是根据一个基础学习者的错误构建的，而没有仔细调整超参数，则模型中的早期树会对后续发展产生太大影响。`learning_rate`限制个别树木的影响。一般来说，随着`n_estimators`，树木的数量上升，`learning_rate`应该会下降。

确定最佳的`learning_rate`值需要改变`n_estimators`。首先，让我们保持`n_estimators`不变，看看`learning_rate`自己做了什么。`learning_rate`范围从`0`到`1`。`learning_rate`值为`1`意味着不进行调整。`0.1`的默认值表示树的影响权重为 10%。

以下是一个合理的范围:

`learning_rate_values = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 1.0]`

接下来，我们将通过构建一个新的`GradientBoostingRegressor`并对其进行评分来遍历这些值，以查看分数的比较情况:

```
for value in learning_rate_values:
    gbr = GradientBoostingRegressor(max_depth=2,   n_estimators=300, random_state=2, learning_rate=value)
    gbr.fit(X_train, y_train)
    y_pred = gbr.predict(X_test)
    rmse = MSE(y_test, y_pred)**0.5
    print('Learning Rate:', value, ', Score:', rmse)
```

学习率值和分数如下:

```
Learning Rate: 0.001 , Score: 1633.0261400367258
Learning Rate: 0.01 , Score: 831.5430182728547
Learning Rate: 0.05 , Score: 685.0192988749717
Learning Rate: 0.1 , Score: 653.7456840231495
Learning Rate: 0.15 , Score: 687.666134269379
Learning Rate: 0.2 , Score: 664.312804425697
Learning Rate: 0.3 , Score: 689.4190385930236
Learning Rate: 0.5 , Score: 693.8856905068778
Learning Rate: 1.0 , Score: 936.3617413678853
```

正如从输出中看到的，`0.1`的缺省值`learning_rate`给出了 300 棵树的最佳分数。

现在让我们改变一下`n_estimators`。使用前面的代码，我们可以生成包含 30、300 和 3000 棵树的`n_estimators`的`learning_rate`图，如下图所示:

![Figure 4.3 – learning_rate plot for 30 trees](img/B15551_04_03.jpg)

图 4.3–30 棵树的学习率图

正如你所见，有 30 棵树时，`learning_rate`值在`0.3`左右达到峰值。

现在，让我们来看看 3000 棵树的`learning_rate`图:

![Fig 4.4 -- learning_rate plot for 3,000 trees](img/B15551_04_04.jpg)

图 4.4-3000 棵树的学习率图

对于 3000 棵树，`learning_rate`值在第二个值处达到峰值，该值被给定为`0.05`。

这些图表强调了一起调整`learning_rate`和`n_estimators`的重要性。

## 基础学员

梯度推进回归器中的初始决策树被称为**基学习器**，因为它位于集成的基础。它是这个过程中的第一个学习者。术语*学习者*在这里表示*弱学习者*转变为*强学习者*。

虽然基础学习者不需要为了准确性而进行微调，如第二章 、*决策树*中所述，但是调整基础学习者来提高准确性是完全可能的。

例如，我们可以选择`1`、`2`、`3`或`4`的`max_depth`值，并按如下方式比较结果:

```
depths = [None, 1, 2, 3, 4]
for depth in depths:
    gbr = GradientBoostingRegressor(max_depth=depth, n_estimators=300, random_state=2)
    gbr.fit(X_train, y_train)
    y_pred = gbr.predict(X_test)
    rmse = MSE(y_test, y_pred)**0.5
    print('Max Depth:', depth, ', Score:', rmse) 
```

结果如下:

```
Max Depth: None , Score: 867.9366621617327
Max Depth: 1 , Score: 707.8261886858736
Max Depth: 2 , Score: 653.7456840231495
Max Depth: 3 , Score: 646.4045923317708
Max Depth: 4 , Score: 663.048387855927
```

`3`的`max_depth`值给出最佳结果。

其他基础学习者超参数，如 [*第 2 章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) 、*决策树深度*中所述，可以以类似的方式进行调整。

## 子样本

`subsample`是样本的子集。因为样本是行，所以行的子集意味着在构建每棵树时，所有的行可能都没有被包括在内。通过将`subsample`从`1.0`更改为更小的小数，树在构建阶段只选择该百分比的样本。例如，`subsample=0.8`将为每棵树选择 80%的样本。

继续`max_depth=3`，我们尝试一系列子样本百分比来改善结果:

```
samples = [1, 0.9, 0.8, 0.7, 0.6, 0.5]
for sample in samples:
    gbr = GradientBoostingRegressor(max_depth=3, n_estimators=300, subsample=sample, random_state=2)
    gbr.fit(X_train, y_train)
    y_pred = gbr.predict(X_test)
    rmse = MSE(y_test, y_pred)**0.5
    print('Subsample:', sample, ', Score:', rmse)
```

结果如下:

```
Subsample: 1 , Score: 646.4045923317708
Subsample: 0.9 , Score: 620.1819001443569
Subsample: 0.8 , Score: 617.2355650565677
Subsample: 0.7 , Score: 612.9879156983139
Subsample: 0.6 , Score: 622.6385116402317
Subsample: 0.5 , Score: 626.9974073227554
```

拥有 300 棵树的`0.7`的`subsample`值和`3`的`max_depth`产生了迄今为止最好的分数。

当`subsample`不等于`1.0`时，模型被归类为**随机梯度下降**，其中*随机*表示模型中固有一些随机性。

## 随机试验

我们有一个好的工作模型，但是我们还没有执行网格搜索，如 [*第 2 章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) 、*决策树深度*所述。我们的初步分析表明，以`max_depth=3`、`subsample=0.7`、`n_estimators=300`和`learning_rate = 0.1`为中心的网格搜索是一个很好的起点。我们已经表明，随着`n_estimators`上升，`learning_rate`应该下降:

1.  这里是一个可能的起点:

    ```
    params={'subsample':[0.65, 0.7, 0.75],
            'n_estimators':[300, 500, 1000],
             'learning_rate':[0.05, 0.075, 0.1]}
    ```

    由于`n_estimators`从起始值 300 开始上升，`learning_rate`从`0.1`的起始值开始下降。让我们保留`max_depth=3`来限制方差。

    对于 27 种可能的超参数组合，我们使用`RandomizedSearchCV`来尝试其中的 10 种组合，希望找到一个好的模型。

    注意

    虽然`GridSearchCV`有 27 种组合是可行的，但在某种程度上，你会有太多的可能性，而`RandomizedSearchCV`将变得必不可少。我们在这里使用`RandomizedSearchCV`来练习和加速计算。

2.  让我们导入`RandomizedSearchCV`并初始化一个梯度推进模型:

    ```
    from sklearn.model_selection import RandomizedSearchCV
    gbr = GradientBoostingRegressor(max_depth=3, random_state=2)
    ```

3.  接下来，用`gbr`和`params`初始化`RandomizedSearchCV`，除了迭代次数、得分和折叠次数。回想一下，`n_jobs=-1`可以加速计算，`random_state=2`可以保证结果的一致性；

    ```
    rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, random_state=2)
    ```

4.  现在，在训练集上拟合模型,并获得最佳参数和分数:

    ```
    rand_reg.fit(X_train, y_train)
    best_model = rand_reg.best_estimator_
    best_params = rand_reg.best_params_
    print("Best params:", best_params)
    best_score = np.sqrt(-rand_reg.best_score_)
    print("Training score: {:.3f}".format(best_score))
    y_pred = best_model.predict(X_test)
    rmse_test = MSE(y_test, y_pred)**0.5
    print('Test set score: {:.3f}'.format(rmse_test))
    ```

    结果如下:

    ```
    Best params: {'learning_rate': 0.05, 'n_estimators': 300, 'subsample': 0.65}
    Training score: 636.200
    Test set score: 625.985
    ```

    从这里开始，单独或成对地改变参数是值得尝试的。尽管目前最好的模型有`n_estimators=300`，但是通过仔细调整`learning_rate`值，提高这个超参数肯定会获得更好的结果。`subsample`也许可以拿来做实验。

5.  经过几轮实验，我们获得了以下模型:

    ```
    gbr = GradientBoostingRegressor(max_depth=3, n_estimators=1600, subsample=0.75, learning_rate=0.02, random_state=2)
    gbr.fit(X_train, y_train)
    y_pred = gbr.predict(X_test)
    MSE(y_test, y_pred)**0.5 
    ```

    结果如下:

    ```
    596.9544588974487
    ```

在`1600`的`n_estimators`值较大，在`0.02`的`learning_rate`值较小，在`0.75`的值与`subsample`值相当，在`3`的`max_depth`值相同的情况下，我们得到了最佳的`597`。

也许可以做得更好。我们鼓励你去尝试！

现在，让我们看看 XGBoost 与梯度增强有什么不同，它使用的是到目前为止介绍过的相同超参数。

## XGBoost

XGBoost 是梯度提升的高级版本，具有相同的一般结构，这意味着它通过对树的残差求和，将弱学习器转换为强学习器。

超参数与上一节的唯一区别是 XGBoost 将`learning_rate`称为`eta`。

让我们用相同的超参数构建一个 XGBoost 回归器来比较结果。

从`xgboost`导入`XGBRegressor`，然后对模型进行如下初始化和评分:

```
from xgboost import XGBRegressor
xg_reg = XGBRegressor(max_depth=3, n_estimators=1600, eta=0.02, subsample=0.75, random_state=2)
xg_reg.fit(X_train, y_train)
y_pred = xg_reg.predict(X_test)
MSE(y_test, y_pred)**0.5
```

结果是这样的:

```
584.339544309016
```

分数更好。至于分数为什么更好的原因将在下一章揭晓， [*第五章*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117) ， *XGBoost 揭开*。

准确性和速度是建立机器学习模型时最重要的两个概念，我们已经多次证明 XGBoost 非常准确。XGBoost 通常优于梯度增强，因为它始终能提供更好的结果，而且速度更快，如下面的案例研究所示。

# 走近大数据——梯度推进与 XGBoost

在现实世界中，数据集可能非常庞大，有数万亿个数据点。由于一台机器的资源有限，将工作限制在一台计算机上可能是不利的。在处理大数据时，云通常用于利用并行计算机。

当数据集突破计算极限时，它就变得很大。到目前为止，在本书中，通过将数据集限制为具有一百或更少列的数万行，应该没有显著的时间延迟，除非您遇到错误(每个人都会遇到)。

在这一节中，我们将考察**系外行星**随时间的变化。该数据集有 5087 行和 3189 列，记录了恒星生命周期不同时间的光通量。将列和行相乘得到 150 万个数据点。使用 100 棵树的基线，我们需要 1.5 亿个数据点来建立一个模型。

在这一部分，我的 2013 款 MacBook Air 的等待时间约为 5 分钟。新电脑应该更快。我选择了系外行星数据集，这样等待时间在不占用你的电脑很长时间的情况下发挥了重要作用。

## 介绍系外行星数据集

系外行星数据集取自 Kaggle，日期大约在 2017 年:[https://www . ka ggle . com/keplers machines/Kepler-labelled-time-series-data](https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data)。数据集包含关于恒星光线的信息。每一行都是一颗独立的星星，每一列都显示出不同的光线模式。除了光线模式，如果恒星拥有一颗系外行星，一颗系外行星列也被标记为`2`；否则贴上`1`的标签。

数据集记录了来自数千颗恒星的光通量。**光通量**，通常被称为**光通量**，是恒星被感知的亮度。

注意

感知亮度不同于实际亮度。例如，非常远的非常亮的恒星可能光通量很小(看起来很暗)，而非常近的中等亮度的恒星，如太阳，可能光通量很大(看起来很亮)。

当一颗恒星的光通量周期性变化时，这颗恒星可能正在被一颗系外行星围绕旋转。假设是，当一颗系外行星在一颗恒星前面运行时，它会阻挡一小部分光线，使感知到的亮度略微降低。

小费

发现系外行星是罕见的。关于一颗恒星是否拥有系外行星的预测栏很少有正面的案例，这导致了不平衡的数据集。不平衡的数据集需要额外的预防措施。我们将在 [*第七章*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161) 、*用 XGBoost* 发现系外行星中涉及不平衡数据集，在那里我们将更详细地讨论这个数据集。

接下来，让我们访问系外行星数据集，并为机器学习做准备。

## 预处理系外行星数据集

系外行星数据集已经上传到我们的 GitHub 页面，网址为[https://GitHub . com/packt publishing/Hands-On-Gradient-Boosting-with-XG boost-and-Scikit-learn/tree/master/chapter 04](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter04)。

以下是为机器学习加载和预处理系外行星数据集的步骤:

1.  将`exoplanets.csv`下载到 Jupyter 笔记本所在的文件夹中。然后，打开文件看一看:

    ```
    df = pd.read_csv('exoplanets.csv')
    df.head() 
    ```

    数据帧将如下图所示:

    ![Fig 4.5 – Exoplanet DataFrame](img/B15551_04_05.jpg)

    图 4.5-系外行星数据帧

    由于篇幅限制，没有显示所有列。通量列是浮动的，而`Label`列是系外行星恒星的`2`和非系外行星恒星的`1`。

2.  让我们确认所有的列都是带有`df.info()`的数字:

    ```
    df.info()
    ```

    结果如下:

    ```
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 5087 entries, 0 to 5086
    Columns: 3198 entries, LABEL to FLUX.3197
    dtypes: float64(3197), int64(1)
    memory usage: 124.1 MB
    ```

    从输出中可以看到，`3197`列是浮点型的，`1`列是 T2，所以所有列都是数字型的。

3.  现在，让我们用下面的代码来确认空值的数量:

    ```
    df.isnull().sum().sum()
    ```

    输出如下所示:

    ```
    0
    ```

    输出显示没有空值。

4.  由于所有的列都是数值型的，没有空值，我们可以将数据分成训练集和测试集。注意，第 0 列是目标列`y`，所有其他列是预测列`X`:

    ```
    X = df.iloc[:,1:]
    y = df.iloc[:,0]
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)
    ```

是时候建立一个梯度推进分类器来预测恒星是否存在系外行星了。

## 构建梯度增强分类器

梯度推进分类器的工作方式与梯度推进回归器相同。区别主要在于得分。

除了`accuracy_score`之外，让我们从导入`GradientBoostingClassifer`和`XGBClassifier`开始，这样我们可以比较这两个模型:

```
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
```

接下来，我们需要一种使用计时器来比较模型的方法。

## 计时模式

Python 附带了一个`time`库，可以用来标记时间。总的想法是在计算前后标记时间。这些时间之间的差异告诉我们计算花费了多长时间。

`time`库导入如下:

```
import time
```

在`time`库中，`.time()`方法以秒为单位标记时间。

例如，通过使用`time.time()`分配计算前后的开始和结束时间，查看运行`df.info()`需要多长时间:

```
start = time.time()
df.info()
end = time.time()
elapsed = end - start
print('\nRun Time: ' + str(elapsed) + ' seconds.')
```

输出如下:

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5087 entries, 0 to 5086
Columns: 3198 entries, LABEL to FLUX.3197
dtypes: float64(3197), int64(1)
memory usage: 124.1 MB
```

运行时如下:

```
Run Time: 0.0525362491607666 seconds.
```

你的结果会与我们的不同，但希望是在同一范围内。

现在让我们使用前面的代码标记时间，将`GradientBoostingClassifier`和`XGBoostClassifier`与系外行星数据集进行速度比较。

小费

Jupyter 笔记本自带魔法功能，在命令前用`%`符号表示。`%timeit`就是这样一个神奇的功能。`%timeit`不是计算运行一次代码需要多长时间，而是计算多次运行代码需要多长时间。关于魔法功能的更多信息，请参见[ipython.readthedocs.io/en/stable/interactive/magics.html](http://ipython.readthedocs.io/en/stable/interactive/magics.html)。

## 比较速度

是时候让`GradientBoostingClassifier`和`XGBoostClassifier`与系外行星数据集赛跑了。我们设置了`max_depth=2`和`n_estimators=100`来限制模型的大小。先说`GradientBoostingClassifier`:

1.  首先，我们将标记开始时间。在建立和评分模型后，我们将标记结束时间。根据您的计算机速度，以下代码可能需要大约 5 分钟的时间来运行:

    ```
    start = time.time()
    gbr = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=2)
    gbr.fit(X_train, y_train)
    y_pred = gbr.predict(X_test)
    score = accuracy_score(y_pred, y_test)
    print('Score: ' + str(score))
    end = time.time()
    elapsed = end - start
    print('\nRun Time: ' + str(elapsed) + ' seconds')
    ```

    结果是这样的:

    ```
    Score: 0.9874213836477987
    Run Time: 317.6318619251251 seconds
    ```

    在我的 2013 款 MacBook Air 上运行花了 5 分多钟。对于旧电脑上的 1.5 亿个数据点来说，这已经不错了。

    注意

    虽然 98.7%的分数通常在准确性方面非常突出，但不平衡数据集的情况并非如此，正如您将在第七章 、*使用 XGBoost* 发现系外行星中看到的那样。

2.  接下来，我们将使用相同的超参数构建一个`XGBClassifier`模型，并以相同的方式标记时间:

    ```
    start = time.time()
    xg_reg = XGBClassifier(n_estimators=100, max_depth=2, random_state=2)
    xg_reg.fit(X_train, y_train)
    y_pred = xg_reg.predict(X_test)
    score = accuracy_score(y_pred, y_test)
    print('Score: ' + str(score))
    end = time.time()
    elapsed = end - start
    print('Run Time: ' + str(elapsed) + ' seconds')
    ```

    结果如下:

    ```
    Score: 0.9913522012578616
    Run Time: 118.90568995475769 seconds
    ```

在我的 2013 款 MacBook Air 上，XGBoost 用时不到 2 分钟，速度提高了一倍多。也精确了半个百分点。

当涉及到大数据时，两倍快的算法可以节省数周或数月的计算时间和资源。在大数据领域，这一优势是巨大的。

在 boosting 领域，XGBoost 由于其无与伦比的速度和令人印象深刻的准确性而成为首选模型。

至于系外行星数据集，将在第七章 、*使用 XGBoost* 发现系外行星中再次讨论，这是一个重要的案例研究，揭示了处理不平衡数据集的挑战以及应对这些挑战的各种潜在解决方案。

注意

最近买了一台 2020 的 MacBook Pro，更新了所有软件。使用相同代码的时间差异是惊人的:

梯度推进运行时间:197.38 秒

XGBoost 运行时间:8.66 秒

相差超过 10 倍！

# 总结

在本章中，您学习了装袋和增压的区别。通过从头开始构建梯度增强回归器，您了解了梯度增强的工作原理。您实现了各种梯度增强超参数，包括`learning_rate`、`n_estimators`、`max_depth`和`subsample`，这导致了随机梯度增强。最后，你使用大数据通过比较`GradientBoostingClassifier`和`XGBoostClassifier`的时间来预测恒星是否有系外行星，其中`XGBoostClassifier`出现的速度是两倍到十倍以上，并且更加准确。

学习这些技能的好处是，你现在明白什么时候应用 XGBoost，而不是类似的机器学习算法，比如梯度提升。现在，您可以通过适当利用核心超参数，包括`n_estimators`和`learning_rate`，构建更强大的 XGBoost 和梯度增强模型。此外，你已经发展了计算时间的能力，而不是依赖直觉。

恭喜你！您已经完成了 XGBoost 的所有预备章节。到目前为止，目的是在更大的 XGBoost 叙述中向您介绍机器学习和数据分析。目的是展示 XGBoost 是如何从集合方法、增强、梯度增强和大数据中出现的。

下一章从 XGBoost 的高级介绍开始，除了 XGBoost 为提高速度而进行的硬件修改之外，您还将了解 XGBoost 算法背后的数学细节。您还将在希格斯玻色子发现的历史相关案例研究中使用原始 Python API 构建 XGBoost 模型。接下来的章节重点介绍了令人兴奋的细节、优势、细微差别、技巧和提示，以构建快速、高效、强大、行业就绪的 XGBoost 模型，供您在未来几年使用。