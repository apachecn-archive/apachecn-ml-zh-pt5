

# *第五章*:xboost 亮相

在这一章中，你将最终看到**极限梯度提升**，或者说 **XGBoost** ，就像它本来的样子。XGBoost 是在我们已经建立的机器学习叙事的背景下提出的，从决策树到梯度推进。本章的前半部分着重于 XGBoost 给树集成算法带来的显著进步背后的理论。下半部分的重点是在**希格斯玻色子 Kaggle 竞赛**中建立 XGBoost 模型，向世界展示了 XGBoost。

具体来说，您将识别使 XGBoost 更快的速度增强，发现 XGBoost 如何处理缺失值，并学习 XGBoost 的**正则化参数选择**背后的数学推导。您将为构建 XGBoost 分类器和回归器建立模型模板。最后，您将查看**大型强子对撞机**，希格斯玻色子就是在这里被发现的，在这里您将使用原始的 XGBoost Python API 对数据进行加权并做出预测。

本章涵盖以下主要主题:

*   设计 XGBoost

*   分析 XGBoost 参数

*   构建 XGBoost 模型

*   寻找希格斯玻色子——案例研究

# 设计 XGBoost

XGBoost 是梯度增强的一个重大升级。在本节中，您将了解 XGBoost 区别于梯度推进和其他树集成算法的关键特性。

## 历史叙事

随着大数据的加速，寻找令人敬畏的机器学习算法来产生准确、最佳的预测的探索开始了。决策树产生的机器学习模型过于精确，无法很好地推广到新数据。通过**装袋**和**助推**将许多决策树结合起来，集成方法被证明更加有效。从树集合轨迹中出现的领先算法是梯度推进。

梯度推进的一致性、力量和突出的结果说服了华盛顿大学的陈天琦来增强它的能力。他将新算法称为 XGBoost，是**极端梯度推进**的缩写。陈的新形式的梯度推进包括内置的正则化和令人印象深刻的速度增益。

在 Kaggle 竞赛中获得初步成功后，2016 年，陈天琦和卡洛斯·格斯特林(Carlos Guestrin)撰写了 *XGBoost:可扩展的树提升系统*，向更大的机器学习社区展示了他们的算法。你可以在 https://arxiv.org/pdf/1603.02754.pdf 的[查看原文。下一节总结了要点。](https://arxiv.org/pdf/1603.02754.pdf)

## 设计特点

如 [*第四章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093)*从梯度提升到 XGBoost* 所示，在处理大数据时，对更快算法的需求显而易见。*极限梯度提升*中的*极限*是指将计算极限推到极限。突破计算极限不仅需要模型构建方面的知识，还需要磁盘读取、压缩、缓存和内核方面的知识。

虽然这本书的重点仍然是构建 XGBoost 模型，但我们将在 XGBoost 算法的引擎盖下看一眼，以区分关键的进步，例如处理缺失值、速度增益和精度增益，这些都使 XGBoost 更快、更准确、更令人满意。接下来让我们看看这些关键的进步。

### 处理缺失值

你在 [*第一章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)*机器学习景观*中花了大量时间，练习不同的方法纠正**空值**。这是所有机器学习从业者的必备技能。

然而，XGBoost 能够为您处理丢失的值。有一个`missing`超参数，可以设置为任意值。当给定一个缺失的数据点时，XGBoost 会对不同的分割选项进行评分，并选择结果最好的一个。

### 加速

XGBoost 是专门为速度而设计的。速度的提高允许机器学习模型更快地建立，这在处理数百万、数十亿或数万亿行数据时尤为重要。这在大数据世界中并不罕见，每天，行业和科学都会积累比以往任何时候都多的数据。以下新设计特性使 XGBoost 在速度上比同类集成算法有很大优势:

*   **近似分裂查找算法**

*   **稀疏感知分裂查找**

*   **并行计算**

*   **缓存感知访问**

*   **块压缩和分片**

让我们更详细地了解一下这些特性。

#### 近似分裂查找算法

决策树需要最佳的分裂来产生最佳的结果。一个*贪婪算法*在每一步选择最佳分割，并且不回溯查看先前的分支。请注意，决策树分裂通常是以贪婪的方式执行的。

XGBoost 提出了一个精确的贪婪算法和一个新的近似分裂查找算法。分裂查找算法使用**分位数**，即分裂数据的百分比，来提出候选分裂。在全局建议中，在整个训练中使用相同的分位数，而在局部建议中，为每一轮分裂提供新的分位数。

先前已知的算法，**分位数草图**，适用于同等权重的数据集。XGBoost 提出了一种新的基于合并和剪枝的加权分位数草图，并给出了理论保证。虽然这种算法的数学细节超出了本书的范围，但是我们鼓励你在[https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf)查阅原始 XGBoost 论文的附录。

#### 稀疏感知的分裂发现

`pd.get_dummies`将**分类列**转换为**数值列**。这导致了一个更大的数据集，其中有许多值为 0。这种将分类列转换为数字列的方法，其中 1 表示存在，0 表示不存在，通常称为 one-hot e 编码。你将在 [*第十章*](B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230) ， *XGBoost 模型部署*中练习一次热编码。

稀疏矩阵被设计为仅存储具有非零和非空值的数据点。这节省了宝贵的空间。稀疏感知分裂表示在寻找分裂时，XGBoost 更快，因为它的矩阵是稀疏的。

根据原始论文*XGBoost:A Scalable Tree Boosting System*，稀疏感知分裂查找算法在**全状态 10K** 数据集上的执行速度比标准方法快 50 倍。

#### 并行计算

提升对于**并行计算**来说并不理想，因为每棵树都依赖于前一棵树的结果。然而，也有可能发生并行化。

当多个计算单元同时处理同一个问题时，就出现了并行计算。XGBoost 将数据分类并压缩成块。这些块可以分布到多台机器上，或者分布到外部存储器(核外)。

使用块对数据进行排序会更快。分裂查找算法利用了块，并且由于块，分位数的搜索更快。在每一种情况下，XGBoost 都提供了并行计算来加速建模过程。

#### 缓存感知访问

你电脑上的数据被分成**缓存**和**主存**。您最常用的缓存是为高速内存保留的。不常用的数据会保留在低速内存中。不同的缓存级别具有不同数量级的延迟，如下所述:[https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832)。

说到梯度统计，XGBoost 使用**缓存感知预取**。XGBoost 分配一个内部缓冲区，获取梯度统计数据，并执行小批量累加。根据 *XGBoost:一个可扩展的树提升系统*的说法，对于具有大量行的数据集，预取延长了读/写依赖性并减少了大约 50%的运行时间。

#### 块压缩和分片

XGBoost 通过**块压缩**和**块分片**提供额外的速度增益。

块压缩通过压缩列来帮助计算量大的磁盘读取。数据块分片通过将数据分片到多个磁盘中来减少读取时间，这些磁盘在读取数据时会交替出现。

### 精度增益

XGBoost 增加了内置的正则化，以实现超过梯度增强的精度增益。**正则化**是添加信息以减少方差和防止过拟合的过程。

虽然可以通过超参数微调来正则化数据，但是也可以尝试正则化算法。例如，`Ridge`和`Lasso`是`LinearRegression`的正则化机器学习替代方案。

XGBoost 将正则化作为学习目标的一部分，与梯度增强和随机森林形成对比。正则化的参数惩罚复杂性并平滑最终权重以防止过度拟合。XGBoost 是梯度增强的正则化版本。

在下一节中，您将了解 XGBoost 学习目标背后的数学，它将正则化与损失函数结合起来。虽然您不需要了解数学就能有效地使用 XGBoost，但是数学知识可以提供更深入的理解。如果需要，您可以跳过下一部分。

# 分析 XGBoost 参数

在本节中，我们将分析 XGBoost 使用的参数，通过数学推导来创建最先进的机器学习模型。

我们将保持在 [*第 2 章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) 、*决策树深入*中呈现的参数和超参数之间的区别。在训练模型之前选择超参数，而在训练模型时选择参数。换句话说，参数就是模型从数据中学习到的东西。

下面的推导摘自 XGBoost 官方文档，[https://XGBoost . readthedocs . io/en/latest/tutorials/model . html](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)上的*Boosted Trees*简介。

## 学习目标

机器学习模型的学习目标决定了模型与数据的吻合程度。在 XGBoost 的情况下，学习目标由两部分组成:损失函数**和正则项**。****

 ****从数学上讲，XGBoost 的学习目标可以定义如下:

![](img/Formula_05_001.jpg)

这里，![](img/Formula_05_002.png)是损失函数，它是用于回归的**均方误差** ( **MSE** )，或者用于分类的对数损失，![](img/Formula_05_003.png)是正则化函数，一个防止过拟合的惩罚项。将正则项作为目标函数的一部分将 XGBoost 与大多数树系综区分开来。

让我们更详细地看看目标函数，考虑回归的 MSE。

### 损失函数

损失函数定义为回归的 MSE，可以用求和符号表示，如下所示:

![](img/Formula_05_004.jpg)

这里，![](img/Formula_05_005.png)是第![](img/Formula_05_006.png)行的目标值，![](img/Formula_05_007.png)是由机器学习模型预测的第![](img/Formula_05_008.png)行的值。求和符号![](img/Formula_05_009.png)表示从![](img/Formula_05_010.png)开始到![](img/Formula_05_011.png)结束的所有行的总和，即行数。

给定树的预测![](img/Formula_05_012.png)需要从树根开始到树叶结束的函数。在数学上，这可以表示如下:

![](img/Formula_05_013.jpg)

这里， *x* i 是一个向量，其条目是第![](img/Formula_05_014.png)行的列，![](img/Formula_05_015.png)意味着函数![](img/Formula_05_016.png)是所有可能的购物车函数集合![](img/Formula_05_017.png)的成员。 **CART** 是**分类和回归树**的缩写。CART 提供了所有树叶的真实值，甚至是分类算法。

在梯度增强中，确定第![](img/Formula_05_006.png)行预测的函数包括所有先前函数的和，如 [*第 4 章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093) ，*从梯度增强到 XGBoost* 中所述。因此，可以这样写:

![](img/Formula_05_019.jpg)

这里， *T* 是被助推树木的数量。换句话说，为了获得对第![](img/Formula_05_020.png)棵树的预测，除了对新树的预测之外，还要对先前树的预测求和。符号![](img/Formula_05_021.png)坚持认为这些功能属于![](img/Formula_05_022.png)，即所有可能的购物车功能的集合。

第![](img/Formula_05_023.png)棵提升树的学习目标现在可以重写如下:

![](img/Formula_05_024.jpg)

这里，![](img/Formula_05_025.png)是第![](img/Formula_05_026.png)棵提升树的总损失函数，![](img/Formula_05_027.png)是正则化项。

由于 boosted 树是对之前树的预测求和，所以除了新树的预测，还必须是![](img/Formula_05_028.png)的情况。这就是加法训练背后的理念。

通过将其代入前面的学习目标，我们得到以下结果:

![](img/Formula_05_029.png)

对于最小二乘回归情况，这可以重写如下:

![](img/Formula_05_030.jpg)

将多项式相乘，我们得到以下结果:

![](img/Formula_05_031.jpg)

这里，![](img/Formula_05_032.png)是不依赖于![](img/Formula_05_033.png)的常数项。就多项式而言，这是一个带有变量![](img/Formula_05_034.png)的二次方程。回想一下，我们的目标是找到![](img/Formula_05_035.png)的最优值，这个最优函数将根(样本)映射到叶(预测)。

任何足够光滑的函数，比如二次多项式(二次)，都可以用泰勒多项式(T2)来近似。XGBoost 使用牛顿法和二阶泰勒多项式得出以下结果:

![](img/Formula_05_036.jpg)

这里，![](img/Formula_05_037.png)和![](img/Formula_05_038.png)可以写成以下偏导数:

![](img/Formula_05_039.png)

![](img/Formula_05_040.png)

关于 XGBoost 如何使用**泰勒展开式**的一般性讨论，请查看[https://stats . stack exchange . com/questions/202858/XGBoost-loss-function-approximation-with-Taylor-expansion](https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion)。

XGBoost 通过采用一个仅使用![](img/Formula_05_041.png)和![](img/Formula_05_038.png)作为输入的解算器来实现这个学习目标函数。由于损失函数是通用的，相同的输入可用于回归和分类。

这就剩下正则化函数![](img/Formula_05_043.png)。

### 正则化函数

设![](img/Formula_05_044.png)为叶子的向量空间。然后，![](img/Formula_05_045.png)，将树根映射到树叶的函数，可以根据![](img/Formula_05_044.png)进行重铸，如下所示:

![](img/Formula_05_047.png)

这里， *q* 是将数据点分配给叶子的函数， *T* 是叶子的数量。

在实践和实验之后，XGBoost 确定了以下正则化函数，其中![](img/Formula_05_048.png)和![](img/Formula_05_049.png)是用于减少过拟合的惩罚常数:

![](img/Formula_05_050.png)

### 目标函数

将损失函数与正则化函数相结合，学习目标函数变成如下:

![](img/Formula_05_051.png)

我们可以如下定义分配给第![](img/Formula_05_052.png)叶的数据点的索引集:

![](img/Formula_05_053.png)

目标函数可以写成如下形式:

![](img/Formula_05_054.png)

最后，设置![](img/Formula_05_055.png)和![](img/Formula_05_056.png)，在重新排列指标和组合相似项后，我们得到目标函数的最终形式，如下:

![](img/Formula_05_057.png)

通过对![](img/Formula_05_058.png)求导并设置左侧等于零来最小化目标函数，我们得到如下:

![](img/Formula_05_059.png)

这可以代入目标函数，得到如下结果:

![](img/Formula_05_060.png)

这是 XGBoost 用来确定模型与数据拟合程度的结果。

祝贺你完成了一个漫长而富有挑战性的推导！

# 构建 XGBoost 模型

在前两节中，您学习了 XGBoost 如何通过参数派生、正则化、速度增强和新特性(如补偿空值的`missing`参数)来工作。

在本书中，我们主要用 scikit-learn 构建 XGBoost 模型。scikit-learn XGBoost 包装器于 2019 年发布。在完全沉浸于 scikit-learn 之前，构建 XGBoost 模型需要更陡峭的学习曲线。例如，将 NumPy 数组转换为`dmatrices`，是利用 XGBoost 框架的强制要求。

然而，在 scikit-learn 中，这些转换发生在幕后。在 scikit-learn 中构建 XGBoost 模型与在 scikit-learn 中构建其他机器学习模型非常相似，正如您在本书中所经历的那样。除了基本工具如`train_test_split`、`cross_val_score`、`GridSearchCV`和`RandomizedSearchCV`之外，所有标准的 scikit-learn 方法都可用。

在本节中，您将开发用于构建 XGBoost 模型的模板。今后，这些模板可以作为构建 XGBoost 分类器和回归器的起点。

我们将为两个经典数据集构建模板:用于分类的**虹膜数据集**和用于回归的**糖尿病数据集**。这两个数据集都很小，内置于 scikit-learn 中，并在整个机器学习社区中经过了频繁测试。作为模型构建过程的一部分，您将显式地定义默认的超参数，这些参数会给 XGBoost 带来很高的分数。这些超参数是明确定义的，以便您可以了解它们在中的内容，为将来调整它们做准备。

## 虹膜数据集

Iris 数据集是机器学习社区的主要组成部分，由统计学家 Robert Fischer 于 1936 年引入。它的易访问性、小尺寸、干净的数据和值的对称性使它成为测试分类算法的流行选择。

我们将通过使用 `datasets`库和`load_iris()`方法直接从 scikit-learn 下载 Iris 数据集，如下所示:

```
import pandas as pd
import numpy as np
from sklearn import datasets
iris = datasets.load_iris()
```

Scikit-learn 数据集存储为`pandas`数据帧更多用于数据分析和数据可视化。将 NumPy 数组作为数据帧查看需要使用`pandas` `DataFrame`方法。这个 scikit-learn 数据集预先被分成预测器和目标列。将它们放在一起需要在转换之前用代码`np.c_`连接 NumPy 数组。还会添加列名，如下所示:

```
df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])
```

您可以使用`df.head()`查看数据帧的前五行:

```
df.head()
```

生成的数据帧将如下所示:

![Figure 5.1 – The Iris dataset](img/B15551_05_01.jpg)

图 5.1–虹膜数据集

预测器列是不言自明的，测量萼片和花瓣的长度和宽度。根据 scikit-learn 文档，[https://scikit-learn . org/stable/auto _ examples/datasets/plot _ iris _ dataset . html](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)，目标列由三种不同的鸢尾花组成， **setosa** 、 **versicolor** 和 **virginica** 。有 150 行。

要为机器学习准备数据，请导入`train_test_split`，然后相应地拆分数据。您可以使用原始的 NumPy 数组，`iris['data']`和`iris['target']`，作为`train_test_split`的输入:

```
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=2)
```

现在我们已经分割了数据，让我们构建分类模板。

### XGBoost 分类模板

下面的模板用于构建 XGBoost 分类器，假设数据集已经被分成`X_train`、`X_test`、`y_train`和`y_test`集合:

1.  从`xgboost`库导入`XGBClassifier`:

    ```
    from xgboost import XGBClassifier
    ```

2.  根据需要导入分类评分方法。

    虽然`accuracy_score`是标准的，但其他评分方法，如`auc` ( **曲线下面积**)，将在后面讨论:

    ```
    from sklearn.metrics import accuracy_score
    ```

3.  用超参数初始化 XGBoost 分类器。

    微调超参数的重点是 [*第六章*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136) 、 *XGBoost 超参数*。在本章中，最重要的默认超参数已在前面明确说明:

    ```
    xgb = XGBClassifier(booster='gbtree', objective='multi:softprob', max_depth=6, learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)
    ```

    上述超参数的简要描述如下:

    a)`booster='gbtree'`:助推器是`'gbtree'`代表梯度助推树，XGBoost 默认基础学习器。与其他基础学习者合作不常见，但也是可能的，这是我们在 [*第 8 章*](B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189) 、 *XGBoost 备选基础学习者*中采用的策略。

    b) `objective='multi:softprob'`:目标的标准选项可在 XGBoost 官方文档[【https://xgboost.readthedocs.io/en/latest/parameter.html】](https://xgboost.readthedocs.io/en/latest/parameter.html)中的*学习任务参数*下查看。当数据集包括多个类时，`multi:softprob`目标是`binary:logistic`的标准替代方案。它计算分类的概率并选择最高的一个。如果没有明确说明，XGBoost 通常会为您找到合适的目标。

    c) `max_depth=6`:一棵树的`max_depth`决定了每棵树有多少个分支。这是进行平衡预测时最重要的超参数之一。XGBoost 使用默认值`6`，不像随机森林，除非明确编程，否则随机森林不提供值。

    d) `learning_rate=0.1`:在 XGBoost 内，这个超参数通常被称为`eta`。这个超参数通过将每棵树的权重降低到给定的百分比来限制方差。`learning_rate`超参数在 [*第四章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093) 、*从梯度升压到 XGBoost* 中有详细探讨。

    e) `n_estimators=100`:在集成方法中流行，`n_estimators`是模型中被增强的树的数量。在减少`learning_rate`的同时增加这个数字可以产生更健壮的结果。

4.  使分类器适合数据。

    这就是奇迹发生的地方。整个 XGBoost 系统、前两节中探讨的细节、最佳参数的选择(包括正则化约束)和速度增强(如近似分裂查找算法)以及分块和分片都发生在这一行强大的 scikit-learn 代码中:

    ```
    xgb.fit(X_train, y_train)
    ```

5.  预测 *y* 值为`y_pred`:

    ```
    y_pred = xgb.predict(X_test)
    ```

6.  通过比较`y_pred`和`y_test`对模型进行评分:

    ```
    score = accuracy_score(y_pred, y_test)
    ```

7.  显示您的结果:

    ```
    print('Score: ' + str(score))
    Score: 0.9736842105263158
    ```

不幸的是，没有 Iris 数据集分数的官方列表。太多了，一个地方编不完。使用默认超参数的虹膜数据集上的初始分数`97.4`百分比非常好(参见[https://www.kaggle.com/c/serpro-iris/leaderboard](https://www.kaggle.com/c/serpro-iris/leaderboard))。

前面段落中提供的 XGBoost 分类器模板并不是权威性的，而是前进的起点。

## 糖尿病数据集

现在您已经熟悉了 scikit-learn 和 XGBoost，您正在发展相当快速地构建和评估 XGBoost 模型的能力。在本节中，使用`cross_val_score`和 scikit-learn 的糖尿病数据集提供了一个 XGBoost 回归器模板。

在构建模板之前，将预测列作为`X`导入，将目标列作为`y`导入，如下所示:

```
X,y = datasets.load_diabetes(return_X_y=True)
```

现在我们已经导入了预测值和目标列，让我们开始构建模板。

### XGBoost 回归器模板(交叉验证)

下面是使用交叉验证在 scikit-learn 中构建 XGBoost 回归模型的基本步骤，假设已经定义了预测列`X`和目标列`y`:

1.  导入`XGBRegressor`和`cross_val_score`:

    ```
    from sklearn.model_selection import cross_val_score
    from xgboost import XGBRegressor
    ```

2.  初始化`XGBRegressor`。

    这里，我们用 MSE`objective='reg:squarederror'`初始化`XGBRegressor`。明确给出了最重要的超参数默认值:

    ```
    xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror', max_depth=6, learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)
    ```

3.  用`cross_val_score`对回归变量进行拟合和评分。

    使用`cross_val_score`，使用模型、预测值列、目标列和评分作为输入，在一个步骤中完成拟合和评分:

    ```
    scores = cross_val_score(xgb, X, y, scoring='neg_mean_squared_error', cv=5)
    ```

4.  显示结果。

    回归的得分通常显示为**均方根误差** ( **RMSE** )以保持单位不变:

    ```
    rmse = np.sqrt(-scores)
    print('RMSE:', np.round(rmse, 3))
    print('RMSE mean: %0.3f' % (rmse.mean()))
    ```

    结果如下:

    ```
    RMSE: [63.033 59.689 64.538 63.699 64.661]
    RMSE mean: 63.124
    ```

没有一个比较的基准，我们不知道这个分数意味着什么。使用`.describe()`方法将目标列`y`转换为`pandas`数据帧将给出预测列的四分位数和一般统计信息，如下所示:

```
pd.DataFrame(y).describe()
```

以下是预期的输出:

![Figure 5.2 – Describing the statistics of y, the Diabetes target column](img/B15551_05_02.jpg)

图 5.2–描述糖尿病目标列 y 的统计数据

得分`63.124`小于 1 个标准差，这是一个不错的结果。

现在您已经有了 XGBoost 分类器和回归器模板，可以用于构建模型。

既然您已经习惯了在 scikit-learn 中构建 XGBoost 模型，那么是时候深入研究高能物理学了。

# 寻找希格斯玻色子——案例研究

在这一节，我们将回顾希格斯玻色子 Kaggle 竞赛，它将 XGBoost 带入了机器学习的聚光灯下。为了做好准备，在进入模型开发之前，先给出历史背景。我们构建的模型包括竞赛时 XGBoost 提供的默认模型和 Gabor Melis 提供的获胜解决方案的参考。Kaggle 帐户不是本文所必需的，因此我们不会花时间向您展示如何提交。如果你感兴趣，我们已经提供了指南。

## 物理学背景

在流行文化中，希格斯玻色子被称为*上帝粒子*。希格斯玻色子是由彼得·希格斯在 1964 年提出的理论，用来解释为什么粒子有质量。

寻找希格斯玻色子的努力在 2012 年欧洲粒子物理研究所(瑞士日内瓦)的大型强子对撞机(T1)中达到高潮。诺贝尔奖颁发了，物理学的标准模型比以往任何时候都要高，这个模型解释了物理学中除重力以外的所有已知力。

希格斯玻色子是通过质子以极高的速度相互碰撞并观察结果而发现的。观测数据来自 **ATLAS** 探测器，该探测器记录了每秒*数亿次质子-质子碰撞*产生的数据，根据竞赛的技术文档，*学习发现:希格斯玻色子机器学习挑战*，[https://Higgs ml . lal . in2p 3 . fr/files/2014/04/documentation _ v 1.8 . pdf](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf)。

发现希格斯玻色子后，下一步是精确测量其衰变的特征。ATLAS 实验从包裹在背景噪音中的数据中发现希格斯玻色子衰变为两个**τ**粒子。为了更好地理解数据，ATLAS 呼吁机器学习社区。

## Kaggle 竞赛

Kaggle 竞赛是一项旨在解决特定问题的机器学习竞赛。机器学习比赛在 2006 年变得很出名，当时网飞向任何能够将他们的电影推荐提高 10%的人提供 100 万美元。2009 年，100 万美元的奖金被授予*贝尔科尔*的*务实混乱*团队([https://www . wired . com/2009/09/bellkors-practical-Chaos-wins-100 万-网飞奖/](https://www.wired.com/2009/09/bellkors-pragmatic-chaos-wins-1-million-netflix-prize/) )。

许多企业、计算机科学家、数学家和学生开始意识到机器学习在社会中日益增长的价值。机器学习竞赛变得炙手可热，公司主持人和机器学习从业者都从中受益。从 2010 年开始，许多早期采用者去 Kaggle 尝试机器学习竞赛。

2014 年，Kaggle 与 ATLAS(【https://www.kaggle.com/c/higgs-boson】)宣布了*希格斯玻色子机器学习挑战*。1，875 支队伍参加了比赛，奖金总额为 13，000 美元。

在 Kaggle 比赛中，提供了训练数据，以及所需的评分方法。团队在提交他们的结果之前，在训练数据上建立机器学习模型。未提供测试数据的目标列。然而，允许多次提交，并且会返回分数，以便竞争者可以在最终日期之前改进他们的模型。

Kaggle 竞赛是测试机器学习算法的沃土。与工业不同，Kaggle 竞赛吸引了成千上万的竞争对手，使得获奖的机器学习模型得到了很好的测试。

## XGBoost 和希格斯挑战

XGBoost 于 2014 年 3 月 27 日向公众发布，比希格斯挑战提前了 6 个月。在比赛中，XGBoost 一路飙升，帮助竞争对手登上 Kaggle 排行榜，同时节省了宝贵的时间。

让我们访问数据，看看竞争对手在做什么。

## 数据

我们没有使用 Kaggle 提供的数据，而是使用 CERN 开放数据门户提供的原始数据:http://opendata.cern.ch/record/328 T2。CERN 数据和 Kaggle 数据的区别在于 CERN 数据集要大得多。我们将选择前 250，000 行，并进行一些修改以匹配 Kaggle 数据。

你可以直接从[https://github . com/packt publishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/chapter 05](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter05)下载 CERN 希格斯玻色子数据集。

将`atlas-higgs-challenge-2014-v2.csv.gz`文件读入`pandas`数据帧。请注意，我们只选择了前 250，000 行，并且使用了`compression=gzip`参数，因为数据集被压缩成了一个`csv.gz`文件。访问数据后，查看前五行，如下所示:

```
df = pd.read_csv('atlas-higgs-challenge-2014-v2.csv.gz', nrows=250000, compression='gzip')
df.head()
```

输出中最右边的列应该如下面的屏幕截图所示:

![Figure 5.3 – CERN Higgs boson data – Kaggle columns included](img/B15551_05_03.jpg)

图 5.3–CERN 希格斯玻色子数据–包括 Kaggle 列

注意`Kaggleset`和`KaggleWeight`列。由于 Kaggle 数据集较小，Kaggle 对其权重列使用了不同的数字，在上图中表示为`KaggleWeight`。`Kaggleset`下的`t`值表示它是 Kaggle 数据集的训练集的一部分。换句话说，这两列`Kaggleset`和`KaggleWeight`是 CERN 数据集中的列，设计用于包含将用于 Kaggle 数据集的信息。在这一章中，我们将把我们的 CERN 数据子集限制在 Kaggle 训练集中。

为了匹配 Kaggle 训练数据，我们删除`Kaggleset`和`Weight`列，将`KaggleWeight`转换为`'Weight'`，并将`'Label'`列移动到最后一列，如下所示:

```
del df[‹Weight›]
del df[‹KaggleSet›]
df = df.rename(columns={«KaggleWeight»: «Weight»})
```

移动`Label`列的一种方法是将其存储为一个变量，删除该列，并通过将它赋给新变量来添加一个新列。每当向数据帧分配新列时，新列都会出现在末尾:

```
label_col = df['Label']
del df['Label']
df['Label'] = label_col
```

现在所有的更改都已完成，CERN 的数据与 Kaggle 的数据相匹配。继续查看前五行:

```
df.head()
```

这里是预期输出的左侧:

![Figure 5.4 – CERN Higgs boson data – physics columns](img/B15551_05_04.jpg)

图 5.4–CERN 希格斯玻色子数据–物理栏

许多列没有显示，并且在多个地方出现了一个不寻常的值`-999.00`。

`EventId`之后的列包括前缀为`PRI`的变量，代表*原语*，是探测器在碰撞过程中直接测得的值。相比之下，标有`DER`的列是这些测量值的数值推导。

所有列名和类型由`df.info()`显示:

```
df.info()
```

以下是输出示例，中间的列被截断以节省空间:

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 250000 entries, 0 to 249999
Data columns (total 33 columns):
 #   Column                       Non-Null Count   Dtype  
---  ------                       --------------   -----  
 0   EventId                      250000 non-null  int64  
 1   DER_mass_MMC                 250000 non-null  float64
 2   DER_mass_transverse_met_lep  250000 non-null  float64
 3   DER_mass_vis                 250000 non-null  float64
 4   DER_pt_h                     250000 non-null  float64
…
 28  PRI_jet_subleading_eta       250000 non-null  float64
 29  PRI_jet_subleading_phi       250000 non-null  float64
 30  PRI_jet_all_pt               250000 non-null  float64
 31  Weight                       250000 non-null  float64
 32  Label                        250000 non-null  object  
dtypes: float64(30), int64(3)
memory usage: 62.9 MB
```

所有列都有非空值，只有最后一列`Label`不是数字。各列可按如下方式分组:

*   列`0`:`EventId`——与机器学习模型无关。

*   专栏`1-30`:源自 LHC 碰撞的物理专栏。这些栏目的详细信息可以在 http://higgsml.lal.in2p3.fr/documentation 的技术文档链接中找到。这些是机器学习预测器列。

*   列`31`:`Weight`–该列用于缩放数据。这里的问题是希格斯玻色子事件非常罕见，所以准确率高达 99.9%的机器学习模型可能无法找到它们。砝码可以补偿这种不平衡，但是砝码不能用于测试数据。处理权重的策略将在本章稍后讨论，在 [*第 7 章*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161) ，*用 XGBoost* 发现系外行星。

*   列`32`:`Label`–这是目标列，标记为`s`为信号，`b`为背景。训练数据是从真实数据中模拟出来的，因此会有比其他方式更多的信号。信号是希格斯玻色子衰变的发生。

数据的惟一问题是目标列`Label`不是数字。通过将`s`值替换为`1`，将`b`值替换为`0`，将`Label`列转换为数字列，如下所示:

```
df['Label'].replace(('s', 'b'), (1, 0), inplace=True)
```

既然所有的列都是非空值的数值，那么您可以将数据分成预测值和目标列。回想一下，预测值列的索引为 1–30，目标列是最后一列，索引为`32`(或-1)。注意不应该包括`Weight`列，因为它不可用于测试数据:

```
X = df.iloc[:,1:31]
y = df.iloc[:,-1]
```

## 评分

希格斯挑战不是普通的卡格尔竞赛。除了高能物理对于特征工程(一条我们不会追求的路线)的理解难度，评分方法也不标准。希格斯玻色子挑战需要优化**近似中值显著性** ( **AMS** )。

AMS 定义如下:

![](img/Formula_05_061.png)

这里，![](img/Formula_05_062.png)是真阳性率，![](img/Formula_05_063.png)是假阳性率，![](img/Formula_05_064.png)是给定为`10`的常数正则项。

好在 XGBoost 为比赛提供了 AMS 评分方法，所以不需要正式定义。高 AMS 源于许多真阳性和少数假阴性。在[http://higgsml.lal.in2p3.fr/documentation](http://higgsml.lal.in2p3.fr/documentation)的技术文件中给出了 AMS 而非其他评分方法的理由。

小费

可以建立自己的评分方法，但通常不需要。在极少数情况下，您需要建立自己的评分方法，您可以查看[https://sci kit-learn . org/stable/modules/model _ evaluation . html](https://scikit-learn.org/stable/modules/model_evaluation.html)了解更多信息。

## 重量

在为希格斯玻色子建立一个机器学习模型之前，理解和利用权重很重要。

在机器学习中，权重可以用来提高不平衡数据集的准确性。考虑希格斯挑战中的`s`(信号)和`b`(背景)列。现实中，`s` < < `b`，所以信号在背景噪音中是非常罕见的。比如说，信号比背景噪音要稀少 1000 倍。您可以创建一个权重列，其中`b` = 1 且`s` = 1/1000，以补偿这种不平衡。

根据比赛的技术文件，重量栏是一个`s`(信号)项目。

权重应该首先被缩放以匹配测试数据，因为测试数据提供了由测试集生成的信号和背景事件的预期数量。测试数据有 550，000 行，是训练数据提供的 250，000 行(`len(y)`)的两倍多。通过将重量栏乘以增加的百分比，可以调整重量以匹配测试数据，如下所示:

```
df['test_Weight'] = df['Weight'] * 550000 / len(y)
```

接下来，XGBoost 提供了一个超参数`scale_pos_weight`，它考虑了比例因子。比例因子是背景噪声权重之和除以信号权重之和。可以使用`pandas`条件符号计算比例因子，如下所示:

```
s = np.sum(df[df['Label']==1]['test_Weight'])
b = np.sum(df[df['Label']==0]['test_Weight'])
```

在前面的代码中，`df[df['Label']==1]`将数据帧缩小到其中`Label`列等于`1`的行，然后`np.sum`使用`test_Weight`列将这些行的值相加。

最后，要查看实际速率，用`b`除以`s`:

```
b/s
593.9401931492318
```

总之，权重表示由数据生成的信号和背景事件的预期数量。我们调整权重以匹配测试数据的大小，然后将背景权重之和除以信号权重之和，以建立`scale_pos_weight=b/s`超参数。

小费

有关权重的更详细讨论，请查看来自 KDnuggets 的精彩介绍，网址为[https://www . kdnugges . com/2019/11/machine-learning-what-why-how-weighting . html](https://www.kdnuggets.com/2019/11/machine-learning-what-why-how-weighting.html)。

## 模型

是时候建立一个 XGBoost 模型来预测信号了——也就是模拟希格斯玻色子衰变的发生。

在比赛的时候，XGBoost 是新的，scikit-learn 包装器还没有推出。即使在今天(2020 年)，网上关于用 Python 实现 XGBoost 的大部分信息都是在 scikit-learn 之前的。由于您可能会在网上遇到 scikit-learn 前的 XGBoost Python API，并且这是所有竞争对手在 Higgs Challenge 中使用的 API，因此我们仅在本章中介绍使用原始 Python API 的代码。

以下是为希格斯玻色子挑战建立 XGBoost 模型的步骤:

1.  将`xgboost`作为`xgb`导入:

    ```
    import xgboost as xgb
    ```

2.  将 XGBoost 模型初始化为一个`-999.0`未知值。在 XGBoost 中，可以将未知值设置为`missing`超参数，而不是将这些值转换为中值、平均值、众数或其他零替换值。在模型构建阶段，XGBoost 自动选择导致最佳分割的值。

3.  `weight`超参数可以等于新列`df['test_Weight']`，如`weight`部分所定义:

    ```
    xgb_clf = xgb.DMatrix(X, y, missing=-999.0, weight=df['test_Weight'])
    ```

4.  设置附加超参数。

    以下超参数是 XGBoost 为竞赛提供的默认值:

    a)初始化一个名为`param`的空白字典:

    ```
    param = {}
    ```

    b)将目标定义为`'binary:logitraw'`。

    这意味着二元模型是由逻辑回归概率创建的。该目标将模型定义为一个分类器，并允许对目标列进行排序，这是该特定 Kaggle 竞赛的提交内容所必需的:

    ```
    param['objective'] = 'binary:logitraw'
    ```

    c)使用背景权重除以信号权重来缩放正样本。这将有助于模型在测试集上表现得更好:

    ```
    param['scale_pos_weight'] = b/s
    ```

    d)学习率`eta`被给定为`0.1`:

    ```
    param['eta'] = 0.1
    ```

    e) `max_depth`给定为`6`:

    ```
    param['max_depth'] = 6
    ```

    f)出于显示目的，将评分方法设置为`'auc'`:

    ```
    param['eval_metric'] = 'auc'
    ```

    虽然 AMS 分数会被打印出来，但评估指标是以`auc`给出的，它代表`auc`是真阳性对假阳性曲线，当它等于`1`时是完美的。与准确性类似，`auc`是分类的标准评分标准，尽管它通常优于准确性，因为准确性对于不平衡的数据集是有限的，正如在 [*第 7 章*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161) 、*用 XGBoost* 发现系外行星中所讨论的。

5.  创建一个包含上述项目的参数列表，以及评估指标(`auc`)和`ams@0.15`，XGBoost 使用 15%阈值实现 AMS 得分:

    ```
    plst = list(param.items())+[('eval_metric', 'ams@0.15')]
    ```

6.  创建一个包含初始化的分类器和`'train'`的观察列表，这样您就可以在树继续增长时查看分数:

    ```
    watchlist = [ (xg_clf, 'train') ]
    ```

7.  将助推轮数设置为`120`:

    ```
    num_round = 120
    ```

8.  训练并保存模型。通过将参数列表、分类器、回合数和观察列表作为输入来训练模型。使用`save_model`方法保存模型，这样您就不必再次经历耗时的训练过程。然后，运行代码，观察分数如何随着树的增加而提高:

    ```
    print ('loading data end, start to boost trees')
    bst = xgb.train( plst, xgmat, num_round, watchlist )
    bst.save_model('higgs.model')
    print ('finish training')
    ```

    您的结果结束时应该有以下输出:

    ```
    [110]	train-auc:0.94505	train-ams@0.15:5.84830
    [111]	train-auc:0.94507	train-ams@0.15:5.85186
    [112]	train-auc:0.94519	train-ams@0.15:5.84451
    [113]	train-auc:0.94523	train-ams@0.15:5.84007
    [114]	train-auc:0.94532	train-ams@0.15:5.85800
    [115]	train-auc:0.94536	train-ams@0.15:5.86228
    [116]	train-auc:0.94550	train-ams@0.15:5.91160
    [117]	train-auc:0.94554	train-ams@0.15:5.91842
    [118]	train-auc:0.94565	train-ams@0.15:5.93729
    [119]	train-auc:0.94580	train-ams@0.15:5.93562
    finish training
    ```

恭喜你建造了一个可以预测希格斯玻色子衰变的 XGBoost 分类器！

该模型以`94.58`百分比`auc`和`5.9`的 AMS 执行。就 AMS 而言，比赛的最高值在前三名。当提交测试数据时，该模型实现了大约`3.6`的 AMS。

您刚刚构建的模型是 Tanqi Chen 在竞赛期间为 XGBoost 用户提供的基线。比赛的获胜者 Gabor Melis 用这个基线建立了他的模型。从在[https://github.com/melisgl/higgsml](https://github.com/melisgl/higgsml)查看获胜解决方案并点击 **xgboost-scripts** 可以看出，对基线模型的更改并不显著。像大多数 Kaggle 竞争对手一样，Melis 也执行了特征工程，向数据中添加更多相关列，我们将在第九章*XGBoost ka ggle Masters*中介绍这种做法。

有可能在截止日期后构建和训练自己的模型，并通过 Kaggle 提交。对于 Kaggle 竞赛，必须对提交的内容进行排名、适当索引，并与需要进一步解释的 Kaggle API 主题一起提交。如果你想提交实际比赛的模型，XGBoost 排名代码可能会对你有所帮助，可从[https://github . com/dmlc/XGBoost/blob/master/demo/ka ggle-Higgs/Higgs-pred . py](https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-pred.py)获得。

# 总结

在本章中，您了解了 XGBoost 是如何通过缺失值、稀疏矩阵、并行计算、分片和分块来提高梯度增强的准确性和速度的。您了解了 XGBoost 目标函数背后的数学推导，该函数确定了梯度下降和正则化的参数。您从经典的 scikit-learn 数据集构建了`XGBClassifier`和`XGBRegressor`模板，获得了非常好的分数。最后，您构建了 XGBoost 为 Higgs 挑战提供的基线模型，该模型导致了获胜的解决方案，并将 XGBoost 推到了聚光灯下。

现在，您已经对 XGBoost 的整体叙述、设计、参数选择和模型构建模板有了坚实的理解，在下一章中，您将微调 XGBoost 的超参数以获得最佳分数。****