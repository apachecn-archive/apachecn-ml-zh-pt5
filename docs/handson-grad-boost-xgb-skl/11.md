

# 八、XGBoost 替代基础学习器

在本章中，您将分析和应用不同的`gbtree`，基础学习器的附加选项包括`gblinear`和`dart`。此外，XGBoost 有自己的随机森林实现，作为基础学习器和树集成算法，您将在本章中进行实验。

通过学习如何应用可选的基础学习器，您将大大扩展 XGBoost 的应用范围。您将有能力建立更多的模型，并学习开发线性、基于树和随机森林机器学习算法的新方法。本章的目标是让您熟练地与其他基础学习器一起构建 XGBoost 模型，这样您就可以利用高级 XGBoost 选项找到一系列情况下的最佳模型。

在本章中，我们涵盖了以下主要主题:

*   探索其他基础学习器

*   应用`gblinear`

*   比较`dart`

*   寻找 XGBoost 随机森林

# 技术要求

本章的代码和数据集可以在[https://github . com/packt publishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/chapter 08](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08)找到。

# 探索其他基础学习器

基础学习器是 XGBoost 用来构建其集合中第一个模型的机器学习模型。使用单词 *base* 是因为它是最先出现的模型，使用单词 *learner* 是因为模型在从错误中学习后对自身进行迭代。

决策树已经成为 XGBoost 的首选基础学习器，因为它能不断提高树的成绩。决策树的流行从 XGBoost 扩展到其他集成算法，如随机森林和`ExtraTreesClassifier`和`ExtraTreesRegressor`(【https://scikit-learn.org/stable/modules/ensemble.html】)。

在 XGBoost 中，默认的基础学习器称为`gbtree`，是几个基础学习器中的一个。还有`gblinear`，一个梯度推进的线性模型，和`dart`，一个决策树的变种，包括一个基于神经网络的脱落技术。此外，还有 XGBoost 随机森林。在下一节中，我们将探讨这些基础学习器之间的差异，然后在后续部分中应用它们。

## gblinear

决策树对于**非线性数据**来说是最佳的，因为它们可以通过根据需要多次分割数据来轻松访问数据点。决策树通常更适合作为基础学习器，因为真实数据通常是非线性的。

然而，可能有这样的情况，a `gblinear`作为一个**线性基础学习器**的选项。

增强线性模型背后的一般思想与增强树模型相同。建立一个基础模型，并根据残差训练每个后续模型。最后，将各个模型相加得到最终结果。线性基础学习器的主要区别在于集合中的每个模型都是线性的。

Like `gblinear`也在线性回归中加入正则项。XGBoost 的创始人兼开发者田启钦在 GitHub 上评论说，可能会用多轮提升`gblinear`来用*拿回单套索回归*(【https://github.com/dmlc/xgboost/issues/332】)。

`gblinear`也可通过**逻辑回归**用于分类问题。这是因为逻辑回归也是通过寻找最佳系数(加权输入)建立的，如在**线性回归**中，并通过 **sigmoid 方程**求和(参见 [*第 1 章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022) 、*机器学习前景*)。

我们将在本章的*应用 gblinear* 一节中探讨`gblinear`的细节和应用。现在，让我们来了解一下`dart`。

## 镖

**辍学者遇到多重可加回归树**，简称为 **DART** 的是由加州大学伯克利分校的 K. V. Rashmi 和微软的 Ran Gilad-Bachrach 于 2015 年在下面的论文中介绍的:[http://proceedings.mlr.press/v38/korlakaivinayak15.pdf](http://proceedings.mlr.press/v38/korlakaivinayak15.pdf)。

Rashmi 和 Gilad-Bachrach 强调**多重可加回归树** ( **MART** )是一个成功的模型，它遭受了对早期树的过度依赖。他们没有关注标准惩罚术语**收缩**，而是使用**神经网络**的**退出**技术。简而言之，dropout 技术从神经网络的每一层学习中消除节点(数学点)，从而减少过拟合。换句话说，辍学技术通过消除每一轮的信息来减缓学习过程。

在 DART 中，在每一轮新的提升中，DART 不是将所有先前树的残差相加来构建新模型，而是选择先前树的随机样本，并通过缩放因子![](img/Formula_08_001.png)来归一化树叶，其中![](img/Formula_08_002.png)是被丢弃的树的数量。

DART 是决策树的一种变体。DART 的 XGBoost 实现类似于`gbtree`,增加了额外的超参数以适应退出。

关于 DART 的数学细节，请参考本节第一段中突出显示的原始论文。

在本章后面的*比较 dart* 部分，您将与`DART`基础学习器一起练习构建机器学习模型。

## XGBoost 随机森林

我们将在本节中探索的最后一个选项是 XGBoost 随机森林。通过将`num_parallel_trees`设置为大于`1`的整数，可以将随机森林实现为基本学习器，并且可以将随机森林实现为定义为`XGBRFRegressor`和`XGBRFClassifier`的 XGBoost 内的类选项。

请记住，梯度提升旨在改善基础相对较弱的学习器的错误，而不是像随机森林那样的基础较强的学习器。尽管如此，在一些边缘情况下，随机的森林基础学习器可能是有利的，所以这是一个很好的选择。

作为额外的奖励，XGBoost 提供了`XGBRFRegressor`和`XGBRFClassifier`作为随机森林机器学习算法，它们不是基础学习器，而是它们自己的算法。这些算法的工作方式与 scikit-learn 的随机森林类似(参见 [*第 3 章*](B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070) ，*用随机森林打包*)。主要的区别是 XGBoost 包含了默认的超参数来抵消过度拟合，以及它们自己的构建单个树的方法。XGBoost 随机森林一直处于实验阶段，但是从 2020 年末开始，它们的表现开始超过 scikit-learn 的随机森林，你将在本章中看到这一点。

在本章的最后一节，我们将实验 XGBoost 的随机森林，既作为基础学习器，也作为他们自己的模型。

现在您已经对 XGBoost 基础学习器有了一个大致的了解，让我们一次应用一个。

# 应用 gblinear

寻找最适合线性模型的真实数据集是一项挑战。通常情况下，真实数据是杂乱的，更复杂的模型，如树集成，会产生更好的分数。在其他情况下，线性模型可能会更好地概括。

机器学习算法的成功取决于它们在真实世界数据中的表现。在下一个部分，我们将首先将`gblinear`应用于糖尿病数据集，然后通过构建将其应用于线性数据集。

## 将 gblinear 应用于糖尿病数据集

糖尿病数据集是由 scikit-learn 提供的 442 名糖尿病患者的回归数据集。预测栏包括年龄、**【身体质量指数】** ( **体重指数**)、**血压** ( **血压**)和五项血清测量值。目标栏是 1 年后疾病的进展。可以在这里阅读原论文关于数据集的内容:[http://web . Stanford . edu/~ hastie/Papers/LARS/leas tangle _ 2002 . pdf](http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)。

Scikit-learn 的数据集已经分为预测值和目标列。它们经过机器学习的预处理，分别加载预测列`X`和目标列`y`。

以下是处理该数据集和本章其余部分所需的导入的完整列表:

```
import pandas as pd
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor, XGBClassifier, XGBRFRegressor, XGBRFClassifier
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.linear_model import Lasso, Ridge
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error as MSE
```

我们开始吧！要使用糖尿病数据集，请执行以下操作:

1.  您首先需要使用`load_diabetes`定义`X`和`y`，并将`return_X_y`参数设置为等于`True`:

    ```
    X, y = load_diabetes(return_X_y=True)
    ```

    计划是使用`cross_val_score`和`GridSearchCV`，所以让我们提前创建折叠以获得一致的分数。在 [*第 6 章*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136) ， *XGBoost 超参数*中，我们使用了`StratifiedKFold`，它将目标列分层，确保每个测试集包含相同数量的类。

    这种方法适用于分类，但不适用于回归，在回归中，目标列采用连续值，并且不涉及类。`KFold`通过在数据中创建一致的分割，在没有分层的情况下实现类似的目标。

2.  现在，混洗数据，并使用以下参数将`5`与`KFold`分开:

    ```
    kfold = KFold(n_splits=5, shuffle=True, random_state=2)  
    ```

3.  使用`cross_val_score`构建一个函数，该函数将机器学习模型作为输入，并将`5`倍的平均分数作为输出返回，确保设置了`cv=kfold`:

    ```
    def regression_model(model):
        scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kfold)
        rmse = (-scores)**0.5
        return rmse.mean()
    ```

4.  要使用`gblinear`作为基础模型，只需在回归函数中为`XGBRegressor`设置`booster='gblinear'`:

    ```
    regression_model(XGBRegressor(booster='gblinear'))
    ```

    分数如下:

    ```
    55.4968907398679
    ```

5.  让我们对照其他线性模型(包括使用`Ridge`的`LinearRegression`、`Lasso`和使用`LinearRegression`的)来检查这个分数，如下所示:

    ```
    regression_model(LinearRegression())
    ```

    分数如下:

    ```
    55.50927267834351
    ```

    b) `Lasso`如下:

    ```
    regression_model(Lasso())
    ```

    分数如下:

    ```
    62.64900771743497
    ```

    c) `Ridge`如下:

    ```
    regression_model(Ridge())
    ```

    分数如下:

    ```
    58.83525077919004
    ```

    如你所见，以`gblinear`为基础学习器的`XGBRegressor`表现最好，还有`LinearRegression`。

6.  现在将`booster='gbtree'`放在`XGBRegressor`中，这是默认的基础学习器:

    ```
    regression_model(XGBRegressor(booster='gbtree'))
    ```

    分数如下:

    ```
    65.96608419624594
    ```

如你所见，在这种情况下，`gbtree`基础学习器的表现不如`gblinear`基础学习器，这表明线性模型是理想的。

让我们看看是否可以用`gblinear`作为基础学习器来修改超参数以获得一些收益。

### GB 线性超参数

调整超参数时，理解`gblinear`和`gbtree`之间的差异很重要。第六章*【XGBoost 超参数】中的许多 XGBoost 超参数*是树超参数，不适用于`gblinear`。例如，`max_depth`和`min_child_weight`是专门为树设计的超参数。

下表总结了为线性模型设计的 XGBoost `gblinear`超参数。

#### 注册 _λ

Scikit-learn 使用`reg_lambda`而不是`lambda`，T1 是 Python 中 lambda 函数的保留关键字。这是`Ridge`使用的标准 L2 正则化。接近`0`的值往往效果最好:

*   *默认值:0*

*   *范围:[0，inf)*

*   *增加防止过度拟合*

*   *别名:λ*

#### 注册 _ 阿尔法

Scikit-learn 接受`reg_alpha`和`alpha`。这是`Lasso`使用的标准 L1 正则化。接近`0`的值往往效果最好:

*   *默认值:0*

*   *范围:[0，inf)*

*   *增加防止过度拟合*

*   *别名:阿尔法*

#### 更新器

这是 XGBoost 在每一轮 boosting 中使用建立线性模型的算法。`shotgun`使用`hogwild`带坐标下降的并行性产生非确定性解决方案。相比之下，`coord_descent`是具有确定性解的普通坐标下降:

*   *默认:散弹枪*

*   *射程:霰弹枪，坐标下降*

    注意

    *坐标下降*是一个机器学习术语，定义为通过一次找到一个坐标的梯度来最小化误差。

#### 特征选择器

`feature_selector`通过以下选项确定如何选择重量:

a)`cyclic`–反复循环特性

b)`shuffle`–每轮随机洗牌循环

c)`random`–坐标下降时的坐标选择器是随机的

d)`greedy`–耗时；选择具有最大梯度值的坐标

e)`thrifty`–近似贪婪，根据重量变化重新排序功能

*   *默认:循环*

*   *范围必须与更新程序一起使用，如下所示:*

    a) `shotgun` : `cyclic`，`shuffle`

    b) `coord_descent` : `random`，`greedy`，`thrifty`

    注意

    对于大型数据集来说，`greedy`的计算代价很高，但是通过改变参数`top_k`(`greedy`)可以减少`greedy`考虑的特征数量。

#### top_k

`top_k`是`greedy`和`thrifty`在坐标下降过程中选择的特征的数量；

*   *默认值:0(所有特性)*

*   *范围:[0，最大特征数]*

    注意

    有关 XGBoost `gblinear`超参数的更多信息，请参考官方 XGBoost 文档页面，网址为[https://XGBoost . readthe docs . io/en/latest/parameter . html # parameters-for-linear-booster-booster-GB linear](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear)。

### 线性网格搜索

既然您已经熟悉了`gblinear`可能使用的超参数的范围，那么让我们在一个定制的`grid_search`函数中使用`GridSearchCV`来寻找最佳参数:

1.  下面是我们的`grid_search`函数的一个版本，来自 [*第六章*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136) ， *XGBoost 超参数*:

    ```
    def grid_search(params, reg=XGBRegressor(booster='gblinear')):
        grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=kfold)
        grid_reg.fit(X, y)
        best_params = grid_reg.best_params_
        print("Best params:", best_params)
        best_score = np.sqrt(-grid_reg.best_score_)
        print("Best score:", best_score)
    ```

2.  让我们从用标准范围修改`alpha`的开始:

    ```
    grid_search(params={'reg_alpha':[0.001, 0.01, 0.1, 0.5, 1, 5]})
    ```

    输出如下所示:

    ```
    Best params: {'reg_alpha': 0.01}
    Best score: 55.485310447306425
    ```

    分数差不多，有非常轻微的提高。

3.  接下来，让我们用相同的范围修改`reg_lambda`:

    ```
    grid_search(params={'reg_lambda':[0.001, 0.01, 0.1, 0.5, 1, 5]})
    ```

    输出如下所示:

    ```
    Best params: {'reg_lambda': 0.001}
    Best score: 56.17163554152289
    ```

    这里的这个分数很相似，但是稍微差一点。

4.  现在我们用`feature_selector`和`updater`串联。默认情况下，`updater=shotgun`和`feature_selector=cyclic`。当 `updater=shotgun`时，`feature_selector`唯一的其他选项是`shuffle`。

    让我们看看`shuffle`是否能比`cyclic`表现得更好:

    ```
    grid_search(params={'feature_selector':['shuffle']})
    ```

    输出如下所示:

    ```
    Best params: {'feature_selector': 'shuffle'}
    Best score: 55.531684115240594
    ```

    在这种情况下，`shuffle`并没有表现得更好。

5.  现在我们把`updater`改成`coord_descent`。因此，`feature_selector`可能会与`random`、`greedy`或`thrifty`展开较量。输入以下代码，尝试`grid_search`中的所有`feature_selector`选项:

    ```
    grid_search(params={'feature_selector':['random', 'greedy', 'thrifty'], 'updater':['coord_descent'] })
    ```

    输出如下所示:

    ```
    Best params: {'feature_selector': 'thrifty', 'updater': 'coord_descent'}
    Best score: 55.48798105805444
    This is a slight improvement from the base score.
    ```

    要检查的最后一个超参数是`top_k`，它定义了`greedy`和`thrifty`在坐标下降过程中检查的特征数量。从`2`到`9`的范围是可接受的，因为总共有 10 个特征。

6.  在`greedy`和`thrifty`的`grid_search`内输入`top_k`的范围，以找到最佳选项:

    ```
    grid_search(params={'feature_selector':['greedy', 'thrifty'], 'updater':['coord_descent'], 'top_k':[3, 5, 7, 9]})
    ```

    输出如下所示:

    ```
    Best params: {'feature_selector': 'thrifty', 'top_k': 3, 'updater': 'coord_descent'}
    Best score: 55.478623763746256
    ```

这是迄今为止最好的成绩。

在继续之前，注意附加的超参数并不局限于树，比如`n_estimators`和`learning_rate`也可以使用。

现在让我们看看`gblinear`是如何处理线性数据集的。

## 线性数据集

确保数据集是线性的一种方法是通过构造。我们可以选择一系列的`X`值，比如说`1`到`99`，然后将它们乘以一个带有一些随机性的比例因子。

以下是构建线性数据集的步骤:

1.  从`1`到`100`设置`X`值的范围:

    ```
    X = np.arange(1,100)
    ```

2.  使用 NumPy 声明一个随机种子，以确保结果的一致性:

    ```
    np.random.seed(2) 
    ```

3.  创建一个定义为`y`的空列表:

    ```
    y = []
    ```

4.  遍历`X`，将每个条目乘以一个从`-0.2`到`0.2`的随机数:

    ```
    for i in X:
           y.append(i * np.random.uniform(-0.2, 0.2))
    ```

5.  将`y`转换为用于机器学习的`numpy`数组:

    ```
    y = np.array(y)
    ```

6.  重塑`X`和`y`的形状，使它们包含与数组中的成员一样多的行和一列，因为列是 scikit-learn 的机器学习输入:

    ```
    X = X.reshape(X.shape[0], 1)
    y = y.reshape(y.shape[0], 1)
    ```

    我们现在有了一个线性数据集，它包含了关于`X`和`y`的随机性。

让我们以`gblinear`为基础学习器，再次运行`regression_model`函数:

```
regression_model(XGBRegressor(booster='gblinear', objective='reg:squarederror'))
```

分数如下:

```
6.214946302686011
```

现在以`gbtree`为基础学习器运行`regression_model`功能:

```
regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror'))
```

分数如下:

```
9.37235946501318
```

如你所见，`gblinear`在我们构建的线性数据集中表现得更好。

为了更好地衡量，让我们在同一个数据集上尝试`LinearRegression`:

```
regression_model(LinearRegression())
```

分数如下:

```
6.214962315808842
```

在这种情况下，`gblinear`表现稍好，也许可以忽略不计，得分比`LinearRegression`低`0.00002`分。

## 分析 gblinear

`gblinear`是一个令人信服的选项，但是只有当您有理由相信线性模型可能比基于树的模型表现更好时，才应该使用它。在真实和构建的数据集上，`gblinear`的表现确实比`LinearRegression`略胜一筹。在 XGBoost 中，当数据集很大且呈线性时，`gblinear`是基础学习器的一个强有力的选择。`gblinear`也是分类数据集的一个选项，您将在下一节中应用该选项。

# 对比镖

基本学习器`dart`与`gbtree`相似，都是梯度提升树。主要的区别是`dart`在每一轮提升过程中移除树木(称为丢弃)。

在本节中，我们将在回归和分类问题中应用基础学习器`dart`并将其与其他基础学习器进行比较。

## 带 xgb 回归器的 DART

让我们看看`dart`如何在糖尿病数据集上执行:

1.  首先，像以前一样使用`load_diabetes`重新定义`X`和`y`:

    ```
    X, y = load_diabetes(return_X_y=True)
    ```

2.  要使用`dart`作为 XGBoost 基本学习器，请在`regression_model`函数中设置`XGBRegressor`参数`booster='dart'`:

    ```
    regression_model(XGBRegressor(booster='dart', objective='reg:squarederror'))
    ```

    分数如下:

    ```
    65.96444746130739
    ```

`dart`基础学习器给出与`gbtree`基础学习器相同的结果，精确到小数点后两位。结果的相似性是由于小数据集和`gbtree`默认超参数的成功避免了过度拟合，而不需要剔除技术。

让我们来看看`dart`与`gbtree`在一个更大的分类数据集上的表现。

## 带 XGBClassifier 的 dart

在本书的多个章节中，您都使用了人口普查数据集。我们在 [*第 1 章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022) 、*机器学习场景*中修改的数据集的干净版本，已经为您预装了 [*第 8 章*](B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189) 、 *XGBoost 备选基础学习器*的代码，位于[https://github . com/packt publishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/master 现在让我们开始测试`dart`在更大的数据集上的表现:](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter08)

1.  将人口普查数据集加载到数据帧中，并使用最后一个索引(`-1`)作为目标列，将预测值和目标列拆分为`X`和`y`:

    ```
    df_census = pd.read_csv('census_cleaned.csv')
    X_census = df_census.iloc[:, :-1]
    y_census = df_census.iloc[:, -1]
    ```

2.  定义一个使用`cross_val_score`的新分类函数，将机器学习模型作为输入，将平均分数作为输出，类似于本章前面定义的回归函数:

    ```
    def classification_model(model):
        scores = cross_val_score(model, X_census, y_census, scoring='accuracy', cv=kfold)
        return scores.mean()
    ```

3.  现在使用`XGBClassifier`和`booster='gbtree'`以及`booster='dart'`调用函数两次来比较结果。请注意，由于数据集更大，运行时间会更长:

    a)我们先用`booster='gbtree'`调用`XGBClassifier`:

    ```
    classification_model(XGBClassifier(booster='gbtree'))
    ```

    分数如下:

    ```
    0.8701208195968675
    ```

    b)现在，让我们用`booster='dart'`调用`XGBClassifier`:

    ```
    classification_model(XGBClassifier(booster='dart')
    ```

    分数如下:

    ```
    0.8701208195968675
    ```

这令人惊讶。`dart`对所有 16 位小数给出与`gbtree`完全相同的结果！目前还不清楚是树木被砍伐还是树木的砍伐没有任何影响。

我们可以调整超参数以确保树被丢弃，但是首先，让我们看看`dart`与`gblinear`相比如何。回想一下，`gblinear`也可以通过使用sigmoid 函数来进行分类，就像逻辑回归一样:

1.  用`XGBClassifier`调用`classification_model`函数，设置`booster='gblinear'`:

    ```
    classification_model(XGBClassifier(booster='gblinear'))
    ```

    分数如下:

    ```
    0.8501275704120015
    ```

    这种线性基学习器的性能不如树基学习器。

2.  我们来看看`gblinear`与 logistic 回归相比如何。由于数据集很大，最好将逻辑回归的`max_iter`超参数从`100`调整为`1000`，以便有更多的时间收敛并消除警告。注意，在这种情况下，增加`max_iter`会增加精度:

    ```
    classification_model(LogisticRegression(max_iter=1000))
    ```

    分数如下:

    ```
    0.8008968643699182
    ```

    在这种情况下，与逻辑回归相比保持明显优势。值得强调的是，XGBoost 的`gblinear`分类选项为逻辑回归提供了一个可行的替代方案。

既然已经看到了作为基础学习器`dart`与`gbtree`和`gblinear`的比较，让修改`dart`的超参数。

## DART 超参数

`dart`包括所有的`gbtree`超参数以及它自己的一组额外的超参数，用于调整退出树的百分比、频率和概率。详见[https://XGBoost . readthedocs . io/en/latest/parameter . html # additional-parameters-for-dart-booster-booster-dart](https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart)的 XGBoost 文档。

以下部分是对`dart`特有的 XGBoost 超参数的总结。

#### 样本类型

`sample_type`的选项包括`uniform`，均匀掉落树木，以及`weighted`，按重量比例掉落树木:

*   *默认:“统一”*

*   *范围:【“统一”、“加权”】*

*   *决定如何选择被删除的树*

#### 规格化类型

`normalize_type`的选项包括`tree`，其中新树的权重与被删除的树相同，以及`forest`，其中新树的权重与被删除的树的总和相同:

*   *默认:【树】*

*   *范围:【“树”、“森林”】*

*   *根据掉落的树木计算树木的重量*

#### 速率下降

`rate_drop`允许用户精确设置按百分比丢弃多少棵树:

*   *默认值:0.0*

*   *范围:[0.0，1.0]*

*   *倒下树木的百分比*

#### 一滴

当设置为`1`时，`one_drop`确保在推进过程中至少有一棵树被丢弃:

*   *默认值:0*

*   *范围:[0，1]*

*   *用于确保下降*

#### 跳过 _ 丢弃

`skip_drop`给出了完全跳过辍学的概率。在官方文档中，XGBoost 说`skip_drop`的优先级高于`rate_drop`或`one_drop`。默认情况下，每个树都以相同的概率被丢弃，因此对于给定的提升回合，有可能没有树被丢弃。`skip_drop`允许更新该概率以控制退出轮次的数量:

*   *默认值:0.0*

*   *范围:[0.0，1.0]*

*   *跳级的概率*

现在让我们修改`dart`超参数来区分分数。

## 修改 dart 超参数

为了确保在每一轮提升中至少有棵树被丢弃，我们可以设置`one_drop=1`。现在通过`classification_model`功能对人口普查数据集执行此操作:

```
classification_model(XGBClassifier(booster='dart', one_drop=1))
```

结果如下:

```
0.8718714338474818
```

这是十分之一个百分点的进步，表明每轮提升至少掉一棵树是有利的。

现在，我们正在删除树以更改分数，让我们返回到更小更快的糖尿病数据集来修改剩余的超参数:

1.  使用`regression_model`功能，将`sample_type`从`uniform`更改为`weighted`:

    ```
    regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', sample_type='weighted'))
    ```

    分数如下:

    ```
    65.96444746130739
    ```

    这比之前得分的`gbtree`车型多了 0.002 分。

2.  将`normalize_type`更改为`forest`,以在更新权重时包括树的总和:

    ```
    regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', normalize_type='forest'))
    ```

    分数如下:

    ```
    65.96444746130739
    ```

    分数没有变化，浅数据集可能会发生这种情况。

3.  将`one_drop`更改为`1`,保证每个提升轮至少有一棵树倒下；

    ```
    regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))
    ```

    分数如下:

    ```
    61.81275131335009
    ```

    这是一个明显的进步，获得了四个满分。

当涉及到`rate_drop`、将被丢弃的树木的百分比时，百分比的范围可以与`grid_search`函数一起使用，如下所示:

```
grid_search(params={'rate_drop':[0.01, 0.1, 0.2, 0.4]}, reg=XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))
```

结果如下:

```
Best params: {'rate_drop': 0.2}
Best score: 61.07249602732062
```

这是迄今为止最好的结果。

我们可以用`skip_drop`实现一个类似的范围，它给出了给定树被*而不是*丢弃的概率:

```
grid_search(params={'skip_drop':[0.01, 0.1, 0.2, 0.4]}, reg=XGBRegressor(booster='dart', objective='reg:squarederror'))
```

结果如下:

```
Best params: {'skip_drop': 0.1}
Best score: 62.879753748627635
```

这是一个的好成绩，但是`skip_drop`并没有带来实质性的收获。

现在您已经看到了`dart`是如何工作的，让我们来分析一下结果。

## 分析飞镖

`dart`在 XGBoost 框架内提供了一个引人注目的选项。由于`dart`接受所有的`gbtree`超参数，在修改超参数时，很容易将基础学习器从`gbtree`改为`dart`。实际上，这样做的好处是您可以试验新的超参数，包括`one_drop`、`rate_drop`、`normalize`和其他参数，看看您是否可以获得额外的收益。作为一名基础学习器，在使用 XGBoost 进行研究和建模时，绝对值得一试。

现在你对`dart`已经有了很好的理解，是时候进入随机森林了。

# 寻找 XGBoost 随机森林

在 XGBoost 中有两种策略来实现随机森林。第一种是用随机森林作为基础学习器，第二种是用 XGBoost 原来的随机森林，`XGBRFRegressor`和`XGBRFClassifier`。我们从最初的主题开始，随机森林作为替代的基础学习器。

## 随机森林作为基础学习器

没有一个选项可以将 booster 超参数设置为随机森林。相反，超参数`num_parallel_tree`可以从其默认值`1`增加，以将`gbtree`(或`dart`)转换为增强随机森林。这里的想法是，每个提升轮将不再由一棵树组成，而是由许多平行的树组成，这些树依次组成一个森林。

以下是 XGBoost 超参数`num_parallel_tree`的快速总结。

#### 数量 _ 并行 _ 树

`num_parallel_tree`给出了在每一轮提升过程中生成的树的数量，可能超过 1:

*   *默认值:1*

*   *范围:[1，inf)*

*   *给出并行增强的树的数量*

*   *大于 1 的值将提升器变为随机森林*

通过每轮包含多棵树，基础学习器不再是一棵树，而是一片森林。因为 XGBoost 包括与随机森林相同的超参数，所以当`num_parallel_tree`超过 1 时，基本学习器被适当地分类为随机森林。

让我们看看 XGBoost 随机森林基础学习器在实践中是如何工作的:

1.  用`XGBRegressor`调用`regression_model`并设置`booster='gbtree'`。此外，设置`num_parallel_tree=25`意味着每个提升回合由一片`25`树组成:

    ```
    regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror', num_parallel_tree=25))
    ```

    分数如下:

    ```
    65.96604877151103
    ```

    分数是可观的，在这种情况下，它几乎与提升单个`gbtree`相同。原因是梯度提升是为了吸取之前树的错误而设计的。从一个健壮的随机森林开始，学习的东西很少，收益也很小。

    理解梯度推进作为一种算法的优势来自学习过程这一基本点是至关重要的。因此，为`num_parallel_tree`尝试一个更小的值是有意义的，比如`5`。

2.  在同一个回归模型中设置`num_parallel_tree=5`:

    ```
    regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror', num_parallel_tree=5))
    ```

    分数如下:

    ```
    65.96445649315855
    ```

    从技术上讲，这个分数比由 25 棵树组成的森林产生的分数高 0.002 分。虽然提升不多，但是一般来说，在构建 XGBoost 随机森林的时候，`num_parallel_tree`的值越低越好。

既然您已经看到了随机森林是如何在 XGBoost 中作为基础学习器实现的，那么是时候构建随机森林作为原始的 XGBoost 模型了。

## 作为 XGBoost 模型的随机森林

在中，除了`XGBRegressor`和`XGBClassifier`，`XGBoost`还附带了`XGBRFRegressor`和`XGBRFClassifier`来建造随机森林。

根据 https://xgboost.readthedocs.io/en/latest/tutorials/rf.html[的官方 XGBoost 文档，random forest scikit-learn 包装器仍处于试验阶段，默认设置可能会随时更改。在撰写本文时，2020 年，包括以下`XGBRFRegressor`和`XGBRFClassifier`默认值。](https://xgboost.readthedocs.io/en/latest/tutorials/rf.html)

#### n _ 估计量

使用`XGBRFRegressor`或`XGBRFClassifier`建造随机森林时，使用`n_estimators`而不是`num_parallel_tree`。请记住，当使用`XGBRFRegressor`和`XGBRFClassifier`时，你不是在梯度推进，而是像传统的随机森林一样，只在一轮中对树进行打包:

*   *默认值:100*

*   *范围:[1，inf)*

*   *自动转换为随机森林的 num _ parallel _ tree】*

#### 学习率

`learning_rate`通常是为学习的模型设计的，包括提升器，而不是`XGBRFRegressor`或`XGBRFClassifier`，因为它们由一轮树组成。然而，从 1 改变`learning_rate`将改变分数，因此通常不建议修改该超参数:

*   *默认值:1*

*   *范围:[0，1]*

#### 子样本，列样本节点

Scikit-learn 的随机森林将这些默认值保持在`1`，使得默认值`XGBRFRegressor`和`XGBRFClassifier`不容易过度拟合。这是 XGBoost 和 scikit-learn 随机森林默认实现之间的主要区别:

*   *默认值:0.8*

*   *范围:[0，1]*

*   *减少有助于防止过度拟合*

现在，让我们看看 XGBoost 的随机森林在实践中是如何工作的:

1.  首先，将`XGBRFRegressor`放在`regression_model`函数内:

    ```
    regression_model(XGBRFRegressor(objective='reg:squarederror'))
    ```

    分数如下:

    ```
    59.447250741400595
    ```

    这个分数比前面介绍的`gbtree`模型好一点，比本章介绍的最好的线性模型差一点。

2.  作为比较，让我们通过将`RandomForestRegressor`放在同一个函数中来看看它是如何执行的:

    ```
    regression_model(RandomForestRegressor())
    ```

    分数如下:

    ```
    59.46563031802505
    ```

    这个分数比`XGBRFRegressor`略差。

现在，让我们使用更大的普查数据集进行分类，将 XGBoost random 森林与 scikit-learn 的标准随机森林进行比较:

1.  将`XGBRFClassifier`放在`classification_model`函数中，看看它对用户收入的预测有多好:

    ```
    classification_model(XGBRFClassifier())
    ```

    分数如下:

    ```
    0.856085650471878
    ```

    这是一个很好的分数，与之前给出的 87%略有出入。

2.  现在将`RandomForestClassifier`放在同一个函数中来比较结果:

    ```
    classification_model(RandomForestClassifier())
    ```

    分数如下:

    ```
    0.8555328202034789
    ```

    这比 XGBoost 的实现稍差。

由于 XGBoost 的随机森林仍处于开发阶段，我们将在此停下来分析结果。

## 分析 XGBoost 随机森林

通过将`num_parallel_tree`增加到大于`1`的值，你可以随时尝试随机森林作为你的 XGBoost 基础学习器。尽管如此，正如您在本节中所看到的，bo osting 旨在从弱模型而不是强模型中学习，因此`num_parallel_tree`的值应该保持接近`1`。尝试随机森林作为基础学习器应该谨慎使用。如果提升单个树不能产生最佳分数，随机森林基础学习器是一个选择。

或者，XGBoost 随机森林的`XGBRFRegressor`和`XGBRFClassifier`可以实现为 scikit-learn 的随机森林的替代物。XGBoost 的新`XGBRFRegressor`和`XGBRFClassifier`优于 scikit-learn 的`RandomForestRegressor`和`RandomForestClassifier`，尽管比较非常接近。鉴于 XGBoost 在机器学习社区的整体成功，使用`XGBRFRegressor`和`XGBRFClassifier`作为可行的选项是绝对值得的。

# 总结

在这一章中，你通过将所有 XGBoost 基础学习器，包括`gbtree`、`dart`、`gblinear`和随机森林，应用于回归和分类数据集，极大地扩展了 XGBoost 的范围。您预览、应用和调整了基础学习器特有的超参数，以提高分数。此外，您使用线性构造的数据集和`XGBRFRegressor`和`XGBRFClassifier`试验了`gblinear`来构建 XGBoost 随机森林，而没有任何提升。现在您已经和所有基础学习器一起学习了，您对 XGBoost 范围的理解已经达到了高级水平。

在下一章中，您将分析 Kaggle masters 的提示和技巧，以进一步提高您的 XGBoost 技能！