

# 九、XGBoost Kaggle大师

在这一章中，你将从`VotingClassifier`和`VotingRegressor`中学到构建非相关机器学习集成的有价值的技巧和诀窍，以及**堆叠**一个最终模型的优势。

在本章中，我们将讨论以下主要话题:

*   探索 Kaggle 竞赛

*   设计新的数据列

*   构建非相关系综

*   堆叠最终模型

# 技术要求

本章的代码可以在[https://github . com/packt publishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/chapter 09](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09)找到。

# 探索 Kaggle 竞赛

“我只使用了 XGBoost(尝试了其他产品，但没有一个表现足够好，以至于最终出现在我的合奏中)。”

–*王，Kaggle 冠军*

([https://www . cnblogs . com/yymn/p/4847130 . html](https://www.cnblogs.com/yymn/p/4847130.html)

在本节中，我们将通过查看 Kaggle 竞赛的简史来研究 Kaggle 竞赛，它们是如何构成的，以及与验证/测试集相区别的坚持/测试集的重要性。

## 在高尔夫比赛中获胜

XGBoost 建立了其作为领先机器学习算法的声誉，因为它在赢得 Kaggle 比赛中取得了无与伦比的成功。XGBoost 经常与深度学习模型一起出现在获胜的组合中，如**神经网络**，除了直接获胜。XGBoost Kaggle 竞赛获胜者的样本列表出现在*分布式(深度)机器学习社区*网页上，网址为[https://github . com/dmlc/XGBoost/tree/master/demo # Machine-Learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)。要获得更多 XGBoost Kaggle 竞赛获奖者的列表，可以对 Kaggle 竞赛的*获奖解决方案*([https://www . ka ggle . com/sudalairajkumar/Winning-solutions-of-ka ggle-competitions](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions))进行排序，以研究获奖模型。

注意

虽然 XGBoost 经常出现在获胜者中，但其他机器学习模型也出现了。

正如在 [*第五章*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117)*XGBoost 亮相*中提到的，Kaggle 竞赛是机器学习的竞赛，机器学习的从业者相互竞争，以获得尽可能好的分数并赢得现金奖励。当 XGBoost 在 2014 年的*希格斯玻色子机器学习挑战赛*中爆发时，它立即跃上排行榜，成为 Kaggle 比赛中最受欢迎的机器学习算法之一。

在 2014 年至 2018 年期间，XGBoost 在表格数据上的表现一直优于竞争对手，表格数据是指按行和列组织的数据，而非结构化数据如图像或文本，神经网络在这些数据中具有优势。随着 2017 年 **LightGBM** 的出现，一个闪电般快速的微软版本的梯度增强，XGBoost 终于与表格数据有了一些真正的竞争。

以下介绍性论文 *LightGBM:一个高效的梯度提升决策树*，由八位作者撰写，推荐用于 LightGBM 的介绍:[https://papers . nips . cc/paper/6907-light GBM-A-Highly-Efficient-Gradient-Boosting-Decision-Tree . pdf](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)。

在 Kaggle 竞赛中实现一个伟大的机器算法，比如 XGBoost 或 LightGBM 是不够的。类似地，微调模型的超参数通常是不够的。虽然单个模型预测很重要，但设计新数据和组合最佳模型以获得更高的分数同样重要。

## ka ggle 竞赛的结构

有必要了解 Kaggle 竞赛的结构，以深入了解为什么非相关系综构建和堆叠等技术如此普遍。此外，探索 Kaggle 比赛的结构将会给你信心，如果你选择继续这条路的话，你会有信心参加 Kaggle 比赛。

小费

Kaggle 推荐*房价:高级回归技术*，[https://www . ka ggle . com/c/house-Prices-Advanced-Regression-Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)，面向寻求从基础过渡到高级竞赛的机器学习学生。这是众多不提供现金奖励的知识竞赛之一。

Kaggle 网站上有 Kaggle 比赛。以下是 XGBoost 用户 Owen Zhang 获得的 2015 年 *Avito 上下文广告点击量*的网站:。包括 Owen Zhang 在内的几个 XGBoost Kaggle 竞赛获奖者都是 2015 年的，说明 XGBoost 在田启钦的标志性论文*2016 年发表的《XGBoost:可扩展的树提升系统*:之前的发行量。

下面是顶部的 *Avito 上下文广告点击量*网站:

![Figure 9.1 – Avito Context Ad Clicks Kaggle competition website](img/B15551_09_01.jpg)

图 9.1–Avito 上下文广告点击 Kaggle 竞争网站

该概述页面对本次竞赛的解释如下:

*   **概述**(以蓝色突出显示)旁边的附加链接包括**数据**，您可以在其中访问比赛数据。

*   **笔记本**，Kagglers 们在这里发布解决方案和入门笔记本。

*   **讨论**，Kagglers 发帖答疑。

*   **排行榜**，显示最高分数。

*   **规则**，解释比赛如何进行。

*   此外，请注意最右侧的**延迟提交**链接，这表明即使比赛已经结束，提交的内容仍然可以接受，这是一项通用的 Kaggle 政策。

为了下载数据，你需要注册一个免费账户来参加比赛。数据通常分为两个数据集，`training.csv`，用于构建模型的训练集，和`test.csv`，用于对模型评分的测试集。提交模型后，您会在公共排行榜上获得一个分数。比赛结束时，提交一个最终模型，与一个私有测试集进行对比，以确定获胜的解决方案。

## 拖延套

区分为 Kaggle 比赛建立机器学习模型和自己建立模型是很重要的。到目前为止，我们已经将数据集分为训练集和测试集，以确保我们的模型具有良好的泛化能力。然而，在 Kaggle 竞赛中，模型必须在竞争环境中进行测试。因此，来自测试集的数据仍然是隐藏的。

以下是 Kaggle 的训练集和测试集之间的差异:

*   这是你自己训练和给模特打分的地方。应该使用`train_test_split`或`cross_val_score`将这个训练集分割成自己的训练集和测试集，以构建能够很好地推广新数据的模型。训练期间使用的测试集通常被称为**验证集**，因为它们验证模型。

*   这是一个单独的坚持组。直到你有了一个最终的模型，可以用它从未见过的数据进行测试，你才可以使用测试集。隐藏测试集的目的是保持竞赛的完整性。测试数据对参与者是隐藏的，只有在参与者提交一个模型后，结果才会显示出来。

为研究或行业建立模型时，保留一个测试总是好的做法。当使用已经看到的数据对模型进行测试时，该模型有过度适应测试集的风险，这种可能性经常出现在 Kaggle 比赛中，此时竞争对手专注于将他们在公共排行榜中的位置提高千分之几个百分点。

Kaggle竞赛与现实世界在这方面有交集。建立机器学习模型的目的是利用未知数据做出准确的预测。例如，如果一个模型在训练集上给出 100%的准确率，但在未知数据上只给出 50%的准确率，那么这个模型基本上是没有价值的。

在测试集上验证模型和在保留集上测试模型之间的区别非常重要。

以下是自行验证和测试机器学习模型的一般方法:

1.  **将数据分为训练集和保留集**:保留保留集，抵制看它的诱惑。

2.  **将训练集拆分为训练集和测试集，或者使用交叉验证**:将新模型放入训练集并验证模型，如此往复以提高分数。

3.  **获得最终模型后，在搁置集**上测试:这是模型的真正测试。如果分数低于预期，返回*步骤 2* 并重复。重要的是，不要使用拒绝集作为新的验证集，来回调整超参数。当这种情况发生时，模型会自我调整以匹配拒绝集，这首先就违背了拒绝集的目的。

在 Kaggle 比赛中，将机器学习模型调整得过于接近测试集是行不通的。Kaggle 经常将测试集拆分成一个额外的公共和私有组件。公共测试集让参与者有机会对他们的模型进行评分，并在此过程中进行改进、调整和重新提交。直到比赛的最后一天，私人测试集才会揭晓。虽然排名是为公共测试集显示的，但竞赛获胜者是根据未公开测试集的结果宣布的。

赢得 Kaggle 比赛需要在私人测试集上获得尽可能好的分数。在 Kaggle 竞赛中，每一个百分点都很重要。对这种精确度的需求，有时会被业界嘲笑，导致了创新的机器学习实践来提高分数。正如本章所介绍的，理解这些技术可以导致更强大的模型和对机器学习整体的更深入理解。

# 工程新栏目

“我几乎总能找到我想做的事情的开源代码，我的时间花在研究和功能工程上会更好。”

–*欧文·张，Kaggle 冠军*

([https://medium . com/ka ggle-blog/profiling-top-ka gglers-Owen-Zhang-current-1-in-the-world-805 b 941 dbb 13](https://medium.com/kaggle-blog/profiling-top-kagglers-owen-zhang-currently-1-in-the-world-805b941dbb13))

许多 Kagglers 和数据科学家承认在研究和特性工程上花费了大量时间。在本节中，我们将使用`pandas`来设计新的数据列。

## 什么是特征工程？

机器学习模型和它们用来训练的数据一样好。当数据不足时，建立一个健壮的机器学习模型是不可能的。

一个更能说明问题的是，这些数据是否可以改进。当从其他列中提取新数据时，这些新的数据列被称为*工程*。

特征工程是从原始列开发新数据列的过程。问题不是你是否应该实现特征工程，而是你应该实现多少特征工程。

让我们在预测**优步**和 **Lyft** 乘坐的出租车费用的数据集上练习特征工程。

## 优步和 Lyft 的数据

除了举办比赛，Kaggle 还举办了大量数据集，包括如下的公共数据集，预测优步和 Lyft 出租车价格:[https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices](https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices):

1.  要开始，首先导入本节所需的所有库和模块，并消除警告:

    ```
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import cross_val_score
    from xgboost import XGBClassifier, XGBRFClassifier
    from sklearn.ensemble import RandomForestClassifier, StackingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, StratifiedKFold
    from sklearn.metrics import accuracy_score
    from sklearn.ensemble import VotingClassifier
    import warnings
    warnings.filterwarnings('ignore')
    ```

2.  接下来，加载`'cab_rides.csv'` CSV 文件并查看前五行。将`nrows`限制为`10000`以加快的计算。总共有 60 万+行:

    ```
    df = pd.read_csv('cab_rides.csv', nrows=10000)
    df.head()
    ```

    以下是预期的输出:

![Figure 9.2 – Cab rides dataset](img/B15551_09_02.jpg)

图 9.2–驾驶室乘坐数据集

该显示显示了广泛的列，包括分类特征和时间戳。

### 空值

像往常一样，在进行任何计算之前检查空值:

1.  回想一下`df.info()`也提供了关于列类型的信息:

    ```
    df.info()
    ```

    输出如下:

    ```
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 10000 entries, 0 to 9999
    Data columns (total 10 columns):
     #   Column            Non-Null Count  Dtype  
    ---  ------            --------------  -----  
     0   distance          10000 non-null  float64
     1   cab_type          10000 non-null  object 
     2   time_stamp        10000 non-null  int64  
     3   destination       10000 non-null  object 
     4   source            10000 non-null  object 
     5   price             9227 non-null   float64
     6   surge_multiplier  10000 non-null  float64
     7   id                10000 non-null  object 
     8   product_id        10000 non-null  object 
     9   name              10000 non-null  object 
    dtypes: float64(3), int64(1), object(6)
    memory usage: 781.4+ KB
    ```

    正如您从输出中看到的，空值存在于`price`列中，因为非空浮点数少于`10,000`。

2.  值得检查空值以查看是否可以获得关于数据的更多信息:

    ```
    df[df.isna().any(axis=1)]
    ```

    以下是输出的前五行:

    ![Figure 9.3 – Null values in the cab rides dataset](img/B15551_09_03.jpg)

    图 9.3–驾驶室乘坐数据集中的空值

    正如你所看到的，这些行没有什么特别明显的。可能是因为乘车的价格从来没有被记录下来。

3.  因为`price`是目标列，所以这些行可以通过使用`inplace=True`参数的`dropna`来删除，以确保丢弃发生在数据帧内:

    ```
    df.dropna(inplace=True)
    ```

您可以通过再次使用`df.na()`或`df.info()`来验证不存在空值。

### 特征工程时间列

**时间戳**列往往代表 **Unix 时间**，其中是自 1970 年 1 月 1 日以来的毫秒数。可以从时间戳列中提取特定的时间数据,这可以帮助预测出租车费用，例如月份、一天中的时间、是否是高峰时间等等:

1.  首先，使用`pd.to_datetime`将时间戳列转换成时间对象，然后查看前五行:

    ```
    df['date'] = pd.to_datetime(df['time_stamp'])
    df.head()
    ```

    以下是预期的输出:

    ![Figure 9.4 – The cab rides dataset after time_stamp conversion](img/B15551_09_04.jpg)

    图 9.4-时间戳转换后的出租车乘车数据集

    这个数据有问题。知道 Lyft 和优步在 1970 年不存在并不需要太多领域的专业知识。多余的小数位是转换不正确的线索。

2.  在尝试了几个乘数对进行适当的转换后，我发现`10**6`给出了适当的结果:

    ```
    df['date'] = pd.to_datetime(df['time_stamp']*(10**6))
    df.head()
    ```

    以下是预期的输出:

    ![Figure 9.5 – The cab rides dataset after 'date' conversion](img/B15551_09_05.jpg)

    图 9.5–日期转换后的出租车乘坐数据集

3.  使用日期时间列，您可以在导入`datetime`后提取新列，如`month`、`hour`和`day of week`，如下所示:

    ```
    import datetime as dt
    df['month'] = df['date'].dt.month
    df['hour'] = df['date'].dt.hour
    df['dayofweek'] = df['date'].dt.dayofweek
    ```

    现在，您可以使用这些列来设计更多的列，例如是周末还是高峰时间。

4.  以下函数通过检查`'dayofweek'`是否等同于`5`或`6`来确定一周中的天是否为周末，根据官方文档:[https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas。Series.dt.weekday.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekday.html):

    ```
    def weekend(row):
        if row['dayofweek'] in [5,6]:
            return 1
        else:
            return 0
    ```

5.  接下来，将函数作为新列`df['weekend']`应用于数据帧，如下所示:

    ```
    df['weekend'] = df.apply(weekend, axis=1)
    ```

6.  可以实施相同的策略来创建高峰时间列，方法是查看该时间是否在上午 6–10 点(时间`6–10`)和下午 3–7 点(时间`15–19`)之间:

    ```
    def rush_hour(row):
        if (row['hour'] in [6,7,8,9,15,16,17,18]) & 
            (row['weekend'] == 0):
            return 1
        else:
            return 0
    ```

7.  现在，将该函数应用于新的`'rush_hour'`列:

    ```
    df['rush_hour'] = df.apply(rush_hour, axis=1)
    ```

8.  最后五行显示了新列的变化，如`df.tail()`所示:

    ```
    df.tail()
    ```

    下面是从显示新列的输出中摘录的内容:

![Figure 9.6 – The last five rows of the cab rides dataset after feature engineering](img/B15551_09_06.jpg)

图 9.6-特征工程后驾驶室乘坐数据集的最后五行

提取和设计新时间列的过程可以继续。

注意

当设计大量新列时，有必要检查一下新特性是否有很强的相关性。数据的相关性将在本章的后面部分探讨。

既然您已经理解了特性工程时间列的实践，让我们来特性工程分类列。

### 特征工程分类列

之前，我们使用`pd.get_dummies`将分类列转换成数字列。Scikit-learn 的`OneHotEncoder`特性是另一个选项，旨在使用稀疏矩阵将分类数据转换为 0 和 1，您将在 [*第 10 章*](B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230) 、 *XGBoost 模型部署*中应用该技术。虽然使用这些选项中的任何一个将分类数据转换成数值数据是标准的，但是也存在替代方法。

虽然 0 和 1 作为分类列的数值有意义，但因为 0 表示不存在，1 表示存在，所以其他值可能会提供更好的结果。

一种策略是将分类列转换成它们的频率，这相当于每个类别在给定列中出现的次数百分比。因此，不是一列类别，而是每个类别被转换成它在列中的百分比。

接下来让我们看看将分类值转换成数值的步骤。

#### 工程频率栏

要设计一个分类列，比如`'cab_type'`，首先查看每个类别的值的数量:

1.  使用`.value_counts()`方法查看类型的频率:

    ```
    df['cab_type'].value_counts()
    ```

    结果如下:

    ```
    Uber    4654
    Lyft    4573
    Name: cab_type, dtype: int64
    ```

2.  使用`groupby`将计数放入新列。`df.groupby(column_name)`是`groupby`，而`[column_name].transform`指定要转换的列，后跟括号中的聚合:

    ```
    df['cab_freq'] = df.groupby('cab_type')['cab_type'].transform('count')
    ```

3.  用新列除以总行数，得到频率:

    ```
    df['cab_freq'] = df['cab_freq']/len(df)
    ```

4.  验证是否按预期进行了更改:

    ```
    df.tail()
    ```

    下面是来自显示新列的输出的摘录:

![Figure 9.7 – The cab rides dataset after engineering the frequency of cabs](img/B15551_09_07.jpg)

图 9.7-设计出租车频率后的出租车乘坐数据集

驾驶室频率现在显示预期输出。

### Kaggle 提示–平均编码

我们将用一种经过竞争测试的特性工程方法来结束本部分，这种方法被称为**平均编码**或**目标编码**。

均值编码根据均值目标变量将分类列转换为数字列。例如，如果橙色导致 7 个目标值 1 和 3 个目标值 0，则平均编码列将是 7/10 = 0.7。由于使用目标值时存在数据泄漏，因此需要额外的正则化离子技术。

**当训练集和测试集之间或者预测器和目标列之间的信息被共享时，数据泄漏**发生。这里的风险是目标列被直接用来影响预测列，这在机器学习中通常是一个坏主意。然而，均值编码已经被证明能够产生出色的结果。当数据集很深，并且输入数据的平均值分布大致相同时，它可以工作。正则化是一种额外的预防措施，用来减少过度拟合的可能性。

幸运的是，scikit-learn 提供了`TargetEncoder`来为您处理均值转换:

1.  首先，从`category_encoders`导入`TargetEndoder`。如果这不起作用，使用以下代码安装`category_encoders`:

    ```
    pip install --upgrade category_encoders
    from category_encoders.target_encoder import TargetEncoder
    ```

2.  接下来，初始化`encoder`，如下所示:

    ```
    encoder = TargetEncoder()
    ```

3.  最后，引入一个新列，并在编码器上使用`fit_transform`方法应用均值编码。包含正在更改的列和目标列作为参数:

    ```
    df['cab_type_mean'] = encoder.fit_transform(df['cab_type'], df['price'])
    ```

4.  现在，验证更改是否符合预期:

    ```
    df.tail()
    ```

    以下是查看新列后的输出摘录:

![Figure 9.8 – The cab rides dataset after mean encoding](img/B15551_09_08.jpg)

图 9.8-均值编码后的出租车乘车数据集

最右边的一列`cab_type_mean`，正如所料。

有关均值编码的更多信息，请参考本次 Kaggle 研究:[https://www . ka ggle . com/vprokopev/mean-likelihood-encodings-a-comprehensive-study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)。

这里的想法并不是说均值编码比一次性编码更好，而是说均值编码是一种经过验证的技术，在 Kaggle 比赛中表现很好，可能值得实施来尝试和提高分数。

### 更多功能工程

这里没有理由停止。更多的功能工程可能包括使用`groupby`和附加编码器对其他列进行统计测量。其他分类列，如目的地和到达列，可以转换为纬度和经度，然后转换为新的距离度量，如出租车距离或**文森特**距离，其中考虑了球面几何。

在 Kaggle 竞赛中，参与者可能会设计成千上万个新列，希望获得额外的几位小数。如果您有大量的工程列，您可以使用`.feature_importances_`选择最重要的列，如 [*第 2 章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) 、*决策树深度*中所述。您还可以消除高度相关的列(在下一节*构建不相关的集成*中解释)。

对于这个特殊的出租车乘车数据集，有一个包含天气的附加 CSV 文件。但是如果没有天气文件呢？您可以随时研究所提供日期的天气数据，并包括您自己的天气数据。

特征工程是任何数据科学家构建稳健模型的基本技能。这里涵盖的策略只是现有选项的一小部分。特征工程涉及研究、实验、领域专业知识、标准化列、对新列的机器学习性能的反馈，以及在最后缩小最终列的范围。

现在，您已经理解了特性工程的各种策略，让我们继续构建不相关的集成。

# 构建不相关的系综

“在我们的最终模型中，我们将 XGBoost 作为一个集合模型，其中包括 20 个 XGBoost 模型、5 个随机森林、6 个随机决策树模型、3 个正则化贪婪森林、3 个逻辑回归模型、5 个 ANN 模型、3 个弹性网络模型和 1 个 SVM 模型。”

–*宋，Kaggle 冠军*

([https://hunch243.rssing.com/chan-68612493/all_p1.html](https://hunch243.rssing.com/chan-68612493/all_p1.html)

Kaggle 比赛的获奖模特很少是个人模特；他们几乎总是合奏。我说的集成并不是指增强或打包模型，比如随机森林或 XGBoost，而是指包含任何不同模型的纯集成，包括 XGBoost、随机森林等。

在本节中，我们将把机器学习模型结合到不相关的集成中，以获得准确性并减少过拟合。

## 型号范围

用于预测患者是否患有乳腺癌的威斯康星州乳腺癌数据集有 569 行 30 列，可在[https://sci kit-learn . org/stable/modules/generated/sk learn . datasets . load _ Breast _ Cancer . html 查看？highlight = load _ 乳腺癌 _ 癌症](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer)。

以下是使用几个分类器准备和评分数据集的步骤:

1.  从 scikit-learn 导入`load_breast_cancer`数据集，以便我们可以快速开始构建模型:

    ```
    from sklearn.datasets import load_breast_cancer
    ```

2.  通过设置`return_X_y=True`参数，将预测列分配给`X`，将目标列分配给`y`:

    ```
    X, y = load_breast_cancer(return_X_y=True)
    ```

3.  使用`StratifiedKFold`准备 5 重交叉验证以确保一致性:

    ```
    kfold = StratifiedKFold(n_splits=5)
    ```

4.  现在，构建一个简单的分类函数，将模型作为输入，并将平均交叉验证分数作为输出返回:

    ```
    def classification_model(model):
        scores = cross_val_score(model, X, y, cv=kfold)
        return scores.mean()
    ```

5.  获得几个默认分类器的分数，包括 XGBoost，以及它的备选基础学习器、随机森林和逻辑回归:

    a)用 XGBoost 进行评分:

    ```
    classification_model(XGBClassifier())
    ```

    分数如下:

    ```
    0.9771619313771154
    ```

    b)用`gblinear`评分:

    ```
    classification_model(XGBClassifier(booster='gblinear'))
    ```

    分数如下:

    ```
    0.5782952957615277
    ```

    c)用`dart`评分:

    ```
    classification_model(XGBClassifier(booster='dart', one_drop=True))
    ```

    分数如下:

    ```
    0.9736376339077782
    ```

    注意，对于 dart booster，我们设置了`one_drop=True`以确保树确实被丢弃。

    d)用`RandomForestClassifier`评分:

    ```
    classification_model(RandomForestClassifier(random_state=2))
    ```

    分数如下:

    ```
    0.9666356155876418
    ```

    e)用`LogisticRegression`评分:

    ```
    classification_model(LogisticRegression(max_iter=10000))
    ```

    分数如下:

    ```
    0.9490451793199813
    ```

大多数模型都表现不错，其中 XGBoost 分类器得分最高。然而，`gblinear`基础学习器的表现不是特别好，所以我们不会继续使用它。

在实践中，这些模型中的每一个都应该被调整。因为我们已经在多个章节中讨论了超参数调优，所以这里不再讨论这个选项。然而，超参数的知识可以给尝试具有一些调整值的快速模型带来信心。例如，如以下代码所示，可以在 XGBoost 上尝试将`max_depth`降低到`2`，将`n_estimators`增加到`500`，并确保`learning_rate`设置为`0.1`:

```
classification_model(XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1))
```

分数如下:

```
0.9701133364384411
```

这是一个非常好的分数。虽然不是最高的，但在我们合奏中可能是有价值的。

现在我们有了各种各样的模型，让我们了解一下它们之间的相互关系。

## 相关性

本节的目的是不选择所有模型用于集合，而是选择不相关的模型。

先来了解一下**关联**代表什么。

相关性是`-1`和`1`之间的统计度量，表示两组点之间的线性关系的强度。`1`的相关性是一条完美的直线，而`0`的相关性显示没有任何线性关系。

一些关于相关性的视觉效果会让事情变得更清楚。以下图片摘自 https://en.wikipedia.org/wiki/Correlation_and_dependence[的*相关性和依赖性*页面:](https://en.wikipedia.org/wiki/Correlation_and_dependence)

*   具有列出的相关性的散点图如下所示:

![Figure 9.9 – Listed Correlations](img/B15551_09_09.jpg)

图 9.9-列出的相关性

许可证信息

由 DenisBoigelot 原创上传者是 image creator——自己的作品，CC0，[https://commons.wikimedia.org/w/index.php?curid=15165296](https://commons.wikimedia.org/w/index.php?curid=15165296)。

*   Anscombe 的四重奏-四个相关度为 **0.816** 的散点图如下:

![Figure 9.10 – Correlation of 0.816](img/B15551_09_10.jpg)

图 9.10-0.816 的相关性

许可证信息

由 Anscombe.svg:史高斯(使用下标标注):大道–anscombe . SVG，CC BY-SA 3.0，[https://commons.wikimedia.org/w/index.php?curid=9838454](https://commons.wikimedia.org/w/index.php?curid=9838454)

第一个例子表明相关性越高，点通常越接近直线。第二个例子表明，相同相关性的数据点可能有很大不同。换句话说，相关性提供了有价值的信息，但并不能说明全部。

既然你理解了相关性的含义，让我们将相关性应用于构建机器学习集成。

## 机器学习集成中的相关性

现在我们来选择哪些模特会出现在我们的服装中。

在集成中，机器学习模型之间的高度相关性是不期望的。但是为什么呢？

考虑两个分类器各有 1000 个预测的情况。如果这些分类器都做出相同的预测，那么从第二个分类器中不会获得新的信息，使其变得多余。

使用*多数规则*实现，只有当大多数分类器出错时，预测才是错误的。因此，我们希望有多种多样的模型，这些模型得分很高，但给出不同的预测。如果大多数模型给出相同的预测，则相关性很高，并且将新模型添加到集合中没有什么价值。在强模型可能出错的地方寻找预测的差异，给了集合产生更好结果的机会。当模型不相关时，预测将会不同。

为了计算机器学习模型之间的相关性，我们首先需要数据点进行比较。机器学习模型产生的不同数据点就是它们的预测。获得预测后，我们将它们连接成一个数据帧，然后应用`.corr`方法一次性获得所有相关性。

以下是寻找机器学习模型之间相关性的步骤:

1.  定义一个为每个机器学习模型返回预测的函数:

    ```
    def y_pred(model):
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        score = accuracy_score(y_pred, y_test)
        print(score)
        return y_pred
    ```

2.  使用`train_test_split`为一次预测准备数据:

    ```
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)
    ```

3.  使用之前定义的函数获得所有候选分类器的预测值:

    a) `XGBClassifier`使用如下:

    ```
    y_pred_gbtree = y_pred(XGBClassifier())
    ```

    准确度得分如下:

    ```
    0.951048951048951
    ```

    b) `XGBClassifier`与`dart`使用如下:

    ```
    y_pred_dart = y_pred(XGBClassifier(booster='dart', one_drop=True))
    ```

    准确度得分如下:

    ```
    0.951048951048951
    ```

    c) `RandomForestClassifier`使用如下:

    ```
    y_pred_forest = y_pred(RandomForestClassifier())
    ```

    准确度得分如下:

    ```
    0.9370629370629371
    ```

    d) `LogisticRegression`使用以下:

    ```
    y_pred_logistic = y_pred(LogisticRegression(max_iter=10000))
    ```

    准确度得分如下:

    ```
    0.9370629370629371
    y_pred_xgb = y_pred(XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1))
    ```

    准确度得分如下:

    ```
    0.965034965034965
    ```

4.  使用`np.c`(`c`是串联的缩写)将预测连接到一个新的数据帧中:

    ```
    df_pred = pd.DataFrame(data= np.c_[y_pred_gbtree, y_pred_dart, y_pred_forest, y_pred_logistic, y_pred_xgb], columns=['gbtree', 'dart','forest', 'logistic', 'xgb'])
    ```

5.  使用`.corr()`方法在数据帧上运行相关性:

    ```
    df_pred.corr()
    ```

    您应该会看到以下输出:

![Figure 9.11 – Correlations between various machine learning models](img/B15551_09_11.jpg)

图 9.11–各种机器学习模型之间的相关性

如你所见，对角线上的所有相关性都是`1.0`，因为模型和它本身之间的相关性必须是完全线性的。所有其他值都相当高。

没有明确的截止来获得非相关阈值。它最终取决于相关值和可供选择的模型数量。对于这个例子，我们可以选择下两个与我们的最佳模型`xgb`最不相关的模型，它们是随机森林和逻辑回归。

现在我们已经选择了我们的模型，我们将使用接下来介绍的`VotingClassifier`系综把它们组合成一个系综。

## 投票分类器集成

Scikit-learn 的`VotingClassifier` ensemble 是，旨在结合多个分类模型，并使用多数规则为每个预测选择输出。请注意，scikit-learn 还附带了`VotingRegressor`，它通过取每个模型的平均值来组合多个回归模型。

以下是在 scikit-learn 中创建合奏的步骤:

1.  初始化空列表:

    ```
    estimators = []
    ```

2.  初始化第一个模型:

    ```
    logistic_model = LogisticRegression(max_iter=10000)
    ```

3.  将模型作为元组以`(model_name, model)`的形式添加到列表中:

    ```
    estimators.append(('logistic', logistic_model))
    ```

4.  根据需要多次重复*步骤 2* 和 *3* :

    ```
    xgb_model = XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1)
    estimators.append(('xgb', xgb_model))
    rf_model = RandomForestClassifier(random_state=2)
    estimators.append(('rf', rf_model))
    ```

5.  使用型号列表作为输入，初始化`VotingClassifier`(或`VotingRegressor`):

    ```
    ensemble = VotingClassifier(estimators)
    ```

6.  使用`cross_val_score`对分类器进行评分:

    ```
    scores = cross_val_score(ensemble, X, y, cv=kfold)
    print(scores.mean())
    ```

    分数如下:

    ```
    0.9754075454122031
    ```

如你所见，分数提高了。

既然你已经理解了构建非相关机器学习集成的目的和技术，那么让我们继续讨论一种类似但潜在有利的技术，叫做堆叠。

# 堆叠模型

"对于堆叠和提升，我使用 xgboost，同样主要是因为熟悉和它被证明的结果."

–*大卫·奥斯汀，Kaggle 冠军*

([https://www . pyimagesearch . com/2018/03/26/interview-David-Austin-1st-place-25000-kaggles-popular-competition/](https://www.pyimagesearch.com/2018/03/26/interview-david-austin-1st-place-25000-kaggles-popular-competition/))

在这最后一部分，我们将研究 Kaggle 赢家最常用的一个最强大的技巧，叫做堆叠。

## 什么是堆叠？

堆叠在两个不同的层次上结合了机器学习模型:基础层次，其模型对所有数据进行预测；元层次，其将基础模型的预测作为输入，并使用它们来生成最终预测。

换句话说，堆叠中的最终模型不将原始数据作为输入，而是将基础机器学习模型的预测作为输入。

堆叠模型在 Kaggle 竞赛中获得了巨大的成功。大多数 Kaggle 比赛都有合并截止日期，个人和团队可以一起加入。这些合并可以作为团队而不是个人带来更大的成功，因为竞争对手可以建立更大的群体，并将他们的模型堆叠在一起。

注意，叠加不同于标准集合，因为元模型在最后结合了预测。因为元模型将预测值作为输入，所以通常建议使用简单的元模型，例如用于回归的线性回归和用于分类的逻辑回归。

现在您对什么是堆叠有了一个概念，让我们用 scikit-learn 应用堆叠。

## 在 scikit 中堆叠-学习

幸运的是，scikit-learn 附带了一个堆叠回归器和分类器，使得这个过程相当简单。大致思路和上一节的系综模型很像。选择各种基础模型，然后为元模型选择线性回归或逻辑回归。

以下是使用 scikit-learn 堆叠的步骤:

1.  创建基础模型的空列表:

    ```
    base_models = []
    ```

2.  使用语法`(name, model)`将所有基础模型作为元组添加到基础模型列表中:

    ```
    base_models.append(('lr', LogisticRegression()))
    base_models.append(('xgb', XGBClassifier()))
    base_models.append(('rf', RandomForestClassifier(random_state=2)))
    ```

    堆叠时可以选择更多的模型，因为没有多数规则限制，并且线性权重更容易根据新数据进行调整。最佳方法是使用非相关性作为宽松的指导方针，并试验不同的组合。

3.  选择一个元模型，最好是线性回归用于回归，逻辑回归用于分类:

    ```
    meta_model = LogisticRegression()
    ```

4.  使用`base_models`的`estimators`和`meta_model`的`final_estimator`初始化`StackingClassifier`(或`StackingRegressor`):

    ```
    clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)
    ```

5.  使用`cross_val_score`或任何其他评分方法验证堆叠模型:

    ```
    scores = cross_val_score(clf, X, y, cv=kfold)
    print(scores.mean())
    ```

    分数如下:

    ```
    0.9789318428815401
    ```

这是迄今为止最强的结果。

正如你所看到的，叠加是一个令人难以置信的强大的方法，并且优于上一节的非相关系综。

# 总结

在本章中，您从 Kaggle 竞赛的获胜者那里学到了一些久经考验的技巧和诀窍。除了探索 Kaggle 竞赛和理解拒绝集合的重要性之外，您还获得了在特征工程时间列、特征工程分类列、均值编码、构建非相关集成和堆叠方面的基本实践。这些先进的技术在精英 Kagglers 中广泛传播，它们可以在为研究、竞争和行业开发机器学习模型时给你带来优势。

在下一章也是最后一章中，我们将从竞争世界转移到技术世界，在那里我们将从头到尾使用变压器和管道构建一个 XGBoost 模型，以完成一个可用于行业部署的模型。