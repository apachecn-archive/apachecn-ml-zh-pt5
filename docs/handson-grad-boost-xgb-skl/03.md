

# *第二章*:深度决策树

在本章中，您将熟练使用**决策树**，这是构建 XGBoost 模型的主要机器学习算法。您还将获得关于**超参数微调**的科学和艺术的第一手经验。由于决策树是 XGBoost 模型的基础，所以您在本章中学习的技能对于构建健壮的 XGBoost 模型至关重要。

在本章中，您将构建并评估**决策树分类器**和**决策树回归器**，根据方差和偏差可视化并分析决策树，并微调决策树超参数。此外，您将把决策树应用到预测患者心脏病的案例研究中。

本章涵盖以下主要主题:

*   用 XGBoost 介绍决策树

*   探索决策树

*   对比方差和偏差

*   调整决策树超参数

*   预测心脏病——案例研究

# 使用 XGBoost 引入决策树

XGBoost 是一种**集成方法**，意味着它是由不同的机器学习模型组合在一起工作。在 XGBoost 中，组成整体的单个模型被称为**基础学习者**。

决策树是最常用的 XGBoost 基础学习器，在机器学习领域是独一无二的。与线性回归和逻辑回归( [*第一章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022) ，*机器学习景观*)中用数字权重乘以列值不同，决策树通过询问关于列的问题来拆分数据。事实上，构建决策树就像玩一个 20 个问题的游戏。

例如，决策树可能有一个温度列，该列可以分为两组，一组温度高于 70 度，另一组温度低于 70 度。下一次分裂可能是基于季节，如果是夏天，跟随一个分支，否则跟随另一个分支。现在数据已经被分成四个独立的组。这种通过分支将数据分成新组的过程持续进行，直到算法达到期望的精度水平。

决策树可以创建数千个分支，直到它将每个样本唯一地映射到训练集中的正确目标。这意味着训练集可以具有 100%的准确性。然而，这样的模型不能很好地推广到新的数据。

决策树容易过度适应数据。换句话说，决策树可能与训练数据的映射关系过于密切，这个问题将在本章后面的方差和偏差中探讨。超参数微调是防止过度拟合的一种解决方案。另一个解决方案是聚合许多树的预测，这是**随机森林**和 XGBoost 采用的策略。

虽然随机森林和 XGBoost 将是后续章节的重点，但我们现在深入研究决策树。

# 探索决策树

决策树通过将数据分成*个分支*来工作。这些分支被追踪到*树叶*，在那里进行预测。有了一个实际的例子，理解树枝和树叶是如何产生的就容易多了。在深入细节之前，让我们建立第一个决策树模型。

## 第一个决策树模型

我们首先构建一个决策树，使用来自 [*第一章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022) 、*机器学习前景*的人口普查数据集来预测某人是否赚了超过 5 万美元:

1.  首先，打开一个新的 Jupyter 笔记本，从以下导入开始:

    ```
    import pandas as pd
    import numpy as np
    import warnings
    warnings.filterwarnings('ignore')
    ```

2.  接下来，打开已经上传到[https://github . com/packt publishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/chapter 02](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter02)的文件`'census_cleaned.csv'`。如果你从 Packt GitHub 页面下载了这本书的所有文件，如*前言*中所推荐的，你可以导航到 [*第 2 章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) *，深度决策树*，在启动 Anaconda 之后，以你导航到其他章节的相同方式。否则，请访问我们的 GitHub 页面并立即克隆文件:

    ```
    df_census = pd.read_csv('census_cleaned.csv')
    ```

3.  将数据上传到 DataFrame 后，按照下面的声明您的预测值和目标列`X`和`y`:

    ```
    X = df_census.iloc[:,:-1]
    y = df_census.iloc[:,-1]
    ```

4.  接下来，导入`train_test_split`将数据分割成用`random_state=2`设置的训练和测试，以确保结果一致:

    ```
    from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)
    ```

    与其他机器学习分类器一样，当使用决策树时，我们初始化模型，使其适合训练集，并使用`accuracy_score`进行测试。

`accuracy_score`决定了正确预测数除以预测总数。如果 20 个预测中有 19 个是正确的，那么`accuracy_score`就是 95%。

首先，导入`DecisionTreeClassifier`和`accuracy_score`:

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
```

接下来，我们用标准步骤构建决策树分类器:

1.  用`random_state=2`初始化机器学习模型，以确保结果一致:

    ```
    clf = DecisionTreeClassifier(random_state=2)
    ```

2.  在训练集上拟合模型:

    ```
    clf.fit(X_train, y_train)
    ```

3.  对测试集进行预测:

    ```
    y_pred = clf.predict(X_test)
    ```

4.  将预测与测试集进行比较:

    ```
    accuracy_score(y_pred, y_test) 
    ```

    `accuracy_score`如下所示:

    ```
    0.8131679154894976
    ```

81%的准确度与来自第一章*中的 [*相同数据集的逻辑回归的准确度相当。*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)*

 *既然您已经看到了如何构建决策树，那么让我们来看看它的内部。

## 决策树内部

决策树有很好的视觉效果，揭示了它们的内部工作原理。

这是一个来自人口普查数据集的决策树，只有两个分支:

![Figure 2.1 – Census dataset decision tree](img/B15551_02_01.jpg)

图 2.1-人口普查数据集决策树

树顶是根，**真** / **假**箭头是分支，数据点是节点。在树的末端，节点被分类为叶子。让我们深入研究前面的图表。

### 根

树根在顶端。第一行写着**婚姻状况 _ 已婚公民-配偶< =5** 。**婚姻状况**是一个二进制列，所以所有值都是 **0** (负)或 **1** (正)。第一次分裂是基于某人是否结婚。树的左边是**真**分支，表示用户未婚，右边是**假**分支，表示用户已婚。

### 基尼标准

根的第二行表示`Gini`索引为 0 意味着 0 个错误。基尼系数为 1 意味着所有的错误。基尼指数为 0.5，表明元素分布均等，意味着预测并不比随机猜测好多少。越接近 0，误差越低。从根本上说，0.364 的基尼系数意味着训练集与类别 1 的 36.4%不平衡。

基尼指数的公式如下:

![Figure 2.2 – gini index equation](img/Formula_02_001.jpg)

图 2.2-基尼指数方程

![](img/Formula_02_002.png)是拆分产生正确值的概率，c 是类的总数:在前面的示例中为 2。从另一个角度来看，![](img/Formula_02_003.png)是集合中具有正确输出标签的部分。

### 样本、值、类别

树的根表示有 24420 个样本。这是训练集中样本的总数。下面一行写着**【18575，5845】**。排序是先 0 后 1，因此 18，575 个样本的值为 0(它们产生的值小于 50K)，5，845 个样本的值为 1(它们产生的值大于 50K)。

### 真/假节点

沿着第一个分支，你会看到左侧的**真**，右侧的假**。真-左和假-右的模式贯穿整个树。**

在第二行的左侧节点中，分割的 **capital_gain < = 7073.5** 被应用于后续节点。剩下的信息来自前一个分支上面的拆分。在 13160 名未婚人士中，12311 人的收入低于 5 万英镑，而 849 人的收入超过 5 万英镑。基尼系数为 0.121(T4)，这是一个非常好的分数。

### 柱

有可能一棵树只有一个裂缝。这样的树被称为**树桩**。虽然树桩本身并不是强大的预测器，但树桩在用作助推器时可以变得强大，正如 [*第四章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093) *所述，从梯度助推到 XGBoost* 。

### 离开

树末端的节点是叶子。树叶包含了所有的最终预测。

最左叶的基尼指数**为 0.093** ，正确预测 12938 例中的 12304 例，为 95%。我们有 95%的信心，资本收益低于 7，073.50 英镑的未婚用户收入不会超过 5 万英镑。

其他叶可以类似地解释。

现在让我们看看这些预测错在哪里。

# 对比方差和偏差

假设您有下图中显示的数据点。你的任务是拟合一条直线或曲线，让你预测新的点。

这是一个随机点的图表:

![Figure 2.3 – Graph of random points](img/B15551_02_03.jpg)

图 2.3-随机点图表

一个想法是使用线性回归，最小化每个点和直线之间的距离的平方，如下图所示:

![Figure 2.4 – Minimizing distance using Linear Regression](img/B15551_02_04.jpg)

图 2.4–使用线性回归最小化距离

直线一般有较高的**偏差**。在机器学习中，偏差是一个数学术语，来自于在将模型应用于现实生活问题时对误差的估计。直线的偏差很大，因为预测仅限于直线，无法解释数据的变化。

在许多情况下，一条直线不够复杂，不足以做出准确的预测。当这种情况发生时，我们说机器学习模型以高偏差对数据进行了欠拟合。

第二种选择是用八次多项式拟合这些点。由于只有九个点，八次多项式将完美地拟合数据，如下图所示:

![Figure 2.5 – Eight-degree poynomial](img/B15551_02_05.jpg)

图 2.5-八度多项式

该模型具有高**方差**。在机器学习中，方差是一个数学术语，表示在给定一组不同的训练数据的情况下，模型将改变多少。形式上，方差是随机变量与其均值之间的方差的度量。给定训练集中九个不同的数据点，八次多项式将完全不同，导致高方差。

具有高方差的模型通常会过度拟合数据。这些模型不能很好地推广到新的数据点，因为它们与训练数据太接近了。

在大数据的世界里，过度拟合是一个大问题。更多的数据会产生更大的训练集，决策树等机器学习模型太适合训练数据了。

作为最后一个选择，考虑一个三次多项式来拟合数据点，如下图所示:

![Figure 2.6 – Third-degree polynomial](img/B15551_02_06.jpg)

图 2.6–三次多项式

这个三次多项式在方差和偏差之间提供了一个很好的平衡，通常遵循曲线，但适应变化。低方差意味着不同的训练集不会产生差异很大的曲线。低偏差表示将该模型应用于真实情况时的误差不会太高。在机器学习中，低方差和低偏差的组合是理想的。

在方差和偏差之间取得良好平衡的最佳机器学习策略之一是微调超参数。

# 调整决策树超参数

超参数与参数不同。

在机器学习中，当模型被调整时，参数被调整。例如，线性和逻辑回归中的权重是在构建阶段调整的参数，以最小化误差。相比之下，超参数是在构建阶段之前选择的。如果没有选择超参数，则使用默认值。

## 决策树回归器

了解超参数的最好方法是通过实验。尽管在所选择的超参数范围背后有理论，但结果胜过理论。不同的数据集看到不同的超参数值的改进。

在选择超参数之前，让我们通过以下步骤使用`DecisionTreeRegressor`和`cross_val_score`找到基线分数:

1.  下载`'bike_rentals_cleaned'`数据集，并将其分成`X_bikes`(预测列)和`y_bikes`(训练列):

    ```
    df_bikes = pd.read_csv('bike_rentals_cleaned.csv')X_bikes = df_bikes.iloc[:,:-1]y_bikes = df_bikes.iloc[:,-1]
    ```

2.  导入`DecisionTreeRegressor`和`cross_val_score`:

    ```
    from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import cross_val_score
    ```

3.  初始化`DecisionTreeRegressor`并在`cross_val_score`中拟合模型:

    ```
    reg = DecisionTreeRegressor(random_state=2)
    scores = cross_val_score(reg, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=5)
    ```

4.  计算到`1233.36`。这比《T4》*第一章* *、机器学习景观*中从线性回归得到的`972.06`，以及从 XGBoost 得到的`887.31`都要差。

模型是否因为方差过高而过度拟合数据？

这个问题可以通过观察决策树单独对训练集进行预测的效果来回答。下面的代码在对测试集进行预测之前检查定型集的错误:

```
reg = DecisionTreeRegressor()reg.fit(X_train, y_train)y_pred = reg.predict(X_train)
from sklearn.metrics import mean_squared_error reg_mse = mean_squared_error(y_train, y_pred)reg_rmse = np.sqrt(reg_mse)reg_rmse
```

结果如下:

```
0.0
```

`0.0`的 RMSE 意味着模型完美地拟合了每个数据点！这个完美的分数加上交叉验证误差`1233.36`证明决策树过度拟合了具有高方差的数据。训练集非常适合，但测试集却严重失误。

超参数可以纠正这种情况。

## 一般超参数

所有 scikit-learn 型号的超参数详情可在 scikit-learn 的官方文档页面上查看。

以下是摘自 DecisionTreeRegressor 网站的([https://sci kit-learn . org/stable/modules/generated/sk learn . tree . DecisionTreeRegressor . html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html))。

注意

*sklearn* 是 *scikit-learn* 的简称。

![Figure 2.7\. Excerpt of DecisionTreeRegressor official documentation page](img/B15551_02_07.jpg)

图 2.7。决策树回归器官方文档页面摘录

官方文档解释了超参数背后的含义。注意这里的`Parameters`是*超参数*的简称。当你独自工作时，查阅官方的文档是你最可靠的资源。

让我们一次检查一个超参数。

### 最大深度

`max_depth`定义树的深度，由分割的次数确定。默认情况下，`max_depth`没有限制，因此可能会有数百或数千个分割导致过度拟合。通过将`max_depth`限制为较小的数字，方差减少了，并且模型更好地推广到新数据。

如何能为`max_depth`选择最好的数字？

你总是可以尝试`max_depth=1`，然后`max_depth=2`，然后`max_depth=3`，等等，但是这个过程会很累。相反，你可以使用一个叫做`GridSearchCV`的奇妙工具。

### GridSearchCV

`GridSearchCV`使用交叉验证搜索超参数的网格，以提供最佳结果。

作为任何机器学习算法，这意味着它适合训练集，并在测试集上得分。主要区别在于`GridSearchCV`在最终确定模型之前检查所有超参数。

用`GridSearchCV`的关键是建立一个超参数值的字典。没有一组正确的值可供尝试。一种策略是选择一个最小和最大的值，在它们之间有均匀间隔的数字。因为我们试图减少过度拟合，所以一般的想法是在较低的一侧为`max_depth`尝试更多的值。

导入`GridSearchCV`并为`max_depth`定义如下超参数列表:

```
from sklearn.model_selection import GridSearchCV params = {'max_depth':[None,2,3,4,6,8,10,20]}
```

`params`字典包含一个写为字符串的键`'max_depth'`和一个值，即我们选择的数字列表。注意，`None`是默认的，意味着对`max_depth`没有限制。

小费

一般来说，减少最大超参数和增加最小超参数将减少变化并防止过度拟合。

接下来，初始化一个`DecisionTreeRegressor`，并将其与`params`和评分标准一起放入`GridSearchCV`中:

```
reg = DecisionTreeRegressor(random_state=2)grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)grid_reg.fit(X_train, y_train)
```

现在`GridSearchCV`已经适合数据，您可以查看如下最佳超参数:

```
best_params = grid_reg.best_params_print("Best params:", best_params)
```

结果如下:

```
Best params: {'max_depth': 6}
```

正如所见，`6`的`max_depth`值导致了训练集中的最佳交叉验证分数。

可以使用`best_score`属性显示训练分数:

```
best_score = np.sqrt(-grid_reg.best_score_)print("Training score: {:.3f}".format(best_score))
```

分数如下:

```
Training score: 951.938
```

测试分数可能显示如下:

```
best_model = grid_reg.best_estimator_
y_pred = best_model.predict(X_test) 
rmse_test = mean_squared_error(y_test, y_pred)**0.5
print('Test score: {:.3f}'.format(rmse_test))
```

分数为如下:

```
Test score: 864.670
```

差异已经大大减少。

### 最小样本叶

`min_samples_leaf`通过增加一片叶子可能拥有的样本数量来给提供一个限制。与`max_depth`一样，`min_samples_leaf`旨在减少过度拟合。

当没有限制时，`min_samples_leaf=1`是缺省值，这意味着叶子可能由唯一的样本组成(容易过度拟合)。增加`min_samples_leaf`会减少方差。如果`min_samples_leaf=8`，所有的叶子必须包含八个或更多的样本。

测试`min_samples_leaf`的一系列值需要经历与之前相同的过程。我们没有复制和粘贴，而是编写了一个函数，使用`GridSearchCV`显示最佳参数、训练分数和测试分数，并将`DecisionTreeRegressor(random_state=2)`分配给`reg`作为默认参数:

```
def grid_search(params, reg=DecisionTreeRegressor(random_state=2)):
    grid_reg = GridSearchCV(reg, params,   
    scoring='neg_mean_squared_error', cv=5, n_jobs=-1):
    grid_reg.fit(X_train, y_train)
       best_params = grid_reg.best_params_    print("Best params:", best_params)    best_score = np.sqrt(-grid_reg.best_score_)    print("Training score: {:.3f}".format(best_score))
    y_pred = grid_reg.predict(X_test)    rmse_test = mean_squared_error(y_test, y_pred)**0.5
    print('Test score: {:.3f}'.format(rmse_test))
```

小费

当编写自己的函数时，包含默认的关键字参数是有利的。default 关键字参数是一个具有默认值的命名参数，在以后的使用和测试中可以对其进行更改。默认关键字参数极大地增强了 Python 的功能。

当选择超参数的范围时，了解模型所基于的训练集的大小是很有帮助的。Pandas 附带了一个很好的方法，`.shape`，它返回的行和列的数据:

```
X_train.shape
```

数据的行和列如下:

```
(548, 12)
```

由于训练集有`548`行，这有助于确定`min_samples_leaf`的合理值。让我们试试`[1, 2, 4, 6, 8, 10, 20, 30]`作为我们`grid_search`的输入:

```
grid_search(params={'min_samples_leaf':[1, 2, 4, 6, 8, 10, 20, 30]})
```

分数如下:

```
Best params: {'min_samples_leaf': 8}
Training score: 896.083
Test score: 855.620
```

因为测试分数比训练分数好，所以方差减少了。

当我们把`min_samples_leaf`和`max_depth`放在一起会发生什么？让我们看看:

```
grid_search(params={'max_depth':[None,2,3,4,6,8,10,20],'min_samples_leaf':[1,2,4,6,8,10,20,30]})
```

分数如下:

```
Best params: {'max_depth': 6, 'min_samples_leaf': 2}
Training score: 870.396
Test score: 913.000
```

结果可能是一个惊喜。即使训练分数提高了，测试分数却没有提高。`min_samples_leaf`从`8`减少到`2`，而`max_depth`保持不变。

小费

这是超参数调优的一个有价值的教训:超参数不应该孤立地选择。

至于减少前面的例子中的方差，将`min_samples_leaf`限制为大于 3 的值可能会有帮助:

```
grid_search(params={'max_depth':[6,7,8,9,10],'min_samples_leaf':[3,5,7,9]})
```

分数如下:

```
Best params: {'max_depth': 9, 'min_samples_leaf': 7}
Training score: 888.905
Test score: 878.538
```

如你所见，考试分数提高了。

我们现在将探索剩余的决策树超参数，而不进行单独测试。

### max_leaf_nodes

`max_leaf_nodes`类似于和`min_samples_leaf`吗？它不是指定每片叶子的样本数，而是指定叶子的总数。因此，`max_leaf_nodes=10`意味着模型的叶子不能超过 10 片。它可以更少。

### 最大 _ 功能

`max_features`是减少方差的有效超参数。它不是考虑每一个可能的分割特征，而是从每一轮选择的特征中进行选择。

看到`max_features`的标准选项如下:

*   `'auto'`是默认值，没有任何限制。

*   `'sqrt'`是特征总数的平方根。

*   `'log2'`是以 2 为基数的特征总数的对数。32 列解析为 5 列，因为 2 ^5 = 32。

### 最小 _ 样本 _ 分割

另一个劈法是`min_samples_split`。如名称所示，`min_samples_split`提供了在进行分割之前所需样本数量的限制。默认为`2`，因为两个样本可能会被分割成一个样本，以单个叶子结束。如果限制增加到`5`，则不允许对具有五个或更少样本的节点进行进一步分割。

### 分流器

`splitter`、`'random'`、`'best'`有两个选项。拆分器告诉模型如何选择特征来拆分每个分支。默认的`'best'`选项选择能获得最大信息的特性。相比之下，`'random'`选项选择随机分割。

将`splitter`改为`'random'`是防止过度拟合和使树多样化的好方法。

### 标准

用于拆分决策树回归器的`criterion`和分类器是不同的。`criterion`提供了机器学习模型用来确定应该如何分割的方法。这是劈叉的计分方法。对于每个可能的拆分，`criterion`会计算一个可能拆分的数字，并将其与其他选项进行比较。得分最高的一方获胜。

决策树回归器的选项有`mse`(均方误差)`friedman_mse`(包含弗里德曼调整)和`mae`(平均绝对误差)。默认为`mse`。

对于分类器来说，`gini`(之前描述过的)和`entropy`通常会给出类似的结果。

#### 最小 _ 杂质 _ 减少

以前的称为为`min_impurity_split`，`min_impurity_decrease`在杂质大于或等于该值时导致分裂。

*杂质*是对每个节点的预测有多纯的度量。准确度为 100%的树的杂质为 0.0。准确度为 80%的树的杂质为 0.20。

杂质是决策树中的一个重要概念。在整个造树过程中，杂质应该不断减少。为每个节点选择导致杂质最大程度减少的分裂。

默认值为`0.0`。这个数字可以增加，这样当达到某个阈值时，树就会停止生长。

#### 最小重量分数叶

`min_weight_fraction_leaf`是成为一片叶子所需的总权重的最小加权分数。根据文件，当没有提供样品重量时*样品重量相等。*

出于实用目的，`min_weight_fraction_leaf`是另一个超参数，它可以减少 ces 方差并防止过度拟合。默认值为 0.0。假设权重相等，1%的限制，0.01，将要求 500 个样本中至少有 5 个是叶子。

#### ccp 阿尔法

这里将不讨论`ccp_alpha`超参数，因为它是为在树建成后进行修剪而设计的。有关完整的讨论，请查看最小成本复杂性修剪:[https://sci kit-learn . org/stable/modules/tree . html # minimal-cost-complex-pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)。

## 把所有这些放在一起

当微调超参数时，几个因素开始起作用:

*   分配的时间

*   超参数的数量

*   所需精确度的小数位数

所花费的时间、微调的超参数数量和所需的精度取决于您、数据集和手头的项目。因为超参数是相互关联的，所以不需要全部修改它们。微调一个较小的范围可能会得到更好的结果。

既然您已经理解了决策树和决策树超参数的基础知识，那么是时候应用您所学的知识了。

小费

决策树超参数太多，无法一致地使用它们。以我的经验来看，`max_depth`、`max_features`、`min_samples_leaf`、`max_leaf_nodes`、`min_impurity_decrease`、`min_samples_split`往往就足够了。

# 预测心脏病——病例研究

你已经被一家医院要求使用机器学习来预测心脏病。你的工作是开发一个模型，并强调医生和护士可以关注的两到三个重要特征，以改善患者的健康。

您决定使用带有微调超参数的决策树分类器。模型建立后，您将使用`feature_importances_`来解释结果，这个属性决定了预测心脏病时最重要的特征。

## 心脏病数据集

心脏病数据集已上传至 GitHub，命名为`heart_disease.csv`。这是对 UCI 机器学习知识库(【https://archive.ics.uci.edu/ml/index.php】)提供的原始心脏病数据集(【https://archive.ics.uci.edu/ml/datasets/Heart+Disease】[)的一个微小修改，为了方便起见，空值被清除了。](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)

上传文件并显示前五行，如下所示:

```
df_heart = pd.read_csv('heart_disease.csv')df_heart.head()
```

上述代码生成了下表:

![Figure 2.8 – heart_disease.csv output](img/B15551_02_08.jpg)

图 2.8–heart _ disease . CSV 输出

目标列，方便地标记为“`target`”是二元的，`1`表示病人有心脏病，`0`表示他们没有。

以下是预测值列的含义，取自之前链接的数据源:

*   `age`:以年为单位的年龄

*   `sex`:性别(`1` =男性；`0` =女)

*   `cp`:胸痛型(`1` =典型心绞痛、`2` =非典型心绞痛、`3` =非心绞痛、`4` =无症状)

*   `trestbps` : 静息血压(入院时以毫米汞柱为单位)

*   `chol`:血清胆固醇 mg/dl 6 fbs:(空腹血糖> 120 mg/dl) ( `1` =真；`0` =假)

*   `fbs`:空腹血糖> 120 mg/dl ( `1` =真；`0` =假)

*   `restecg`:静息心电图结果(`0` =正常，`1`= ST-T 波异常(T 波倒置和/或 ST 抬高或压低> 0.05 mV)，`2` =显示根据 Estes 标准可能或明确的左心室肥厚)

*   `thalach`:达到最大心率

*   `exang`:运动诱发心绞痛(`1` =是；`0` =否)

*   `oldpeak`:运动相对于休息诱发的 ST 段压低

*   `slope`:运动 ST 段峰值的斜率(`1` =上坡，`2` =平，`3` =下坡)

*   `ca`:透视着色的主要血管数(0-3)

*   `thal` : `3` =正常；`6` =修复缺陷；`7` =可逆缺陷

将数据分为训练集和测试集，为机器学习做准备:

```
X = df_heart.iloc[:,:-1]y = df_heart.iloc[:,-1]from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)
```

你现在可以做预测了。

## 决策树分类器

在实现超参数之前，有一个用于比较的基线模型是有帮助的。

使用带有`DecisionTreeClassifier`的`cross_val_score`,如下所示:

```
model = DecisionTreeClassifier(random_state=2)
scores = cross_val_score(model, X, y, cv=5)
print('Accuracy:', np.round(scores, 2))
print('Accuracy mean: %0.2f' % (scores.mean()))
Accuracy: [0.74 0.85 0.77 0.73 0.7 ]
```

结果如下:

```
Accuracy mean: 0.76
```

初始准确率为 76%。让我们看看通过超参数微调可以获得哪些收益。

### 随机搜索 CLF 函数

当微调许多超参数时，`GridSearchCV`会花费太多时间。scikit-learn 库提供了`RandomizedSearchCV`作为一个很好的选择。`RandomizedSearchCV`的工作方式与`GridSearchCV`相同，但它不是尝试所有的超参数，而是尝试随机数量的组合。这并不意味着详尽无遗。它意味着在有限的时间内找到最佳组合。

这里有一个使用`RandomizedSearchCV`返回最佳模型和分数的函数。输入有`params`(要测试的超参数字典)、`runs` (要检查的超参数组合数量)和`DecisionTreeClassifier`:

```
def randomized_search_clf(params, runs=20, clf=DecisionTreeClassifier(random_state=2)):    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs,    cv=5, n_jobs=-1, random_state=2)    rand_clf.fit(X_train, y_train)
    best_model = rand_clf.best_estimator_
    best_score = rand_clf.best_score_  
    print("Training score: {:.3f}".format(best_score))
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print('Test score: {:.3f}'.format(accuracy))
    return best_model
```

现在，让我们挑选一个范围的超参数。

## 选择超参数

选择超参数没有单一的正确方法。实验是游戏的名字。这是一个初始列表，放在`randomized_search_clf`函数中。选择这些数字的目的是减少差异并尝试扩大范围:

```
randomized_search_clf(params={'criterion':['entropy', 'gini'],'splitter':['random', 'best'], 'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01],'min_samples_split':[2, 3, 4, 5, 6, 8, 10],'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],'max_features':['auto', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],'max_depth':[None, 2,4,6,8],'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]})
Training score: 0.798
Test score: 0.855
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=8, max_features=0.8, max_leaf_nodes=45, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.04, min_samples_split=10,min_weight_fraction_leaf=0.05, presort=False, random_state=2, splitter='best')
```

这是一个明确的改进，并且该模型在测试集上推广良好。让我们看看是否可以通过缩小范围来做得更好。

## 缩小范围

缩小范围是改善超参数的一种策略。

例如，使用从最佳模型中选择的基线`max_depth=8`，我们可以将范围缩小到从`7`到`9`。

另一个策略是停止检查那些默认运行良好的超参数。例如，`entropy`不推荐超过`'gini'`，因为差别非常小。`min_impurity_split`和`min_impurity_decrease`也可以保留默认值。

这是一个新的超参数范围，增加了`100`次运行:

```
randomized_search_clf(params={'max_depth':[None, 6, 7],'max_features':['auto', 0.78], 'max_leaf_nodes':[45, None], 'min_samples_leaf':[1, 0.035, 0.04, 0.045, 0.05],'min_samples_split':[2, 9, 10],'min_weight_fraction_leaf': [0.0, 0.05, 0.06, 0.07],}, runs=100)
Training score: 0.802
Test score: 0.868
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,max_features=0.78, max_leaf_nodes=45, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.045, min_samples_split=9, min_weight_fraction_leaf=0.06, presort=False, random_state=2, splitter='best')
```

该模型在训练和测试得分上更加准确。

然而，为了有一个合适的比较基准，将新模型放到`cross_val_clf`中是很重要的。这可以通过复制和粘贴前面的模型来实现:

```
model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7, max_features=0.78, max_leaf_nodes=45, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.045, min_samples_split=9, min_weight_fraction_leaf=0.06, presort=False, random_state=2, splitter='best')
scores = cross_val_score(model, X, y, cv=5)
print('Accuracy:', np.round(scores, 2))
print('Accuracy mean: %0.2f' % (scores.mean()))
Accuracy: [0.82 0.9  0.8  0.8  0.78]
```

结果如下:

```
Accuracy mean: 0.82
```

这比默认模式高出六个百分点。当谈到预测心脏病时，更多的准确性可以挽救生命。

## 特性 _ 重要性 _

难题的最后一块是传达机器学习模型最重要的特征。决策树有一个很好的属性，`feature_importances_`，它能做到这一点。

首先，我们需要最终确定最佳模式。我们的函数返回了最佳模型，但尚未保存。

测试时，不要混合搭配训练集和测试集，这一点很重要。然而，选择最终模型后，在整个数据集上拟合模型可能是有益的。为什么？因为目标是在从未见过的数据上测试模型，在整个数据集上拟合模型可能会带来额外的准确性。

让我们使用最佳超参数定义模型，并在整个数据集上进行拟合:

```
best_clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=9,max_features=0.8, max_leaf_nodes=47,min_impurity_decrease=0.0, min_impurity_split=None,min_samples_leaf=1, min_samples_split=8,min_weight_fraction_leaf=0.05, presort=False,random_state=2, splitter='best')
best_clf.fit(X, y)
```

为了确定最重要的特性，我们可以在`best_clf`上运行`feature_importances_`属性:

```
best_clf.feature_importances_
array([0.04826754, 0.04081653, 0.48409586, 0.00568635, 0\.        , 0., 0., 0.00859483, 0., 0.02690379, 0., 0.18069065, 0.20494446])
```

解释这些结果并不容易。以下代码将这些列以及最重要的特性压缩到一个字典中，然后以相反的顺序显示它们，以获得易于理解的清晰输出:

```
feature_dict = dict(zip(X.columns, best_clf.feature_importances_))
# Import operator import operator
Sort dict by values (as list of tuples)sorted(feature_dict.items(), key=operator.itemgetter(1), reverse=True)[0:3]
[('cp', 0.4840958610240171),
 ('thal', 0.20494445570568706),
 ('ca', 0.18069065321397942)]
```

三个最重要的特征如下:

*   `'cp'`:胸痛型(`1` =典型心绞痛、`2` =非典型心绞痛、`3` =非心绞痛、`4` =无症状)

*   `'thalach'`:达到最大心率

*   `'ca'`:透视着色的主要血管数(0-3)

这些数字可能被解释为他们对方差的解释，所以`'cp'`占方差的 48%，比`'thal'`和`'ca'`加起来还多。

你可以告诉医生和护士，你的模型使用胸痛、最大心率和荧光透视作为三个最重要的特征，以 82%的准确率预测患者是否患有心脏病。

# 总结

在这一章中，通过研究决策树，你已经向掌握 XGBoost 迈出了一大步，决策树是 XGBoost 的基础学习者。通过用`GridSearchCV`和`RandomizedSearchCV`微调超参数，您构建了决策树回归器和分类器。您可视化了决策树，并根据方差和偏差分析了它们的误差和准确性。此外，您还了解了一个不可或缺的工具，`feature_importances_`，它用于传达您的模型的最重要的特性，这也是 XGBoost 的一个属性。

在下一章，你将学习如何构建随机森林，这是我们的第一个系综方法，也是 XGBoost 的竞争对手。随机森林的应用对于理解 bagging 和 boosting 之间的差异、生成与 XGBoost 相当的机器学习模型以及了解最初促进 XGBoost 开发的随机森林的局限性非常重要。*