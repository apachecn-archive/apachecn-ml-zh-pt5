

# 一、机器学习的版图

欢迎来到*XGBoost 和 Scikit-Learn* 的实际操作梯度增强，这本书将教你 XGBoost 的基础、技巧和诀窍，这是从表格数据进行预测的最佳机器学习算法。

这本书的重点是 **XGBoost** ，也叫**极限梯度提升**。XGBoost 的结构、功能和原始功能将在每一章中详细阐述。这几章展开来讲述一个不可思议的故事:XGBoost 的故事。到本书结束时，您将成为利用 XGBoost 从真实数据中进行预测的专家。

在第一章中，XGBoost 以预览的形式出现。它在**机器学习**回归和分类的大背景下客串了一把，为即将到来的事情做好准备。

本章关注的是为机器学习准备数据，这个过程也被称为**数据争论**。除了构建机器学习模型，您还将了解如何使用高效的 **Python** 代码来加载数据、描述数据、处理空值、将数据转换为数字列、将数据拆分为训练集和测试集、构建机器学习模型、实现**交叉验证**，以及将**线性回归**和**逻辑回归**模型与 XGBoost 进行比较。

本章介绍的概念和库贯穿全书。

本章包括以下主题:

*   预览 XGBoost

*   争论的数据

*   预测回归

*   预测分类

# 预览 XGBoost

机器学习在 20 世纪 40 年代凭借第一个神经网络获得了认可，随后在 20 世纪 50 年代获得了第一个机器学习 checker 冠军。平静了几十年后，当深蓝 T2 在 20 世纪 90 年代击败国际象棋世界冠军加里卡斯帕罗夫时，机器学习领域开始腾飞。随着计算能力的激增，20 世纪 90 年代和 21 世纪初产生了过多的学术论文，揭示了新机器学习算法，如**随机森林**和 **AdaBoost** 。

boosting 背后的一般思想是通过迭代改进错误，将弱学习器转化为强学习器。**梯度增强**背后的关键思想是使用梯度下降来最小化残差的误差。这种进化链，从标准的机器学习算法到梯度推进，是本书前四章的重点。

XGBoost 是极端梯度增强的缩写。*极端*部分指的是推动计算的极限，以实现准确性和速度的提高。XGBoost 的人气飙升很大程度上是因为它在 **Kaggle 比赛**中无与伦比的成功。在 Kaggle 比赛中，竞争对手建立机器学习模型，试图做出最佳预测并赢得丰厚的现金奖励。与其他型号相比，XGBoost 已经击败了竞争对手。

理解 XGBoost 的细节需要理解梯度推进背景下的机器学习的前景。为了描绘完整的画面，我们从头开始，从机器学习的基础开始。

## 什么是机器学习？

机器学习是计算机从数据中学习的能力。2020 年，机器学习预测人类行为，推荐产品，识别人脸，胜过扑克专业人士，发现系外行星，识别疾病，操作自动驾驶汽车，个性化互联网，直接与人类交流。机器学习正在引领人工智能革命，并影响着几乎每一家大公司的底线。

在实践中，机器学习意味着实现计算机算法，当新数据进来时，这些算法的权重会进行调整。机器学习算法从数据集学习，以预测物种分类、股票市场、公司利润、人类决策、亚原子粒子、最佳交通路线等。

机器学习是我们将大数据转化为准确、可操作的预测的最佳工具。然而，机器学习不是在真空中发生的。机器学习需要数据的行和列。

# 数据角力

数据争论是一个综合术语，包含机器学习开始前数据预处理的各个阶段。数据加载、数据清理、数据分析和数据操作都包含在数据争论的范围内。

第一章详细介绍了数据争论。这些例子旨在涵盖标准的数据争论挑战，这些挑战可以由 Python 处理数据分析的专用库 **pandas** 快速处理。虽然不需要有与**熊猫**相处的经验，但对**熊猫**的基本知识将会有所帮助。所有代码都有解释，以便刚接触**熊猫**的读者可以理解。

## 数据集 1–自行车租赁

自行车租赁数据集是我们的第一个数据集。数据来源是世界著名的数据仓库 UCI 机器学习库(【https://archive.ics.uci.edu/ml/index.php】)，对公众免费开放。我们的自行车租赁数据集已经从原始数据集([https://archive . ics . UCI . edu/ml/datasets/bike+sharing+dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset))进行了调整，加入了空值，这样您就可以练习纠正它们。

### 访问数据

数据争论的第一步是访问数据。这可以通过以下步骤实现:

1.  下载数据。这本书的所有文件都存储在 GitHub 上。您可以通过按桌面上的`Data`文件夹将所有文件下载到您的本地计算机。

2.  打开一个 Jupyter 笔记本。您可以在前言中找到下载 Jupyter 笔记本的链接。点击终端中的`jupyter notebook`。web 浏览器打开后，您应该会看到文件夹和文件的列表。转到与自行车租赁数据集相同的文件夹，并选择**新:笔记本:Python 3** 。以下是一个视觉指南:

    ![Figure 1.2 – Visual guide to accessing the Jupyter Notebook](img/B15551_01_02.jpg)

    图 1.2–查看 Jupyter 笔记本的视觉指南

    小费

    如果您在打开 Jupyter 笔记本时遇到困难，请参见 Jupyter 的官方故障排除指南:[https://Jupyter-Notebook . readthedocs . io/en/stable/troubleshoot . html](https://jupyter-notebook.readthedocs.io/en/stable/troubleshooting.html)。

3.  在 Jupyter 笔记本的第一个单元格中输入以下代码:

    ```
    import pandas as pd 
    ```

    按下 *Shift* + *Enter* 运行单元。现在你可以在写`pd`的时候访问`pandas`库。

4.  使用`pd.read_csv`加载数据。加载数据需要一个`read`方法。`read`方法将数据存储为 DataFrame，一个用于查看、分析和操作数据的`pandas`对象。加载数据时，将文件名放在引号中，然后运行单元格:

    ```
    df_bikes = pd.read_csv('bike_rentals.csv')
    ```

    如果您的数据文件与 Jupyter 笔记本在不同的位置，您必须提供一个文件目录，例如`Downloads/bike_rental.csv`。

    现在，数据已经正确地存储在名为`df_bikes`的数据帧中。

    小费

    **标签完成**:当在 Jupyter 笔记本上编码，输入几个字符后，按*标签*按钮。对于 CSV 文件，您应该会看到文件名出现。用光标突出显示该名称，然后按*键输入*。如果文件名是唯一可用的选项，您可以按*键输入*。制表符补全将使您的编码体验更快、更可靠。

5.  使用`.head()`显示数据。最后一步是查看数据，以确保它已经正确加载。`.head()`是一个 DataFrame 方法，显示 DataFrame 的前五行。您可以将任意正整数放在括号中，以查看任意数量的行。输入以下代码，按 *Shift* + *Enter* :

    ```
    df_bikes.head()
    ```

    下面是前几行以及预期输出的屏幕截图:

![Figure 1.3 –The bike_rental.csv output](img/B15551_01_03.jpg)

图 1.3–bike _ rental . CSV 输出

现在我们已经有了对数据的访问，让我们来看看理解数据的三种方法。

## 了解数据

既然数据已经被加载，是时候理解这些数据了。理解这些数据对于今后做出明智的决策至关重要。这里有三种很好的理解数据的方法。

### 。头部()

您已经看到了中的`.head()`，这是一种广泛使用的解释列名和列号的方法。如前面的输出所示，`dteday`是一个日期，而`instant`是一个有序索引。

### 。描述()

使用`.describe()`可以查看数字统计，如下所示:

```
df_bikes.describe()
```

以下是预期的输出:

![Figure 1.4 – The .describe() output](img/B15551_01_04.jpg)

图 1.4–的。描述()输出

您可能需要向右滚动才能看到所有的列。

比较平均值和中位数(50%)给出了偏态的指示。如你所见，`mean`和`median`彼此接近，所以数据大致对称。每一列的`max`和`min`值，以及四分位数和标准偏差(`std`)也被呈现。

### 。信息()

另一个伟大的方法是`.info()`，它显示关于列和行的一般信息:

```
df_bikes.info()
```

以下是预期的输出:

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 731 entries, 0 to 730
Data columns (total 16 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   instant     731 non-null    int64  
 1   dteday      731 non-null    object 
 2   season      731 non-null    float64
 3   yr          730 non-null    float64
 4   mnth        730 non-null    float64
 5   holiday     731 non-null    float64
 6   weekday     731 non-null    float64
 7   workingday  731 non-null    float64
 8   weathersit  731 non-null    int64  
 9   temp        730 non-null    float64
 10  atemp       730 non-null    float64
 11  hum         728 non-null    float64
 12  windspeed   726 non-null    float64
 13  casual      731 non-null    int64  
 14  registered  731 non-null    int64  
 15  cnt         731 non-null    int64  
dtypes: float64(10), int64(5), object(1)
memory usage: 91.5+ KB
```

如您所见，`.info()`为提供了行数、列数、列类型和非空值。由于非空值的数量因列而异，因此必须存在空值。

## 修正空值

如果空值没有被纠正，那么可能会出现意想不到的错误。在这一小节中，我们将介绍各种可用于纠正空值的方法。我们的例子不仅是为了处理空值，也是为了突出`pandas`的广度和深度。

以下方法可用于纠正空值。

### 查找空值的数量

下面的代码显示了空值的总数:

```
df_bikes.isna().sum().sum()
```

结果是这样的:

```
12
```

注意，需要两个`.sum()`方法。第一种方法对每一列的空值求和，而第二种方法对列计数求和。

### 显示空值

您可以使用以下代码显示所有包含空值的行:

```
 df_bikes[df_bikes.isna().any(axis=1)]
```

这段代码可以分解如下:`df_bikes[conditional]`是满足括号中条件的`df_bikes`的子集。`.df_bikes.isna().any`收集所有的空值，而`(axis=1)`指定列中的值。在熊猫中，行是`axis 0`，列是`axis 1`。

以下是预期的输出:

![Figure 1.5 – Bike Rentals dataset null values](img/B15551_01_05.jpg)

图 1.5-自行车租赁数据集空值

正如您在输出中看到的，在最后一行的`windspeed`、`humidity`和`temperature`列中有空值。

小费

如果这是你第一次和**熊猫**一起工作，可能需要时间来适应符号。查看 Packt 的*与熊猫*的动手数据分析，获得精彩介绍:【https://subscription.packtpub.com/book/data/9781789615326】[。](https://subscription.packtpub.com/book/data/9781789615326)

### 更正空值

纠正空值取决于列和数据集。让我们复习一些策略。

#### 用中间值/平均值替换

一种常见的策略是用中值或平均值替换空值。这里的想法是用平均列值替换空值。

对于`'windspeed'`列，空值可以替换为`median`值，如下所示:

```
df_bikes['windspeed'].fillna((df_bikes['windspeed'].median()), inplace=True)
```

`df_bikes['windspeed'].fillna`表示将填充`'windspeed'`列的空值。`df_bikes['windspeed'].median()`是`'windspeed'`列的中间值。最后，`inplace=True`确保改变是永久性的。

小费

中位数通常是比平均值更好的选择。中位数保证一半数据大于给定值，一半数据小于给定值。相比之下，平均值容易受到异常值的影响。

在前一个单元格中，`df_bikes[df_bikes.isna().any(axis=1)]`显示了第`56`行和第`81`行的`windspeed`值为空。这些行可以使用`.iloc`显示，简称**索引位置**:

```
df_bikes.iloc[[56, 81]]
```

以下是预期的输出:

![Figure 1.6 – Rows 56 and 81](img/B15551_01_06.jpg)

图 1.6–第 56 行和第 81 行

不出所料，空值已经被风速中值所取代。

小费

用户在使用`.iloc`的时候经常会犯用单括号或双括号的错误，对一个索引使用单括号如下:`df_bikes.iloc[56]`。现在，`df_bikes`也接受括号内的列表，允许多个索引。多个索引需要如下双括号:`df_bikes.iloc[[56, 81]]`。请看[https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas。DataFrame.iloc.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)获取更多文档。

#### 具有中值/平均值的分组依据

通过使用由组成的**组，可以在修正空值时使更加细致入微。**

groupby 按共享值组织行。因为有四个共享的季节分布在各个行中，所以按季节分组总共会产生四行，每个季节一行。但是每个季节都来自许多不同的行，具有不同的值。我们需要一种方法来组合或聚合这些值。集合的选择包括`.sum()`、`.count()`、`.mean()`和`.median()`。我们用`.median()`。

使用`.median()`骨料按季节对`df_bikes`进行分组的方法如下:

```
df_bikes.groupby(['season']).median()
```

以下是预期的输出:

![Figure 1.7 – The output of grouping df_bikes by season](img/B15551_01_07.jpg)

图 1.7–按季节对 df_bikes 分组的输出

如您所见，列值是中间值。

为了校正`hum`列中的空值，简称为**湿度**，我们可以按季节取湿度的中间值。

用于修正`hum`列中的空值的代码是`df_bikes['hum'] = df_bikes['hum'].fillna()`。

进入`fillna`的代码是所需的值。从`groupby`获得的值需要如下的`transform`方法:

```
df_bikes.groupby('season')['hum'].transform('median')
```

下面是一个长步骤中的组合代码:

```
df_bikes['hum'] = df_bikes['hum'].fillna(df_bikes.groupby('season')['hum'].transform('median'))
```

您可以通过检查`df_bikes.iloc[[129, 213, 388]]`来验证转换。

#### 从特定行获取中值/平均值

在某些情况下，用特定行的数据替换空值可能是有利的。

在校正温度时，除了查阅历史记录外，取前一天和后一天的平均温度应该可以给出一个很好的估计。

要查找`'temp'`列的空值，请输入以下代码:

```
df_bikes[df_bikes['temp'].isna()]
```

以下是预期的输出:

![Figure 1.8 – The output of the 'temp' column](img/B15551_01_08.jpg)

图 1.8–temp 列的输出

如您所见，索引`701`包含空值。

要找到`701`指数前一天和后一天的平均温度，完成以下步骤:

1.  将`700`和`702`行的温度相加，然后除以`2`。对`'temp'`和`'atemp'`列执行此操作:

    ```
    mean_temp = (df_bikes.iloc[700]['temp'] + df_bikes.iloc[702]['temp'])/2
    mean_atemp = (df_bikes.iloc[700]['atemp'] + df_bikes.iloc[702]['atemp'])/2
    ```

2.  替换空值:

    ```
    df_bikes['temp'].fillna((mean_temp), inplace=True)
    df_bikes['atemp'].fillna((mean_atemp), inplace=True)
    ```

您可以自己在上验证空值是否已按预期填充。

#### 推断日期

我们纠正空值的最后一个策略涉及到日期。当提供实际日期时，日期值可以外推。

`df_bikes['dteday']`是日期列；然而，`df_bikes.info()`揭示的列类型是一个对象，通常表示为一个字符串。诸如年和月这样的日期对象必须从`datetime`类型中推断出来。使用`to_datetime`方法可以将`df_bikes['dteday']`转换为`'datetime'`类型，如下所示:

```
df_bikes['dteday'] = pd.to_datetime(df_bikes['dteday'],infer_datetime_format=True)
```

`infer_datetime_format=True`允许**熊猫**决定存储哪种日期时间对象，这在大多数情况下是一个安全的选择。

要外推单个列，首先导入`datetime`库:

```
import datetime as dt
```

我们现在可以使用一些不同的方法来推断空值的日期。标准方法是将“`mnth`”列转换为从“dteday”列推断的正确月份。这有利于纠正转换中可能出现的任何额外错误，当然假设'`dteday`'列是正确的。

代码如下:

```
ddf_bikes['mnth'] = df_bikes['dteday'].dt.month
```

核实这些变化很重要。由于空日期值在最后一行，我们可以使用`.tail()`，一个类似于`.head()`的 DataFrame 方法，显示最后五行:

```
df_bikes.tail()
```

以下是预期的输出:

![Figure 1.9 – The output of the extrapolated date values](img/B15551_01_09.jpg)

图 1.9-外推日期值的输出

如您所见，月份值都是正确的，但是年份值需要更改。

“`dteday`栏最后五行的年份都是`2012`，但是“`yr`栏提供的对应年份是`1.0`。为什么？

数据是标准化的，这意味着它被转换成介于`0`和`1`之间的值。

标准化的数据通常更有效，因为机器学习权重不必针对不同的范围进行调整。

您可以使用。loc 方法来填充正确的值。`.loc`方法用于按行和列定位条目，如下所示:

```
df_bikes.loc[730, 'yr'] = 1.0
```

既然您已经练习了如何纠正空值，并且已经获得了使用**熊猫**的重要经验，那么是时候处理非数字列了。

### 删除非数字列

对于机器学习，所有数据列都应该是数值。根据`df.info()`，唯一不是数字的列是`df_bikes['dteday']`。此外，这是多余的，因为所有日期信息都存在于其他列中。

可以按如下方式删除该列:

```
df_bikes = df_bikes.drop('dteday', axis=1)
```

既然我们已经有了所有的数字列，没有空值，我们就可以开始机器学习了。

# 预测回归

机器学习算法旨在使用来自一个或多个输入列的数据来预测一个输出列的值。这些预测依赖于由正在解决的一般类别的机器学习问题所确定的数学方程。大多数监督学习问题被归类为回归或分类。在本节中，在回归的背景下介绍机器学习。

## 预测自行车租赁

在自行车租赁数据集中，`df_bikes['cnt']`是某一天自行车租赁的数量。预测这个专栏对自行车租赁公司来说会有很大的用处。我们的问题是根据某一天是假日还是工作日、预测的温度、湿度、风速等数据来预测某一天自行车租赁的正确数量。

根据数据集，`df_bikes['cnt']`是`df_bikes['casual']`和`df_bikes['registered']`之和。如果将`df_bikes['registered']`和`df_bikes['casual']`作为输入列，预测将总是 100%准确，因为这些列的总和总是正确的。尽管理论上完美的预测是理想的，但是包含现实中未知的输入列是没有意义的。

如前所述，除了用于`'casual'`和`'registered'`的之外，所有当前的列都可以用于预测`df_bikes['cnt']`。使用如下的`.drop`方法放下`'casual'`和`'registered'`柱:

```
df_bikes = df_bikes.drop(['casual', 'registered'], axis=1)
```

数据集现在准备好了。

## 保存数据以备将来使用

自行车租赁数据集将在本书的中多次使用。您可以将干净的数据集导出为 CSV 文件以供将来使用，而不是每次都运行此笔记本来执行数据争论:

```
df_bikes.to_csv('bike_rentals_cleaned.csv', index=False)
```

`index=False`参数防止索引创建额外的列。

## 声明预测值和目标列

机器学习通过对每个预测器列(输入列)执行数学运算来确定目标列(输出列)来工作。

标准的做法是用大写字母`X`对预测列进行分组，用小写字母`y`对目标列进行分组。因为我们的目标列是最后一列，所以可以通过使用索引符号进行切片来将数据分为预测值和目标列:

```
X = df_bikes.iloc[:,:-1]y = df_bikes.iloc[:,-1]
```

逗号分隔列和行。第一个冒号`:`表示包含所有行。逗号后的`:-1`表示从第一列开始，一直到最后一个列，不包括它。第二个`-1`只占最后一列。

## 了解回归

实际上，预测自行车租赁的数量可能会导致任何非负整数**。当目标列包括一个无限值的范围时，机器学习问题被分类为**回归**。**

 **最常见的回归算法是线性回归。线性回归将每个预测列作为一个**多项式变量**，并将该值乘以**系数**(也称为**权重**)来预测目标列。**梯度下降**在引擎盖下工作，将误差降到最低。线性回归的预测可以是任何实数。

在运行线性回归之前，我们必须将数据分成训练集和测试集。定型集使用目标列将数据与算法相匹配，以最小化错误。模型建立后，会根据测试数据进行评分。

拿出测试集来给模型打分的重要性怎么强调都不为过。在大数据的世界里，**将数据过度拟合**到训练集是很常见的，因为有太多的数据点需要训练。过度拟合通常是不好的，因为模型会根据异常值、异常情况和暂时趋势进行自我调整。强大的机器学习模型在很好地归纳新数据和准确提取手头数据的细微差别之间取得了良好的平衡，这一概念在第二章 *【深度决策树】的 [*中进行了详细探讨。*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)*

## 访问 scikit-learn

所有机器学习库将通过 **scikit-learn** 处理。Scikit-learn 的范围、易用性和计算能力使其成为世界上最广泛的机器学习库之一。

从 scikit 导入`train_test_split`和`LinearRegression`-学习如下:

```
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
```

接下来，将数据分为训练集和测试集:

```
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)
```

注意`random_state=2`参数。每当你看到`random_state=2`，这意味着你正在选择一个伪随机数发生器的种子以确保可重复的结果。

## 静音警告

在建立你的第一个机器学习模型之前，关闭所有警告。Scikit-learn 包含警告以通知用户未来的更改。一般来说，不建议让警告静音，但是因为我们的代码已经过测试，所以建议您在 Jupyter 笔记本中节省空间。

警告可以按如下方式静音:

```
import warnings
warnings.filterwarnings('ignore')
```

是时候建立你的第一个模型了。

## 建模线性回归

线性回归模型可通过以下步骤构建:

1.  初始化机器学习模型:

    ```
    lin_reg = LinearRegression()
    ```

2.  在训练集上拟合模型。这就是机器学习模型建立的地方。请注意，`X_train`是预测列，`y_train`是目标列。

    ```
    lin_reg.fit(X_train, y_train)
    ```

3.  对测试集进行预测。使用`lin_reg`上的`.predict`方法，将测试集中的预测器列`X_test`的预测存储为`y_pred`:

    ```
    y_pred = lin_reg.predict(X_test)
    ```

4.  将预测与测试集进行比较。给模型打分需要一个比较的基础。线性回归的标准是`mean_squared_error`，预测值和实际值之差的平方和，以及平方根，以保持单位不变。`mean_squared_error`可以被导入，平方根可以用**数字 Python** 来取，通常被称为 **NumPy** ，这是一个设计用于与**熊猫**一起工作的速度惊人的库。

5.  导入`mean_squared_error`和 NumPy，然后计算均方误差，取平方根:

    ```
    from sklearn.metrics import mean_squared_error
    import numpy as np
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    ```

6.  打印您的结果:

    ```
    print("RMSE: %0.2f" % (rmse))
    ```

    结果如下:

    ```
    RMSE: 898.21
    ```

    下面是构建您的第一个机器学习模型的所有代码的截图:

![Figure 1.10 – Code to build your machine learning model](img/B15551_01_10.jpg)

图 1.10-构建机器学习模型的代码

在不知道每天租金的预期范围的情况下，很难知道`898`租金的误差是好是坏。

可以在`df_bikes['cnt']`列上使用`.describe()`方法来获得范围和更多信息:

```
df_bikes['cnt'].describe()
```

以下是输出:

```
count     731.000000
mean     4504.348837
std      1937.211452
min        22.000000
25%      3152.000000
50%      4548.000000
75%      5956.000000
max      8714.000000
Name: cnt, dtype: float64
```

有了`22`到`8714`的区间，`4504`的均值，`1937`的标准差，`898`的 RMSE 不算差，但也不算伟大。

## XGBoost

线性回归是可以用来解决回归问题的众多算法之一。其他回归算法可能会产生更好的结果。一般的策略是用不同的回归量做实验来比较分数。在这本书里，你将会尝试各种各样的回归变量，包括决策树、随机森林、梯度推进以及这本书的重点 XGBoost。

本书后面将提供对 XGBoost 的全面介绍。现在，请注意 XGBoost 包括一个名为`XGBRegressor`的回归变量，它可以用于任何回归数据集，包括刚刚评分的自行车租赁数据集。现在让我们使用`XGBRegressor`来比较自行车租赁数据集和线性回归的结果。

您应该已经在前言中安装了 XGBoost。如果您还没有这样做，请现在安装 XGBoost。

## xgb 回归器

安装 XGBoost 后，XGBoost 回归器可按如下方式导入:

```
from xgboost import XGBRegressor
```

构建`XGBRegressor`的一般步骤与`LinearRegression`相同。唯一的区别是初始化`XGBRegressor`而不是`LinearRegression`:

1.  初始化机器学习模型:

    ```
    xg_reg = XGBRegressor()
    ```

2.  在训练集上拟合模型。如果您在这里收到来自 XGBoost 的一些警告，不要担心:

    ```
    xg_reg.fit(X_train, y_train)
    ```

3.  对测试集进行预测:

    ```
    y_pred = xg_reg.predict(X_test)
    ```

4.  将预测与测试集进行比较:

    ```
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    ```

5.  打印您的结果:

    ```
    print("RMSE: %0.2f" % (rmse))
    ```

    输出如下所示:

    ```
    RMSE: 705.11
    ```

`XGBRegressor`性能大幅提升！

XGBoost 经常表现比别人好的原因将在 [*第五章*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117) *中探究，XGBoost 揭开*。

## 交叉验证

一个测试分数是不可靠的，因为将数据分成不同的训练和测试集将会给出不同的结果。实际上，将数据分成训练集和测试集是任意的，不同的`random_state`将给出不同的 RMSE。

解决不同分裂之间分数差异的一种方法是 **k 倍交叉验证**。的思路是将数据多次拆分成不同的训练集和测试集，然后取分数的平均值。被称为**折叠**的分裂数由 **k** 表示。使用 k = 3、4、5 或 10 个分割是标准的。

以下是交叉验证的直观描述:

![Figure 1.11 – Cross-validation](img/B15551_01_11.jpg)

图 1.11-交叉验证

(重绘自[https://commons . wikimedia . org/wiki/File:K-fold _ cross _ validation _ en . SVG](https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.svg))

交叉验证的工作原理是在第一个训练集上拟合一个机器学习模型，并根据第一个测试集对其进行评分。为第二次分裂提供不同的训练集和测试集，从而产生具有其自己分数的新的机器学习模型。第三次拆分产生一个新模型，并根据另一个测试集对其进行评分。

训练集中会有重叠，但测试集中不会。

选择折叠次数是灵活的，取决于数据。五次折叠是标准的，因为每次有 20%的测试集被保留。10 折，只有 10%的数据被保留；然而，90%的数据可用于训练，并且平均值不易受到异常值的影响。对于较小的数据集，三次折叠可能会更好。

最后，将会有 k 个不同的分数针对 k 个不同的测试集来评估该模型。取 k 个折叠的平均值给出了比任何单个折叠更可靠的分数。

`cross_val_score`是实现交叉验证的一种便捷方式。`cross_val_score`将机器学习算法作为输入，连同预测器和目标列，以及可选的附加参数，包括评分度量和期望的折叠数。

### 线性回归交叉验证

让我们用交叉验证和`LinearRegression`。

首先，从`cross_val_score`库中导入`cross_val_score`:

```
from sklearn.model_selection import cross_val_score
```

现在，在以下步骤中使用交叉验证来构建机器学习模型并对其进行评分:

1.  初始化机器学习模型:

    ```
    model = LinearRegression()
    ```

2.  执行`cross_val_score`，输入型号、`X`、`y`、`scoring='neg_mean_squared_error'`和折数`cv=10`:

    ```
    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)
    ```

    小费

    为什么是`scoring='neg_mean_squared_error'`？Scikit-learn 旨在训练模型时选择最高分。这对于精确度来说很有效，但是对于误差来说就不是最好的了。通过取每个均方误差的负值，最低值就是最高值。这是后来用`rmse = np.sqrt(-scores)`补偿的，所以最终结果是正的。

3.  通过取负分数的平方根来找到 RMSE:

    ```
    rmse = np.sqrt(-scores)
    ```

4.  显示结果:

    ```
    print('Reg rmse:', np.round(rmse, 2))
    print('RMSE mean: %0.2f' % (rmse.mean()))
    ```

    输出如下所示:

    ```
    Reg rmse: [ 504.01  840.55 1140.88  728.39  640.2   969.95 
    1133.45 1252.85 1084.64  1425.33]
    RMSE mean: 972.02
    ```

线性回归的平均误差为`972.06`。这比之前得到的`980.38`略胜一筹。这里的重点不是分数是好是坏。重点是，这是对线性回归如何对未知数据进行的更好的估计。

为了更好地估计分数，总是推荐使用交叉验证。

关于打印功能

当运行自己的机器学习代码时，全局`print`函数通常不是必需的，但如果您想要打印出多行并格式化输出，如下所示，它会很有帮助。

### 使用 XGBoost 进行交叉验证

现在让我们用和`XGBRegressor`进行交叉验证。步骤相同，除了初始化模型:

1.  初始化机器学习模型:

    ```
    model = XGBRegressor()
    ```

2.  执行`cross_val_score`，将模型、`X`、`y`、得分、折叠数`cv`作为输入:

    ```
    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)
    ```

3.  通过取负分数的平方根来找到 RMSE:

    ```
    rmse = np.sqrt(-scores)
    ```

4.  打印结果:

    ```
    print('Reg rmse:', np.round(rmse, 2))
    print('RMSE mean: %0.2f' % (rmse.mean()))
    ```

    输出如下所示:

    ```
    Reg rmse: [ 717.65  692.8   520.7   737.68  835.96 1006.24  991.34  747.61  891.99 1731.13]
    RMSE mean: 887.31
    ```

`XGBRegressor`再次赢得，以大约 10%的优势击败的线性回归。

# 预测分类

你学到了 XGBoost 可能在回归方面有优势，但是分类呢？XGBoost 有一个分类模型，但是它的表现会像经过充分测试的分类模型(如逻辑回归)一样准确吗？让我们找出答案。

## 什么是分类？

与回归不同，当预测具有有限数量输出的目标列时，机器学习算法被归类为分类算法。可能的输出包括以下内容:

*   是，不是

*   垃圾邮件，不是垃圾邮件

*   0, 1

*   红色、蓝色、绿色、黄色、橙色

## 数据集 2–人口普查

我们将更快地浏览第二个数据集，人口普查收入数据集([https://archive.ics.uci.edu/ml/datasets/Census+Income](https://archive.ics.uci.edu/ml/datasets/Census+Income))，来预测个人收入。

## 数据角力

在实现机器学习之前，必须对数据集进行预处理。测试新算法时，所有数字列都不能有空值，这一点很重要。

### 数据加载

由于这个数据集直接托管在 UCI 机器学习网站上，因此可以使用`pd.read_csv`直接从互联网上下载:

```
df_census = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data')
df_census.head()
```

以下是预期的输出:

![Figure 1.12 – The Census Income DataFrame](img/B15551_01_12.jpg)

图 1.12-人口普查收入数据框架

输出显示列标题代表第一行的条目。发生这种情况时，可以用`header=None`参数重新加载数据:

```
df_census = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header=None)
df_census.head()
```

下面是没有标题的预期输出:

![Figure 1.13 – The header=None parameter output](img/B15551_01_13.jpg)

图 1.13–header = None 参数输出

如您所见，仍然缺少列名。它们被列在人口普查收入数据集网站([https://archive.ics.uci.edu/ml/datasets/Census+Income](https://archive.ics.uci.edu/ml/datasets/Census+Income))的*属性信息*下。

列的名称可以更改如下:

```
df_census.columns=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']
df_census.head()
```

以下是包含列名的预期输出:

![Figure 1.14 – Expected column names](img/B15551_01_14.jpg)

图 1.14–预期的列名

如您所见，列名已经恢复。

### 空值

检查空值的一个好方法是查看 DataFrame `.info()`方法:

```
df_census.info()
```

输出如下所示:

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 32561 entries, 0 to 32560
Data columns (total 15 columns):
 #   Column          Non-Null Count  Dtype 
---  ------          --------------  ----- 
 0   age             32561 non-null  int64 
 1   workclass       32561 non-null  object
 2   fnlwgt          32561 non-null  int64 
 3   education       32561 non-null  object
 4   education-num   32561 non-null  int64 
 5   marital-status  32561 non-null  object
 6   occupation      32561 non-null  object
 7   relationship    32561 non-null  object
 8   race            32561 non-null  object
 9   sex             32561 non-null  object
 10  capital-gain    32561 non-null  int64 
 11  capital-loss    32561 non-null  int64 
 12  hours-per-week  32561 non-null  int64 
 13  native-country  32561 non-null  object
 14  income          32561 non-null  object
dtypes: int64(6), object(9)
memory usage: 3.7+ MB
```

因为所有列都有相同数量的非空行，所以我们可以推断没有空值。

### 非数字列

`dtype`对象的所有列必须被转换成数字列。一个`get_dummies`方法获取每一列的非数字的唯一值，并将它们转换成它们自己的列，`1`表示存在，`0`表示不存在。例如，如果一个名为“书籍类型”的数据帧的列值是“精装本”、“平装本”或“电子书”，`pd.get_dummies`将创建三个新列，分别称为“精装本”、“平装本”和“电子书”，替换“书籍类型”列。

这是一个“书籍类型”数据框架:

![Figure 1.15 – A "Book Types" DataFrame](img/B15551_01_15.jpg)

图 1.15–一个“书籍类型”数据框架

以下是`pd.get_dummies`之后的相同数据帧:

![Figure 1.16 – The new DataFrame](img/B15551_01_16.jpg)

图 1.16–新的数据框架

`pd.get_dummies`将创建许多新列，所以值得检查一下是否有任何列可能被删除。快速查看一下`df_census`数据，可以发现一个`'education'`列和一个`education_num`列。`education_num`栏是`'education'`的数值转换。由于信息相同，可以删除`'education'`列:

```
df_census = df_census.drop(['education'], axis=1)
```

现在使用`pd.get_dummies`将非数字列转换成数字列:

```
df_census = pd.get_dummies(df_census)
df_census.head()
```

这里是的预期输出:

![Figure 1.17 – pd.get_dummies – non-numerical to numerical columns](img/B15551_01_17.jpg)

图 1.17–PD . get _ dummies–非数字到数字列

如您所见，新列是使用引用原始列的`column_value`语法创建的。比如`native-country`是原创栏目，台湾是众多值中的一个。如果这个人来自台湾，新的`native-country_Taiwan`列的值为`1`，否则为`0`。

小费

使用`pd.get_dummies`可能会增加内存使用量，这可以通过在相关数据帧上使用`.info()`方法并检查最后一行来验证。存储`1`，不存储`0`的值。有关稀疏矩阵的更多信息，请参见 [*第十章*](B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230) 、 *XGBoost 模型部署*，或者访问位于[https://docs.scipy.org/doc/scipy/reference/](https://docs.scipy.org/doc/scipy/reference/)的 SciPy 官方文档。

### 目标列和预测列

因为所有的列都是数值，没有空值，所以是时候将数据分成目标列和预测列了。

目标栏是某人是否赚了 50K。在`pd.get_dummies`之后，`df_census['income_<=50K']`和`df_census['income_>50K']`两列用于确定某人是否赚了 50K。由于这两列都可以，我们删除了`df_census['income_ <=50K']`:

```
df_census = df_census.drop('income_ <=50K', axis=1)
```

现在将数据分成`X`(预测列)和`y`(目标列)。注意，`-1`用于索引，因为最后一列是目标列:

```
X = df_census.iloc[:,:-1]y = df_census.iloc[:,-1]
```

是时候构建机器学习分类器了！

## 逻辑回归

逻辑回归是最基本的分类算法。数学上，逻辑回归的工作方式类似于线性回归。对于每一列，逻辑回归都会找到一个适当的权重或系数，以最大限度地提高模型的准确性。主要区别在于，逻辑回归使用了 **sigmoid 函数**，而不是像线性回归那样对各项求和。

这是 sigmoid 函数和相应的图形:

![Figure 1.18 – Sigmoid function graph](img/B15551_01_18.jpg)

图 1.18–Sigmoid 函数图

乙状结肠通常用于分类。所有大于 0.5 的值都匹配到 1，所有小于 0.5 的值都匹配到 0。

用 scikit-learn 实现逻辑回归几乎与实现线性回归相同。主要的区别在于预测值列应该适合不同的类别，并且误差应该以精确度来表示。额外的好处是，默认情况下，误差是以准确度表示的，因此不需要明确的评分参数。

您可以导入逻辑回归，如下所示:

```
from sklearn.linear_model import LogisticRegression
```

### 交叉验证功能

让我们在逻辑回归上使用交叉验证来预测某人是否赚超过 50K。

不要复制和粘贴，让我们使用`cross_val_score`构建一个交叉验证分类函数，该函数将机器学习算法作为输入，将准确度分数作为输出:

```
def cross_val(classifier, num_splits=10):    model = classifier     scores = cross_val_score(model, X, y, cv=num_splits)    print('Accuracy:', np.round(scores, 2))    print('Accuracy mean: %0.2f' % (scores.mean()))
```

现在调用逻辑回归函数:

```
cross_val(LogisticRegression())
```

输出如下所示:

```
Accuracy: [0.8  0.8  0.79 0.8  0.79 0.81 0.79 0.79 0.8  0.8 ]
Accuracy mean: 0.80
```

80%的准确率已经不错了。

让我们看看【XGBoost 是否能做得更好。

小费

任何时候你发现自己在复制和粘贴代码，寻找一个更好的方法！计算机科学的一个目标是避免重复。编写自己的数据分析和机器学习函数，从长远来看，会让你的生活更轻松，工作更高效。

## XGBoost 分类器

XGBoost 有一个回归器和一个分类器。要使用分类器，请导入以下算法:

```
from xgboost import XGBClassifier
```

现在运行`cross_val`函数中的分类器，添加一个重要的内容。由于有 94 列，XGBoost 是一个集合方法，这意味着它为每次运行组合了许多模型，每个模型包括 10 个拆分，我们将把模型的数量`n_estimators`限制为`5`。正常情况下，XGBoost 非常快。事实上，它被认为是最快的提升合奏方法，我们将在本书中验证这一点。然而，对于我们最初的目的来说，`5`估计量虽然没有默认的`100`那么稳健，但已经足够了。选择`n_estimators`的细节将是第四章 *的一个重点，从梯度升压到 XGBoost* :

```
cross_val(XGBClassifier(n_estimators=5))
```

输出如下所示:

```
Accuracy: [0.85 0.86 0.87 0.85 0.86 0.86 0.86 0.87 0.86 0.86]
Accuracy mean: 0.86
```

如您所见，XGBoost 的得分高于开箱即用的逻辑回归。

# 总结

你的 XGBoost 之旅正式开始了！本章一开始，你学习了数据争论和**熊猫**的基础知识，这是所有机器学习从业者的基本技能，重点是纠正空值。接下来，通过比较线性回归和 XGBoost，您学习了如何在 scikit-learn 中构建机器学习模型。然后，您准备了一个用于分类的数据集，并将逻辑回归与 XGBoost 进行了比较。在这两种情况下，XGBoost 都是明显的赢家。

祝贺您构建了您的第一个 XGBoost 模型！使用 **pandas** 、NumPy 和 scikit-learn 库开始数据争论和机器学习的过程已经完成。

在 [*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) *深度决策树*中，您将通过构建决策树、XGBoost 机器学习模型的基础学习器以及微调超参数来提高结果，从而提高您的机器学习技能。**