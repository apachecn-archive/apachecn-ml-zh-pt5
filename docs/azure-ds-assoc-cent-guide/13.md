# 十、了解模型结果

在这一章中，你将学习如何分析你的机器学习模型的结果来解释为什么模型会做出这样的推断。理解模型预测值的原因是避免黑盒模型部署的关键，并且能够理解您的模型可能具有的限制。在本章中，您将了解 Azure 机器学习的可用解释功能，并将模型解释结果可视化。您还将学习如何分析潜在的模型错误，并检测模型表现不佳的群组。最后，您将探索一些工具，这些工具将帮助您评估您的模型的公平性，并允许您减轻潜在的问题。

在本章中，我们将讨论以下主题:

*   创建负责任的机器学习模型
*   解释模型的预测
*   分析模型误差
*   检测潜在的模型公平性问题

# 技术要求

您需要访问 Azure 订阅。在该订阅中，您将需要一个`packt-azureml-rg`。你将需要一个`Contributor`或者`Owner`T3。如果你按照第二章 、*部署 Azure 机器学习工作区资源*中的说明，这些资源应该对你可用。

你还需要对 **Python** 语言有一个基本的了解。本章中的代码片段针对 Python 版或更高版本。您还应该熟悉在 Azure Machine Learning Studio 中使用笔记本的体验，这在前面的章节中已经介绍过了。

本章假设您已经创建了一个名为`cpu-sm-cluster`的计算集群，如 [*第 7 章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102) 、*AzureML Python SDK*的*使用计算目标*部分所述。

你可以在本书的资源库[http://bit.ly/dp100-ch10](http://bit.ly/dp100-ch10)找到本章的所有笔记本和代码片段。

# 创建负责任的机器学习模型

机器学习可以让你创建能够影响决策和塑造未来的模型。随着强大的力量而来的是巨大的责任，这就是人工智能治理变得必要的地方，这通常被称为负责任的人工智能原则和实践。Azure 机器学习在以下三个支柱下提供了支持负责任地创建人工智能的工具:

*   **了解**:在发布任何机器学习模型之前，你需要能够解读和解释模型的行为。此外，您需要评估和减轻针对特定群组的潜在模型不公平性。本章着重于帮助你理解你的模型的工具。
*   **Protect**: Here, you put mechanisms in place to protect people and their data. When training a model, data from real people is used. For example, in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117), *Experimenting with Python Code*, you trained a model on top of medical data from diabetic patients. Although the specific training dataset didn't have any **Personally Identifiable Information** (**PII**), the original dataset contained this sensitive information. There are open source libraries such as **SmartNoise** that offer basic building blocks that can be used to implement data handling mechanisms using vetted and mature differential privacy research techniques.

    例如，使用 SmartNoise 构建的查询引擎可以允许数据科学家在敏感数据之上执行聚合查询，并在结果中添加统计*噪声*，以防止意外识别数据集中的单个行。其他开源库，如如 **presidio** ，提供了一种不同的数据保护方法，允许您快速识别和匿名化私人信息，如信用卡号、姓名、位置、财务数据等。这些库更侧重于原始文本输入，即您通常在构建**自然语言处理** ( **NLP** )模型时使用的输入。他们提供了模块，在使用这些模块训练模型之前，你可以使用这些模块来匿名化你的数据。保护人员及其数据的另一种方法是加密数据，并使用加密的数据集执行整个模型训练过程，而无需解密。这通过**同态加密** ( **HE** )是可行的，同态加密是一种加密技术，允许在加密数据之上执行某些数学运算，而不需要访问私有(解密)密钥。计算结果是加密的，只能由私钥的所有者透露。这意味着使用 **HE** ，可以将两个加密值 **A** 和 **B** 相加，得到值 **C** ，这个值只能用加密值 **A** 和 **B** 的私钥解密，如下图所示:

![Figure 10.1 – Using HE to perform operations on top of encrypted data
](img/B16777_10_001.jpg)

图 10.1–使用 HE 对加密数据执行操作

*   控制:控制和记录端到端的过程是所有软件工程活动的基本原则。DevOps 实践通常用于确保端到端的流程自动化和治理。DevOps 中的一个关键实践是记录流程中每个步骤的正确信息，让您在每个阶段做出负责任的决策。Azure 机器学习工作区允许您为您在端到端机器学习过程中创建的各种工件添加标签和描述。下图显示了如何向您在 [*第 9 章*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136) 、*优化 ML 模型*中执行的 **AutoML** 运行添加描述:

![Figure 10.2 – Adding descriptions to runs to document them
](img/B16777_10_002.jpg)

图 10.2–向运行添加描述以记录它们

类似于向运行添加描述，您可以向您生成的各种工件添加标签，比如模型。标签是键/值对，例如`PyTorch`是`Framework`标签键的值。您可能希望将以下信息记录为型号**数据表**的一部分:

*   模型的预期用途
*   模型架构，包括使用的框架
*   使用的训练和评估数据
*   训练模型性能指标
*   公平信息，你将在本章中读到

这些信息可以是标签的一部分，而**数据表**可以是通过这些标签自动生成的降价文档。

在这一节中，您了解了可以帮助您创建负责任的 AI 的工具和技术的概述。这三个支柱同样重要，但对于 DP100 考试，您将重点关注理解类别中的工具，从模型可解释性开始，您将在下一节中了解更多。

# 解读模型的预测

能够解释模型的预测有助于数据科学家、审计员和企业领导人通过查看驱动模型预测的最重要因素来理解模型行为。它还使他们能够执行假设分析，以验证要素对预测的影响。Azure 机器学习工作区与 **InterpretML** 相集成来提供这些功能。

InterpretML 是一个开源社区，提供工具来执行模型可解释性。该社区包含几个项目。最著名的如下:

*   **Interpret** 和 **Interpret-Community** 知识库，它们专注于解释使用表格数据的模型，比如你在这本书里一直在做的糖尿病数据集。在本节中，您将使用解释社区存储库。
*   **解释文本**将的可解释性努力扩展到文本分类模型中。
*   **不同的反事实解释** ( **骰子**)机器学习允许你检测出你需要在一个数据行中执行的最少数量的变化来改变模型的输出。例如，假设您有一个刚刚拒绝了贷款申请的贷款审批模型。客户询问如何才能获得贷款批准。 **DiCE** 可以提供批准贷款的最小变动，比如减少信用卡数量或者增加 1%的年薪。

在解释机器学习模型时，有两种方法:

*   `DecisionTreeClassifier`提供了`feature_importances_`属性，允许您了解特性如何影响模型的预测。 **InterpretML** 社区提供了几个更高级的 **glassbox** 模型实现。这些模型一旦被训练，就允许你检索一个解释者，并回顾哪个特征驱动了什么结果，也被称为**可解释性结果**。这些模型的解释器是无损的，这意味着它们准确地解释了每个特性的重要性。
*   **黑盒解释**:如果你正在训练的模型没有自带解释器，你可以创建一个黑盒解释器来解释模型的结果。您必须提供经过训练的模型和测试数据集，解释者观察值排列如何影响模型的预测。例如，在贷款批准模型中，这可能会调整被拒绝记录的年龄和收入，以观察这是否会改变预测。通过执行这些实验获得的信息被用来解释特性的重要性。这种技术可以应用于任何机器学习模型，因此它被认为是模型不可知的。由于它们的性质，这些解释器是有损耗的，这意味着它们可能不能准确地表示每个特性的重要性。科学文献中有几个众所周知的黑盒技术，比如 **Shapley 加法解释**(**SHAP**)**局部可解释模型不可知解释**(**LIME**)**部分相关**(**PD**)**排列特征重要性**(**PFI**)**黑色框解释器的一个子类是**灰色框解释器**，它利用关于模型结构的信息来获得更好更快的解释。例如，有专门针对树模型的解释器(**树解释器**)、线性模型(**线性解释器**)、以及甚至深度神经网络(**深度解释器**)。**

模型解释者可以提供两种类型的解释:

*   **局部-** 或**实例级特征重要性**关注特征对特定预测的贡献。例如，它可以帮助回答模型拒绝特定客户贷款的原因。并非所有技术都支持本地解释。例如，基于 **PFI** 的不支持实例级特征重要性。
*   **全局-** 或**聚合级特征重要性**解释了模型在考虑模型所做的所有预测后的总体表现。例如，它可以回答关于贷款审批哪个特性是最重要的。

既然您已经有了模型解释背后的基本理论，现在是您获得一些实践经验的时候了。您将从训练一个简单的 **sklearn** 模型开始。

## 训练贷款审批模型

在本节中，您将根据您将生成的贷款审批数据集训练一个分类模型。在接下来的部分中，您将使用这个模型来分析其结果。让我们开始吧:

1.  Navigate to the `chapter10` and then create a notebook called `chapter10.ipynb`, as shown here:![Figure 10.3 – Creating the chapter10 notebook in the chapter10 folder
    ](img/B16777_10_003.jpg)

    图 10.3–在第 10 章文件夹中创建第 10 章笔记本

2.  You will need to install the latest packages of the `interpret-community` library, Microsoft's responsible AI widgets, and `#` or delete the cell:![Figure 10.4 – Restarting the Jupyter kernel after installing the necessary packages
    ](img/B16777_10_004.jpg)

    图 10.4–安装必要的软件包后重新启动 Jupyter 内核

3.  After restarting you kernel, add a new cell in the notebook. Generate a loans dataset using the following code:

    ```
    from sklearn.datasets import make_classification
    import pandas as pd
    import numpy as np
    features, target = make_classification(
        n_samples=500, n_features=3, n_redundant=1, shift=0,
        scale=1,weights=[0.7, 0.3], random_state=1337)
    def fix_series(series, min_val, max_val):
        series = series - min(series)
        series = series / max(series)
        series = series * (max_val - min_val) + min_val
        return series.round(0)
    features[:,0] = fix_series(features[:,0], 0, 10000)
    features[:,1] = fix_series(features[:,1], 0, 10)
    features[:,2] = fix_series(features[:,2], 18, 85)
    classsification_df = pd.DataFrame(features, dtype='int')
    classsification_df.set_axis(
       ['income','credit_cards', 'age'],
       axis=1, inplace=True)
    classsification_df['approved_loan'] = target
    classsification_df.head()
    ```

    此代码将生成具有以下正态分布特征的数据集:

    *   `income`最小值为`0`，最大值为`10000`。
    *   `credit_cards`最小值为`0`，最大值为`10`。
    *   `age`最小值为`18`，最大值为`85`。

    您将预测的标签是`approved_loan`，这是一个布尔值，500 个样本(`n_samples`)中只有 30% ( `weights`)是批准的贷款。

4.  Later in this chapter, you are going to run an **AutoML** experiment against this dataset. Register the dataset, as you saw in [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102), *The AzureML Python SDK*. Add the following code in your notebook:

    ```
    from azureml.core import Workspace, Dataset
    ws = Workspace.from_config()
    dstore = ws.get_default_datastore()
    loans_dataset = \
    Dataset.Tabular.register_pandas_dataframe(
        dataframe=classsification_df,
        target=(dstore,"/samples/loans"),
        name="loans",
        description="A genarated dataset for loans")
    ```

    如果您访问注册的数据集，您可以查看数据集的配置文件，如下所示:

    ![Figure 10.5 – Generated dataset profile
    ](img/B16777_10_005.jpg)

    图 10.5–生成的数据集配置文件

5.  To be able to train and evaluate the model, you will need to split the dataset into train and test datasets. Use the following code to do so:

    ```
    from sklearn.model_selection import train_test_split
    X = classsification_df[['income','credit_cards', 'age']]
    y = classsification_df['approved_loan'].values
    x_train, x_test, y_train, y_test = \
            train_test_split(X, y, 
                           test_size=0.2, random_state=42)
    ```

    首先，将数据集一分为二，一部分包含要素，另一部分包含您试图预测的标注。然后，您使用`train_test_split`方法将 500 个样本数据分成一个包含 500 * 0.2 = 100 个测试记录的数据和一个包含剩余 400 个样本的训练集。

6.  下一步是初始化模型，并使其符合训练数据集。在 [*第九章*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136) ，*优化 ML 模型*中，你学习了 Azure 机器学习的`datatransformer`步骤是一个将`MinMaxScaler`应用于所有特性的`ColumnTransformer`。该转换器缩放每个特征的值。
7.  第`model`步是你正在训练的实际模型，是一个`RandomForestClassifier`。

然后，您必须调用实例化管道的`fit`方法，根据训练数据集对其进行训练。

重要说明

您不需要使用`Pipeline`来从本章讨论的可解释性特性中获益。您可以通过将模型分配给`model_pipeline`变量来直接使用模型，而不是创建管道；例如，`model_pipeline=RandomForestClassifier()`。添加`datatransformer`步骤是为了帮助您理解 AutoML 如何构建其管道。使用`MinMaxScaler`和增加了最终模型的准确性。请随意尝试不同的缩放器，以观察结果模型中的差异。

11.  Now that you have a trained model, you can test it. Let's test against three fictional customers:
    *   45 岁，有两张信用卡，月收入`2000`
    *   一个 45 岁的人，有 9 张信用卡，月收入`2000`
    *   45 岁，有两张信用卡，月收入`10000`

    为此，请在新的笔记本单元格中使用以下代码:

    ```
    test_df = pd.DataFrame(data=[
        [2000, 2, 45],
        [2000, 9, 45],
        [10000, 2, 45]
    ], columns= ['income','credit_cards', 'age'])
    test_pred = model_pipeline.predict(test_df)
    print(test_pred)
    ```

    打印结果为`[0 1 1]`，这意味着第一个客户的贷款将被拒绝，而第二个和第三个客户的贷款将被批准。这表明`income`和`credit_cards`特征可能在模型预测中起重要作用。

12.  Since the trained model is a decision tree and belongs to the glassbox model category, you can get the importance of the features that were calculated during the training process. Use the following code in a new notebook cell:

    ```
    model_pipeline.named_steps['model'].feature_importances_
    ```

    这段代码获取对实际的`RandomForestClassifier`实例的引用，并调用一个`feature_importances_`。它的输出类似于`array([0.66935129, 0.11090936, 0.21973935])`，显示出`income`(第一个值)是最重要的特征，但是与我们在*步骤 7* 中观察到的情况相比，`age`(第三个值)似乎比`credit_cards`(第二个值)更重要。

    重要说明

    该模型的训练过程是不确定的，这意味着您的结果将与本书示例中看到的结果不同。这些数字应该是相似的，但不是相同的。

在本节中，您训练了一个简单的`feature_importances_`属性。在下一节中，您将使用更高级的技术来解释任何模型。

## 使用表格解释器

到目前为止，您已经使用了sk learn 库的功能来训练和理解模型的结果。从现在开始，您将使用解释社区的包来更准确地解释您的训练模型。您将使用 **SHAP** ，这是一种黑盒技术，它告诉您哪些特性在将预测从**拒绝**移动到**批准**中扮演什么角色，反之亦然。让我们开始吧:

1.  In a new notebook cell, add the following code:

    ```
    from interpret.ext.blackbox import TabularExplainer
    explainer = TabularExplainer(
                  model_pipeline.named_steps['model'],
                  initialization_examples=x_train, 
                  features= x_train.columns,
                  classes=["Reject", "Approve"],
                  transformations=
                    model_pipeline.named_steps['datatransformer']) 
    ```

    这段代码创建了一个`TabularExplainer`，这是一个围绕 SHAP 解释技术的包装类。这意味着该对象将根据传入的模型选择最佳的 SHAP 解释方法。在这种情况下，由于模型是基于树的，它将选择**树解释器**。

2.  Using this explainer, you are going to get the **local** or **instance-level feature importance** to gain more insights into why the model gave the results it did in *Step 7* of the *Training a loans approval model* section. In a new notebook cell, add the following code:

    ```
    local_explanation = explainer.explain_local(test_df)
    sorted_local_values = \
        local_explanation.get_ranked_local_values()
    sorted_local_names = \
        local_explanation.get_ranked_local_names()
    for sample_index in range(0,test_df.shape[0]):
        print(f"Test sample number {sample_index+1}")
        print("\t", test_df.iloc[[sample_index]]
                             .to_dict(orient='list'))
        prediction = test_pred[sample_index]
        print("\t", f"The prediction was {prediction}")
        importance_values = \
            sorted_local_values[prediction][sample_index]
        importance_names = \
            sorted_local_names[prediction][sample_index]
        local_importance = dict(zip(importance_names,
                                    importance_values))
        print("\t", "Local feature importance")
        print("\t", local_importance)
    ```

    这段代码产生的结果如下面的屏幕截图所示。如果您关注第 2 个的**测试样本，您会注意到，它显示出**信用卡**特征是将特定样本预测为**批准** ( **预测为 1** )的最重要原因(参见 **0.33** 值)。同一个样本中的**收入**(其值约为 **-0.12** )的负值表明该特征推动模型**拒绝**贷款:**

    ![Figure 10.6 – Local importance features show the importance of each feature for each test sample
    ](img/B16777_10_006.jpg)

    图 10.6-局部重要性特征显示了每个测试样本的每个特征的重要性

3.  还可以得到`income`、`age`，然后是`credit_cards`，它们对应的重要性值大概分别是`0.28`、`0.09`、`0.06`(实际值在你的执行中可能会有所不同)。请注意，这些值与您在*训练贷款审批模型*部分的*步骤 8* 中获得的值不同，尽管顺序保持不变。这是正常的，因为`Method used: shap.tree`表明`TabularExplainer`使用**树解释器**解释了模型，如本节*步骤 1* 所述。
4.  Finally, you must render the explanation dashboard to review the `global_explanation` results you generated in *Step 3*. Add the following code in your notebook:

    ```
    from raiwidgets import ExplanationDashboard
    ExplanationDashboard(global_explanation, model_pipeline, dataset=x_test, true_y=y_test)
    ```

    这将呈现一个交互式小部件，您可以使用它来根据您提供的测试数据集理解您的模型的预测。点击**聚合特性重要性**选项卡，您应该会看到与在*步骤 3* 中看到的结果相同的结果:

![Figure 10.7 – The explanation dashboard provided by the interpret community
](img/B16777_10_007.jpg)

图 10.7–解释社区提供的解释面板

您将在*查看解释结果*部分更详细地了解该仪表板。

到目前为止，您已经训练了一个模型，并使用了 **SHAP** 解释技术来解释您的模型预测的特征重要性，无论是在全局还是在特定推理的局部级别。在下一节中，您将了解更多关于 Interpret-Community 包中可用的替代解释技术。

## 了解表格数据解释技术

在上一节中，您使用表格解释器自动选择一个可用的**SHAP**技术。目前，解释社区支持以下 SHAP 解释者:

*   **树解释器**用于解释决策树模型。
*   **线性解释器**解释线性模型，也可以解释特征间的相关性。
*   **深度解释器**为深度学习模型提供近似解释。
*   **内核解释器**是最通用也是最慢的一个。它可以解释任何函数的输出，使其适用于任何模型。

SHAP 解释技术的另一种选择是建立一个更容易解释的代理模型，比如解释社区提供的 T2 玻璃盒子模型，来重现给定黑盒的输出，然后解释这个代理。这个技巧是由**模仿者**讲解者使用的，你需要提供以下玻璃盒子模型之一:

*   **lgbmexplaineablemodel**，其中是一个 **LightGBM** (一个基于决策树的快速、高性能框架)可解释模型
*   **线性解释模型**，这是一个线性解释模型
*   **sgdexplanablemodel**，其中是一个随机梯度下降可解释模型
*   **决策树解释模型**，其中是决策树解释模型

如果你想在上一节的第一步中使用模拟解释器，代码应该是这样的:

```
from interpret.ext.glassbox import (
    LGBMExplainableModel,
    LinearExplainableModel,
    SGDExplainableModel,
    DecisionTreeExplainableModel
)
from interpret.ext.blackbox import MimicExplainer
mimic_explainer = MimicExplainer(
           model=model_pipeline, 
           initialization_examples=x_train,
           explainable_model= DecisionTreeExplainableModel,
           augment_data=True, 
           max_num_of_augmentations=10,
           features=x_train.columns,
           classes=["Reject", "Approve"], 
           model_task='classification')
```

您可以从您在*第 1 行*看到的`import`语句中选择任何代理模型。在这个例子中，您使用的是`DecisionTreeExplainableModel`这个。为了获得全局的解释，代码与您在*步骤 3* 中编写的代码相同，如下所示:

```
mimic_global_explanation = \
       mimic_explainer.explain_global(x_test)
print("Feature names:", 
       mimic_global_explanation.get_ranked_global_names())
print("Feature importances:",
       mimic_global_explanation.get_ranked_global_values())
print(f"Method used: {mimic_explainer._method}")
```

尽管要素重要性的顺序相同，但计算出的要素重要性值不同，如下所示:

![Figure 10.8 – Mimic explainer feature importance calculated using the decision tree glassbox model
](img/B16777_10_008.jpg)

图 10.8–使用决策树玻璃盒模型计算的模拟解释者特征重要性

类似于`mimic_explainer`计算**局部**或**实例级特征重要性**，使用与上一节*步骤 2* 相同的代码。解释可以在下面的截图中看到:

![Figure 10.9 – Local feature importance calculated using the decision 
tree glassbox model of the Mimic explainer
](img/B16777_10_009.jpg)

图 10.9-使用模拟解释器的决策树玻璃盒模型计算的局部特征重要性

解释社区提供的最后一种解释技术基于 **PFI** 。这种技术会改变每个要素的值，并观察模型的预测如何变化。要创建一个 PFI 解释器来解释您的模型，您将需要以下代码:

```
from interpret.ext.blackbox import PFIExplainer
pfi_explainer = PFIExplainer(model_pipeline,
                             features=x_train.columns,
                             classes=["Reject", "Approve"])
```

获得全局解释需要传入`true_labels`参数，这是数据集的基本事实，即实际值:

```
pfi_global_explanation = \
        pfi_explainer.explain_global(x_test, 
                                     true_labels=y_test)
print("Feature names:", 
        pfi_global_explanation.get_ranked_global_names())
print("Feature importances:",
        pfi_global_explanation.get_ranked_global_values())
print(f"Method used: {pfi_explainer._method}")
```

这段代码的结果可以在这里看到。由于方式`credit_cards`和`age`要素在您的结果中可能正好相反，因为它们具有非常相似的要素重要性值:

![Figure 10.10 – Global feature importance calculated by the PFI explainer
](img/B16777_10_010.jpg)

图 10.10-由 PFI 解释器计算的全局特征重要性

重要说明

由于 **PFI** 解释器的性质，你*不能*用它来创建**局部**或**实例级特征重要性**。如果在考试中，有人问你这种技术是否能提供局部解释，请记住这一点。

在这一节中，您了解了解释社区包支持的所有解释技术。在下一节中，您将探索解释仪表板提供的功能，以及该仪表板如何嵌入 Azure Machine Learning Studio。

## 查看解释结果

Azure Machine Learning 提供了与 interpret 社区工作的丰富集成。其中一个集成点是解释仪表板，它嵌入在每次运行中。您可以使用`azureml.interpret`包中的`ExplanationClient`将模型解释上传到您的工作区，或者从您的工作区下载模型解释。要上传您在使用表格解释器部分的*中使用`TabularExplainer`创建的全局解释，请导航到`chapter10.ipynb`笔记本，并使用以下代码在文件的末尾添加一个新单元格:*

```
from azureml.core import Workspace, Experiment
from azureml.interpret import ExplanationClient
ws = Workspace.from_config()
exp = Experiment(workspace=ws, name="chapter10")
run = exp.start_logging()
client = ExplanationClient.from_run(run)
client.upload_model_explanation(
    global_explanation, true_ys= y_test,
    comment='global explanation: TabularExplainer')
run.complete()
print(run.get_portal_url())
```

这段代码在`chapter10`实验中开始新的运行。从那次运行中，您创建了一个`ExplanationClient`，用于上传您生成的模型解释和基本事实(`true_ys`)，这有助于仪表板评估模型的性能。

如果您访问该代码打印出的门户链接，您将导航到一个运行，在这里，在**解释**选项卡中，您将需要选择左侧的**解释 ID** ，然后通过访问**聚合特性重要性**选项卡来查看解释仪表板，如下所示:

![Figure 10.11 – Reviewing the global explanations stored within the Azure Machine Learning workspace
](img/B16777_10_011.jpg)

图 10.11–查看存储在 Azure 机器学习工作区中的全局解释

`ExplanationClient`是 Azure 机器学习的`chapter10.ipynb`笔记本使用的，并在新单元格中添加以下代码块:

```
from azureml.core import Workspace, Dataset, Experiment
from azureml.train.automl import AutoMLConfig
ws = Workspace.from_config()
compute_target = ws.compute_targets["cpu-sm-cluster"]
loans_dataset = Dataset.get_by_name(
                            workspace=ws, name='loans')
train_ds,validate_ds = loans_dataset.random_split(
                             percentage=0.8, seed=1337)
```

这段代码看起来很像你在 [*第九章*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136) ，*优化 ML 模型*中使用的代码，在*用代码*运行 AutoML 实验部分。在这个代码块中，您将获得一个对 Azure 机器学习工作区的引用，即`loans`数据集，然后您将数据集分成训练集和验证集。

在同一个或新的单元格中，添加以下代码块:

```
experiment_config = AutoMLConfig(
    task = "classification",
    primary_metric = 'accuracy',
    training_data = train_ds,
    label_column_name = "approved_loan",
    validation_data = validate_ds,
    compute_target = compute_target,
    experiment_timeout_hours = 0.25,
    iterations = 4,
    model_explainability = True)
automl_experiment = Experiment(ws, 'loans-automl')
automl_run = automl_experiment.submit(experiment_config)
automl_run.wait_for_completion(show_output=True)
```

在这个代码块中，您正在启动`model_explainability`(默认情况下是`True`)。一旦 **AutoML** 流程结束，该选项安排最佳模型的模型解释。运行完成后，导航到运行的 UI 并打开**模型**选项卡，如下所示:

![Figure 10.12 – Explanations become available for the best model in the AutoML run
](img/B16777_10_012.jpg)

图 10.12-AutoML 运行中最佳模型的解释变得可用

点击最佳模型的**查看解释**链接，导航至训练特定模型的子运行的**解释**选项卡。一旦你进入**解释**标签，你会注意到 **AutoML** 存储了两个全局解释:一个用于原始特征，一个用于工程特征。您可以通过选择屏幕左侧的适当 ID 在这两种解释之间切换，如下面的截图所示。原始要素是原始数据集中的要素。工程特征是预处理后得到的特征。工程特征是模型的内部输入。如果您选择下方的**解释 ID** 并访问**聚集特征重要性**区域，您将注意到 **AutoML** 已经将信用卡号转换为分类特征。此外，与您在模型训练中生成的三个特征相比，模型的输入是 12 个特征。

您可以在此查看这些功能及其相应的功能重要性:

![Figure 10.13 – Global explanations for engineered features
](img/B16777_10_013.jpg)

图 10.13-工程特征的总体解释

因为工程化的特征更难理解，所以转到顶部的**解释 ID** ，这是你到目前为止已经使用过的三个原始特征。导航到**数据集资源管理器**选项卡，如下所示:

![Figure 10.14 – Dataset explorer in the raw features explanations
](img/B16777_10_014.jpg)

图 10.14-原始要素说明中的数据集资源管理器

在这里，我们可以看到下面的:

1.  **模仿者**讲解者用于讲解具体模型(是一个**xgboost 分类器**，如图*图 10.12* 所示)。用作代理模型的 **glassbox** 模型是一个**lgbmexplaineablemodel**，如前面截图的左上角所示。
2.  您可以编辑群组或定义新群组，以便能够通过从**选择要探索的数据集群组**下拉列表中选择特定子群组来集中分析。要定义新群组，您需要指定要应用的数据集过滤标准。例如，在前面的截图中，我们定义了一个名为 **age_45** 的群组，它有一个单独的过滤器(年龄== 45)。测试数据集中有 **4 个数据点**被该解释仪表板使用。
3.  您可以通过点击前面截图中标记为 **3** 的高亮区域来修改 *x* 轴和 *y* 轴字段。这允许您更改视图，并深入了解特征与预测值或基础事实的相关性、特征之间的相关性，以及对您的模型理解分析有意义的任何其他视图。

在**聚合特征重要性**选项卡中，如此处所示，您可以查看所有数据或您定义的特定群组的特征重要性:

![Figure 10.15 – Aggregate feature importance for the raw features with 
the cohorts and dependency of the rejected loans based on income
](img/B16777_10_015.jpg)

图 10.15-原始特征的总体特征重要性，以及基于收入的被拒绝贷款的群组和依赖性

在这个例子中，**收入**特征对于**年龄 _45** 群体比普通大众更重要，普通大众由**所有数据**表示。如果您单击一个特性重要性栏，下面的图表会更新，向您显示该特性如何影响模型拒绝贷款请求的决策( **Class 0** )。在本例中，您可以看到从 0 到略高于 5，000 的收入*推动*模型拒绝贷款，而从 6，000 开始的收入具有负面影响，这意味着他们试图*推动*模型批准贷款。

在解释面板中有大量的特性，新的特性一直出现在解释社区中。在本节中，您回顾了仪表板最重要的功能，这些功能有助于您理解模型做出预测的原因，以及如何潜在地调试性能较差的极限情况。

在下一节中，您将了解错误分析，这是微软整体负责任的 AI widgets 包的一部分。此工具允许您了解模型的盲点，即模型表现不佳的情况。

# 分析模型误差

**错误分析**是一个模型评估/调试工具，使您能够更深入地了解您的机器学习模型错误。错误分析可帮助您识别数据集中错误率高于其他记录的群组。您可以更仔细地观察错误分类和错误的数据点，以调查是否可以发现任何系统模式，例如是否没有特定群组的可用数据。错误分析也是描述系统当前缺点并与其他利益相关者和审计员沟通的一种强有力的方式。

该工具由几个可视化组件组成，可以帮助您了解错误出现的位置。

导航到`chapter10.ipynb`笔记本。从**菜单**中，在**编辑器**子菜单中，点击**在 Jupyter 中编辑**在 Jupyter 中打开同一个笔记本并继续编辑，如下图所示:

![Figure 10.16 – Editing a notebook in Jupyter for better compatibility with the widget
](img/B16777_10_016.jpg)

图 10.16-在 Jupyter 中编辑笔记本以更好地兼容小部件

重要说明

在写这本书的时候，由于笔记本体验强加的安全限制，错误分析仪表板不能在笔记本体验上工作，这阻止了某些功能的正常工作。如果你试图在笔记本上运行它，它不会产生必要的可视化效果。这就是为什么你要在 Jupyter 里打开笔记本，在你读这本书的时候可能不需要的东西。

在 Jupyter 环境中，使用以下代码在文件末尾添加一个新单元格:

```
from raiwidgets import ErrorAnalysisDashboard
ErrorAnalysisDashboard(global_explanation, model_pipeline, 
                       dataset=x_test, true_y=y_test)
```

请注意，这段代码与您用来触发解释仪表板的代码非常相似。

重要说明

确保从任何其他编辑体验中关闭笔记本，例如 Azure Machine Learning Studio 中的笔记本体验。如果文件被另一个编辑器意外修改，您可能会丢失一些代码。

工具在全局视图中打开，如下所示:

![Figure 10.17 – The error analysis dashboard loaded within the Jupyter environment
](img/B16777_10_017.jpg)

图 10.17–Jupyter 环境中加载的错误分析仪表板

在这个视图中，您正在查看模型在整体数据上的错误率。在这个视图中，您可以看到一个二叉树，它描述了可解释子组之间的数据分区，这些子组具有出乎意料的高或低的错误率。在我们的例子中，模型的所有错误都发生在收入小于或等于 **6144** 的情况下，占 **7.25%** 的错误率，这意味着月收入小于 **6144** 的贷款中有 **7.25%** 被错误分类。错误覆盖率是所有错误中落入该节点的部分，在这种情况下，所有错误都位于该节点( **100%** )。节点中的数字显示了数据表示。这里，属于该节点的 **69** 条记录中有 **5** 个样本是错误的。

一旦您在**树形图**中选择了一个节点，您可以点击**群组设置**或**群组信息**，并将这些记录保存为感兴趣的群组。该群组可用于解释仪表板。点击**解释**按钮，您将进入**数据浏览器**视图，如下所示:

![Figure 10.18 – The data explorer for the specific cohort selected in the tree map
](img/B16777_10_018.jpg)

图 10.18-树形图中所选特定群组的数据浏览器

这个视图已经预先选择了节点的群组。它具有与解释仪表板类似的功能，例如查看影响所选群组的整体模型预测的特征重要性。这个视图还提供了**本地解释**选项卡，它允许您理解单个错误记录，甚至执行假设分析，以了解模型何时会正确地分类该记录。

通过点击小部件左上角的**错误浏览器**链接，您将导航回**树形图**视图。从**错误浏览器:**下拉菜单中，选择**热图**而不是当前选择的**树形图**。这将引导您进入错误热图视图，如下所示:

![Figure 10.19 – Error heat map view
](img/B16777_10_019.jpg)

图 10.19–误差热图视图

该视图根据左上角选择的特征以一维或二维方式分割数据。热图以较暗的红色显示误差较大的单元格，以引起用户对误差差异较大的区域的注意。带条纹的单元格表示没有评估样本，这可能表示隐藏的错误。

在这一部分中，您已经了解了错误分析仪表板的功能，以及它如何帮助您了解您的模型在哪里出错。该工具可以帮助您识别这些错误，并通过设计新功能、收集更好的数据、丢弃一些当前训练数据或执行更好的超参数调整来减轻这些错误。

在下一节中，您将了解到关于 Fairlearn 的,这是一个帮助您评估模型的公平性并减少任何观察到的不公平问题的工具。

# 检测潜在的模型公平性问题

由于多种原因，机器学习模型可能表现不公平:

*   社会中的历史偏见可能反映在用于训练模型的数据中。
*   模型开发人员所做的决定可能有偏差。
*   缺少用于训练模型的代表性数据。例如，来自特定人群的数据点可能太少。

由于很难确定导致模型行为不公平的实际原因，因此模型行为不公平的定义是由其对人的影响来定义的。模型会造成两种严重的伤害:

*   **分配伤害**:当模型拒绝给一群人机会、资源或信息时，这种就会发生。例如，在招聘过程中，或者在我们到目前为止一直在做的贷款例子中，你可能没有机会被聘用或获得贷款。
*   **服务质量损害**:当系统不能为每个人提供相同的服务质量时，就会发生这种。例如，它降低了特定人群面部检测的准确性。

基于此，很明显，模型公平性问题不能自动解决，因为没有数学公式。 **Fairlearn** 是一个工具包，提供帮助评估和减轻分类和回归模型预测的公平性的工具。

在我们的例子中，如果我们将年龄组视为一个敏感特性，我们可以使用以下代码基于其准确性来分析模型的行为:

```
from fairlearn.metrics import MetricFrame
from sklearn.metrics import accuracy_score
y_pred = model_pipeline.predict(x_test)
age = x_test['age']
model_metrics = MetricFrame(accuracy_score, y_test, 
                             y_pred, sensitive_features=age)
print(model_metrics.overall)
print(model_metrics.by_group[model_metrics.by_group < 1])
```

这段代码获取您在*训练贷款审批模型*部分中训练的模型的预测，并为`x_test`数据集创建预测。然后，它将所有来自`x_test['age']`特征的值分配给`age`变量。然后，通过使用`MetricFrame`，我们可以为整个测试数据集计算模型的`accuracy_score`度量，它存储在`overall`属性中，或者按组计算精度，它存储在`by_group`属性中。此代码打印总体准确度分数和小于 1 的组的准确度分数。结果如以下截图所示:

![Figure 10.20 – The model has a 0.96 accuracy but for 65-year-olds its accuracy is 0.5
](img/B16777_10_020.jpg)

图 10.20–该模型的精确度为 0.96，但对于 65 岁的人来说，精确度为 0.5

虽然数据集是生成的，但你可以看到该模型对 65 岁老人的准确率只有 50%。请注意，尽管该模型是针对 18 到 85 岁的人进行训练的，但在数据集中只检测到 35 个亚组，这表明我们可能没有准确地测试它。

与`ExplanationDashboard`和`ErrorAnalysisDashboard`类似，负责任的 AI widgets ( `raiwidgets`)包提供了一个`FairnessDashboard`，可以用来分析模型结果的公平性。

重要说明

在写这本书的时候，`FairnessDashboard`在 Jupyter 工作。在笔记本体验中，有一些技术故障。在 Jupyter 中打开您的笔记本，从`FairnessDashboard`中获得最佳体验。

在新的单元格中，添加以下代码，以使用您在前面的代码中定义的区分年龄的功能来调用公平性仪表板:

```
from raiwidgets import FairnessDashboard
FairnessDashboard(sensitive_features=age, 
                  y_true=y_test, y_pred=y_pred)
```

发布后，小工具将引导您完成公平评估流程，您需要定义以下内容:

*   **敏感特性**:这里您必须配置敏感特性。正如我们之前看到的，敏感特性用于将数据分组。在这种情况下，它会提示您为年龄组(18-29 岁、30-40 岁、41-52 岁、53-64 岁和 64-75 岁)创建五个条块，您可以修改宁滨过程，甚至通过选择它提供的**视为分类**选项来请求它单独处理每个年龄。
*   **性能指标**:性能指标用于评估您的模型整体和各组的质量。在这种情况下，您可以像之前一样选择精度。即使在向导完成后，您也可以对此进行更改。
*   **公平性度量**:公平性度量代表性能度量的极值之间的差异或比率，或者仅仅是任何组的最差值。这种度量的一个例子是**准确度分数比率**，它是任意两个组之间的最小比率准确度分数。即使在向导完成后，您也可以对此进行更改。

由此产生的仪表板允许您钻取您的模型对子组的影响。它由两个区域组成——摘要表和可视化区域——您可以在其中选择不同的图形表示，如下所示:

![Figure 10.21 – Fairness dashboard showing the accuracy of the model in various age groups
](img/B16777_10_021.jpg)

图 10.21-显示不同年龄组模型准确性的公平仪表板

一旦你发现了你的模型的公平性问题，你可以使用 fair learn(T2)库来减轻它们。Fairlearn 图书馆提供了两种方法:

*   `ThresholdOptimizer`，调整底层模型的输出，实现一个显式的约束，比如均衡赔率的约束。在我们的二元分类模型中，均等化优势意味着真阳性率和假阳性率应该在各组之间匹配。
*   `fit` **sklearn** 方法接受的`sample_weight`参数。

使用这些技术，您可以通过牺牲一些模型的性能来平衡模型的公平性，以满足您的业务需求。

**Fairlearn** 包不断发展，已经集成到 Azure Machine Learning SDK 和 Studio web experience 中，使数据科学家能够将模型公平性见解上传到 Azure Machine Learning 运行历史中，并观察 Azure Machine Learning Studio 中的 **Fairlearn** 仪表板。

在本节中，您学习了如何检测您的模型可能存在的潜在不公平行为。您还了解了可以在 **Fairlearn** 包中实现的可能的缓解技术。这总结了 Azure 机器学习工作区和开源社区提供的工具，这些工具允许你理解你的模型，并帮助你创建 AI。

# 总结

在这一章中，你已经得到了可以帮助你理解你的模型的各种工具的概述。您从 Interpret-Community 包开始，它允许您理解模型为什么做出预测。您了解了各种解释技术，并探索了解释仪表板，它提供了诸如特性重要性之类的视图。然后您看到了错误分析面板，它允许您确定模型在哪里表现不佳。最后，您了解了公平评估技术、使您能够探索潜在不公平结果的相应仪表板，以及可用于缓解潜在公平问题的方法。

在下一章，你将了解 Azure 机器学习管道，它允许你以可重复的方式编排模型训练和模型结果解释。

# 问题

在每章中，您会发现几个问题来帮助您进行与每章中讨论的主题相关的知识检查:

1.  You are using `TabularExplainer` to interpret a `DecisionTreeClassifier`. Which underlying SHAP explainer will be used?

    a.`DecisionTreeExplainer`

    b.`TreeExplainer`

    c.`KernelExplainer`

    d.`LinearExplainer`

2.  You want to interpret a `DecisionTreeClassifier` using `MimicExplainer`. Which of the following models can you use for the `explainable_model` parameter?

    a.`LGBMExplainableModel`

    b.`LinearExplainableModel`

    c.`SGDExplainableModel`

    d.`DecisionTreeExplainableModel`

    e.上述全部

3.  Can you use `PFIExplainer` to produce local feature importance values?

    a.是

    b.不

# 延伸阅读

本节提供了一个有用的 web 资源列表，这些资源将帮助您增加 Azure Machine Learning SDK 和本章中使用的各种代码片段的知识:

*   **用于差分隐私的 SmartNoise** 图书馆:【https://github.com/opendp/smartnoise-core 
*   何资源:[https://www . Microsoft . com/en-us/research/project/homo morphic-encryption/](https://www.microsoft.com/en-us/research/project/homomorphic-encryption/)
*   部署加密的推理 web 服务:[https://docs . Microsoft . com/en-us/azure/machine-learning/how-to-homo morphic-encryption-seal](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-homomorphic-encryption-seal)
*   **Presidio** ，数据保护和匿名化 API:【https://github.com/Microsoft/presidio】T2
*   数据科学项目中 aDevOps 流程的样本库，也称为**MLOps**:【https://aka.ms/mlOps】T2
*   **申报型号的型号卡**:【https://arxiv.org/pdf/1810.03993.pdf 
*   **InterpretML** 网站，链接到社区的 GitHub 知识库:【https://interpret.ml/ 
*   **错误分析**主页，包括如何使用工具包的指南:【https://erroranalysis.ai/】T2
*   **Fairlearn** 首页:【https://fairlearn.org/】T2