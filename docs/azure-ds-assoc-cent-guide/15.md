# 第十二章:用代码操作模型

在这一章中，你将学习如何操作你在本书中已经训练过的机器学习模型。您将探索两种方法:通过托管可用于进行推理的 REST API 来公开实时端点，并扩展您的管道创作知识，以便在大数据的基础上并行、高效地进行推理。您将从在工作区中注册一个模型来跟踪工件开始。然后，你会发布一个 REST API 这将允许您的模型与第三方应用程序集成，如 **Power BI** 。接下来，您将创建一个管道，在几分钟内以非常经济的方式处理 50 万条记录。

在本章中，我们将讨论以下主题:

*   了解各种部署选项
*   在工作空间中注册模型
*   部署实时端点
*   创建批处理推理管道

# 技术要求

您将需要访问 Azure 订阅。在该订阅中，您将需要一个`packt-azureml-rg`。你将需要一个`Contributor`或者`Owner`T3。如果你按照第二章 、*部署 Azure 机器学习工作区资源*中的说明，这些资源应该已经对你可用了。

此外，你需要对 **Python** 语言有一个基本的了解。本章中的代码片段针对 Python 版或更高版本。您还应该熟悉在 AzureML studio 中使用笔记本；这一点在 [*第 7 章*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102) ，*AzureML Python SDK*中有所涉及。

本章假设您已经注册了您在第十章 、*理解模型结果*中生成的**贷款**数据集。它还假设您已经创建了一个名为 **cpu-sm-cluster** 的计算集群，如第 7 章[](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102)**azure ml Python SDK*的*使用计算目标*部分所述。*

 *重要说明

AzureML 在不断更新。如果您在使用本书中的代码示例时遇到任何问题，请尝试通过将以下代码添加到新的笔记本单元格中来升级您的 AzureML SDK:

`!pip install --upgrade azureml-core azureml-sdk[notebooks]`

然后，重新启动 Jupyter 内核，正如你在*训练中所学的一个贷款审批模型*章节的 [*第十章*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147) ，*了解模型结果*。此外，尝试从本书的 GitHub 页面下载最新版本的笔记本。如果问题仍然存在，请在本书的 GitHub 页面上打开一个问题。

你可以在 http://bit.ly/dp100-ch12 的 GitHub 上找到这一章的所有笔记本和代码片段。

# 了解各种部署选项

自从 [*第八章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117)*试验 Python 代码*以来，我们一直在研究 Python 代码。到目前为止，您已经训练了各种模型，基于度量评估了，并使用 **joblib** 库的`dump`方法保存了训练好的模型。AzureML 工作空间允许您通过在模型注册中心注册来存储和版本化那些工件，我们在第 5 章 ，*中讨论过让机器做模型训练*。注册模型允许您对保存的模型和关于特定模型的元数据进行版本化，例如根据各种度量标准的性能。您将在*在工作区*注册模型部分学习如何从 SDK 注册模型。

一旦注册了模型，您必须决定您想要如何操作该模型，要么通过部署实时端点，要么通过创建批处理过程，如*图 12.1* 所示:

![Figure 12.1 – A path from training to operationalization
](img/B16777_12_001.jpg)

图 12.1-从培训到运作的路径

就模型处理传入数据的方式而言，有两个主要类别:

*   `predict_proba`你训练的分类器的方法。在本章的*部署实时端点*一节中，你会读到更多关于这个场景的内容。
*   **批量推理**:在 [*第五章*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072) ，*让机器做模型训练*，你训练了一个 AutoML 模型来搅动客户预测。该模型使用诸如消费者在过去 6 至 12 个月的活动等特征进行训练。让我们假设你想评估你所有的客户，并为那些可能流失的客户开展营销活动。您必须运行一次性流程，读取所有客户信息，计算所需的特性，然后调用每个特性的模型来生成预测。结果可以存储在一个 CSV 文件中，供营销部门使用。在这种方法中，您只需要在短时间内使用模型，也就是说，只在进行预测时使用。您不需要实时端点，例如您在第 5 章[](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072)*、*中部署的端点，让机器进行模型训练*，因为您不需要模型进行特别的推断。您可以在本章的*创建批处理推理管道*一节中了解更多关于这个场景的信息。*

 *所有模型都可以用于实时或批量推理。由您来决定您是需要特别的模型推理，还是需要生成和存储推理结果的预定过程。以批处理模式操作模型往往更具成本效益，因为您可以利用低优先级的计算集群来执行推理。在这种情况下，您不需要花钱就能拥有一个等待实时推断的实时端点基础设施。

在下一节中，您将通过培训和注册您将在本章剩余部分使用的模型来开始操作化的道路。

# 在工作空间中注册模型

注册一个模型可以让你保留训练过的模型的不同版本。每个模型版本都有工件和元数据。在元数据中，您可以保留引用来试验运行和数据集。这允许您跟踪用于训练模型的数据、训练模型的运行 ID 以及实际的模型工件本身之间的传承，如图*图 12.2* 所示:

![Figure 12.2 – Building the lineage from the training dataset all the way to the registered model
](img/B16777_12_002.jpg)

图 12.2–从训练数据集一直到注册模型构建谱系

在这个部分中，您将训练一个模型，并且将它注册到您的 AzureML 工作空间中。执行以下步骤:

1.  导航到 AzureML studio web 界面的**笔记本**部分。
2.  Create a folder, named `chapter12`, and then create a notebook named `chapter12.ipynb`, as shown in *Figure 12.3*:![Figure 12.3 – Adding the chapter12 notebook to your working files
    ](img/B16777_12_003.jpg)

    图 12.3–将第 12 章笔记本添加到您的工作文件中

3.  Add and execute the following code snippets in separate notebook cells. You will start by getting a reference to the workspace resources:

    ```
    from azureml.core import Workspace, Experiment
    ws = Workspace.from_config()
    loans_ds = ws.datasets['loans']
    experiment = Experiment(ws, "chapter-12-train")
    ```

    在前面的代码中，您获得了一个对工作区的引用，即`chapter-12-train`。

4.  Split the dataset into training and validation using the following code:

    ```
    training_data, validation_data = loans_ds.random_split(
                           percentage = 0.8, seed=42)
    X_train = training_data.drop_columns('approved_loan') \
                .to_pandas_dataframe()
    y_train = training_data.keep_columns('approved_loan') \
                .to_pandas_dataframe().values.ravel()
    X_validate = validation_data.drop_columns('approved_loan') \
                    .to_pandas_dataframe()
    y_validate = validation_data.keep_columns('approved_loan') \
                    .to_pandas_dataframe().values.ravel()
    ```

    该代码将数据集分成 80%的定型数据和 20%的验证数据。`seed`参数初始化`random_split`方法的内部随机状态，允许您对数据分割进行硬编码，并在每次调用该代码时生成相同的`training_data`和`validation_data`。

    这里，`X_train`是一个包含`income`、`credit_cards`和`age`特性的`pandas`、`DataFrame`(即除了`approved_loan`之外的所有列)。

    相比之下，`y_train`包含了你想要预测的值。首先，加载一个只包含`approved_loan`列的`pandas` `DataFrame`。然后，将数据帧转换成一个`values`属性。这个数组的每一行都有一个元素数组。例如， *[[0]，[1]]* 表示两条记录:一条未审批的贷款，值为 *0* ，一条已审批的贷款，值为 *1* 。接下来，您调用`ravel`方法来展平数组，这将给定的示例转换为 *[0，1]* 。虽然您可以使用直接使用`pandas` `DataFrame`来训练模型，但是一条警告消息将通知您自动约定已经发生，提示您使用您在该单元格中观察到的`ravel`方法。

    对将用于评估模型性能的`X_validate`数据帧和`y_validate`数组重复相同的过程。

5.  Train a model and log the achieved accuracy using the following code:

    ```
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score
    run = experiment.start_logging()
    sk_model = LogisticRegression()
    sk_model.fit(X_train, y_train)
    y_predicted = sk_model.predict(X_validate)
    accuracy = accuracy_score(y_validate, y_predicted)
    print(accuracy)
    run.log("accuracy", accuracy)
    run.complete()
    ```

    在这里，您从实验中的一次运行开始，如*步骤 3* 中所定义的。您将使用这个运行来注册模型训练过程的度量、日志和工件。然后，您训练一个`LogisticRegression`模型，并使用`accuracy_score`函数来计算训练好的模型的精度。接下来，打印计算出的精度，并将其作为运行中的一个指标记录下来。最终，你的`complete`跑来敲定了它的执行。

6.  Now that you have a trained model referenced by the `sk_model` variable, you are going to save it using the following code:

    ```
    import os
    import joblib
    os.makedirs('./model', exist_ok=True)
    joblib.dump(value=sk_model,
                filename=
                  os.path.join('./model/','model.joblib'))
    ```

    首先，创建一个名为`model`的文件夹。该文件夹的名称并不重要。在那个文件夹中，你使用`joblib`库`dump`训练好的模型。模型存储在名为`model.joblib`的文件中。

    重要说明

    `.joblib`文件扩展名不标准，只要一致就可以随便用。有些人使用`.pkl`文件扩展名，这在过去被使用，因为我们使用 Python 的内置`pickle`模块来序列化 Python 对象结构。如今，`joblib`库是由 **scikit-learn** 提出的推荐方式，因为它在序列化大型 NumPy 数组方面更高效，这在训练好的模型中很常见。

7.  Now that you have the artifacts ready, you can register the model using the following code:

    ```
    from sklearn import __version__ as sk_version
    from azureml.core import Model
    run.upload_folder("model", "./model")
    model = run.register_model(
            model_name="chapter12-loans",
            model_path="./model/",
            tags={ "accuracy": accuracy},
            properties={ "accuracy": accuracy},
            model_framework= Model.Framework.SCIKITLEARN,
            model_framework_version= sk_version,
            datasets=[("training", loans_ds)]
    )
    ```

    在第一行中，您导入了`sklearn`包的`__version__`变量，这是一个显示您的环境中当前加载的版本的字符串。然后，为该变量创建一个别名(使用`as`语句)，并在代码中将其引用为`sk_version`。这是您用来训练模型的`sklearn`库的版本。此外，您从 AzureML SDK 导入了`Model`类，以便在下面的代码行中将其用作引用。

    导入您的引用后，您将在*步骤 6* 中创建的本地`./model`文件夹的内容上传到运行的输出中，位于名为`model`的文件夹下。这允许 AzureML 访问您将要注册的工件；否则，您将收到一个`ModelPathNotFoundException`错误。

    准备好所有的先决条件后，您就可以注册模型了。这个模型将被命名为`chapter12-loans`(`model_name`参数)，使用的是刚刚上传到运行输出的`model`文件夹(`model_path`参数)中的工件。您将精确度指定为该模型的标签(`tags`参数)和属性(`properties`参数)。您指示您使用了`SCIKITLEARN`框架(`model_framework`参数)来训练模型，并且您指定了您使用的框架版本(`model_framework_version`参数)。在最后一行，您指定您使用了`loans_ds`数据集作为`training`数据集(`datasets`参数)。

    重要说明

    如果您尝试重新运行同一个单元格，将会出现一个*资源冲突*错误，因为您不能覆盖已经存在于运行输出文件夹中的文件。如果您通过使用`#`作为行前缀来注释掉`upload_folder`行，并重新运行该单元，您将使用特定运行中已经存在的工件来注册同一模型的新版本。

8.  导航至`Model.Framework.SCIKITLEARN`。这种类型的部署被认为是无代码部署，这是 AzureML 为受支持的框架提供的一种功能。否则，需要指定一个评分文件；这是我们将在*部署实时端点*部分讨论的内容。
9.  If you want to register a pretrained model that you downloaded from the internet, you will not have a `Run` object to call the `register_model` method. You can use the `register` method of the `Model` class, as demonstrated in the following code snippet:

    ```
    from azureml.core import Model
    offline_model = Model.register(
            ws,
            model_name="chapter12-pre-trained-loans",
            model_path="./model/",
            properties={"accuracy": 0.828},
            model_framework= "ScikitLearn",
            model_framework_version= "0.22.2.post1"
    ) 
    ```

    在前面的代码中，您在 AzureML 工作空间(`ws`变量)中将位于*本地* `model`文件夹(`model_path`参数)中的工件注册为名为`chapter12-pre-trained-loans`(`model_name`参数)的模型。这是一个使用`sklearn`库的版本`0.22.2.post1`(`model_framework_version`参数)(`model_framework_version`参数)训练的模型。此外，其精确度`0.828`作为模型属性存储。

10.  If you had a process to train new models, such as the scheduled pipeline that you created in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working with Pipelines*, you will have to verify whether the newly trained model has better metrics than the one already registered. Then, if it is better, proceed with the registration of the model. To do that, you can use code similar to the following:

    ```
    from sklearn.linear_model import RidgeClassifier
    new_model = RidgeClassifier(solver='svd')
    new_model.fit(X_train, y_train)
    y_predicted = new_model.predict(X_validate)
    accuracy = accuracy_score(y_validate, y_predicted)
    registered_model = Model(ws, name="chapter12-loans")
    r_version = registered_model.version
    r_acc = float(registered_model.properties['accuracy'])
    if accuracy > r_acc:
        print(f"New model has better accuracy {accuracy}")
    else:
        print(f"Registered model with version {r_version}" \
               " has better accuracy {r_acc}")
    ```

    在前面的代码中，您训练了一个基于`RidgeClassifier`的模型，它使用了在*步骤 7* 中注册的`chapter12-loans`。`registered_model`变量与您在*步骤 7* 中获得的`model`变量具有相同的引用；只是，这一次，您使用`Model`类而不是通过注册一个模型来创建那个引用。从该模型中，您可以读取`version`属性和`accuracy`属性。您可以从模型的`tags`字典而不是`properties`字典中检索精度。您将精度值转换为浮点数，因为标记和属性将其值存储为字符串。接下来，将新模型的精度与已经注册的精度进行比较(存储在`r_acc`变量中)。如果新型号比注册的型号好，则打印一条消息。在这种情况下，您重复*步骤 6* 和*步骤 7* 来存储模型，然后注册模型的新的改进版本。

    重要说明

    要注册新的型号版本，只需要用相同的名称注册新型号即可。通过注册一个同名的新模型，AzureML 会自动为你创建一个新版本。

11.  Optionally, as a last step, delete the locally stored model using the following code:

    ```
    import shutil
    shutil.rmtree('./model',ignore_errors=True)
    ```

    这段代码删除了您在*步骤 6* 中创建的`model`文件夹，包括您不再需要的序列化模型。`ignore_errors`参数允许您运行这个单元格，即使文件夹不存在也不会产生任何错误。

在本节中，您在笔记本上的 Jupyter 内核中训练了一个模型。然后，您在您的工作区内注册了模型。您可以在第十一章 、*使用管道*的*创作管道*部分的*步骤 11* 中使用相同的注册码来注册`run=Run.get_context()`方法，然后您需要上传序列化模型并注册模型，就像您在*步骤 7* 中所做的那样。作为一个额外的活动，尝试修改`train_model.py`脚本和`chapter11.ipynb`来创建一个管道，该管道注册了在管道中被训练的模型。这个活动的一个潜在解决方案可以在`train_model_and_register.py`脚本中找到。这可以在位于[http://bit.ly/dp100-ch11](http://bit.ly/dp100-ch11)的 GitHub 库的`step02`文件夹中找到。

在下一节中，您将开始操作您在这一节中注册的模型，方法是将它部署为一个服务于实时推理的 web 服务。

# 部署实时端点

让我们想象一下，您有一个电子银行解决方案，其中包含客户申请贷款的流程。你要正确设定顾客的期望，并为他们可能的拒绝做好准备。当客户提交贷款申请表时，您要调用您在工作区部分的*注册模型中注册的模型，即名为 **chapter12-loans** 的模型，并传入客户在申请表上填写的信息。如果模型预测贷款不会被批准，贷款请求的确认页面上会出现一条消息，让客户为贷款请求可能被拒绝做好准备。*

*图 12.5* 显示了一个过于简化的架构，用于描述从客户开始到模型实时端点的请求流:

![Figure 12.5 – An oversimplified e-banking architecture showing the flow 
of requests from the customer to the model
](img/B16777_12_005.jpg)

图 12.5–一个过于简化的电子银行架构，显示了从客户到模型的请求流程

部署模型最简单的方法是通过 AzureML 为特定的机器学习框架提供的无代码部署方法，包括您在上一节中使用的的`sklearn`库。执行以下步骤:

1.  转到`chapter12.ipynb`笔记本，添加下面的代码，以获得对您在上一节中创建的 **chapter12-loans** 模型的最新版本的引用:

    ```
    from azureml.core import Workspace, Model
    ws = Workspace.from_config()
    model = Model(ws, name="chapter12-loans")
    ```

2.  To deploy a real-time endpoint, use the following code:

    ```
    no_code_service = Model.deploy(ws, "no-code-loans",
                                   [model])
    no_code_service.wait_for_deployment(show_output=True)
    ```

    这段代码部署一个名为`no-code-loans`的新实时端点服务，然后等待部署完成。

3.  To get the scoring URI for the newly deployed endpoint, use the following code:

    ```
    print(no_code_service.scoring_uri)
    ```

    这是一个格式为[http://guid.region.azurecontainer.io/score](http://guid.region.azurecontainer.io/score)的 URL，用 **JSON** 有效载荷接受 **POST** 请求，如下所示:

    ```
    {"data": [[2000,2,45]]}
    ```

    这个有效载荷将为一个月收入为 2000、拥有两张信用卡并且年龄为 45 岁的客户触发一个推断请求。您可以使用诸如**邮递员**或 **curl** 之类的工具来创建这样一个 HTTP 请求并调用端点。

4.  Instead of making an HTTP request using a tool such as `curl`, you can use the `no_code_service` reference and invoke the `run` method by passing in the JSON payload that you would normally send to the service:

    ```
    import json
    input_payload = json.dumps({
        'data': [[2000, 2, 45], [2000, 9, 45]],
        'method': 'predict'
    })
    output = no_code_service.run(input_payload)
    print(output)
    ```

    前面的代码导入了`json`库，这有助于将对象序列化为 JSON 字符串。您使用`dumps`方法创建有效负载。请注意，有效载荷与您在*步骤 3* 中看到的简单版本略有不同。在这个例子中，不是传递单个客户的信息，而是传递两个客户的信息:一个是您之前传递的，另一个是有 *9* 信用卡而不是 *2* 的。此外，您要指定调用哪个方法。默认情况下，模型的方法名是`predict`，这是您在前面章节中用来进行推论的方法。最后，打印输出，看起来应该如下所示:

    ```
    {'predict': [0, 1]}
    ```

    前面的结果表明，第一笔贷款将被拒绝，而第二笔贷款将被批准。

    大多数分类模型提供了另一种叫做`predict_proba`的方法，它返回一个数组，其中包含每个标签的概率。在`loans`批准案例中，这个数组将只包含 2 个总计为 1 的概率，即贷款获得批准的概率和被拒绝的概率。如果您将方法名称从`predict`更改为`predict_proba`并重新执行该单元，您将得到以下结果:

    ```
    {'predict_proba': [[0.998, 0.002], [0.173, 0.827]]}
    ```

    前面的结果表明，该模型对第一笔贷款将被拒绝的置信度为 99.8%，对第二笔贷款将被批准的置信度为 82.7%。

5.  或者，导航至`chapter12-demanding-loans`。您指定它需要`1` CPU 和`1.5` GB 内存。注意，如果您在工作区部分的*注册模型的*步骤 11* 中删除了`model`文件夹，这段代码将无法注册新的模型，因为它将无法找到模型工件。*
6.  为了节省成本，您应该使用下面的代码删除服务:

    ```
    no_code_service.delete()
    ```

到目前为止，您已经使用无代码方法部署了一个实时端点，它将模型部署为一个容器实例。只有在使用特定的支持模型来训练模型时，这才是可行的。在下一节中，您将学习如何使用更高级的选项来部署模型。

## 了解模型部署选项

在上一节中，您使用无代码方法部署了模型。在幕后，AzureML 使用了一个包含所有必需的模型依赖关系的`environment`，在我们的例子中是`sklearn`，生成一个 Python 脚本来加载模型并在数据到达端点时做出推断，并使用一个`AciServiceDeploymentConfiguration`类发布了一个 ACI 服务。

如果您有一个用不支持的框架训练的模型，或者如果您想要更好地控制部署模型，您可以使用 AzureML SDK 类部署模型，如图 12.7 所示:

![Figure 12.7 – The components required in a real-time endpoint deployment
](img/B16777_12_007.jpg)

图 12.7–实时端点部署所需的组件

这里，**推理配置**类指定了模型依赖关系。它需要一个入口脚本来加载模型并处理传入的请求，以及一个入口脚本将在其中执行的环境。这个环境包含模型加载和进行推理所需的所有依赖项。

入口脚本应该有以下两种方法:

*   `Init`:在这个步骤中，脚本将训练好的模型加载到内存中。根据您如何将模型的状态存储到磁盘，您可以使用相应的方法来反序列化模型。例如，如果您使用了`joblib`库来序列化您的模型，那么您可以使用同一个库的`load`方法来将其加载到内存中。有些模型提供了自己的序列化和反序列化方法，但过程是相同的；已定型模型的状态保存在一个或多个文件中，您可以稍后使用这些文件将已定型模型加载到内存中。根据模型的大小，初始化阶段可能需要很长时间。较小的`sklearn`模型应该在几毫秒内加载到内存中，而较大的神经网络可能需要几秒钟才能加载。
*   `run`:这是当实时端点接收到数据集进行推断时调用的方法。在这个方法中，您必须使用加载在`init`代码中的模型来调用它提供的预测方法，以便对传入的数据进行推断。如前所述，大多数模型都提供了`predict`方法，您可以调用该方法并将其传递到您想要进行推理的数据中。大多数分类模型都提供了一个额外的方法，称为`predict_proba`，它返回每个类别的概率。自动预测模型提供了`forecast`方法，而不是`predict`方法。在进行预测时，神经网络有一种不同的方法。例如，在 TensorFlow 的第一个版本中，您必须通过一个`session.run()`方法调用来调用一个预测方法。

一旦您配置了模型依赖项，您需要决定您想要在哪里部署模型。AzureML SDK 提供了三个类:`LocalWebserviceDeploymentConfiguration`、`AciServiceDeploymentConfiguration`和`AksServiceDeploymentConfiguration`。这些允许你在本地机器上部署成 ACI 或者 **Azure Kubernetes 服务** ( **AKS** )，如图*图 12.8* 所示:

![Figure 12.8 – Picking the right compute target for your model
](img/B16777_12_008.jpg)

图 12.8–为您的模型选择正确的计算目标

正如您可能已经收集到的，在*图 12.8* 中，您可以通过指定您希望服务监听的端口来部署到您的本地计算机。这是一个很好的方法，可以用来调试模型的任何潜在加载问题，或者验证与本地计算机上的其余系统的集成。下一个选择是使用 ACI，这意味着测试环境或小规模生产环境。在`AciServiceDeploymentConfiguration`类中只能使用 CPU，不能使用 GPU。通过将`auth_enabled`参数设置为`True`，您可以使用基于密钥的认证来保护端点。这个身份验证方法要求您将一个静态密钥作为**授权**头传递到 HTTP 请求中。

另一方面，`AksServiceDeploymentConfiguration`在 AKS 集群中部署服务。如果您的模型可以使用 GPU，并且您要部署到的集群具有支持 GPU 的节点，那么您就可以使用 GPU。这种部署配置允许您在基于密钥的身份验证和基于令牌的身份验证之间进行选择。基于令牌的认证要求最终用户从保护 Azure 工作区的 **Azure Active Directory** 获取访问令牌，这将允许您访问部署在其中的端点。与基于密钥的身份验证(ACI 中唯一可用的选项)相比，此令牌是短暂的，并且隐藏了呼叫者的身份。AKS 部署的另一个生产就绪特性是动态伸缩的能力，以处理传入请求数量的波动。在手头的电子银行场景中，客户倾向于在工作时间访问电子银行解决方案，而系统在晚上基本上是空闲的。此外，在月底，传入流量达到峰值。在这种工作负载下，您希望能够扩展您的端点，以在需要时适应流量的增加。AKS 可以自动启动模型的多个容器，并在传入流量显著增加时在它们之间进行流量负载平衡。当流量恢复正常时，它只能保留一个容器作为潜在传入流量的热备用。

现在您对部署选项有了更好的理解，您将使用您在*图 12.7* 中看到的类在 ACI 中部署相同的模型:

1.  The first thing you will need to create is the entry script. Underneath the **chapter12** folder, create a new folder named **script** and place a **score.py** file inside it, as shown in *Figure 12.9*:![Figure 12.9 – Adding the score.py file for the real-time endpoint
    ](img/B16777_12_009.jpg)

    图 12.9–为实时端点添加 score.py 文件

2.  In the `init` method, you are getting the path of the serialized model using the `AZUREML_MODEL_DIR` environment variable. When AzureML spins up the Docker image that will be serving the model, this variable points to the folder where the model is located; for example, `/tmp/azureml_umso8bpm/chapter12-loans/1` could be the location where you find the first version of the `chapter12-loans` model. In that folder, the actual artifact, named `model.joblib`, is located in the `model` folder, which you uploaded in *Step 5* of the *Deploying real-time endpoints* section. You use `os.path.join` to get the final path of the model, and then you load the model in a `global` variable named `model`. If you want to use the AzureML SDK to get the location of the model, you could use `model_path = Model.get_model_path(model_name, version=version)`, which uses the same environment variable under the hood. However, note that you would need to install the AzureML SDK in your environment to be able to import the `Model` class from it; this is something that is not necessary with the preceding code.

    重要说明

    请注意，您正在使用`print`将模型路径和传入的`raw_data`写入控制台。您将在*应用洞察监控*部分了解如何查看这些消息。

    在`run`方法中，当试图读取请求的输入时，使用`try` `except`块来捕捉潜在的错误。如果出现这样的错误，异常将被序列化为一个字符串(使用`str()`方法)，并返回给最终用户。注意，将异常返回给调用者是一种安全反模式，因为您可能会意外地将有价值的信息暴露给潜在的攻击者，但这在调试时很有帮助。您可以使用`print`语句或更高级的库(如`try`块)来反序列化传入的 JSON 负载，而不是返回错误消息，如*部署实时端点*部分的*步骤 3* 所示。然后，通过`init`方法调用已经加载到内存中的`model`对象的`predict`方法。接下来，您将模型结果作为一个列表返回，该列表将被序列化为一个数组。

    重要说明

    您永远不会直接调用`init`或`run`方法。AzureML 将把另一段代码放入最终的 Docker 映像中，这是 HTTP 推理服务器。这个服务器将负责在服务器启动时调用您的`init`方法，并将传入的 HTTP 数据传递给`run`方法。此外，您在`run`方法中返回的结果将被序列化为一个 **JSON** 并返回给调用者。

3.  The next thing you need is an `Environment` that has all of the necessary dependencies to run the `score.py` script that you created. Open your `chapter12.ipynb` notebook and add the following code inside a new cell:

    ```
    from azureml.core import Environment
    from azureml.core.conda_dependencies import CondaDependencies 
    import sklearn
    myEnv= Environment(name="sklearn-inference")
    myEnv.Python.conda_dependencies = CondaDependencies()
    myEnv.Python.conda_dependencies.add_conda_package(
                   f"scikit-learn=={sklearn.__version__}")
    myEnv.Python.conda_dependencies.add_pip_package(
                   "azureml-defaults>=1.0.45")
    ```

    在前面的代码中，您创建了一个`Environment`，如 [*第 8 章*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117) 、*使用 Python 代码*中所示。您添加了`scikit-learn` `azureml-defaults` `pip`包，它包含了将模型作为 web 服务托管的必要功能。因为您正在构建自己的`Environment`，所以您需要添加这个包并至少使用版本 1.0.45。这是您可以用来运行评分脚本的最基本的环境。此外，AzureML 提供了一个您可以使用的管理环境，例如`AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference`，它包含了您使用在`sklearn`版本 0.24.1 中训练的模型做出推理请求所需的一切。

4.  You have defined everything that is needed by the `InferenceConfig` class. Add a new cell and type in the following code to put everything together:

    ```
    from azureml.core.model import InferenceConfig
    inference_config = InferenceConfig(
                          source_directory= "./script",
                          entry_script='score.py', 
                          environment=myEnv)
    ```

    这段代码创建了用模型进行推理所需的配置。它使用位于`script`文件夹中的`score.py`文件，并在`myEnv`环境中执行该文件，这是您在*步骤 3* 中定义的。

5.  Now you have two out of the three components depicted in *Figure 12.7*. In this step, you will create an `AciServiceDeploymentConfiguration` class, and you will deploy the model in ACI. In a new cell, add the following code:

    ```
    from azureml.core.webservice import AciWebservice
    deployment_config = AciWebservice.deploy_configuration(
                        cpu_cores=1, memory_gb=1)
    service = Model.deploy(ws, "aci-loans", [model], 
                    inference_config, deployment_config)
    service.wait_for_deployment(show_output=True)
    ```

    这里，我们使用`AciWebservice`类来获取您想要部署的容器实例的部署配置。在前面的代码中，您指定需要 1 个 CPU 内核和 1gb RAM。然后，您将模型部署到一个名为`aci-loans`的新服务中，并等待部署完成。

    重要说明

    如果您在尝试部署容器时遇到问题，您可以查看打印输出中的错误消息或使用`service.get_logs()`方法。最有可能的是，这是您在`score.py`脚本中的代码库的问题。您可以通过安装`azureml-inference-server-http` pip 包并运行以下命令来本地测试代码:

    `5001`。调试这种情况的另一种方法是使用`LocalWebservice`，我们将在后面讨论。如果您的代码没有问题，那么您可能会遇到内存问题。这应该可以在服务日志中看到。在这种情况下，请参考下一节，了解如何对模型进行概要分析，以确定其资源需求。

6.  To test the deployed service, you can use the following code, which is similar to the one that you used in the previous section:

    ```
    import json
    input_payload = json.dumps({
        'data': [[2000, 2, 45]]
    })
    output = service.run(input_payload)
    print(output)
    ```

    请注意，您在*部署实时端点*部分的*步骤 4* 中使用的有效负载的`method`属性不会对该部署产生任何影响，并且会从有效负载中省略。如果您想支持这个属性，您必须在`score.py`文件的`run`方法中编写代码来读取该属性并调用模型的相应方法。

7.  为了节省成本，在完成测试后，使用下面的代码删除服务:

    ```
    service.delete()
    ```

8.  If you want to deploy the same service in your local computer, you can use the following code:

    ```
    from azureml.core.webservice import LocalWebservice
    deployment_config = LocalWebservice.deploy_configuration(port=1337)
    service = Model.deploy(ws, "local-loans", [model], inference_config, deployment_config)
    service.wait_for_deployment()
    ```

    不使用`AciWebservice`类，而是使用`LocalWebservice`创建一个本地服务来监听端口`1337`。如果您在本地计算机上运行笔记本，您需要访问`http://localhost:1337`并查看服务端点的健康状态。现在您已经在 AzureML 笔记本中运行了这段代码，本地计算机就是您正在处理的计算实例。查看名为`service.delete()`代码的计算实例的端口`1337`，如*步骤 7* 所示。

类似于`AciWebservice`和`LocalWebservice`，你可以使用`AksWebservice`创建一个`AksServiceDeploymentConfiguration`。在部署它时，您需要在`Model.deploy`方法中指定一个额外的参数，即`deployment_target`参数。该参数允许您指定您想要将模型部署到的`AksCompute`推理集群。

除了您之前看到的本地计算机、ACI 和 AKS 部署选项，AzureML 还提供了多种其他部署选项。例如， **Azure Functions** 允许你在一个无服务器的基础设施中运行你的模型，而 **Azure App Services** 将模型作为一个传统的 web 应用来托管，它总是准备好为传入的请求提供服务。另一方面，您可以使用**物联网边缘**，这允许您在边缘设备上部署服务，如 Raspberry Pi 或基于 GPU 的 Jetson Nano。最后，您甚至可以将模型打包到 Docker 容器映像中，该映像可以在隔离的 air gap 数据中心中运行。

在本节中，您部署了一个 ACI 实时推理端点，请求 1 个 CPU 内核和 1 GB RAM。在下一节中，您将探索如何通过分析模型的性能来优化您的资源需求。

## 分析模型的资源需求

在将产品投入生产之前，通常会进行压力测试。本质上，该测试用请求轰炸实时端点，并测量端点的响应性和性能。您可以对您的模型做一些类似的事情，以了解您需要什么类型的资源来使它们按预期执行。例如，您可能需要确保所有的推理都在 200 毫秒内完成。

在本节中，您将创建一个测试数据集，用于对实时端点进行压力测试，并观察其性能。数据集中的每一行将包含一个推理请求。

导航到您的`chapter12.ipynb`笔记本并执行以下步骤:

1.  In a new cell, add the following code:

    ```
    loans_ds = ws.datasets['loans']
    prof_df = loans_ds.drop_columns('approved_loan') \
                            .to_pandas_dataframe()
    prof_df['sample_request'] = \
        "{'data':[[" + prof_df['income'].map(str) \
      + ","+ prof_df['credit_cards'].map(str) \
      + "," + prof_df['age'].map(str) + "]]}"
    prof_df = prof_df[['sample_request']]
    prof_df.head()
    ```

    这段代码加载`loans`数据集，删除我们不需要的`approved_loan`列，并将其加载到`pandas` `DataFrame`中。接下来，您创建一个名为`sample_request`的新列，它连接这些列以产生如下所示的字符串:

    ```
    {"data": [[2000,2,45]]}
    ```

    然后，您只保留那一列，并打印前 5 行，以验证请求看起来像预期的那样。请注意，数据是否是我们用来训练模型的数据并不重要。甚至可能是随机记录。我们只关心我们将发出的请求的数量，而不关心推理结果将会是什么样子。

2.  Store the newly created dataset inside the workspace using the following code:

    ```
    from azureml.core import Dataset
    dstore = ws.get_default_datastore()
    loan_req_ds = Dataset.Tabular.register_pandas_dataframe(
        dataframe=prof_df,
        target=(dstore,"/samples/loans-requests"),
        name="loans-requests",
        description="Sample requests for the loans model")
    ```

    前面的代码将 DataFrame 注册为`loans-requests`数据集。数据存储在默认数据存储的`/samples/loans-requests`中。`loans_req_ds`变量引用了新注册的`tabular` `Dataset`。

3.  Now that you have the necessary data, you can start the model profiling process using the following code:

    ```
    profile = Model.profile(ws,
                  'chapter12-loan',
                  [model], inference_config,
                  input_dataset=loan_req_ds,
                  cpu=2, memory_in_gb=1)
    profile.wait_for_completion(True)
    print(profile.get_details())
    ```

    注意，profile 方法需要您在上一节的模型部署中使用的`model`和`inference_config`。此外，您需要指定用于执行分析的 ACI 大小。在前面的代码中，您请求 2 个 CPU 和 1gb RAM。分析可能需要很长时间，有时超过 20 分钟。分析完成后，您将会看到结果，包括作为`recommendedCpu`的 1 个 CPU 和作为`recommendedMemoryInGB`值的 0.5 GB RAM。

    重要说明

    模型概要文件的名称在工作空间中应该是唯一的。如果您试图在不更改名称的情况下重新运行第 3 步的代码，将会出现错误。

在幕后，执行了一个名为`ModelProfile` run 的实验，该实验使用该模型部署了一个 ACI 服务。一旦服务启动并运行，流程将发送您在 **loan_req_ds** 数据集中指定的 500 个请求，并记录模型的响应时间，同时监控已部署容器实例的 CPU 和内存利用率。AzureML 可以根据这些统计数据建议您应该为实时端点配置的 CPU 和内存。

在下一节中，您将使用这些值来部署 ACI 服务。接下来，您将探索如何在部署到生产环境后监控其性能，并使用**应用洞察**记录传入的数据。

## 利用应用洞察进行监控

正如您在 [*第二章*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026) ，*部署 Azure 机器学习工作区资源*中所了解到的，当您部署 AzureML 工作区时，一个名为`packtlearningm<random_number>`的 Application Insights 帐户被部署在同一个资源组中。这个 Azure 资源允许你监控应用程序的性能。特别是对于 web 应用程序，例如您正在部署的实时端点，Application Insights 允许您监控请求和响应时间、端点的失败率、代码中出现的任何潜在异常，甚至是您希望从代码库发出的日志跟踪。

在前面的*理解模型部署选项*部分中，您创建了一个包含几个`print`语句的`score.py`文件。这些消息写在端点的控制台内部，可以通过调用`service.get_logs()`方法或导航到部署的**部署日志**选项卡来找到，如图*图 12.10* 所示:

![Figure 12.10 – The model path and incoming raw_data logged in the console of the container instance
](img/B16777_12_010.jpg)

图 12.10–容器实例的控制台中记录的模型路径和传入的 raw_data

这种方法的问题是日志不能持久保存。如果您重新部署容器实例，您将丢失日志。此外，如果您部署了多个模型，您将需要一个集中的位置来共同监控所有这些模型。这是 Application Insights 为您的解决方案带来的众多优势中的两个。

回到您的`chapter12.ipynb`笔记本，重新部署 ACI 容器，并为其启用应用洞察。在新单元格内，添加以下代码:

```
from azureml.core.webservice import AciWebservice
deployment_config = AciWebservice.deploy_configuration(
   cpu_cores=1, memory_gb=0.5, enable_app_insights= True)
service = Model.deploy(ws, "aci-loans", [model], inference_config, deployment_config)
service.wait_for_deployment(show_output=True) 
```

注意，您正在使用在*分析模型资源需求*部分中推荐的`1` CPU 内核和`0.5` GB RAM。此外，请注意，您通过传递`enable_app_insights= True`参数来启用部署配置中的应用程序洞察。如果您已经部署了服务，并且希望为其启用应用程序洞察，则可以使用以下代码来更新其配置:

```
service.update(enable_app_insights=True)
```

让我们向服务发送几个请求，以便更好地了解 Application Insights 能为您做什么。在新单元格内，添加以下代码:

```
import json
input_payload = json.dumps({'data': [[2000, 2, 45], [2000, 9, 45]]})
for x in range(10):
   print(service.run(input_payload))
```

这段代码向服务发送 *10* 个相同的请求，一个接一个，生成一些应该记录在 Application Insights 中的人工流量。找到指向 Azure 门户并直接位于 Application Insights 资源内部的 URL 的最简单方法是访问端点的信息页面，如图*图 12.11* 所示:

![Figure 12.11 – The Application Insights URL that is associated with your AzureML workspace
](img/B16777_12_011.jpg)

图 12.11–与您的 AzureML 工作空间相关联的应用洞察 URL

请注意，这个**应用洞察 url** 链接并不特定于 **aci-loans** 部署。这个链接对你所有的实时端点都是一样的，允许你集中监控你所有的实时端点。点击该链接将进入应用洞察，如图*图 12.12* 所示:

![Figure 12.12 – Application Insights showing the 10 requests that you sent with the last code
](img/B16777_12_012.jpg)

图 12.12–应用洞察显示了您用最后一个代码发送的 10 个请求

在这个仪表板上，您可以单击图表并深入查看信号详情；或者您可以查看您的应用程序正在控制台内部编写的所有跟踪信息。要查看它们，导航到**监控** | **日志**，点击**跟踪**，选择想要调查的时间范围，点击**运行**按钮。您应该看到所有的 **STDOUT** 消息都出现在结果中，并且您可以深入查看细节，如图*图 12.13* 所示:

![Figure 12.13 – Reading all of the traces emitted by your model's real-time 
endpoint in Application Insights
](img/B16777_12_013.jpg)

图 12.13–在 Application Insights 中读取模型的实时端点发出的所有跟踪

在这个**日志**部分，您可以使用一种强大的类似 SQL 的语言 **Kusto** 创建复杂的查询。您甚至可以基于这些查询创建自动的警报，例如，当您在过去 30 分钟内拒绝了超过 100 笔贷款时，您会收到通知。

重要说明

Application Insights 支持一次记录高达 64 KB 的小负载。如果您计划记录更多内容，例如，超过 64 KB 数据的小批量输入，您应该考虑使用 AzureML SDK 的`DataCollector`类。此类允许您将数据直接记录到存储帐户中；但是，只有在 AKS 中部署时，它才可用。

在进入下一部分之前，不要忘记删除已部署的服务，以防止 ACI 服务的任何意外费用。您可以从 studio experience 中的**资产** | **端点**列表中删除服务，或者通过使用以下代码行来删除服务:

```
service.delete()
```

在本节中，您学习了如何在生产环境中部署实时端点后对其进行监控。在*图 12.12* 中，你可能已经注意到有几个`swagger`文件。在下一节中，您将学习如何修复那些失败的请求，并支持与想要使用您的模型结果的第三方应用程序的丰富集成。

## 与第三方应用集成

到目前为止，您已经部署了一个接受数组数组作为输入的 web 服务。这是一个隐晦的输入，您需要向任何想要使用您的实时端点的人解释。在 [*第 5 章*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072) ，*让机器做模型训练*中，你读到了`swagger`文件，它可以用来生成代码来自动消费你的端点。为了生成这样的文件，您可以使用开源的`inference-schema`包，并用元数据装饰您的代码，这些元数据将驱动`swagger.json`文件的生成。

为了使您的模型更容易被第三方应用程序使用，您应该接受以下有效负载:

```
{"data":[{"income": 2000, "credit_cards": 2, "age": 45}]}
```

在这里，您将需要创建一个新版本的评分文件。你可以直接从 GitHub 页面下载修改后的`score_v2.py`版本，而不是克隆和编辑现有的评分文件，如*技术要求*部分所述。在**笔记本**区，右键点击**脚本**文件夹中的 **score.py** 文件，选择**复制**命令，复制该文件，如图*图 12.14* 所示:

![Figure 12.14 – Creating the v2 file of the entry script
](img/B16777_12_014.jpg)

图 12.14–创建入口脚本的 v2 文件

将克隆命名为`score_v2.py`，并修改代码，使其看起来像下面的代码块:

```
import os
import joblib
from inference_schema.schema_decorators import input_schema, output_schema
import pandas as pd
from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType
import numpy as np
from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType
```

在脚本文件的开头，您正在导入额外的助手类，稍后将在代码中使用它们。注意你将不再需要`json`模块:

```
def init():
    global model
    model_path = os.path.join(os.getenv(\
"AZUREML_MODEL_DIR"), "model/model.joblib")
    model = joblib.load(model_path)
```

您不会修改`init`方法:

```
data_sample = pd.DataFrame(
    {
        "income": pd.Series([2000.0], dtype="float64"),
        "credit_cards": pd.Series([1], dtype="int"),
        "age": pd.Series([25], dtype="int")
    }
)
output_sample = np.array([0])
```

在前面的代码块中，您创建了一个`pandas` `DataFrame`，它将作为包含在传入请求的`data`属性中的对象的样本。这个`data_sample`对象有一个`income`特征，就是`float64`，还有`credit_cards`和`age`特征，都是整数。类似地，对于输出，您将`output_sample`定义为 NumPy 数组或数值。您可以在以下代码块的装饰器中使用`data_sample`和`output_sample`对象:

```
@input_schema("data", PandasParameterType(data_sample))
@output_schema(NumpyParameterType(output_sample))
def run(data):
    try:
        result = model.predict(data)
        return result.tolist()
    except Exception as e:
        error = str(e)
        return error
```

这里，您使用带有`@input_schema`装饰器的`data_sample`对象。此外，使用`PandasParameterType`，这表明名为`pandas` `DataFrame`的参数遵循由`data_sample`示例定义的模式。您使用`@output_schema`装饰器来指定您的服务返回一个 NumPy 数组作为输出，类似于`output_sample`。一旦您配置了这些模式，您会注意到您不需要在`run`方法中对传入的有效负载进行反序列化。`data`对象是已经反序列化的`pandas` `DataFrame`。

如果您想处理二进制文件而不是表格数据，例如，处理一个图像，您可以使用`@rawhttp`指令，它会将原始 HTTP 请求传递给您的`run`方法。处理普通的 HTTP 请求给了您更大的灵活性，包括设置响应头；这是在配置**跨源资源共享** ( **CORS** )等安全特性时需要的东西。您可以在本章的*延伸阅读*部分找到资源来了解更多关于这些高级场景的信息。

现在您已经准备好了`score_v2.py`脚本文件的代码，您需要发布实时端点。要为新评分函数创建实时端点，请在笔记本的单元格内添加以下代码:

```
from azureml.core.model import InferenceConfig
from azureml.core.webservice import AciWebservice
myEnv.Python.conda_dependencies.add_pip_package("inference_schema[pandas-support]>=1.1.0")
inference_config = InferenceConfig(source_directory= "./script", entry_script='score_v2.py', environment=myEnv)
deployment_config = AciWebservice.deploy_configuration( cpu_cores=1, memory_gb=0.5)
service = Model.deploy(ws, "aci-loans", [model], inference_config, deployment_config)
service.wait_for_deployment(show_output=True)
```

在前面的代码中，您将附加到`myEnv`依赖关系中的`inference_schema` pip 包，这是您在前面的*理解模型部署选项*部分中定义的。请注意，您正在安装带有额外的`pandas-support`的包，这将包括`pandas`包。您的`score_v2.py`文件所依赖的`numpy`依赖项将由 pip 自动安装，因为它是`pandas`包的依赖项。

接下来，您指定您正在使用`score_v2.py`入口脚本并部署新服务。新服务将有一个`swagger.json`文件，供 Power BI 等第三方应用程序读取并自动理解如何调用您的模型。您可以让神气活现的 URI 在端点页面上指向该文件，如图*图 12.11* 所示。在端点的页面上，您应该注意到 **Test** 选项卡已经得到了增强，可以指导您需要提供哪些字段来调用模型。在代码方面，您可以使用以下有效负载来调用模型:

```
import json
service = ws.webservices['aci-loans']
input_payload = json.dumps({"data":[
    {"income": 2000,"credit_cards": 2,"age": 45},
    {"income": 2000, "credit_cards": 9,"age": 45}
]})
print(service.run(input_payload))
input_payload = json.dumps({'data': [
    [2000, 2, 45], [2000, 9, 45]
]})
print(service.run(input_payload))
```

在继续下一节之前，请确保使用以下代码删除您刚刚部署的 ACI 服务:

```
service.delete()
```

到目前为止，您一直在部署实时推理端点，这些端点可以通过 REST API 满足特定的推理请求。在下一节中，您将学习如何使用`ParallelRunStep`部署能够并行处理大数据的批处理推理管道。

# 创建批量推理管道

在 [*第 11 章*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160) 、*使用管道*中，您学习了如何创建协调多个步骤的管道。可以使用 REST API 调用这些管道，类似于您在上一节中创建的实时端点。一个关键的区别是，在实时端点中，基础设施是持续开启的，等待请求到达，而在发布的管道中，集群只有在管道被触发后才会加速。

您可以使用这些管道在驻留在数据集中的数据之上编排批处理推断。例如，让我们假设你刚刚训练了你在本章中一直使用的`loans`模型。您希望针对所有待定贷款请求运行模型，并存储结果；这样，您就可以针对贷款可能被拒的客户开展电子邮件活动。最简单的方法是创建一个单独的`PythonScriptStep`，它将按顺序处理每个记录并将结果存储在输出文件夹中，正如您在第 11 章 、*的 [*中了解到的那样，使用管道*。您可以不这样做，而是将数据集分成多个批处理，然后在集群的每个节点内运行的多个进程中并行处理它们，如图*图 12.15* 所示:](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160)*

![Figure 12.15 – Parallel processing big datasets by splitting them into smaller 
batches and processing them in parallel
](img/B16777_12_015.jpg)

图 12.15-通过将大数据集分割成较小的批次并并行处理它们来并行处理大数据集

在本节中，您将创建一个批处理管道，它将使用您在本章中训练的`chapter12-loans`模型进行推理。您已经有了一个名为`loans`的数据集，但是它太小了，无法展示`ParallelRunStep`如何通过并行化推理来帮助您加速。通过重复复制相同的数据帧，您将生成一个大 1，024 倍的新数据集。然后，您将创建一个类似于您在第 11 章 、*使用管道*中创建的管道。这一次，您将使用`ParallelRunConfig`和`ParallelRunStep`类来并行处理数据集。配置类需要一个入口脚本，类似于您在上一节中看到的入口脚本。此外，您需要定义以下两种方法:

*   `init()`:该方法加载模型，并为即将到来的批次准备流程。此方法不会产生任何输出。
*   `run(mini_batch)`:这个方法做实际的数据处理。这个方法将被多次调用，每次传递一个不同的`mini_batch`参数。您必须返回一个数组，其中包含您在该函数中作为输出处理的每个项目的一行。例如，如果`mini_batch`参数有 100 行，而您返回一个包含 98 项的数组，那么您将会指出您未能处理其中的 2 条记录。如果正在处理`TabularDataset`，那么`mini_batch`参数可以是一个`pandas` `DataFrame`，如果正在处理`FileDataset`，那么它可以是一个包含需要处理的文件路径的数组。

导航到您的`chapter12.ipynb`笔记本并执行以下步骤:

1.  Start by getting a reference to the workspace, the dataset, and the compute cluster you are going to use for your pipeline:

    ```
    from azureml.core import Workspace
    ws = Workspace.from_config()
    loans_ds = ws.datasets['loans']
    compute_target = ws.compute_targets['cpu-sm-cluster']
    ```

    代码应该是不言自明的，就像你在第 11 章 、*使用管道*中使用的那样。

2.  Create a new, bigger dataset based on the `loans` dataset:

    ```
    from azureml.core import Dataset
    loans_df = loans_ds.drop_columns('approved_loan') \  
                       .to_pandas_dataframe()
    for x in range(10):
        loans_df = loans_df.append(loans_df)
    dstore = ws.get_default_datastore()
    pending_loans_ds =\
    Dataset.Tabular.register_pandas_dataframe(
        dataframe=loans_df,
        target=(dstore,"/samples/pending-loans"),
        name="pending-loans",
        description="Pending loans to be processed")
    ```

    在前面的代码中，您将`loans` `DataFrame`加载到内存中，而没有`approved_loan`列。这个数据集只包含 500 行。然后，将数据集追加到自身 10 次。这将创建一个包含 512，000 行的更大的数据集，您将其注册为`pending-loans`。

3.  Now, it's time to create the script that will be processing this dataset. In the `chapter12` folder, add a `pipeline_step` folder and then add a `tabular_batch.py` file with the following contents:

    ```
    from azureml.core import Model
    import joblib
    def init():
        global model
        model_path = Model.get_model_path("chapter12-loans")
        model = joblib.load(model_path)
    def run(mini_batch):
        print(mini_batch.info())
        mini_batch["approved"] = model.predict(mini_batch)
        return mini_batch.values.tolist()
    ```

    这个脚本有两个方法，如前所述。在`init`方法中，您使用`Model`类的`get_model_path`方法来获取您目前一直使用的模型的位置。从脚本的角度来看，模型将驻留在运行脚本的同一台计算机上的一个文件夹中。然后，使用`joblib`将模型加载到名为`model`的`global`变量中。在`run`方法中，您打印传入数据帧的大小，然后创建一个名为 *approved* 的新列，在其中存储所有的模型推断。您为处理的每一行返回一个包含四元素数组的列表，类似于以下记录:

    ```
    [7298, 2, 35, 1]
    [4698, 7, 70, 0]
    ```

    如果您要处理`FileDataset`而不是本节中正在处理的`TabularDataset`，对应的`file_batch.py`文件将如下所示:

    ```
    def init():
        print('Load model here')
    def run(mini_batch):
        output = []
        for file_path in mini_batch:
            output.append([file_path, 0])
        return output
    ```

    像往常一样，在`init`方法中加载你的模型，例如，一个将实现图像分类的神经网络。在`run`方法中，`mini_batch`参数是一个数组，包含您需要处理的文件的文件路径。您可以遍历这些文件，并使用您的模型进行推理。作为输出，您返回模型的文件名和结果，如下例所示:

    ```
    ['/path/sample_cat.jpg', 0]
    ['/path/sample_dog.jpg', 1]
    ```

    在*第 5 步*中，您将会看到这些结果将会聚集在`ParallelRunConfig`中定义的一个文件中。

4.  You will need to create an environment to execute your pipeline step. Add the following code inside a cell:

    ```
    from azureml.core import Environment
    from azureml.core.conda_dependencies import CondaDependencies 
    import sklearn
    pEnv= Environment(name="sklearn-parallel")
    pEnv.Python.conda_dependencies = CondaDependencies()
    pEnv.Python.conda_dependencies.add_conda_package(f"scikit-learn=={sklearn.__version__}")
    pEnv.Python.conda_dependencies.add_pip_package("azureml-core")
    pEnv.Python.conda_dependencies.add_pip_package("azureml-dataset-runtime[pandas,fuse]")
    ```

    您需要安装`scikit-learn` conda 包，就像您到目前为止一直在做的那样。为了让`ParallelRunConfig`工作，你需要包含`azureml-core`和`azureml-dataset-runtime[pandas,fuse]` `pip`包。

5.  Next, create the `ParallelRunConfig` class that configures how the run will split the workload and what script to use for data processing. Add the following code inside a new notebook cell:

    ```
    from azureml.pipeline.steps import ParallelRunConfig
    parallel_run_config = ParallelRunConfig(
        source_directory='pipeline_step',
        entry_script='tabular_batch.py',
        mini_batch_size='100Kb',
        error_threshold=-1,
        output_action='append_row',
        append_row_file_name="loans_outputs.txt",
        environment=pEnv,
        compute_target=compute_target, 
        node_count=1,
        process_count_per_node=2
    )
    ```

    在这里，您将运行位于`pipeline_step`文件夹中的`tabular_batch.py`脚本。您将把数据集分成更小的批次，大约 100 KB。如果您正在处理一个`FileDataset`，您将需要指定放入每个批处理的文件数量。这里，`error_threshold`指定在处理数据时应该忽略的记录或文件故障的数量。`-1`意味着你可以接受任何数量的处理错误。`output_action`参数接受`append_row`值或`summary_only`值。使用`append_row`值，您可以请求将来自`run`方法调用的所有输出附加到一个名为`parallel_run_step.txt`的输出文件中，除非您通过`append_row_file_name`参数覆盖它，如前面的示例所示。该文件中记录的顺序没有保证，因为记录是并行处理的。通常，您会返回客户 ID，或者贷款申请 ID，以及模型的推断。使用该 ID，您可以将原始记录与模型的预测联系起来。在当前的例子中，我们没有任何 ID；因此，我们返回整行，就像我们在*步骤 3* 的`tabular_batch.py`脚本中所做的那样。

    接下来，您指定将在其中执行这个管道步骤的环境和集群。最后，您指定这个管道步骤将在单个节点中运行，并且它将为每个参与节点启动两个流程。如果您使用两个节点，您将有四个进程并行运行。在当前示例中，两个并行进程足以在几分钟内完成处理。

    如果您有一个繁重的处理脚本，需要超过 60 秒来处理您指定的`mini_batch_size`参数，您可以通过设置`run_invocation_timeout`参数来增加超时值。

6.  As a next step, you will define the output location of `append_row_file_name` that you specified earlier:

    ```
    from azureml.data import OutputFileDatasetConfig
    datastore = ws.get_default_datastore()
    step_output = OutputFileDatasetConfig(
        name= "results_store",
        destination=(datastore, '/inferences/loans/'))
    ```

    您将把这个聚合文件存储在默认的数据存储中，在`/inferences/loans/`文件夹下。

7.  Now it's time to create the first and only step of the pipeline, that is, `ParallelRunStep`:

    ```
    from azureml.pipeline.steps import ParallelRunStep
    parallel_step = ParallelRunStep(
        name='chapter12-parallel-loans',
        inputs=[pending_loans_ds.as_named_input('loans')],
        output=step_output,
        parallel_run_config=parallel_run_config,
        allow_reuse=False
    )
    ```

    将该步骤命名为`chapter12-parallel-loans`，并传递您之前在*步骤 2* 中注册的`pending_loans_ds`数据集。输出存储在您在*步骤 6* 中创建的`OutputFileDatasetConfig`中。指定该步骤不能重复使用(`allow_reuse`)；这允许您多次触发管道，以便总是获得数据集中的最新数据以及最新注册的模型。

8.  使用下面的代码创建并执行一个管道:

    ```
    from azureml.core import Experiment
    from azureml.pipeline.core import Pipeline
    pipeline = Pipeline(workspace=ws, steps=[parallel_step])
    pipeline_run = Experiment(ws, 'chapter12-parallel-run').submit(pipeline)
    ```

9.  You can watch the execution logs by using the `RunDetails` widget with the following code:

    ```
    from azureml.widgets import RunDetails
    RunDetails(pipeline_run).show()
    ```

    或者，您可以使用以下代码等待执行完成:

    ```
    pipeline_run.wait_for_completion(show_output=True)
    ```

10.  从那时起，您可以发布甚至调度管道，如第十一章 、*使用管道*中所讨论的。

你可以在 AzureML studio 中访问管道，观察输出和它产生的日志，如图*图 12.16* 所示。请注意，您将发现一个节点和两个流程。每个流程都有多个`run`方法调用。每次调用`run`方法时，都会传入一个需要 117.3 KB 内存的数据帧，这接近您在前面的*步骤 5* 中请求的 100 KB:

![Figure 12.16 – The logs from the parallel execution showing the information of the mini_batch DataFrame
](img/B16777_12_016.jpg)

图 12.16–显示 mini_batch 数据帧信息的并行执行日志

在本节中，您学习了如何创建一个批处理管道，它可以并行处理大量数据。这就是您需要了解的考试操作选项，包括实时模式和批处理模式。

# 总结

在本章中，您探索了使用本书中培训的机器学习模型的各种方法。您可以进行实时推断，或者以经济高效的方式批量处理大量记录。您首先注册了用于推理的模型。从那里，您可以在 ACI 中部署一个实时端点用于测试，或者在 AKS 中部署一个实时端点用于需要高可用性和自动扩展的生产工作负载。您探索了如何分析您的模型，以确定托管实时端点的推荐容器大小。在此之后，您发现了应用洞察，它允许您监控生产端点并识别潜在的生产问题。通过 Application Insights，您注意到您生成的实时端点没有公开第三方应用程序(如 Power BI)自动使用您的端点所需的`swagger.json`文件。您修改了评分函数，以包含关于模型输入和输出的元数据，从而完成了本章的实时推理部分。

然后，您转到了批处理推理方面，在这里，您创建了一个可以在几分钟内并行处理 50 万条记录的管道。将这种并行化与低优先级计算相结合，您可以在推理更大的数据量时节省大量成本。

恭喜你！您已经完成了探索 AzureML 工作空间基本功能的旅程。现在，您可以在工作空间中进行机器学习实验，并且可以使用适合您试图解决的业务问题的选项来操作生成的模型。有了这些知识，你应该能够顺利通过 *DP-100* 考试、*在 Azure* 上设计和实现数据科学解决方案。

# 问题

在每一章中，你会发现一些问题来验证你对所讨论主题的理解:

1.  You want to deploy a real-time endpoint that will handle transactions from a live betting website. The traffic from this website will have spikes during games and will be very low during the night. Which of the following compute targets should you use?

    a.anticlonusindex 抗痉挛的指数

    b.计算机实例

    c.计算集群

    d.问

2.  You want to monitor a real-time endpoint deployed in AKS and determine the average response time of the service. Which monitoring solution should you use?

    a.anticlonusindex 抗痉挛的指数

    b.Azure 容器注册表

    c.应用洞察

3.  You have a computer vision model, and you want to process 100 images in parallel. You author a pipeline with a parallel step. You want to process 10 images at a time. Which of the following `ParallelRunConfig` parameters should you set?

    a.`mini_batch_size=10`

    b.`error_threshold=10`

    c.`node_count=10`

    d.`process_count_per_node=10`

# 延伸阅读

本节提供了一个有用的 web 资源列表，帮助您扩展 AzureML SDK 和本章中使用的各种代码片段的知识:

*   来自 scikit-learn 的模型持久性指南:[https://scikit-learn . org/stable/modules/model _ persistence . html](https://scikit-learn.org/stable/modules/model_persistence.html)
*   使用 Postman 测试 REST API:[https://www.postman.com/product/api-client/](https://www.postman.com/product/api-client/)
*   发出 web 请求的 **curl** 命令行工具:【https://curl.se/】T2
*   使用 OpenCensus 监控 Python 应用:[https://docs . Microsoft . com/azure/azure-monitor/app/open census-Python](https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python)
*   如何使用推理服务器在本地测试您的条目脚本:[https://docs . Microsoft . com/azure/machine-learning/how-to-inference-server-http](https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http)
*   将模型打包到自治 Docker 容器中:[https://docs . Microsoft . com/azure/machine-learning/how-to-deploy-package-models](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models)
*   ONNX 机器学习格式，用于存储可在多个平台加载的模型:[https://docs . Microsoft . com/azure/machine-learning/concept-ONNX](https://docs.microsoft.com/azure/machine-learning/concept-onnx)
*   应用洞察简介:[https://docs . Microsoft . com/azure/azure-monitor/app/app-Insights-overview](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview)
*   Kusto 查询语言介绍:[https://docs . Microsoft . com/azure/data-explorer/Kusto/concepts/](https://docs.microsoft.com/azure/data-explorer/kusto/concepts/)
*   高级实时端点入口脚本创作指南:[https://docs . Microsoft . com/azure/machine-learning/how-to-deploy-advanced-entry-script](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script)
*   在 Power BI 中集成 AzureML 模型:[https://docs . Microsoft . com/Power-BI/transform-model/data flows/data flows-machine-learning-integration # azure-machine-learning-integration-in-Power-BI](https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi)
*   利用`ParallelRunStep`类训练上百个模型:[https://github . com/Microsoft/solution-accelerator-many-models](https://github.com/microsoft/solution-accelerator-many-models)**