<html><head/><body>
<html>
  <head>
    <title>Chapter 5. The Bare Bones Boosting Algorithms</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0">第五章。最基本的提升算法</h1></div></div></div><p class="calibre7">我们所说的基本增强算法是什么意思？boosting 算法(及其变体)可以说是机器学习工具箱中最重要的算法之一。任何数据分析师都需要了解这种算法，最终对更高准确性的追求必然会推动对 boosting 技术的需求。在<a class="calibre1" href="http://www.kaggle.org">www.kaggle.org</a>论坛上有报道称，针对复杂海量数据的提升算法需要运行数周，大多数获奖的解决方案都基于此。此外，算法运行在现代图形设备机器上。</p><p class="calibre7">考虑到它的重要性，我们将在这里详细研究 boosting 算法。<em class="calibre9">裸机</em>当然不是 boosting 算法的变种。由于 boosting 算法是非常重要和关键的算法之一，我们将首先陈述该算法并以基本的方式实现它，这将显示该算法的每一步。</p><p class="calibre7">我们将从自适应增强算法开始——众所周知并缩写为<strong class="calibre8"> AdaBoost </strong>算法<a id="id202" class="calibre1"/>——并使用非常简单和原始的代码，我们将针对一个分类问题来说明它。插图是在一个<code class="literal">toy</code>数据集上进行的，因此读者可以清楚地跟随这些步骤。</p><p class="calibre7">在下一节中，我们将把分类提升算法扩展到回归问题。对于这个问题，boosting <a id="id203" class="calibre1"/>变种就是著名的<strong class="calibre8">梯度 boosting 算法</strong>。一个有趣的非线性回归问题将通过一系列具有单一分裂的<a id="id204" class="calibre1"/>基本决策树来临时解决，也称为<strong class="calibre8">树桩</strong>。将针对平方误差损失函数的选择来说明梯度提升算法。将阐明 boosting 方法的变量重要性计算。包的细节将在本章的倒数第二节讨论。最后一节将对垃圾邮件数据集的 bagging、random forests 和 boosting 方法进行比较。本章包括以下主题:</p><div><ul class="itemizedlist"><li class="listitem">通用 boosting 算法</li><li class="listitem">自适应增压</li><li class="listitem">梯度增强<div> <ul class="itemizedlist1"> <li class="listitem">基于树桩的梯度增强</li> <li class="listitem">带平方误差损失的梯度增强</li> </ul> </div></li><li class="listitem">助推技术中的可变重要性</li><li class="listitem">使用 gbm 包</li><li class="listitem">bagging、随机森林和 boosting 算法的比较</li></ul></div></div></body></html>


<html>
  <head>
    <title>Chapter 5. The Bare Bones Boosting Algorithms</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec40" class="calibre1"/>技术要求</h1></div></div></div><p class="calibre7">我们将在本章中使用以下库:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">rpart</code></li><li class="listitem"><code class="literal">gbm</code></li></ul></div></div></div></body></html>


<html>
  <head>
    <title>The general boosting algorithm</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec41" class="calibre1"/>通用 boosting 算法</h1></div></div></div><p class="calibre7">前面章节中基于树的<a id="id205" class="calibre1"/>系综，<em class="calibre9">装袋</em>和<em class="calibre9">随机森林</em>，涵盖了决策树的重要扩展。然而，虽然 bagging 通过平均多个决策树提供了更大的稳定性，但偏见仍然存在。这一限制促使 Breiman 在每个分裂点对协变量进行采样，以生成“独立”树的集合，并为随机森林奠定基础。随机森林中的树可以并行开发，就像装袋一样。在多棵树上求平均值的想法是为了确保偏差和方差之间的平衡。Boosting 是决策树的第三个最重要的扩展，可能也是最有效的扩展。它也是基于集合同质的基础学习者(在这种情况下是树)，就像装袋和随机森林一样。然而升压算法的设计是完全不同的。这是一种<em class="calibre9">顺序</em>集成方法，因为前一个学习者的剩余/错误分类点是在算法的下一次运行中临时产生的。</p><p class="calibre7">一般的增强技术由一系列算法组成，这些算法将弱学习器转换成最终的强学习器。在分类问题的背景下，弱学习者是一种比随机猜测更好的技术。该方法将弱学习算法转换为更好的算法，这就是它被命名为<em class="calibre9"> boosting </em>的原因。提升技术旨在提供更接近完美分类器的强大学习器。</p><div><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="bottom" class="calibre21">
<p class="calibre22">分类者</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">子空间<img src="img/00199.jpeg" alt="The general boosting algorithm" class="calibre23"/></p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">子空间<img src="img/00200.jpeg" alt="The general boosting algorithm" class="calibre23"/></p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">子空间<img src="img/00201.jpeg" alt="The general boosting algorithm" class="calibre23"/></p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">子空间<img src="img/00202.jpeg" alt="The general boosting algorithm" class="calibre23"/></p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">准确(性)</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25"><img src="img/00203.jpeg" alt="The general boosting algorithm" class="calibre23"/></td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">Q</p>
</td><td class="calibre25">
<p class="calibre22">Zero point seven five</p>
</td></tr><tr class="calibre20"><td class="calibre25"><img src="img/00204.jpeg" alt="The general boosting algorithm" class="calibre23"/></td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">Q</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">Zero point seven five</p>
</td></tr><tr class="calibre20"><td class="calibre25"><img src="img/00205.jpeg" alt="The general boosting algorithm" class="calibre23"/></td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">Q</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">Zero point seven five</p>
</td></tr><tr class="calibre20"><td class="calibre25"><img src="img/00206.jpeg" alt="The general boosting algorithm" class="calibre23"/></td><td class="calibre25">
<p class="calibre22">Q</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">稀有</p>
</td><td class="calibre25">
<p class="calibre22">Zero point seven five</p>
</td></tr></tbody></table></div><p class="calibre7">表 1 简单的分类器场景</p><p class="calibre7">通过一个简单的例子，就可以理解助推的更广泛的动机。假设从样本空间<img src="img/00207.jpeg" alt="The general boosting algorithm" class="calibre15"/>中抽取大小为<em class="calibre9"> n </em>的随机样本作为 IID。假设随机样本的分布为 d。假设我们在<img src="img/00208.jpeg" alt="The general boosting algorithm" class="calibre15"/>中有<em class="calibre9"> T=4 </em>个分类器，这些分类器用于真值函数<em class="calibre9"> f </em>。</p><p class="calibre7">考虑一个假设场景，其中样本空间<img src="img/00209.jpeg" alt="The general boosting algorithm" class="calibre15"/>由<img src="img/00210.jpeg" alt="The general boosting algorithm" class="calibre15"/>中的四个部分组成，四个分类器的性能如前表所示。增强方法开发背后的思想是以连续的方式临时制作分类器。也就是说，组合分类器是一个接一个地进行的，而不是同时进行的。现在，<img src="img/00211.jpeg" alt="The general boosting algorithm" class="calibre15"/>的误差将被校正为新的分布 D ’,对于区域<img src="img/00212.jpeg" alt="The general boosting algorithm" class="calibre15"/>，分类器的误差被赋予更大的权重。分类器<img src="img/00213.jpeg" alt="The general boosting algorithm" class="calibre15"/>将使用分布 D’并且其在区域<img src="img/00214.jpeg" alt="The general boosting algorithm" class="calibre15"/>中的误差区域实例将被赋予更大的权重，导致分布 D”。提升方法将继续剩余分类器的过程，并给出总体组合器/集成。一个伪 boosting 算法(见周(2012)的第二章)总结如下:</p><div><ol class="orderedlist"><li class="listitem" value="1">步骤 0:初始样本分布为 D，设 D1 = D</li><li class="listitem" value="2">步骤<img src="img/00215.jpeg" alt="The general boosting algorithm" class="calibre15"/>:<div><ul class="itemizedlist1"><li class="listitem"><img src="img/00216.jpeg" alt="The general boosting algorithm" class="calibre15"/>Dt</li><li class="listitem"><img src="img/00217.jpeg" alt="The general boosting algorithm" class="calibre15"/></li><li class="listitem"><em class="calibre9">Dt+1 =改善分配(Dt，</em><img src="img/00218.jpeg" alt="The general boosting algorithm" class="calibre15"/><em class="calibre9">)</em></li></ul></div></li><li class="listitem" value="3">最后一步:<img src="img/00219.jpeg" alt="The general boosting algorithm" class="calibre15"/></li></ol><div/></div><p class="calibre7"><em class="calibre9">中算法的两个步骤改善分配</em>和<em class="calibre9">组合输出</em>显然需要可实施的行动。在下一节中，我们将开发带有清晰数字说明的自适应增强方法。</p></div></body></html>


<html>
  <head>
    <title>The general boosting algorithm</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl1sec42" class="calibre1"/>自适应增压</h2></div></div></div><p class="calibre7">Schapire 和 Freund 发明了自适应增压方法。<strong class="calibre8"> Adaboost </strong>是这种技术的流行缩写。</p><p class="calibre7">通用自适应升压算法如下:</p><div><ul class="itemizedlist"><li class="listitem">Initialize the observation weights uniformly:<div><img src="img/00220.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre26"> </p></li><li class="listitem">对于<em class="calibre9"> m </em>，分类器<em class="calibre9"> hm </em>，从<em class="calibre9"> 1 </em>到<em class="calibre9"> m </em>的数据遍数，执行以下任务:<div> <ul class="itemizedlist1"> <li class="listitem">拟合分类器<em class="calibre9"> hm </em>对训练<a id="id208" class="calibre1"/>数据使用权重<img src="img/00221.jpeg" alt="Adaptive boosting" class="calibre15"/> </li> <li class="listitem">计算每个分类器的误差如下:<div><img src="img/00222.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre11"/></li><li class="listitem">计算<em class="calibre9"/></li></ul></div></li><li class="listitem">Output:<div><img src="img/00226.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre26"> </p></li></ul></div><p class="calibre7">简而言之，算法展开如下:</p><div><ol class="orderedlist"><li class="listitem" value="1">最初，我们开始对所有观察值使用统一的权重<img src="img/00227.jpeg" alt="Adaptive boosting" class="calibre15"/>。</li><li class="listitem" value="2">在下一步中，我们计算<a id="id209" class="calibre1"/>每个考虑中的分类器的加权误差<img src="img/00228.jpeg" alt="Adaptive boosting" class="calibre15"/>。</li><li class="listitem" value="3">需要选择一个分类器(通常是树桩，或者具有单个分裂的决策树),实践是选择具有最大准确度的分类器。</li><li class="listitem" value="4">在<em class="calibre9">改善分配和组合输出</em>的情况下，选择任何准确度的并列分类器。</li><li class="listitem" value="5">接下来，错误分类的观察值被赋予更多的权重，而正确分类的值被向下加权。这里需要记录重要的一点:</li></ol><div/></div><div><h3 class="title2"><a id="note03" class="calibre1"/>注意</h3><p class="calibre7">在权重更新步骤中，被正确分类为观察值的权重之和将等于错误分类的观察值的权重之和。</p></div><p class="calibre7">从计算分类器的误差到权重更新步骤的步骤重复 M 次，并且获得每个分类器的投票能力。对于任何给定的观察值，我们随后通过使用 M 个分类器上的预测值并使用算法中指定的符号函数来进行预测，其中 M 个分类器通过它们各自的投票权进行加权。</p><p class="calibre7">尽管算法可能很简单，但通过一个玩具数据集进行自适应增强<a id="id210" class="calibre1"/>方法的工作是一个有用的练习。数据和计算方法取自杰西卡·诺斯的视频，可在<a class="calibre1" href="https://www.youtube.com/watch?v=gmok1h8wG-Q">https://www.youtube.com/watch?v=gmok1h8wG-Q</a>获得。自适应增强算法的说明现在开始。</p><p class="calibre7">考虑一个有五个三连点的玩具数据集:两个解释变量和一个二进制输出值。变量和数据可以用<img src="img/00229.jpeg" alt="Adaptive boosting" class="calibre15"/>来概括，这里我们有数据点<img src="img/00230.jpeg" alt="Adaptive boosting" class="calibre15"/>、<img src="img/00231.jpeg" alt="Adaptive boosting" class="calibre15"/>、<img src="img/00232.jpeg" alt="Adaptive boosting" class="calibre15"/>、<img src="img/00233.jpeg" alt="Adaptive boosting" class="calibre15"/>、<img src="img/00234.jpeg" alt="Adaptive boosting" class="calibre15"/>。数据将首先输入到 R 中，然后作为预备步骤进行可视化:</p><div><pre class="programlisting">&gt; # ADAPTIVE BOOSTING with a Toy Dataset
&gt; # https://www.youtube.com/watch?v=gmok1h8wG-Q 
&gt; # Jessica Noss
&gt; # The Toy Data
&gt; x1 &lt;- c(1,5,3,1,5)
&gt; x2 &lt;- c(5,5,3,1,1)
&gt; y &lt;- c(1,1,-1,1,1)
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="The TOY Data Depiction")
&gt; text(x1,x2,labels=names(y),pos=1)</pre></div><div><img src="img/00235.jpeg" alt="Adaptive boosting" class="calibre10"/><div><p class="calibre14">图 1:玩具数据集的简单描述</p></div></div><p class="calibre11"> </p><p class="calibre7">树桩是讨论中提到的决策树的一个特例。在这里，我们将使用树桩作为基础学习者。简单地看一下前面的图表有助于我们轻松地找到比随机猜测更准确的树桩。</p><p class="calibre7">例如，我们<a id="id211" class="calibre1"/>可以在<img src="img/00236.jpeg" alt="Adaptive boosting" class="calibre15"/>放置一个树桩，将左侧的所有观察值标记为正，将右侧的所有观察值标记为负。在下面的程序中，绿色阴影区域中的点是树桩预测的正值，而红色阴影区域中的点是负值。同样，我们可以在<img src="img/00237.jpeg" alt="Adaptive boosting" class="calibre15"/>和<img src="img/00238.jpeg" alt="Adaptive boosting" class="calibre15"/>使用额外的树桩。多亏了<code class="literal">symmetry()</code>，预测也可以换成同样的树桩。因此，之前我们将绿色阴影区域放在<img src="img/00239.jpeg" alt="Adaptive boosting" class="calibre15"/>的左侧，并预测值为正值，通过颠倒顺序，树桩<img src="img/00240.jpeg" alt="Adaptive boosting" class="calibre15"/>右侧的区域将被标记为正值。对底片也进行类似的分类。在树桩<img src="img/00241.jpeg" alt="Adaptive boosting" class="calibre15"/>和<img src="img/00242.jpeg" alt="Adaptive boosting" class="calibre15"/>处重复该任务。使用<code class="literal">par</code>、<code class="literal">plot</code>、<code class="literal">text</code>和<code class="literal">rect</code>图形功能，我们在下面呈现这些基础学习者的视觉描述:</p><div><pre class="programlisting">&gt; # Visualizing the stump models
&gt; windows(height=200,width=300)
&gt; par(mfrow=c(2,3))
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&lt;2")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; plim &lt;- par("usr")
&gt; rect(xleft=2,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 2,ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&lt;4")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=4,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 4,ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&lt;6")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=6,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 6,ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&gt;2")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=2,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 2,ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&gt;4")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=4,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 4,ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&gt;6")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=6,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 6,ytop = plim[4],
+      border = "red",col="red",density=20 )</pre></div><p class="calibre7">前面 R <a id="id212" class="calibre1"/>程序的结果如下图所示:</p><div><img src="img/00243.jpeg" alt="Adaptive boosting" class="calibre10"/><div><p class="calibre14">图 2:基于 X1 的残肢分类器</p></div></div><p class="calibre11"> </p><p class="calibre7">注意，在点 2、4 和 6 处，可以为变量<img src="img/00244.jpeg" alt="Adaptive boosting" class="calibre15"/>获得类似的<a id="id213" class="calibre1"/>分类。尽管没有必要给出基于<img src="img/00245.jpeg" alt="Adaptive boosting" class="calibre15"/>的完整 R 程序，我们还是简单的输出了下图。该程序可以在代码包中获得。基于<img src="img/00246.jpeg" alt="Adaptive boosting" class="calibre15"/>的树桩将在接下来的讨论中被忽略:</p><div><img src="img/00247.jpeg" alt="Adaptive boosting" class="calibre10"/><div><p class="calibre14">图 3:基于 X2 的残肢分类器</p></div></div><p class="calibre11"> </p><p class="calibre7">基于<img src="img/00248.jpeg" alt="Adaptive boosting" class="calibre15"/>的残肢选择<a id="id214" class="calibre1"/>导致一些错误分类，我们可以看到 P1、P4 和 P3 的观测值被正确分类，而 P2 和 P5 被错误分类。基于这个树桩的预测可以放在(1，-1，-1，1，-1)。基于<img src="img/00249.jpeg" alt="Adaptive boosting" class="calibre15"/>的残肢对点 P1 和 P4 分类正确，而 P2、P3、P5 分类错误，这里向量形式的预测是(1，-1，1，1，-1)。这里考虑的六个模型将在 R 程序中用 M1、M2、…、M6 来表示，根据前面指定的算法，我们有<img src="img/00250.jpeg" alt="Adaptive boosting" class="calibre15"/>。类似地，我们对其他四个树桩进行预测，并将它们输入到 R 中，如下所示:</p><div><pre class="programlisting">&gt; # The Simple Stump Models
&gt; M1 &lt;- c(1,-1,-1,1,-1)   # M1 = X1&lt;2 predicts 1, else -1
&gt; M2 &lt;- c(1,-1,1,1,-1)    # M2 = X1&lt;4 predicts 1, else -1
&gt; M3 &lt;- c(1,1,1,1,1)      # M3 = X1&lt;6 predicts 1, else -1
&gt; M4 &lt;- c(-1,1,1,-1,1)    # M4 = X1&gt;2 predicts 1, else -1;M4=-1*M1
&gt; M5 &lt;- c(-1,1,-1,-1,1)   # M5 = X1&gt;4 predicts 1, else -1;M5=-1*M2
&gt; M6 &lt;- c(-1,-1,-1,-1,-1) # M6 = X1&gt;6 predicts 1, else -1;M6=-1*M3</pre></div><p class="calibre7">利用六个模型<code class="literal">M1-M6</code>给出的预测，我们可以将它们与<code class="literal">y</code>中的真实标签进行比较，并查看这些模型中哪些<a id="id215" class="calibre1"/>观测值被错误分类:</p><div><pre class="programlisting">&gt; # Stem Model Errors
&gt; Err_M1 &lt;- M1!=y
&gt; Err_M2 &lt;- M2!=y
&gt; Err_M3 &lt;- M3!=y
&gt; Err_M4 &lt;- M4!=y
&gt; Err_M5 &lt;- M5!=y
&gt; Err_M6 &lt;- M6!=y
&gt; # Their Misclassifications
&gt; rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)
          P1    P2    P3    P4    P5
Err_M1 FALSE  TRUE FALSE FALSE  TRUE
Err_M2 FALSE  TRUE  TRUE FALSE  TRUE
Err_M3 FALSE FALSE  TRUE FALSE FALSE
Err_M4  TRUE FALSE  TRUE  TRUE FALSE
Err_M5  TRUE FALSE FALSE  TRUE FALSE
Err_M6  TRUE  TRUE FALSE  TRUE  TRUE</pre></div><p class="calibre7">因此，<code class="literal">TRUE</code>的值意味着名为 points 的列在名为 model 的行中被错误分类。权重<img src="img/00251.jpeg" alt="Adaptive boosting" class="calibre15"/>被初始化，并且加权误差<img src="img/00252.jpeg" alt="Adaptive boosting" class="calibre15"/>被计算用于以下 R 块中的每个模型:</p><div><pre class="programlisting">&gt; # ROUND 1
&gt; # Weighted Error Computation
&gt; weights_R1 &lt;- rep(1/length(y),length(y)) #Initializaing the weights
&gt; Err_R1 &lt;- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%
+   weights_R1
&gt; Err_R1 # Error rate
       [,1]
Err_M1  0.4
Err_M2  0.6
Err_M3  0.2/
Err_M4  0.6
Err_M5  0.4
Err_M6  0.8</pre></div><p class="calibre7">由于对应于模型 3 或<img src="img/00253.jpeg" alt="Adaptive boosting" class="calibre15"/>的误差最小，我们首先选择它，并计算可分配给它的投票权<img src="img/00254.jpeg" alt="Adaptive boosting" class="calibre15"/>如下:</p><div><pre class="programlisting">&gt; # The best classifier error rate
&gt; err_rate_r1 &lt;- min(Err_R1)
&gt; alpha_3 &lt;- 0.5*log((1-err_rate_r1)/err_rate_r1)
&gt; alpha_3
[1] 0.6931472</pre></div><p class="calibre7">因此，升压算法步骤表明<img src="img/00255.jpeg" alt="Adaptive boosting" class="calibre15"/>给我们所需的预测:</p><div><pre class="programlisting">&gt; alpha_3*M3
[1] 0.6931472 0.6931472 0.6931472 0.6931472 0.6931472
&gt; sign(alpha_3*M3)
[1] 1 1 1 1 1</pre></div><p class="calibre7">中央的<a id="id216" class="calibre1"/>观察值<code class="literal">P3</code>仍然分类错误，因此我们继续下一步。</p><p class="calibre7">现在我们需要更新权重<img src="img/00256.jpeg" alt="Adaptive boosting" class="calibre15"/>，对于分类问题，使用以下公式给出简化形式的规则:</p><div><img src="img/00257.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">因此，我们需要一个函数，它将以前运行的权重、错误率和模型的错误分类作为输入，然后将它们作为合并了前面公式的更新权重返回。我们将这样的函数定义如下:</p><div><pre class="programlisting"> 
&gt; # Weights Update Formula and Function
&gt; Weights_update &lt;- function(weights,error,error_rate){
+   weights_new &lt;- NULL
+   for(i in 1:length(weights)){
+     if(error[i]==FALSE) weights_new[i] &lt;- 0.5*weights[i]/(1-error_rate)
+     if(error[i]==TRUE) weights_new[i] &lt;- 0.5*weights[i]/error_rate
+   }
+   return(weights_new)
+ }</pre></div><p class="calibre7">现在，我们将更新权重并计算六个模型的误差:</p><div><pre class="programlisting">&gt; # ROUND 2
&gt; # Update the weights and redo the analyses
&gt; weights_R2 &lt;- Weights_update(weights=weights_R1,error=Err_M3,
+                              error_rate=err_rate_r1)
&gt; Err_R2 &lt;- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%
+   weights_R2
&gt; Err_R2 # Error rates
       [,1]
Err_M1 0.25
Err_M2 0.75
Err_M3 0.50
Err_M4 0.75
Err_M5 0.25
Err_M6 0.50</pre></div><p class="calibre7">这里，模型<code class="literal">M1</code>和<code class="literal">M5</code>在新的权重下具有相等的错误率<a id="id217" class="calibre1"/>，我们简单地选择模型 1，计算其投票能力，并基于更新后的模型进行预测:</p><div><pre class="programlisting">&gt; err_rate_r2 &lt;- min(Err_R2)
&gt; alpha_1 &lt;- 0.5*log((1-err_rate_r2)/err_rate_r2)
&gt; alpha_1
[1] 0.5493061
&gt; alpha_3*M3+alpha_1*M1
[1] 1.242453 0.143841 0.143841 1.242453 0.143841
&gt; sign(alpha_3*M3+alpha_1*M1)
[1] 1 1 1 1 1</pre></div><p class="calibre7">由于点<code class="literal">P3</code>仍被错误分类，我们继续迭代并再次应用循环:</p><div><pre class="programlisting">&gt; # ROUND 3
&gt; # Update the weights and redo the analyses
&gt; weights_R3 &lt;- Weights_update(weights=weights_R2,error=Err_M1,
+                              error_rate=err_rate_r2)
&gt; Err_R3 &lt;- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%
+   weights_R3
&gt; Err_R3 # Error rates
            [,1]
Err_M1 0.5000000
Err_M2 0.8333333
Err_M3 0.3333333
Err_M4 0.5000000
Err_M5 0.1666667
Err_M6 0.6666667
&gt; err_rate_r3 &lt;- min(Err_R3)
&gt; alpha_5 &lt;- 0.5*log((1-err_rate_r3)/err_rate_r3)
&gt; alpha_5
[1] 0.804719
&gt; alpha_3*M3+alpha_1*M1+alpha_5*M5
[1]  0.4377344  0.9485600 -0.6608779  0.4377344  0.9485600
&gt; sign(alpha_3*M3+alpha_1*M1+alpha_5*M5)
[1]  1  1 -1  1  1</pre></div><p class="calibre7">现在分类是完美的，经过三次迭代，我们没有任何误分类或错误。本节编程的目的是以一种基本的方式演示自适应升压算法中的步骤。在下一部分，我们将看看<a id="id218" class="calibre1"/><em class="calibre9">梯度推进</em>技术。</p></div></div></body></html>


<html>
  <head>
    <title>The general boosting algorithm</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl1sec43" class="calibre1"/>梯度推进</h2></div></div></div><p class="calibre7">自适应增强方法<a id="id219" class="calibre1"/>不能应用于回归问题，因为它是为解决分类问题而构建的。梯度推进方法可用于具有合适损失函数的分类和回归问题。事实上，梯度推进方法的使用超出了这两个标准问题。该技术源于 Breiman 的一些观察，并由 Freidman (2000)发展成回归问题。我们将在下一节中介绍基本的代码解释，甚至不介绍算法。设置清楚后，我们将在下面的小节中正式陈述平方误差损失函数的 boosting 算法，并创建一个实现该算法的新函数。</p><p class="calibre7">下图描述了标准正弦波函数。这显然是一种非线性关系。在没有明确使用正弦变换的情况下，我们将看到使用 boosting 算法来学习这个函数。当然，我们需要简单的回归树桩，我们从一个简单的函数<code class="literal">getNode</code>开始，它将给出我们想要的分割:</p><div><img src="img/00258.jpeg" alt="Gradient boosting" class="calibre10"/><div><p class="calibre14">图 boosting 能对非线性正弦数据起作用吗？</p></div></div><p class="calibre11"> </p><div><div><div><div><h3 class="title2">从零开始建造</h3></div></div></div><p class="calibre7">在上一节中，我们使用了<a id="id220" class="calibre1"/>简单分类树桩。在这个例子中，一个简单的视觉检查足以识别树桩，我们很快获得了 12 个分类树桩。对于回归问题，我们先定义一个<code class="literal">getNode</code>函数，它是对 Tattar (2017)第九章定义的函数的一个微小修改。首先建立所需的符号。</p><p class="calibre7">假设我们有 n 对数据点<img src="img/00259.jpeg" alt="Building it from scratch" class="calibre15"/>，并且我们正在尝试学习关系<img src="img/00260.jpeg" alt="Building it from scratch" class="calibre15"/>，其中<em class="calibre9"> f </em>的形式对我们来说是完全未知的。</p><p class="calibre7">对于回归树，分割标准相当简单。对于由 x 值分割的数据，我们计算每个分割部分中的<em class="calibre9"> ys </em>的均差平方和，然后将它们相加。选择分割标准作为 x 值。这最大化了感兴趣变量的均差平方和。R 函数<code class="literal">getNode</code>实现了这种思想:</p><div><pre class="programlisting">&gt; getNode &lt;- function(x,y)	{
+   xu &lt;- sort(unique(x),decreasing=TRUE)
+   ss &lt;- numeric(length(xu)-1)
+   for(i in 1:length(ss))	{
+     partR &lt;- y[x&gt;xu[i]]
+     partL &lt;- y[x&lt;=xu[i]]
+     partRSS &lt;- sum((partR-mean(partR))^2)
+     partLSS &lt;- sum((partL-mean(partL))^2)
+     ss[i] &lt;- partRSS + partLSS
+   }
+   xnode &lt;- xu[which.min(ss)]
+   minss &lt;- min(ss)
+   pR &lt;- mean(y[x&gt;xnode])
+   pL &lt;- mean(y[x&lt;=xnode])
+   return(list(xnode=xnode,yR=pR,yL=pL))
+ }</pre></div><p class="calibre7"><code class="literal">getNode</code>函数的第一步是找到<code class="literal">x</code>的唯一值，然后按降序排序。对于唯一值，我们通过 For 循环计算平方和。循环的第一步是将数据分成左右两部分。</p><p class="calibre7">对于特定的唯一值，在每个分区中计算均差平方和<a id="id221" class="calibre1"/>,然后求和以获得总残差平方和。</p><p class="calibre7">然后我们获得<code class="literal">x</code>的值，它导致最小的残差平方和。分区区域中的预测是这些区域中 y 值的平均值。</p><p class="calibre7"><code class="literal">getNode</code>函数通过返回<code class="literal">x</code>的分割值以及右和左分区的预测来关闭。我们现在准备创建回归树桩。</p><p class="calibre7">正弦波数据首先很容易创建，我们允许 x 值在区间<img src="img/00261.jpeg" alt="Building it from scratch" class="calibre15"/>内变化。y 值就是应用于 x 向量的正弦函数:</p><div><pre class="programlisting">&gt; # Can Boosting Learn the Sine Wave!
&gt; x &lt;- seq(0,2*pi,pi/20)
&gt; y &lt;- sin(x)
&gt; windows(height=300,width=100)
&gt; par(mfrow=c(3,1))
&gt; plot(x,y,"l",col="red",main="Oh My Waves!")</pre></div><p class="calibre7">前面显示的结果将是<em class="calibre9">图 1 </em>。我们继续获取数据的第一次分割，然后在图上显示右侧和左侧分割的平均值。残差来自正弦波，它们也显示在同一显示器上，如下所示:</p><div><pre class="programlisting">&gt; first_split &lt;- getNode(x,y)
&gt; first_split
$xnode
[1] 3.141593
$yR
[1] -0.6353102
$yL
[1] 0.6050574</pre></div><p class="calibre7">现在，我们的第一个分裂点出现在<code class="literal">x</code>值处，<img src="img/00262.jpeg" alt="Building it from scratch" class="calibre15"/>这里，<code class="literal">3.141593</code>。分裂点右侧的预测是<code class="literal">-0.6353102</code>，左侧的预测是<code class="literal">0.6050574</code>。使用分段功能在同一显示屏上绘制预测:</p><div><pre class="programlisting">&gt; segments(x0=min(x),y0=first_split$yL,
+          x1=first_split$xnode,y1=first_split$yL)
&gt; segments(x0=first_split$xnode,y0=first_split$yR,
+          x1=max(x),y1=first_split$yR)</pre></div><p class="calibre7">现在，预测<a id="id222" class="calibre1"/>在这里很容易得到，一个简单的<code class="literal">ifelse</code>函数有助于计算它们。与正弦波的偏差是残差，我们计算出第一组残差和<code class="literal">summary</code>函数给出了残差值的概要:</p><div><pre class="programlisting">&gt; yfit1 &lt;- ifelse(x&lt;first_split$xnode,first_split$yL,first_split$yR)
&gt; GBFit &lt;- yfit1
&gt; segments(x0=x,x1=x,y0=y,y1=yfit1)
&gt; first_residuals &lt;- y-yfit1
&gt; summary(first_residuals)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.60506 -0.25570  0.04752  0.03025  0.32629  0.63531 </pre></div><p class="calibre7">预测的第一步保存在<code class="literal">GBFit</code>对象中，拟合和预测之间的差异在<code class="literal">first_residuals</code>向量中找到。这完成了梯度推进算法的第一次迭代。第一次迭代的残差将成为第二次迭代的回归变量/输出变量。使用<code class="literal">getNode</code>函数，我们执行第二次迭代，它模拟了早期的代码集:</p><div><pre class="programlisting">&gt; second_split &lt;- getNode(x,first_residuals)
&gt; plot(x,first_residuals,"l",col="red",main="The Second Wave!")
&gt; segments(x0=min(x),y0=second_split$yL,
+          x1=second_split$xnode,y1=second_split$yL)
&gt; segments(x0=second_split$xnode,y0=second_split$yR,
+          x1=max(x),y1=second_split$yR)
&gt; yfit2 &lt;- ifelse(x&lt;second_split$xnode,second_split$yL,second_split$yR)
&gt; GBFit &lt;- GBFit+yfit2
&gt; segments(x0=x,x1=x,y0=first_residuals,y1=yfit2)
&gt; second_residuals &lt;- first_residuals-yfit2
&gt; summary(second_residuals)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.51678 -0.24187 -0.02064 -0.01264  0.25813  0.56715 </pre></div><p class="calibre7">这里一个重要的区别是我们不是通过平均而是通过相加来更新预测。请注意，我们正在对第一步的残差进行建模，因此由下一次拟合解释的残差的剩余部分需要相加，而不是求平均值。残差的范围是多少？建议读者将残差值与早期迭代进行比较。对第三次迭代执行类似的扩展:</p><div><pre class="programlisting">&gt; third_split &lt;- getNode(x,second_residuals)
&gt; plot(x,second_residuals,"l",col="red",main="The Third Wave!")
&gt; segments(x0=min(x),y0=third_split$yL,
+          x1=third_split$xnode,y1=third_split$yL)
&gt; segments(x0=third_split$xnode,y0=third_split$yR,
+          x1=max(x),y1=third_split$yR)
&gt; yfit3 &lt;- ifelse(x&lt;third_split$xnode,third_split$yL,third_split$yR)
&gt; GBFit &lt;- GBFit+yfit3
&gt; segments(x0=x,x1=x,y0=second_residuals,y1=yfit3)
&gt; third_residuals &lt;- second_residuals-yfit3
&gt; summary(third_residuals)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.47062 -0.27770 -0.03927 -0.01117  0.18196  0.61331 </pre></div><p class="calibre7">所有的视觉显示如下图所示:</p><div><img src="img/00263.jpeg" alt="Building it from scratch" class="calibre10"/><div><p class="calibre14">图 5:梯度推进算法的三次迭代</p></div></div><p class="calibre11"> </p><p class="calibre7">显然，我们不能每次都在一个详细的执行中进行迭代，循环很重要。代码保存在一个块中，并执行 22 次以上的迭代。每次迭代结束时的输出在图中描述，我们把它们都放在一个外部文件中，<code class="literal">Sine_Wave_25_Iterations.pdf</code>:</p><div><pre class="programlisting">&gt; pdf("Sine_Wave_25_Iterations.pdf")
&gt; curr_residuals &lt;- third_residuals
&gt; for(j in 4:25){
+   jth_split &lt;- getNode(x,curr_residuals)
+   plot(x,curr_residuals,"l",col="red",main=paste0(c("The ", j, "th Wave!")))
+   segments(x0=min(x),y0=jth_split$yL,
+            x1=jth_split$xnode,y1=jth_split$yL)
+   segments(x0=jth_split$xnode,y0=jth_split$yR,
+            x1=max(x),y1=jth_split$yR)
+   yfit_next &lt;- ifelse(x&lt;jth_split$xnode,jth_split$yL,jth_split$yR)
+   GBFit &lt;- GBFit+yfit_next
+   segments(x0=x,x1=x,y0=curr_residuals,y1=yfit_next)
+   curr_residuals &lt;- curr_residuals-yfit_next
+ }
&gt; dev.off()
&gt; summary(curr_residuals)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-0.733811 -0.093432  0.008481 -0.001632  0.085192  0.350122 </pre></div><p class="calibre7">在 25 次迭代之后，我们在<code class="literal">GBFit</code>中得到<a id="id225" class="calibre1"/>的整体拟合，我们可以根据实际 y 值绘制该拟合图，以查看梯度推进算法的执行情况:</p><div><pre class="programlisting">&gt; plot(y,GBFit,xlab="True Y",ylab="Gradient Boosting Fit")</pre></div><div><img src="img/00264.jpeg" alt="Building it from scratch" class="calibre10"/><div><p class="calibre14">图 6:梯度拟合与实际正弦数据</p></div></div><p class="calibre11"> </p><p class="calibre7">对于非线性模型来说，这种拟合相当好。这种方法是为了清楚地理解梯度推进算法。下一小节将讨论和开发 boosting 算法的更一般形式。</p></div><div><div><div><div><h3 class="title2"><a id="ch05lvl2sec30" class="calibre1"/>平方误差损失函数</h3></div></div></div><p class="calibre7">用<img src="img/00265.jpeg" alt="Squared-error loss function" class="calibre15"/>表示数据，将迭代次数/树数固定为数字<em class="calibre9"> B </em>。选择收缩系数<img src="img/00266.jpeg" alt="Squared-error loss function" class="calibre15"/>和树<a id="id227" class="calibre1"/>深度<em class="calibre9"> d </em>。这里简要说明基于平方误差损失函数的梯度提升算法。参见 Efron 和 Hastie (2016)的算法 17.2，如下:</p><div><ul class="itemizedlist"><li class="listitem">将残差<img src="img/00267.jpeg" alt="Squared-error loss function" class="calibre15"/>和梯度推进预测初始化为<img src="img/00268.jpeg" alt="Squared-error loss function" class="calibre15"/></li><li class="listitem">对于<img src="img/00269.jpeg" alt="Squared-error loss function" class="calibre15"/> : <div> <ul class="itemizedlist1"> <li class="listitem">拟合深度回归树<em class="calibre9"> d </em>对于数据<img src="img/00270.jpeg" alt="Squared-error loss function" class="calibre15"/> </li> <li class="listitem">获得预测值为<img src="img/00271.jpeg" alt="Squared-error loss function" class="calibre15"/> </li> <li class="listitem">更新助推预测通过<img src="img/00272.jpeg" alt="Squared-error loss function" class="calibre15"/> </li> <li class="listitem">更新残差<img src="img/00273.jpeg" alt="Squared-error loss function" class="calibre15"/> </li> </ul> </div></li><li class="listitem">返回功能序列<img src="img/00274.jpeg" alt="Squared-error loss function" class="calibre15"/></li></ul></div><p class="calibre7">现在，我们将定义一个函数<code class="literal">GB_SqEL</code>，它将实现由平方误差损失函数驱动的梯度提升算法。函数必须提供五个参数:<code class="literal">y</code>和<code class="literal">x </code>将<a id="id228" class="calibre1"/>构成数据，<code class="literal">depth</code>将指定树的深度(即回归树的分裂数)，<code class="literal">iter</code>表示迭代次数，<code class="literal">shrinkage</code>是<img src="img/00275.jpeg" alt="Squared-error loss function" class="calibre15"/>因子。<code class="literal">GB_SqEL</code>功能设置如下:</p><div><pre class="programlisting">&gt; # Gradiend Boosting Using the Squared-error Loss Function
&gt; GB_SqEL &lt;- function(y,X,depth,iter,shrinkage){
+   curr_res &lt;- y
+   GB_Hat &lt;- data.frame(matrix(0,nrow=length(y),ncol=iter))
+   fit &lt;- y*0
+   for(i in 1:iter){
+     tdf &lt;- cbind(curr_res,X)
+     tpart &lt;- rpart(curr_res~.,data=tdf,maxdepth=depth)
+     gb_tilda &lt;- predict(tpart)
+     gb_hat &lt;- shrinkage*gb_tilda
+     fit &lt;- fit+gb_hat
+     curr_res &lt;- curr_res-gb_hat
+     GB_Hat[,i] &lt;- fit
+   }
+   return(list(GB_Hat = GB_Hat))
+ }</pre></div><p class="calibre7">初始化发生在规范中，并且行<code class="literal">fit &lt;- y*0</code>。在第<code class="literal">maxdepth=depth</code>行中接受算法的深度参数，并使用<code class="literal">rpart</code>函数，我们创建一个必要深度的树。<code class="literal">predict</code>函数在每次迭代中根据需要给出<img src="img/00276.jpeg" alt="Squared-error loss function" class="calibre15"/>的值，而<code class="literal">fit+gb_hat</code>进行必要的更新。注意<code class="literal">GB_Hat[,i]</code>由每次迭代结束时的预测值组成。</p><p class="calibre7">我们将用 Efron 和 Hastie (2016)的<a id="id229" class="calibre1"/>例子来说明算法。所考虑的数据与卢格里格氏病，或<strong class="calibre8">肌萎缩侧索硬化</strong> ( <strong class="calibre8"> ALS </strong>)有关。数据集<a id="id230" class="calibre1"/>拥有 1822 名 als 患者的信息。目标是预测功能评定分数的进展速度<code class="literal">dFRS</code>。这项研究有 369 个预测因子/协变量的信息。这里，我们将使用<code class="literal">GB_SqEL</code>函数来拟合梯度提升技术，并随着迭代次数的增加来分析均方误差。详情和数据可从 https://web.stanford.edu/~hastie/CASI/data.html<a class="calibre1" href="https://web.stanford.edu/~hastie/CASI/data.html">获得。我们现在将平方误差损失函数驱动的升压方法付诸实施:</a></p><div><pre class="programlisting">&gt; als &lt;- read.table("../Data/ALS.txt",header=TRUE)
&gt; alst &lt;- als[als$testset==FALSE,-1]
&gt; temp &lt;- GB_SqEL(y=alst$dFRS,X=alst[,-1],depth=4,
+                 iter=500,shrinkage = 0.02)
&gt; MSE_Train &lt;- 0
&gt; for(i in 1:500){
+   MSE_Train[i] &lt;- mean(temp$GB_Hat[,i]-alst$dFRS)^2
+ }
&gt; windows(height=100,width=100)
&gt; plot.ts(MSE_Train)</pre></div><p class="calibre7">使用<code class="literal">read.table</code>函数，我们将代码包中的数据导入到<code class="literal">als</code>对象中。数据以<code class="literal">.txt</code>格式从数据源<a id="id231" class="calibre1"/>获得。列<code class="literal">testset</code>表示观察值是为了训练目的还是为了测试而标记的。我们选择训练观察值，并删除第一个变量<code class="literal">testset</code>，将其存储在对象<code class="literal">alst</code>中。<code class="literal">GB_SqEL</code>功能应用于具有适当规格的<code class="literal">alst</code>对象。</p><p class="calibre7">每次迭代之后，我们计算均方误差，并将其存储在<code class="literal">GB_Hat</code>中，如前所述。从下图中我们可以看到，随着迭代次数的增加，均方误差减小。这里，算法在近 200 次迭代后稳定下来:</p><div><img src="img/00277.jpeg" alt="Squared-error loss function" class="calibre10"/><div><p class="calibre14">图 7:梯度提升和迭代的 MSE</p></div></div><p class="calibre11"> </p><p class="calibre7">在下一节中，我们将看到两个强大的 R 包的使用。</p></div></div></div></body></html>


<html>
  <head>
    <title>The general boosting algorithm</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl1sec44" class="calibre1"/>使用 adabag 和 gbm 包</h2></div></div></div><p class="calibre7">将助推法作为一种综合<a id="id233" class="calibre1"/>技术确实非常有效。该算法被说明为从零开始的分类和回归问题。一旦<a id="id234" class="calibre1"/>清楚透明地理解了算法，我们就可以使用 R 包来交付结果。许多软件包可用于实现升压技术。然而，我们将在本节中使用两个最流行的包<code class="literal">adabag</code>和<code class="literal">gbm</code>。首先，看一下这两个函数的选项。名字很明显，<code class="literal">adabag</code>实现自适应增强方法，而<code class="literal">gbm</code>处理梯度增强方法。首先，我们在下面的代码中查看这两个函数中的可用选项:</p><div><img src="img/00278.jpeg" alt="Using the adabag and gbm packages" class="calibre10"/><div><p class="calibre14">升压和 gbm 功能</p></div></div><p class="calibre11"> </p><p class="calibre7">公式是通常的论点。<code class="literal">adabag</code>中的参数<code class="literal">mfinal</code>和<code class="literal">gbm</code>中的<code class="literal">n.trees</code>允许指定树的数量或迭代次数。boosting 函数提供了<code class="literal">boos</code>选项，它是使用该迭代中每个观察值的权重绘制的训练集的引导样本。梯度推进是一种更通用的算法，它能够处理比回归结构更多的东西。它也可以用于分类<a id="id236" class="calibre1"/>问题。<code class="literal">gbm</code>功能中的<code class="literal">distribution</code>选项给出了这些选项。类似地，这里可以看到<code class="literal">gbm</code>函数提供了许多其他选项。我们既不会承担解释它们的艰巨任务，也不会将它们应用于复杂的数据集。用于解释和阐述自适应和梯度增强算法的两个数据集将继续使用<code class="literal">boosting</code>和<code class="literal">gbm</code>函数。</p><p class="calibre7">玩具数据集需要更改，我们将多次复制它们，以便我们有足够的观察值来运行<code class="literal">boosting</code>和<code class="literal">gbm</code>函数:</p><div><pre class="programlisting">&gt; # The adabag and gbm Packages
&gt; x1 &lt;- c(1,5,3,1,5)
&gt; x1 &lt;- rep(x1,times=10)
&gt; x2 &lt;- c(5,5,3,1,1)
&gt; x2 &lt;- rep(x2,times=10)
&gt; y &lt;- c(1,1,0,1,1)
&gt; y &lt;- rep(y,times=10)
&gt; toy &lt;- data.frame(x1=x1,x2=x2,y=y)
&gt; toy$y &lt;- as.factor(toy$y)
&gt; AB1 &lt;- boosting(y~.,data=toy,boos=TRUE,mfinal = 10,
+                 maxdepth=1,minsplit=1,minbucket=1)
&gt; predict.boosting(AB1,newdata=toy[,1:2])$class
 [1] "1" "1" "0" "1" "1" "1" "1" "0" "1" "1" "1" "1" "0" "1" "1" "1" "1" "0"
[19] "1" "1" "1" "1" "0" "1" "1" "1" "1" "0" "1" "1" "1" "1" "0" "1" "1" "1"
[37] "1" "0" "1" "1" "1" "1" "0" "1" "1" "1" "1" "0" "1" "1"</pre></div><p class="calibre7"><code class="literal">maxdepth=1</code>函数确保我们只使用树桩作为基本分类器。很容易看出，提升函数<a id="id237" class="calibre1"/>工作得很好，因为所有的观察结果都被正确分类。</p><p class="calibre7">与<code class="literal">boosting</code>函数一样，我们<a id="id238" class="calibre1"/>需要更多的数据点。我们用<code class="literal">seq</code>函数增加这个值，并使用<code class="literal">distribution="gaussian"</code>选项，我们要求<code class="literal">gbm</code>函数符合回归增强技术:</p><div><pre class="programlisting">&gt; x &lt;- seq(0,2*pi,pi/200)
&gt; y &lt;- sin(x)
&gt; sindata &lt;- data.frame(cbind(x,y))
&gt; sin_gbm &lt;- gbm(y~x,distribution="gaussian",data=sindata,
+                n.trees=250,bag.fraction = 0.8,shrinkage = 0.1)
&gt; par(mfrow=c(1,2))
&gt; plot.ts(sin_gbm$fit, main="The gbm Sine Predictions")
&gt; plot(y,sin_gbm$fit,main="Actual vs gbm Predict")</pre></div><p class="calibre7">使用绘图函数，我们将拟合与梯度增强方法进行了比较。下图表明这种配合是合适的。然而，这些情节也表明，这个故事也有不太正确的地方。通过 boosting 方法在<img src="img/00279.jpeg" alt="Using the adabag and gbm packages" class="calibre15"/>和<img src="img/00280.jpeg" alt="Using the adabag and gbm packages" class="calibre15"/>处进行的函数近似还有很多需要改进的地方，实际与预测的曲线表明在 0 处有不连续性/较差的性能。但是，我们不会深入探讨这些问题:</p><div><img src="img/00281.jpeg" alt="Using the adabag and gbm packages" class="calibre10"/><div><p class="calibre14">图 8:使用 gbm 函数的正弦波近似</p></div></div><p class="calibre11"> </p><p class="calibre7">接下来，我们将讨论可变重要性的概念。</p></div></div></body></html>


<html>
  <head>
    <title>The general boosting algorithm</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl1sec45" class="calibre1"/>可变重要性</h2></div></div></div><p class="calibre7">Boosting 方法本质上使用树<a id="id239" class="calibre1"/>作为基础学习器，因此可变重要性的思想在这里得到了推广，就像树、装袋和随机森林一样。我们只是简单地增加了变量在树上的重要性，就像我们对套袋或随机森林所做的那样。</p><p class="calibre7">对于来自<code class="literal">adabag</code>包的增强拟合对象，变量重要性提取如下:</p><div><pre class="programlisting">&gt; AB1$importance
 x1  x2 
100   0 </pre></div><p class="calibre7">这意味着 boosting 方法根本没有使用<code class="literal">x2</code>变量。对于梯度增强对象，重要性由<code class="literal">summary</code>函数给出:</p><div><pre class="programlisting">&gt; summary(sin_gbm)
  var rel.inf
x   x     100</pre></div><p class="calibre7">现在很明显我们只有一个变量，所以解释回归方程很重要，我们当然不需要软件来告诉我们。当然，它在复杂的情况下是有用的。比较是针对基于树的不同集合方法的。让我们进入下一部分。</p></div></div></body></html>


<html>
  <head>
    <title>The general boosting algorithm</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre">
<div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch05lvl1sec46" class="calibre1"/>比较套袋、随机造林和助推</h2></div></div></div><p class="calibre7">在前一章中，我们对<a id="id240" class="calibre1"/>装袋和随机森林<a id="id241" class="calibre1"/>方法进行了比较。使用<code class="literal">gbm</code>函数，我们现在为之前的分析增加了提升精度:</p><div><pre class="programlisting">&gt; data("spam")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(spam),replace = TRUE,
+ prob = c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; spam_Train &lt;- spam[Train_Test=="Train",]
&gt; spam_TestX &lt;- within(spam[Train_Test=="Test",],
+                      rm(type))
&gt; spam_TestY &lt;- spam[Train_Test=="Test","type"]
&gt; spam_Formula &lt;- as.formula("type~.")
&gt; spam_rf &lt;- randomForest(spam_Formula,data=spam_Train,coob=TRUE,
+                         ntree=500,keepX=TRUE,mtry=5)
&gt; spam_rf_predict &lt;- predict(spam_rf,newdata=spam_TestX,type="class")
&gt; rf_accuracy &lt;- sum(spam_rf_predict==spam_TestY)/nrow(spam_TestX)
&gt; rf_accuracy
[1] 0.9436117
&gt; spam_bag &lt;- randomForest(spam_Formula,data=spam_Train,coob=TRUE,
+                          ntree=500,keepX=TRUE,mtry=ncol(spam_TestX))
&gt; spam_bag_predict &lt;- predict(spam_bag,newdata=spam_TestX,type="class")
&gt; bag_accuracy &lt;- sum(spam_bag_predict==spam_TestY)/nrow(spam_TestX)
&gt; bag_accuracy
[1] 0.9350464
&gt; spam_Train2 &lt;- spam_Train
&gt; spam_Train2$type &lt;- ifelse(spam_Train2$type=="spam",1,0)
&gt; spam_gbm &lt;- gbm(spam_Formula,distribution="bernoulli",data=spam_Train2,
+                 n.trees=500,bag.fraction = 0.8,shrinkage = 0.1)
&gt; spam_gbm_predict &lt;- predict(spam_gbm,newdata=spam_TestX,
+                             n.trees=500,type="response")
&gt; spam_gbm_predict_class &lt;- ifelse(spam_gbm_predict&gt;0.5,"spam","nonspam")
&gt; gbm_accuracy &lt;- sum(spam_gbm_predict_class==spam_TestY)/nrow(spam_TestX)
&gt; gbm_accuracy
[1] 0.945753
&gt; summary(spam_gbm)
                                var      rel.inf
charExclamation     charExclamation 21.985502703
charDollar               charDollar 18.665385239
remove                       remove 11.990552362
free                           free  8.191491706
hp                               hp  7.304531600


num415                       num415  0.000000000
direct                       direct  0.000000000
cs                               cs  0.000000000
original                   original  0.000000000
table                         table  0.000000000
charHash                   charHash  0.000000000</pre></div><p class="calibre7"><code class="literal">0.9457</code>的助推<a id="id242" class="calibre1"/>精度高于<code class="literal">0.9436</code>的随机森林<a id="id243" class="calibre1"/>精度。下一章将要探讨的进一步微调将有助于提高精度。变量重要性也可以通过<code class="literal">summary</code>函数<a id="id245" class="calibre1"/>轻松获得<a id="id244" class="calibre1"/>。</p></div></div></body></html>


<html>
  <head>
    <title>Summary</title>
    <meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body id="page" class="calibre"><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec47" class="calibre1"/>总结</h1></div></div></div><p class="calibre7">Boosting 是决策树的另一个分支。这是一种顺序迭代技术，其中来自前一次迭代的误差被更不受惩罚地瞄准。我们从重要的自适应增强算法开始，并使用非常简单的玩具数据来说明基础。然后，该方法被扩展到回归问题，我们用两种不同的方法说明了梯度推进方法。简要阐述了两个包<code class="literal">adabag</code>和<code class="literal">gbm</code>，并再次强调了可变重要性的概念。对于垃圾邮件数据集，我们通过 boosting 获得了更高的准确性，因此 boosting 算法的考虑尤其有用。</p><p class="calibre7">本章考虑了 boosting 算法的不同变体。然而，我们根本没有讨论它为什么会起作用。在下一章中，我们将会更详细地讨论这些方面。</p></div></body></html>
</body></html>