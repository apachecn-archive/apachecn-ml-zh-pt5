<title>Google Machine Learning APIs</title>  

# 谷歌机器学习 API

如前一章所见，机器学习被广泛应用。然而，有一些应用程序很容易构建，而有一些很难构建，尤其是对于不太熟悉机器学习的用户来说。我们将在本章中讨论的一些应用程序属于难以构建的类别，因为为这些应用程序构建机器学习模型的过程是数据密集型、资源密集型的，并且需要该领域的大量知识。

在这一章中，我们将浏览谷歌提供的五个机器学习 API(截至 2018 年 3 月)。这些 API 旨在开箱即用，作为 RESTful APIs。对于下面提到的每个服务，我们将展示什么类型的应用程序可以从中受益，以及如何解释返回的结果:

*   视觉具有标签检测、OCR、人脸检测和情感、标志和地标
*   语音意味着语音到文本
*   NLP 有实体、情感和位置
*   翻译
*   视频智能

<title>Vision API</title>  

# 视觉 API

Vision API 允许我们构建许多与 Vision 相关的应用程序:

*   检测图像中的标签
*   检测图像中的文本
*   人脸检测
*   情绪检测
*   徽标检测
*   地标检测

在我们开始使用上述方法构建应用程序之前，让我们以面部情绪检测为例，快速了解一下如何构建应用程序。

检测情绪的过程包括:

1.  收集大量的图像
2.  用图像中可能表现的情感手工标记图像
3.  训练一个**卷积神经网络** ( **CNN** )(将在以后的章节中讨论)根据输入的图像对情感进行分类

虽然前面的步骤是大量资源密集型的(因为我们需要很多人来收集和手工标记图像)，但是有多种其他方法来获得面部情绪检测。我们不确定谷歌是如何收集和标记图像的，但我们现在将考虑谷歌为我们建立的 API，这样，如果我们想将图像分类为它们所代表的情感，我们就可以利用该 API。

<title>Enabling the API</title>  

# 启用 API

在我们开始构建应用程序之前，我们首先必须启用 API，如下所示:

1.  搜索 Google Cloud Vision API:

![](img/c0b69895-ef96-4500-836c-892128798f1b.png)

2.  启用 Google Cloud Vision API:

![](img/3f0fd833-0aba-4b4a-a7d6-da1cfd61f563.png)

3.  一旦您点击 ENABLE，这个项目(也就是我的第一个项目)的 API 就会被启用，如前面的截图所示。
4.  获取 API 的凭据:

![](img/8af83174-c263-4cab-843e-e61d707c8774.png)

5.  单击创建凭据后，单击服务帐户密钥:

![](img/f0c5cd49-93fb-44db-aae9-12e910f22ad4.png)

6.  单击新服务帐户:

![](img/e34a91c8-b9d0-4e6e-a48f-6d17ae69ff82.png)

7.  输入服务帐户名称(在我的例子中是`kish-gcp`)并选择一个角色作为项目所有者:

![](img/9a1e919d-a00a-4897-9b8d-fe2001b1b8f8.png)

8.  单击 Create 保存密钥的 JSON 文件。

<title>Opening an instance</title>  

# 打开实例

要打开一个实例，请单击虚拟机实例，如下图所示，然后单击激活 google cloud shell 图标:

![](img/f8df15b9-f8a2-4a24-af2b-ec2d45251d15.png)![](img/2370fe86-0ccd-48aa-8185-38b28125c7b9.png)<title>Creating an instance using Cloud Shell</title>  

# 使用云壳创建实例

单击云壳图标后，我们创建一个实例，如下所示:

1.  通过指定以下代码创建一个实例:

```
datalab create --no-create-repository <instance name>
```

2.  在云 Shell 中，前面的代码如下所示:

![](img/7b24116c-0aa3-4f1f-9458-d605dbadc306.png)

3.  输入所有提示的响应后，您需要将端口更改为`8081`以访问 Datalab，操作如下:

![](img/710044dc-2640-47d0-93b1-8ddbc07c34f8.png)

4.  一旦你点击改变端口，你将得到如下窗口。进入`8081`，点击更改和预览，打开数据实验室:

![](img/a2160b6a-880a-4a66-8c76-313dd4705a96.png)

5.  这将打开 Datalab，它的功能使我们能够编写所有类型的命令:`bash`、`bigquery`、`python`等等。

既然已经设置了需求，让我们获取/安装 API 的需求:

1.  访问上一节中的 API 密钥，我们已经下载了所需的密钥。现在，让我们通过点击上传按钮将`.json`文件上传到数据实验室:

![](img/fba08f3a-b025-40eb-825f-6d338fcae55e.png)

2.  一旦上传了`.json`文件，您应该能够从这里通过 Datalab 访问它:

![](img/e4a25f51-1121-4c68-a153-369213f94930.png)

3.  打开笔记本；您可以通过单击“笔记本”选项卡在 Datalab 中打开笔记本，如下所示:

![](img/1c1dbb14-1143-4f17-a662-b9a1d269b821.png)

4.  要安装`google-cloud`，打开笔记本后，将内核从 python2 改为 python3:

![](img/2c842bd5-f941-4f47-a91c-1a54681a9ff2.png)

5.  安装`google-cloud`组件，如下所示:

```
%bash
pip install google-cloud
```

6.  一旦安装了`google-cloud`，通过指定以下内容，确保先前上传的`.json`文件在当前 Python 环境中是可访问的:

```
import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datalab/google-
api.json"
```

7.  为了上传感兴趣的图像，我们将考虑将文件从本地机器传输到 bucket，以及从 bucket 传输到 Datalab。

8.  在谷歌云中搜索`bucket`:

![](img/6337862a-a63e-4cf7-b6c8-6d66838c8e77.png)

9.  现在，命名存储桶并创建它:

![](img/34631cf2-357f-435a-b990-77457e90d3fd.png)

10.  点击上传**文件**将相关文件从本地机器上传到存储桶。

![](img/32220fa6-257a-444a-b7d2-1d24631e8a8f.jpg)

11.  一旦文件上传到 bucket，就从 Datalab 获取它，如下所示:

![](img/4ebd04af-3b8c-499f-873f-b44a33315867.png)

12.  现在，您应该注意到`11.jpg`在 Datalab 中是可访问的。

既然要分析的图像在 Datalab 中是可访问的，那么让我们了解利用云视觉 API 来更好地理解图像的方法:

1.  导入相关包:

```
from google.cloud import vision
```

前面的代码片段确保 Vision 中可用的方法在当前会话中是可访问的。

2.  调用对客户端图像执行 Google Cloud Vision API 检测任务(如人脸、地标、徽标、标签和文本检测)的服务— `ImageAnnotator`:

```
client = vision.ImageAnnotatorClient()
```

3.  验证图像是否按照预期上传:

```
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
%matplotlib inline
img=mpimg.imread('/content/datalab/11.jpg')
plt.axis('off')
plt.imshow(img)
```

![](img/eda014fe-a165-4cbb-b709-d7743b54d0f0.png)

4.  调用`face_detection`方法获取图像的相关细节，如下所示:

```
response = client.face_detection({'source' : {'image_uri': "gs://kish-
bucket/11.jpg"},})
```

5.  对图像注释的响应如下:

![](img/b1b03a99-5a4c-43b2-8d2a-60f8a586d63d.png)![](img/59629615-6973-44ba-9585-fbf796eba63e.png)

6.  现在我们已经运行了我们的方法来检测图像中的人脸，让我们看看输出- `response`。如前所述，`response`的输出是一组属性:

```
response
```

![](img/f2c8d0a0-778a-421f-9a5a-e2e31d1f90e2.png)

以下是详细解释的几个要点:

*   **包围多边形**:包围多边形在脸的周围。边界框的坐标是原始图像的比例，如在`ImageParams`中返回的。计算边界框以根据人类的期望来框住面部。它基于地标结果。注意，如果只有部分面部出现在要注释的图像中，一个或多个 *x* 和/或 *y* 坐标可能不会在`BoundingPoly`中生成(多边形将是无界的)。
*   **人脸检测包围多边形**:包围多边形`fd_bounding_poly`比`BoundingPoly`更紧密，只包围人脸的皮肤部分。通常，它用于从任何检测图像中可见皮肤数量的图像分析中消除面部。
*   **地标**:检测到的人脸地标。

在以下几点中有更多的术语需要解释:

*   `roll_angle`:滚动角度，表示脸部相对于图像顺时针/逆时针旋转的量。范围是[-180，180]。
*   `pan_angle`:偏航角，表示相对于垂直于图像的垂直面，人脸指向的向左/向右的角度。范围是[-180，180]。
*   `tilt_angle`:俯仰角，表示人脸指向的相对于图像水平面的上/下角度。范围是[-180，180]。
*   `detection_confidence`:与检测相关的置信度。
*   `landmarking_confidence`:与地标相关的置信度。
*   `joy_likelihood`:与喜悦相关的可能性。
*   `sorrow_likelihood`:与悲伤有关的可能性。
*   `anger_likelihood`:与愤怒相关的可能性。
*   `surprise_likelihood`:与惊喜相关的可能性。
*   `under_exposed_likelihood`:与暴露相关的可能性。
*   `blurred_likelihood`:与模糊相关的可能性。
*   `headwear_likelihood`:与头饰关联的可能性。

面部标志将进一步提供眼睛、鼻子、嘴唇、耳朵等的位置。

我们应该能够在识别出的人脸周围制作一个边界框。

`face_annotations`的输出如下:

![](img/e8b59b5a-4ea1-4e12-ba0b-a3c54a431b69.png)

从前面的代码中，我们应该能够理解边界框的坐标。在下面的代码中，我们计算边界框的起点，以及边界框相应的宽度和高度。计算完成后，我们将矩形叠加到原始图像上:

```
import matplotlib.patches as patches
import numpy as np
fig,ax = plt.subplots(1)

# Display the image
ax.imshow(img)

# Create a Rectangle patch
x_width = np.abs(response.face_annotations[0].bounding_poly.vertices[1].x-
  response.face_annotations[0].bounding_poly.vertices[0].x)
y_height = np.abs(response.face_annotations[0].bounding_poly.vertices[1].y-
  response.face_annotations[0].bounding_poly.vertices[3].y)

rect =
 patches.Rectangle((response.face_annotations[0].bounding_poly.vertices[0].x,
 response.face_annotations[0].bounding_poly.vertices[0].y),
                         x_width,y_height,linewidth=5,edgecolor='y',facecolor='none')

# Add the patch to the Axes
ax.add_patch(rect)
plt.axis('off')
plt.show()
```

上述代码的输出是脸部周围带有边框的图像，如下所示:

![](img/1b10cd5c-cc20-4c35-9236-765efdb5b670.png)<title>Label detection</title>  

# 标签检测

在前面的代码片段中，我们使用了`face_detection`方法来获取各种坐标。

为了理解图像的标签，我们将使用`label_detection`方法代替`face_detection`，如下所示:

```
response_label = client.label_detection({'source' : {'image_uri': "gs://kish-
bucket/11.jpg"},})
```

![](img/202f4ad6-4b36-410b-aabe-88695abbd757.png)

标签检测的输出是标签的集合，以及与每个标签相关联的分数。

<title>Text detection</title>  

# 文本检测

使用`text_detection`方法可以识别图像中的文本，如下所示:

```
response_text = client.text_detection({'source' : {'image_uri': "gs://kish-
bucket/11.jpg"},})
```

`response_text`的输出如下:

![](img/ad3a9fa3-3b57-4f05-a1d0-9136cc27f118.png)

注意，`text_detection`方法的输出是图像中出现的各种文本的边界框。

另外，请注意，`text_annotations`的描述提供了图像中检测到的文本。

<title>Logo detection</title>  

# 徽标检测

视觉服务还使我们能够通过使用`logo_detection`方法来识别图像中的徽标。

在下面的代码中，您可以看到我们能够通过传递图像位置的 URL 来检测到`wikipedia`的徽标，如下所示:

```
response = client.logo_detection({'source' : {'image_uri':
"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Wikipedia-logo-v2-
en.svg/135px-Wikipedia-logo-v2-en.svg.png"},})
```

`logo_detection`方法的输出如下:

![](img/6b346970-15c3-4286-8f53-ac12b3f5753c.png)<title>Landmark detection</title>  

# 地标检测

注意，在前面的代码行中，我们已经在`logo_detection`方法中指定了图像位置的 URL，并且它产生了对预测的徽标的描述，以及与之相关联的置信度得分。

类似地，可以使用`landmark_detection`方法检测图像中的任何地标，如下所示:

```
response = client.landmark_detection({'source' : {'image_uri': 
 "https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/
  Taj_Mahal_%28Edited%29.jpeg/250px-Taj_Mahal_%28Edited%29.jpeg"},})
```

`landmark_detection`方法的输出如下:

![](img/a58efb19-07d8-4363-9f9c-37bb2458c7b1.png)<title>Cloud Translation API</title>  

# 云翻译 API

云翻译 API 提供了一个简单的编程接口，使用最先进的神经机器翻译将任意字符串翻译成任何支持的语言。翻译 API 响应速度非常快，因此网站和应用程序可以与翻译 API 集成，以便快速、动态地将源文本从源语言翻译成目标语言(例如，从法语翻译成英语)。对于源语言未知的情况，也可以使用语言检测。

<title>Enabling the API</title>  

# 启用 API

为了能够使用谷歌云翻译服务，我们需要启用，具体操作如下:

1.  要启用 Google Cloud 翻译 API，请在控制台中搜索 API:

![](img/b9192c37-46b7-4d04-a53a-9e9018296cb1.png)

2.  启用谷歌云翻译 API:

![](img/c6f06a96-ad9e-4b17-9fd2-9dc7ffa6f099.png)

3.  启用翻译 API 后，下一步是创建访问 API 的凭证。但是，请注意，如果您已经为一个 API 创建了凭证，它们可以用于任何其他 API。让我们继续使用云 Shell 初始化我们的实例:

![](img/5152c3b3-6194-4621-89c8-e542b9653cea.png)

4.  一旦实例启动，我们将在端口`8081`上打开 Datalab。我们提供一个路径到文件`api-key`的位置，如下所示:

```
import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datalab/google-api.json"
```

5.  通过使用下面的语句导入`translate`的各种方法:

```
from google.cloud import translate
```

6.  创建一个`client`对象，该对象创建到云翻译服务的连接，如下所示:

```
client = translate.Client()
```

谷歌云翻译 API 支持三种方法，分别是`get_languages()`、`detect_language()`和`translate()`:

*   `client.get_languages()`方法给出了所有可用语言的列表，以及它们的简写符号，如下所示:

![](img/fff7307a-0a48-4a08-ae02-46c9dffce6fc.png)

*   `client.detect_language()`方法检测编写文本的语言:

![](img/da018e49-6ab0-40a0-a710-b1c4db37929f.png)

注意，在前面的方法中，我们给出了两个文本——一个是西班牙语的，另一个是英语的。前面的输出表示文本的语言，以及与语言检测相关联的置信度。

*   `client.translate()`方法检测源语言并将文本翻译成英语(默认情况下)，如下所示:

![](img/53851a88-716c-4b72-9094-fe2ff2ab68db.png)

*   `client.translate()`方法还为我们提供了一个选项来指定文本需要翻译成的目标语言，如下所示:

![](img/76b2504a-a437-471b-ba5d-01d31127dfcd.png)<title>Natural Language API</title>  

# 自然语言 API

谷歌云自然语言 API 通过在一个易于使用的 REST API 中提供强大的机器学习模型，揭示了文本的结构和意义。您可以使用它来提取文本文档、新闻文章或博客文章中提到的人、地点、事件等信息。你还可以用它来了解社交媒体上对你的产品的看法，或者从呼叫中心或消息应用程序中的客户对话中解析意图。您可以分析请求中上传的文本，或者将其与 Google 云存储上的文档存储相集成。

云自然语言 API 可以通过在控制台中搜索找到，如下所示:

![](img/a3f55923-2eb1-4136-8ca5-83284d52ef7e.png)

在结果页面中启用了云自然语言 API:

![](img/6e269f60-6e7e-421e-a5c1-d11aeb89a1af.png)

与翻译 API 类似，如果已经启用了至少一个 API，我们就不必为此 API 创建凭证。

自然语言处理可以用于提取与各种文本相关联的情感。

情感分析检查给定的文本，并识别文本中的主流情感观点，以确定作者的态度是积极的、消极的还是中立的。通过`analyzeSentiment`方法进行情感分析。

在下面的例子中，让我们了解如何识别语句的情感:

1.  导入相关包:

```
from google.cloud import language
```

2.  初始化对应于语言服务的类:

```
client = language.LanguageServiceClient()
```

Google 自然语言 API 支持以下方法:

*   `analyzeEntities`
*   `analyzeSentiment`
*   `analyzeEntitySentiment`
*   `annotateText`
*   `classifyText`

每种方法都使用一个`Document`来表示文本。让我们在下面的例子中探索一下`analyzeSentiment`方法:

```
text="this is a good text"
from google.cloud.language_v1 import types
document = types.Document(
        content=text,
        type='PLAIN_TEXT')
sentiment = client.analyze_sentiment(document).document_sentiment
sentiment.score
```

注意，我们已经将输入文本转换成了一个`Document`类型，然后分析了文档的情感。

情感得分的输出反映了文本是正面的概率；分数越接近 1，陈述越积极。

类似地，可以传递一个 HTML 文件，如下所示:

![](img/f48ffe89-6faa-4933-8c74-485e248c3fa1.png)

通过将内容更改为`gcs_content_uri`，存储在 Google Cloud bucket 中的文件也可以被引用，如下所示:

![](img/2f8354ee-51fb-4f8e-a652-096019b87500.png)

`analyze_entities()`方法在文本中查找命名实体(即专有名称)。这个方法返回一个`AnalyzeEntitiesResponse`:

```
document = language.types.Document(content='Michelangelo Caravaggio, Italian    painter, is known for "The Calling of Saint Matthew".'
                                   ,type='PLAIN_TEXT') 
response = client.analyze_entities(document=document)

for entity in response.entities:
  print('name: {0}'.format(entity.name)) 
```

上述循环的输出是文档内容中存在的命名实体，如下所示:

![](img/251c1f51-1758-40d2-b0e9-e1fc626ea55e.png)

我们还可以通过使用`analyze_syntax`方法提取给定文本中每个单词的词性，如下所示:

1.  将文档标记为构成文本的相应单词:

```
tokens = client.analyze_syntax(document).tokens
tokens[0].text.content
# The preceding output is u'Michelangelo'
```

2.  然后可以提取`token`的词性，如下所示:

```
pos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM','PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')
for token in tokens:print(u'{}: {}'.format(pos_tag[token.part_of_speech.tag],
                               token.text.content))
```

上述代码的输出是:

![](img/9abf115f-6c7d-4ac1-b8ed-4ce4059c4b33.png)

请注意，大多数单词都被归类到正确的词类中。

<title>Speech-to-text API</title>  

# 语音转文本 API

Google Cloud Speech API 通过在一个易于使用的 API 中应用强大的神经网络模型，使开发人员能够将音频转换为文本。API 可以识别超过 110 种语言和变体。人们可以转录用户对着应用麦克风口述的文本，通过语音实现命令和控制，或者转录音频文件，等等。

要启用语音转文本 API，请在控制台中进行搜索，如下所示:

![](img/7751f4e4-0b32-4450-a80c-d80227d2600f.png)

在生成的网页中，启用 API，如下所示:

![](img/b74baeec-4435-4ffb-9edc-c72f0988558f.png)

与前面提到的 API 类似，为一个 API 获得的凭证可以为其他 Google APIs 复制。因此，我们不必为语音转文本 API 单独创建凭证。

启用 API 后，让我们启动云 Shell 和 Datalab，就像我们在前面几节中所做的那样。

在下面的代码中，我们将一个小音频文件转录成文本:

1.  导入相关的包和 API 密钥:

```
from google.cloud import speech
import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datalab/google-api.json"
from google.cloud.speech import enums
from google.cloud.speech import types
```

2.  调用语音服务，如下所示:

```
client = speech.SpeechClient()
```

3.  我们可以指定要转换的音频，如下所示:

```
audio = types.RecognitionAudio(uri='gs://kish-bucket/how_are_you.flac')
```

注意**免费无损音频编解码器** ( **FLAC** )。

使用位于 https://audio.online-convert.com/convert-to-flac 的[的转换器可以将音频文件(`.wav`)转换为`.flac`文件。](https://audio.online-convert.com/convert-to-flac)

该文件位于我们之前创建的 bucket 中。我们指定音频配置，如下所示:

```
config = types.RecognitionConfig(
encoding=enums.RecognitionConfig.AudioEncoding.FLAC,
sample_rate_hertz=16000,
language_code='en-US')
```

通过传递`audio`内容以及指定的配置来获得响应:

```
response = client.recognize(config, audio)
```

现在可以访问结果，如下所示:

```
for result in response.results: 
  print(result)
```

这样的输出是:

![](img/82460a32-f922-4b33-a502-babcfed47d56.png)

当输入音频文件是短(< 1 分钟)持续时间的音频时，`recognize`方法起作用。

如果`audio`文件持续时间较长，要使用的方法是`long_running_recognize`:

```
operation = client.long_running_recognize(config, audio)
```

然后可以通过指定以下内容来访问`result`:

```
response = operation.result(timeout=90)
```

最后，可以通过打印响应结果来获得转录和置信度，就像前面所做的那样。

<title>Video Intelligence API</title>  

# 视频智能 API

云视频智能 API 通过一个易于使用的 REST API 提取元数据，使视频变得可搜索和可发现。你现在可以搜索目录中每个视频文件的每一个瞬间。它可以快速注释存储在谷歌云存储中的视频，并帮助您识别视频中的关键实体(名词)以及它们出现的时间。

可以按如下方式搜索和启用云视频智能 API:

![](img/fd03c537-4531-4616-b22e-ce7363923d6a.png)![](img/5a9ffecc-7966-4fd5-9ae7-f5d114eb5a1e.png)

我们导入所需的包并将路径添加到`api-key`，如下所示:

```
from google.cloud import videointelligence
import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/datalab/google-api.json"
from google.cloud.speech import enums
from google.cloud.speech import types
```

方法`features`使我们能够指定我们想要在视频中检测的内容类型。可用的功能如下:

![](img/54d48021-a4fb-496e-8b98-10cb80940baa.png)

让我们继续检测视频中我们感兴趣的标签:

```
features = [videointelligence.enums.Feature.LABEL_DETECTION]
```

我们指定视频的`config`和上下文，如下所示:

```
mode = videointelligence.enums.LabelDetectionMode.SHOT_AND_FRAME_MODE
config = videointelligence.types.LabelDetectionConfig(
    label_detection_mode=mode)
context = videointelligence.types.VideoContext(
    label_detection_config=config)
```

然后视频需要从云存储中传递，如下所示:

```
path="gs://kish-bucket/Hemanvi_video.mp4"
operation = video_client.annotate_video(
        path, features=features, video_context=context)
```

`annotate_video`方法的结果访问如下:

```
result = operation.result(timeout=90)
```

视频的注释结果可从以下网址获得:

*   视频段级别
*   视频镜头级别
*   框式水平仪

在片段级别，循环查看各个片段标签注释后，可获得如下结果:

```
segment_labels = result.annotation_results[0].segment_label_annotations
for i, segment_label in enumerate(segment_labels):
    print('Video label description: {}'.format(
        segment_label.entity.description))
    for category_entity in segment_label.category_entities:
        print('\tLabel category description: {}'.format(
            category_entity.description))

    for i, segment in enumerate(segment_label.segments):
        start_time = (segment.segment.start_time_offset.seconds +
                      segment.segment.start_time_offset.nanos / 1e9)
        end_time = (segment.segment.end_time_offset.seconds +
                    segment.segment.end_time_offset.nanos / 1e9)
        positions = '{}s to {}s'.format(start_time, end_time)
        confidence = segment.confidence
        print('\tSegment {}: {}'.format(i, positions))
        print('\tConfidence: {}'.format(confidence))
    print('\n')
```

上述代码的输出是:

![](img/9f133d48-607a-4b76-90f0-abfa9876765b.png)

类似地，可以如下获得射击水平的结果:

```
shot_labels = result.annotation_results[0].shot_label_annotations
for i, shot_label in enumerate(shot_labels):
    print('Shot label description: {}'.format(
        shot_label.entity.description))
    for category_entity in shot_label.category_entities:
        print('\tLabel category description: {}'.format(
            category_entity.description))

    for i, shot in enumerate(shot_label.segments):
        start_time = (shot.segment.start_time_offset.seconds +
                      shot.segment.start_time_offset.nanos / 1e9)
        end_time = (shot.segment.end_time_offset.seconds +
                    shot.segment.end_time_offset.nanos / 1e9)
        positions = '{}s to {}s'.format(start_time, end_time)
        confidence = shot.confidence
        print('\tSegment {}: {}'.format(i, positions))
        print('\tConfidence: {}'.format(confidence))
    print('\n')
```

前面几行代码的输出是:

![](img/8670e457-6af0-4c19-b37d-e62ff23ad8a2.png)

最后，可以如下获得帧级别的结果:

```
frame_labels = result.annotation_results[0].frame_label_annotations
for i, frame_label in enumerate(frame_labels):
    print('Frame label description: {}'.format(
        frame_label.entity.description))
    for category_entity in frame_label.category_entities:
        print('\tLabel category description: {}'.format(
            category_entity.description))

    # Each frame_label_annotation has many frames,
    # here we print information only about the first frame.
    frame = frame_label.frames[0]
    time_offset = (frame.time_offset.seconds +
                   frame.time_offset.nanos / 1e9)
    print('\tFirst frame time offset: {}s'.format(time_offset))
    print('\tFirst frame confidence: {}'.format(frame.confidence))
    print('\n')
```

前面几行代码的输出是:

![](img/8a83da6e-44f2-4ad7-aa58-b6284fc4791d.png)<title>Summary</title>  

# 摘要

在本章中，我们介绍了 Google 提供的主要机器学习 API:视觉、翻译、NLP、语音和视频智能。我们已经了解了每个 API 中的各种方法如何使我们能够复制深度学习的结果，而不必从头开始编码。