

# *第七章:*

# 模型改进

## 学习目标

本章结束时，您将能够:

*   解释并实现机器学习模型中偏差和方差权衡的概念。
*   通过交叉验证执行模型评估。
*   为机器学习模型实现超参数调整。
*   使用各种超参数调整技术提高模型的性能。

在这一章中，我们将重点关注使用交叉验证技术和超参数调整来提高模型的性能。

## 简介

在前一章中，我们探索了一些策略，这些策略帮助我们使用**特征选择**和**维度缩减**来构建改进的模型。这些策略主要关注于提高模型的计算性能和可解释性；但是，为了提高模型在性能指标方面的性能，例如构建稳健和更一般化模型的总体准确性或误差估计，我们需要关注交叉验证和超参数调整。

在这一章中，我们将带你了解机器学习中的基本主题，使用交叉验证和超参数调整来构建通用和健壮的模型，并在 r 中实现它们。

我们将首先用外行的例子详细研究本章中的主题，并利用简单的用例来查看实际的实现。

## 偏差-方差权衡

机器学习的一个有趣、艰巨、重复的部分是**模型评估之旅**。同样，艺术和不同的思维方式也是构建健壮模型所必需的。在本书中，我们简化了模型评估过程，使用了训练和测试数据集，这些数据集是通过将可用数据分割成 **70:30** 或 **80:20** 比率而获得的。尽管这种方法在帮助我们理解模型如何在看不见的数据上执行方面是有效的，但它仍然留下了几个漏洞，可能会使模型在大多数其他情况下无效。我们将需要一种更正式、更彻底、更详尽的方法来验证机器学习模型，使其对未来的预测事件具有鲁棒性。在这一章中，我们将研究**交叉验证**及其评估机器学习模型性能的各种方法。

在我们深入探讨这个主题的细节之前，我们需要探索机器学习中一个至关重要的主题，称为**偏差-方差权衡**。这个话题在大多数机器学习论坛和学术界都有很多讨论。对于机器学习界来说，这是一个至关重要的话题，它是深入研究模型验证和改进的基础。从题目可以很容易地推断出，在机器学习模型中，偏差-方差权衡是模型表现出来的一种行为，不幸的是，在估计模型参数时表现出低偏差的模型在估计模型参数时表现出较高的方差，反之亦然。从外行人的角度理解题目，我们先把题目分解成单个的组件，了解每个组件，然后把所有组件放在一起重构更大的画面。

### 什么是机器学习模型中的偏差和方差？

一般来说，当机器学习模型无法学习数据中展示的重要(或有时复杂)模式时，我们说该模型是**有偏差的**。这种模型过于简化了自己，或者只学习了极其简单的规则，而这些规则可能无助于做出准确的预测。这种模型的最终结果是，不管输入数据的差异如何，预测往往保持大致相同(且不正确)。不幸的是，模型学习到的模式非常简单或有偏差，以至于输入数据的变化不会产生预期的预测。

另一方面，如果我们颠倒基本原理，我们可以很容易地定义机器学习模型中的方差。考虑学习不必要的模式的模型，例如来自数据的噪声，这样即使输入数据中的小变化也会导致预测中非常大的不期望的变化。在这种情况下，我们说模型有很高的方差。

理想的情况是一个低偏差和低方差的模型；即从数据中学习了必要模式的模型。它成功地忽略了噪声，并在输入数据发生合理变化的情况下，在预测行为方面实现了合理和理想的变化。不幸的是，理想的场景很难实现，因此我们来到了主题**偏差-方差权衡**。

将我们研究的所有单个组件放在一起，我们可以说，每一次减少偏差或方差的尝试都会导致另一个维度的增加，导致我们需要在模型性能中的期望偏差和方差之间取得平衡。使用超参数调整方法的组合，可以实现可以合并到机器学习模型中以达到偏差和方差之间的平衡的必要改变。我们将在接下来的章节中研究超参数调优的概念。下面是一个著名的例子，用直观的靶心图来演示偏差-方差概念:

![Figure 7.1: The bias-variance concept with a visual bullseye diagram
](img/C12624_07_01.jpg)

###### 图 7.1:带有可视化牛眼图的偏差-方差概念

在上图中，我们可以看到四个象限来明确区分偏差-方差权衡。该图用于解释回归用例的模型偏差和方差。为一个分类用例直观地推断一个类似的想法可能是有挑战性的；然而，我们通过图示的例子得到了更大的图景。

我们的理想目标是训练一个低偏差、低方差的机器学习模型。然而，当我们具有低偏差和高方差时(前面可视化中的右上象限)，我们会看到输入数据中的小变化在最终结果中的显著大变化。另一方面，当我们有高偏差和低方差(可视化中的左下象限)时，我们可以看到最终结果集中在远离目标的区域，表明输入的变化几乎没有任何变化。最后，我们有高偏差和高方差，也就是说，我们击中远离目标，以及有大变化的小变化的输入。对于一个模型来说，这是最不理想的状态。

## 欠拟合和过拟合

在前面的场景中，我们有一个高偏差，我们在机器学习模型中表示一种称为**欠拟合**的现象。类似地，当我们有高方差时，我们在机器学习模型中表示一种称为**过拟合**的现象。

下图演示了回归模型的**过拟合**、**欠拟合**和**理想平衡**的思想。我们可以看到高偏差导致模型过于简化(即拟合不足)；高方差导致模型过于复杂(即过度拟合)；最后，在偏差和差异之间取得适当的平衡:

![Figure 7.2: Visual demonstration of overfitting, underfitting, and ideal balance
](img/C12624_07_02.jpg)

###### 图 7.2:过拟合、欠拟合和理想平衡的可视化演示

为了更有效地研究机器学习模型中的偏差和方差，我们采用了交叉验证技术。这些技术帮助我们更直观地理解模型性能。

## 定义一个样本用例

为了用一个实际的数据集探索本章中的主题，我们使用一个已经在`mlbench`包中可用的小数据集，称为`PimaIndiansDiabetes`，这是一个用于分类用例的方便的数据集。

该数据集最初来自国家糖尿病、消化和肾脏疾病研究所。可以从数据集定制的用例是当根据少数医学诊断测量来预测患者是否患有糖尿病时。

#### 注意

其他信息可在 http://math . Furman . edu/~ DCS/courses/math 47/R/library/ml bench/html/pimaindiansdiabetes . html 中找到。

数据集大小小于 1000 行的用例是有意选择的。本章探讨的主题需要在商用硬件上进行大量计算，以满足大型数据集的常规使用情况。选择小数据集进行演示有助于大多数使用主流硬件的读者在相当正常的计算时间内获得结果。

### 练习 88:加载和浏览数据

要快速研究`PimaIndiansDiabetes`的整体特征并探究每列内容的性质，请执行以下步骤:

1.  使用以下命令加载`mlbench`、`randomForest`和`dplyr`库:

    ```
    library(mlbench)
    library(randomForest)
    library(dplyr)
    ```

2.  使用以下命令从`PimaIndiansDiabetes`数据集加载数据:

    ```
    data(PimaIndiansDiabetes)
    df<-PimaIndiansDiabetes
    ```

3.  Explore the dimensions of the dataset and study the content within each column using the `str` command:

    ```
    str(df)
    ```

    输出如下所示:

    ```
    'data.frame':768 obs. of  9 variables:
     $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...
     $ glucose : num  148 85 183 89 137 116 78 115 197 125...
     $ pressure: num  72 66 64 66 40 74 50 0 70 96 ...
     $ triceps : num  35 29 0 23 35 0 32 0 45 0 ...
     $ insulin : num  0 0 0 94 168 0 88 0 543 0 ...
     $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ...
     $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...
     $ age     : num  50 31 32 21 33 30 26 29 53 54 ...
     $ diabetes: Factor w/ 2 levels "neg","pos": 2 1 2 1 2 1 2 1 2 2 ...
    ```

我们可以看到，数据集有`768`个观察值和`9`个变量，即 *8 个*自变量和 *1 个*因变量`diabetes`，其值为`pos`表示正，`neg`表示负。

我们将使用这个数据集，并为本章的后续主题开发几个分类模型。

## 交叉验证

交叉验证是一种模型验证技术，有助于评估机器学习模型在独立数据集上进行归纳的性能和能力。它也被称为**旋转验证**，因为它通过从同一分布中提取训练和验证数据，通过多次重复来接近模型的验证。

交叉验证帮助我们:

*   评估模型对未知数据的稳健性。
*   估计理想绩效指标的实际范围。
*   减轻模型的过度拟合和欠拟合。

交叉验证的一般原则是通过将数据划分成组并使用多数进行训练和少数进行测试，在整个数据集上多次迭代测试模型。重复旋转确保模型已经在所有可用的观测值上进行了测试。模型的最终性能指标是从所有轮换的结果中汇总和总结的。

为了研究模型是否具有高偏差，我们可以检查模型在所有旋转中的平均性能。如果平均性能度量表示总体准确性(对于分类)或**平均绝对百分比误差**(对于回归)较低，则存在较高的偏差，并且模型不适合。为了研究模型是否具有高方差，我们可以研究期望的性能指标在轮换中的标准偏差。高标准偏差表明模型具有高方差；也就是说，模型会过度拟合。

交叉验证有几种流行的方法:

*   维持验证
*   k 倍交叉验证
*   保持一出验证(LOOCV)

让我们详细探讨一下这些方法。

## 坚持方法/验证

这是用于验证模型性能的最简单的方法(尽管不是最推荐的)。我们在整本书中都使用了这种方法来测试我们在前几章中的模型性能。这里，我们将可用数据集随机分为训练数据集和测试数据集。训练数据集和测试数据集之间最常用的分流比是 **70:30** 或 **80:20** 。

这种方法的主要缺点是，模型性能纯粹是从部分测试数据集评估的，它可能不是模型性能的最佳表示。模型的评估将完全取决于拆分的类型，因此，最终出现在训练和测试数据集中的数据点的性质，这可能会导致明显不同的结果，从而导致较高的方差。

![Figure 7.3: Holdout validation
](img/C12624_07_03.jpg)

###### 图 7.3:维持验证

以下练习将数据集分为 70%的训练数据集和 30%的测试数据集，并在训练数据集上构建随机森林模型，然后使用测试数据集评估性能。这个方法在*第五章*、*分类*中被广泛使用，所以你不应该对这个过程感到惊讶。

### 练习 89:使用维持验证执行模型评估

在本练习中，我们将利用我们在*练习 1* : *加载和探索数据*中加载到内存中的数据，创建一个简单的随机森林分类模型，并使用维持验证技术执行模型评估。

执行以下步骤:

1.  首先，使用下面的命令将`caret`包导入系统。`caret`包为我们提供了现成的模型评估函数，即`ConfusionMatrix` :

    ```
    library(caret)
    ```

2.  现在，按如下方式设置种子的可再现性:

    ```
    set.seed(2019)
    ```

3.  使用以下命令创建 70% `train`和 30% `test`数据集:

    ```
    train_index<- sample(seq_len(nrow(df)),floor(0.7 * nrow(df)))
    train <- df[train_index,]
    test <- df[-train_index,]
    ```

4.  使用`print`功能显示所需的输出:

    ```
    print("Training Dataset shape:")
    print(dim(train))
    print("Test Dataset shape:")
    print(dim(test))
    ```

5.  通过拟合`train`数据集:

    ```
    model <-randomForest(diabetes~.,data=train, mtry =3)
    ```

    创建随机森林模型
6.  使用以下命令打印模型:

    ```
    print(model)
    ```

7.  对`test`数据集使用`predict`方法，如下所示:

    ```
    y_predicted<- predict(model, newdata = test)
    ```

8.  使用以下命令创建并打印`Confusion-Matrix`:

    ```
    results<-confusionMatrix(y_predicted, test$diabetes, positive= 'pos')
    print("Confusion Matrix  (Test Data)- ")
    print(results$table)
    ```

9.  Print the overall accuracy using the following command:

    ```
    results$overall[1]
    ```

    输出如下所示:

![Figure 7.4: Model assessment using holdout validation
](img/C12624_07_04.jpg)

###### 图 7.4:使用维持验证的模型评估

我们可以看到整体准确率为 77%。这可能不是模型性能的最佳表现，因为我们只对随机测试样本进行了评估。如果我们使用不同的测试样本，结果可能会不同。现在让我们探索克服这种折衷的其他交叉验证方法。

## K 倍交叉验证

这种技术是模型评估最推荐的方法。在这种技术中，我们将数据分成 *k* 组，并使用 *k-1* 组用于训练，剩余的(1 组)用于验证。该过程被重复 *k* 次，其中新的组被用于在每个连续的迭代中进行验证，因此，每个组被用于在一个时间点进行测试。总体结果是跨越 *k* 次迭代的平均误差估计。

因此，*k*-折叠交叉验证克服了维持技术的缺点，因为每个数据点都经过了一次 *k* 次迭代的测试，从而减轻了与分割性质相关的风险。随着 *k* 的值增加，模型的方差减小。k 最常用的值是 5 或 10。这种技术的主要缺点是它训练模型 *k* 次(对于 *k* 次迭代)。因此，模型定型和验证所需的总计算时间大约是保持方法的 *k* 倍。

下图展示了五重交叉验证和所有轮换的汇总结果(假设):

![Figure 7.5: K-fold validation
](img/C12624_07_05.jpg)

###### 图 7.5: K 倍验证

下面的代码片段对上一个示例中使用的相同数据集实现了 5 重交叉验证，并打印了所有折叠的平均准确度。

### 练习 90:使用 K-Fold 交叉验证执行模型评估

我们将利用与前两个练习相同的数据集作为用例，构建一个样本随机森林分类模型，并使用 k-fold 交叉验证评估性能。

要使用 k-fold 交叉验证方法执行模型评估，请执行以下步骤:

1.  首先，使用下面的命令将`caret`包导入到系统中:

    ```
    library(caret)
    ```

2.  接下来，使用以下命令将`seed`设置为`2019`:

    ```
    set.seed(2019)
    ```

3.  现在，使用下面的命令为五重交叉验证定义一个函数:

    ```
    train_control = trainControl(method = "cv", number=5, savePredictions = TRUE,verboseIter = TRUE)
    ```

4.  将`mtry`的值定义为`3`(以匹配我们之前的例子):

    ```
    parameter_values = expand.grid(mtry=3)
    ```

5.  使用以下命令拟合模型:

    ```
    model_rf_kfold<- train(diabetes~., data=df, trControl=train_control, 
                    method="rf",  metric= "Accuracy", 
    tuneGrid = parameter_values)
    ```

6.  接下来，打印整体精度(所有折叠的平均值):

    ```
    model_rf_kfold$results[2]
    ```

7.  Now, print the detailed prediction dataset using the following command:

    ```
    print("Shape of Prediction Dataset")
    print(dim(model_rf_kfold$pred))
    print("Prediction detailed results - ")
    head(model_rf_kfold$pred) #print first 6 rows
    tail(model_rf_kfold$pred) #print last 6 rows
    print("Accuracy across each Fold-")
    model_rf_kfold$resample
    print(paste("Average Accuracy :",mean(model_rf_kfold$resample$Accuracy)))
    print(paste("Std. Dev Accuracy :",sd(model_rf_kfold$resample$Accuracy)))
    ```

    输出如下所示:

    ```
    + Fold1: mtry=3 
    - Fold1: mtry=3 
    + Fold2: mtry=3 
    - Fold2: mtry=3 
    + Fold3: mtry=3 
    - Fold3: mtry=3 
    + Fold4: mtry=3 
    - Fold4: mtry=3 
    + Fold5: mtry=3 
    - Fold5: mtry=3 
    ...
    Accuracy: 0.7590782
    "Shape of Prediction Dataset"
    768   5
    "Prediction detailed results - "
    ...
    "Average Accuracy : 0.759078176725236"
    "Std. Dev Accuracy : 0.0225461480724459"
    ```

我们可以看到，整体精度下降了一点到`76% (rounded off from 75.9)`。这是每次折叠的平均准确度。我们还手工计算了每一次折叠的平均值和标准偏差。每个折叠精度的标准偏差为`2%`，这相当低，因此，我们可以得出方差很低的结论。整体精度不低，所以模型偏差适度低。整体性能还有改进的空间，但是我们的模型目前既没有过度拟合也没有不足拟合。

如果你仔细观察代码，你会发现我们使用了`trainControl`函数，它为我们提供了必要的构造来定义使用`cv`方法的交叉验证的类型，以及等于`5`的折叠数。

我们使用一个额外的构造来表示保存预测的需要，稍后我们可以详细分析它。然后，`trainControl`对象被传递给`caret`包中的`train`函数，在这里我们还用`rf`方法定义了用作随机森林的算法类型，并且作为`tuneGrid`构造的度量在这一点上是不必要的；它用于超参数调整，我们将在后面介绍。然而，默认情况下，`caret`封装中的`train`功能通过使用超参数调谐来简化功能。它在各种迭代中尝试不同的`mtry`值，并返回具有最佳值的最终预测。为了与前面的例子进行比较，我们必须将`mtry`的值限制为`3`。因此，我们使用了`expand.grid`对象来定义在交叉验证过程中使用的`mtry`的值。

当提供了为交叉验证定义的`trainControl`对象时，train 函数将数据分成五个分区，并利用四个分区进行训练，一个分区进行测试。该过程重复五次( *k* 被设置为`5`)，并且在数据集中的每个分区上迭代地测试该模型。

我们可以在模型结果中的`pred`对象(数据框)中看到详细的结果。在这里，我们可以看到每一行数据的观察值(实际值)和预测值。此外，它还注释了折叠中使用的超参数的值，以及它是测试的一部分的折叠数。

`model`对象中的`resample`数据帧记录了交叉验证中每个折叠的准确性和附加指标。我们可以探索我们感兴趣的度量的平均值和标准偏差，以研究偏差和方差。

从 *k* -fold 交叉验证中得出的最终结论是，用例的随机森林模型的准确性是 76%(即所有分区的平均准确性)。

## 保持一出验证

在这项技术中，我们将 *k* -fold 验证发挥到了逻辑的极致。我们选择分区的数量作为可用数据点的数量，而不是创建*k*-分区，其中 *k* 将是 5 或 10。因此，一个分区中只有一个样本。我们使用除了一个样本之外的所有样本进行训练，并在保持的样本上测试模型，并重复此`n`次，其中`n`是训练样本的数量。最后，计算类似于 *k* 倍验证的平均误差。这种技术的主要缺点是模型被训练了 *n* 次，这使得计算量很大。如果我们正在处理一个相当大的数据样本，最好避免这种验证方法。

Hold-one-out 验证也叫**L**eave-**O**ne-**O**ut**C**Ross-**V**验证(LOOCV)。以下视频演示了对 *n* 个样本的保持一出验证:

![Figure 7.6: Hold-one-out validation
](img/C12624_07_06.jpg)

###### 图 7.6:保持一出验证

以下练习使用具有相同实验设置的随机森林对相同数据集执行**保留一项**或留一项交叉验证。

### 练习 91:使用保留一出验证执行模型评估

类似于*练习 2* : *使用维持验证执行模型评估*和*练习 3* : *使用 K-Fold 交叉验证执行模型评估*，我们将继续利用相同的数据集并执行保持一出验证来评估模型性能。

要使用保留一项验证方法执行模型评估，请执行以下步骤:

1.  首先，使用以下命令定义保持一出验证的函数:

    ```
    set.seed(2019)
    train_control = trainControl(method = "LOOCV", savePredictions = TRUE)
    ```

2.  接下来，定义`mtry`的值等于`3`(以匹配我们之前的例子):

    ```
    parameter_values = expand.grid(mtry=3)
    ```

3.  拟合模型:

    ```
    model_rf_LOOCV<- train(diabetes~., data=df, trControl=train_control, 
                        method="rf",  metric= "Accuracy", 
    tuneGrid = parameter_values)
    ```

4.  现在，打印整体精度(所有折叠的平均值):

    ```
    print(model_rf_LOOCV$results[2])
    ```

5.  Print the detailed prediction dataset using the following commands:

    ```
    print("Shape of Prediction Dataset")
    print(dim(model_rf_LOOCV$pred))
    print("Prediction detailed results - ")
    head(model_rf_LOOCV$pred) #print first 6 rows
    tail(model_rf_LOOCV$pred) #print last 6 rows
    ```

    输出如下所示:

    ```
    Accuracy
    1 0.7721354
    [1] "Shape of Prediction Dataset"
    [1] 768   4
    [1] "Prediction detailed results - "
    "Shape of Prediction Dataset"
     768   4
    "Prediction detailed results - "
    ...
    ```

正如我们所看到的，在`77%`的整体准确性几乎与 *K* 折叠交叉验证相同(边际增加`1%`)。这里的 **LOOCV** 结构代表**留一交叉验证**。该过程在计算上是昂贵的，因为它迭代训练过程的次数与数据点的数量一样多(在这种情况下是 768)。

## 超参数优化

**超参数优化**是为机器学习模型优化或寻找最优超参数集的过程。超参数是定义机器学习模型的宏观特征的参数。它基本上是模型的一个元参数。超参数不同于模型参数；模型参数由模型在学习过程中学习，然而，超参数由设计模型的数据科学家设置，并且不能由模型学习。

为了更直观地理解这个概念，让我们深入浅出地探讨一下这个话题。考虑决策树模型的例子。具有根节点、决策节点和叶节点的树结构(类似于逻辑回归中的β系数)通过数据的训练(拟合)来学习。当模型最终收敛(找到模型参数的最佳值集)时，我们就有了最终的树结构，它定义了最终预测的遍历路径。然而，模型的宏观特征是不同的；在决策树的情况下，它将是复杂度参数，由`cp`表示。复杂度参数`cp`限制了树在深度方面的增长；也就是说，如果信息增益或任何其他相关度量没有超过阈值，则不允许节点分支。应用这一新规则将树的深度限制在一个点之外，有助于更好地概括树。因此，复杂度参数是定义模型的宏观特征的参数，该模型然后定制训练过程的书，我们称之为超参数。

每个机器学习算法都有一组不同的相关超参数，这将有助于模型忽略错误(噪声)，从而提高泛化能力。下表显示了机器学习算法中超参数的几个示例:

![Figure 7.7: Hyperparameters in machine learning algorithms
](img/C12624_07_07.jpg)

###### 图 7.7:机器学习算法中的超参数

#### 注意

在 R 和 Python 中提供的实现中，超参数的数量有时是不同的。例如，R 中带有`caret`包的逻辑回归实现不像`sklearn`中的 Python 实现那样调整`c`参数。类似地，Python 的`sklearn`中的随机森林实现允许使用树的深度作为超参数。

有关基于梯度的超参数优化的信息，请访问以下链接:

https://arxiv.org/abs/1502.03492

http://proceedings.mlr.press/v37/maclaurin15.pdf

超参数调整的过程可以总结为寻找超参数的最优值集合的迭代过程，该最优值集合产生用于我们的预测任务的最佳机器学习模型。有几种方法可以实现这一点。事实上，这个过程是迭代的，我们可以肯定，将有几种方法来优化用于找到最佳值集的路径。让我们深入讨论一下超参数调优可以采用的广泛策略。

## 网格搜索优化

为模型寻找最优超参数集的最天真的方法是使用**强力**方法，迭代超参数值的每个组合，然后找到最优组合。这将带来预期的结果，但不是在预期的时间内。在大多数情况下，我们训练的模型会非常大，需要大量的计算时间来训练。遍历每个组合不是一个理想的选择。为了改进蛮力方法，我们有网格搜索优化；顾名思义，在这里，我们定义了一个网格值，该网格值将用于超参数值的穷举组合以进行迭代。

通俗地说，对于网格搜索优化，我们为每个超参数定义一组有限的值，我们有兴趣为模型进行优化。然后，针对所有可能的超参数值的详尽组合来训练该模型，并且选择具有最佳性能的组合作为最优集合。

下图展示了对一组假设参数进行网格搜索优化的想法。使用超参数网格定义组合，并为每个组合训练模型:

![Figure 7.8: The hyperparameter grid and combinations
](img/C12624_07_08.jpg)

###### 图 7.8:超参数网格和组合

网格搜索优化的优势在于，在给定要迭代的候选值的有限集合的情况下，它极大地减少了寻找超参数的最优集合所需的时间(与强力相比)。然而，这是有代价的。网格搜索优化模型假设超参数的最佳值位于为每个超参数提供的候选值列表中。如果我们不提供最佳值作为列表(网格)中的候选值，我们将永远不会有算法的最佳值集。因此，在最终确定候选值列表之前，我们需要为每个超参数的最推荐值列表探索一些建议。超参数优化最适合有经验的数据科学专业人士，他们对各种不同的机器学习问题有很强的判断力。

我们为机器学习模型定义超参数，为模型定制学习手册(拟合)。

### 练习 92:执行网格搜索优化-随机森林

在本练习中，我们将使用`caret`包为模型执行网格搜索优化，其中我们定义了一个网格值，我们希望测试和评估该网格值以获得最佳模型。我们将在与上一主题相同的数据集上使用随机森林算法。

执行以下步骤:

1.  首先，使用下面的命令将`seed`设置为`2019`:

    ```
    set.seed(2019)
    ```

2.  接下来，使用以下命令定义交叉验证方法:

    ```
    train_control = trainControl(method = "cv",  number=5, savePredictions = TRUE)
    ```

3.  现在，定义`parameter_grid`，如下图所示:

    ```
    parameter_grid = expand.grid(mtry=c(1,2,3,4,5,6))
    ```

4.  用交叉验证和网格搜索优化拟合模型:

    ```
    model_rf_gridSearch<- train(diabetes~., data=df, trControl=train_control, 
                   method="rf",  metric= "Accuracy", 
    tuneGrid = parameter_grid)
    ```

5.  Print the overall accuracy (averaged across all folds for each hyperparameter combination):

    ```
    print("Accuracy across hyperparameter Combinations:")
    print(model_rf_gridSearch$results[,1:2])
    ```

    输出如下所示:

    ```
    [1] "Accuracy across hyperparameter Combinations:"
    mtry  Accuracy
    1    1 0.7564893
    2    2 0.7604108
    3    3 0.7642730
    4    4 0.7668704
    5    5 0.7629658
    6    6 0.7590697
    ```

6.  打印详细的预测数据集:

    ```
    print("Shape of Prediction Dataset")
    print(dim(model_rf_gridSearch$pred))
    [1] "Shape of Prediction Dataset"
    [1] 4608    5
    print("Prediction detailed results - ")
    print(head(model_rf_gridSearch$pred)) #print the first 6 rows
    print(tail(model_rf_gridSearch$pred)) #print the last 6 rows
    [1] "Prediction detailed results - "
    predobsrowIndexmtry Resample
    1  neg pos       10    1    Fold1
    2  neg pos       24    1    Fold1
    3  neg neg       34    1    Fold1
    4  neg pos       39    1    Fold1
    5  neg neg       43    1    Fold1
    6  neg neg       48    1    Fold1
    predobsrowIndexmtry Resample
    4603  neg neg      752    6    Fold5
    4604  neg neg      753    6    Fold5
    4605  pos pos      755    6    Fold5
    4606  neg neg      759    6    Fold5
    4607  neg neg      761    6    Fold5
    4608  pos pos      762    6    Fold5
    print("Best value for Hyperparameter 'mtry':")
    print(model_rf_gridSearch$bestTune)
    [1] "Best value for Hyperparameter 'mtry':"
    mtry
    4    4
    print("Final (Best) Model ")
    print(model_rf_gridSearch$finalModel)
    [1] "Final (Best) Model "
    Call:
    randomForest(x = x, y = y, mtry = param$mtry) 
                   Type of random forest: classification
                         Number of trees: 500
    No. of variables tried at each split: 4
    OOB estimate of  error rate: 23.7%
    Confusion matrix:
        neg pos class.error
    neg 423  77    0.154000
    pos 105 163    0.391791
    ```

7.  Plot the grid metrics:

    ```
    library(repr)
    options(repr.plot.width=8, repr.plot.height=5)
    plot(model_rf_gridSearch)
    ```

    输出如下所示:

![Figure 7.9: The accuracy of the random forest model across various values of hyperparameter
](img/C12624_07_09.jpg)

###### 图 7.9:不同超参数值下随机森林模型的精确度

正如我们所见，使用值为 4 的`mtry`超参数获得了最佳精度结果。输出中突出显示的部分将帮助您理解整个外卖流程。我们使用了 5 重交叉验证和网格搜索优化，其中我们用值(1，2，3，4，5 和 6)为`mtry`超参数定义了一个网格。还显示了每个值的精度，我们可以看到来自等于`4`的`mtry`的结果比其他值高一个等级。最后，我们还打印了网格搜索优化过程返回的最终模型。

到目前为止，我们只将随机森林作为实现交叉验证和超参数调优的模型。我们可以将此扩展到任何其他算法。像 **XGBoost** 这样的算法比随机森林(使用 R 实现)有更多的超参数，因此使整个过程在计算上更加昂贵和复杂。在下面的练习中，我们在同一个数据集上执行 5 重交叉验证，以及 XGBoost 的网格搜索优化。代码中突出显示的部分是对 XGBoost 的修改。

#### 自动化超参数调谐:

https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a

### 练习 93:网格搜索优化–XGBoost

与前面的练习类似，我们将对 XGBoost 模型而不是随机森林执行网格搜索优化，并对一组更大的超参数执行网格搜索优化，以找到最佳模型。

要在 XGBoost 模型上执行网格搜索优化，请执行以下步骤:

1.  首先，使用下面的命令将`seed`设置为`2019`:

    ```
    set.seed(2019)
    ```

2.  接下来，使用以下命令导入`dplyr`库:

    ```
    library(dplyr)
    ```

3.  使用以下命令定义交叉验证方法:

    ```
    train_control = trainControl(method = "cv",  number=5, savePredictions = TRUE)
    ```

4.  接下来，如下图所示定义参数网格:

    ```
    parameter_grid = expand.grid(nrounds = c(30,50,60,100),
                                 eta=c(0.01,0.1,0.2,0.3),
    max_depth = c(2,3,4,5),
                                 gamma = c(1),
    colsample_bytree = c(0.7),
    min_child_weight = c(1)  ,
                                 subsample = c(0.6)
                                )
    ```

5.  用交叉验证和网格搜索优化拟合模型:

    ```
    model_xgb_gridSearch<- train(diabetes~., data=df, trControl=train_control, 
                   method="xgbTree",  metric= "Accuracy",
    tuneGrid = parameter_grid)
    ```

6.  打印详细的预测数据集:

    ```
    print("Shape of Prediction Dataset")
    print(dim(model_xgb_gridSearch$pred))
    "Shape of Prediction Dataset"
      49152    11
    print("Prediction detailed results - ")
    head(model_xgb_gridSearch$pred) #print the first 6 rows
    tail(model_xgb_gridSearch$pred) #print the last 6 rows
    [1] "Prediction detailed results - "
    predobsrowIndex  eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds Resample
    1  pos pos        3 0.01         2     1              0.7                1       0.6     100    Fold1
    2  neg neg        6 0.01         2     1              0.7                1       0.6     100    Fold1
    3  neg pos       20 0.01         2     1              0.7                1       0.6     100    Fold1
    4  pos pos       23 0.01         2     1              0.7                1       0.6     100    Fold1
    5  pos pos       25 0.01         2     1              0.7                1       0.6     100    Fold1
    6  pos pos       27 0.01         2     1              0.7                1       0.6     100    Fold1
    predobsrowIndex eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds Resample
    49147  neg pos      732 0.3         5     1              0.7                1       0.6      60    Fold5
    49148  neg pos      740 0.3         5     1              0.7                1       0.6      60    Fold5
    49149  neg neg      743 0.3         5     1              0.7                1       0.6      60    Fold5
    49150  pos pos      749 0.3         5     1              0.7                1       0.6      60    Fold5
    49151  neg pos      751 0.3         5     1              0.7                1       0.6      60    Fold5
    49152  neg neg      763 0.3         5     1              0.7                1       0.6      60    Fold5
    print("Best values for all selected Hyperparameters:")
    model_xgb_gridSearch$bestTune
    [1] "Best values for all selected Hyperparameters:"
    nroundsmax_depth eta gamma colsample_bytreemin_child_weight subsample
    27      60         4 0.1     1              0.7                1       0.6
    ```

7.  Print the overall accuracy (averaged across all folds for each hyperparameter combination):

    ```
    print("Average results across different combination of Hyperparameter Values")
    model_xgb_gridSearch$results %>% arrange(desc(Accuracy)) %>% head(5)
    ```

    输出如下所示:

    ```
    [1] "Average results across different combination of Hyperparameter Values"
       eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds  Accuracy     Kappa AccuracySD
    1 0.10         4     1              0.7                1       0.6      60 0.7695612 0.4790457 0.02507631
    2 0.01         3     1              0.7                1       0.6      30 0.7695442 0.4509049 0.02166056
    3 0.01         2     1              0.7                1       0.6     100 0.7695187 0.4521142 0.03373126
    4 0.30         2     1              0.7                1       0.6      30 0.7682540 0.4782334 0.01943638
    5 0.01         5     1              0.7                1       0.6      30 0.7682455 0.4592689 0.02836553
    KappaSD
    1 0.05067601
    2 0.05587205
    3 0.08038248
    4 0.04249313
    5 0.06049950
    ```

8.  Plot the graph:

    ```
    plot(model_xgb_gridSearch)
    ```

    输出如下所示:

![Figure 7.10: Visualizing the accuracy across various hyperparameter values for the XGBoost model
](img/C12624_07_10.jpg)

###### 图 7.10:可视化 XGBoost 模型的各种超参数值的准确性

这个练习的输出可能看起来很长，但是让我们快速总结一下结果。因为我们在 XGBoost 上执行交叉验证和超参数调优，所以我们需要为更多的超参数提供一个网格。输出中的第一行表明预测数据集的大小为 49152 x 11。这表明交叉验证中每个折叠的超参数的每个组合的详尽预测。我们已经打印了预测数据集的头部和尾部(第一行和最后六行数据)，我们可以看到模型的每个实例的预测结果和相关的超参数，以及相应的折叠。

下表显示了基于模型精度的超参数的最佳值集。我们可以看到，`nrounds=60`、`max_depth=3`、`eta=0.01`、`gamma=1`、`colsample_bytree=0.7`、`min_child_weight=1`和`subsample=0.6`的值为该模型返回了最佳性能。

下表按性能降序显示了每个超参数组合的相应精度。最佳精度位于使用最佳超参数集获得的表格的第一行。我们达到了`76.8%`的精度。

最后，我们绘制了超参数的结果。考虑到大量的超参数，我们有一个更密集的图表来展示结果。但是，我们可以使用`eta=0.01`直接检查象限的结果，并研究最大深度和`nrounds`的变化，得出最佳性能来自相同的超参数组合的结论。

## 随机搜索优化

在随机搜索优化中，我们克服了网格搜索优化的缺点之一，即在网格中为每个超参数选择候选值中的最佳值集。这里，我们选择从分布中随机选择(在超参数连续值的情况下)，而不是我们定义的静态列表。在随机搜索优化中，我们有更广泛的选项可供搜索，因为超参数的连续值是从分布中随机选择的。这在很大程度上增加了找到超参数最佳值的机会。

我们中的一些人可能已经开始理解随机选择总是有可能包含超参数的最佳值。真正的答案是，它并不总是比网格搜索具有绝对优势，但在相当大的迭代次数下，随机搜索比网格搜索找到更优的超参数集的机会会增加。在随机选择值的情况下，随机搜索可能会返回比网格搜索更差的超参数优化值，但是，大多数数据科学专业人员都通过经验验证了这样一个事实，即在迭代次数相当可观的情况下，随机搜索在大多数情况下都优于网格搜索。

在`caret`包中简化了随机搜索优化的实现。我们必须定义一个名为`tuneLength`的参数，它将为随机搜索的迭代次数设置一个最大上限。迭代的次数等于模型被训练的次数，因此次数越多，获得最佳超参数集和相关性能提升的机会就越大。然而，迭代次数越多，执行所需的计算时间就越长。

在下面的练习中，让我们对同一个数据集的随机森林算法执行随机搜索优化。

### 练习 94:在随机森林模型上使用随机搜索优化

我们将用随机搜索优化来扩展机器学习模型的优化过程。在这里，我们只定义我们希望对模型的超参数值的随机组合执行的迭代次数。

本练习的目的是在随机森林模型上执行随机搜索优化。

执行以下步骤:

1.  首先，使用下面的命令将`seed`设置为`2019`:

    ```
    set.seed(2019)
    ```

2.  如下图所示定义交叉验证方法:

    ```
    train_control = trainControl(method = "cv",  number=5, savePredictions = TRUE)
    ```

3.  用交叉验证和随机搜索优化拟合模型:

    ```
    model_rf_randomSearch<- train(diabetes~., data=df, trControl=train_control, 
                            method="rf",  metric= "Accuracy",tuneLength = 15)
    ```

4.  打印详细的预测数据集:

    ```
    print("Shape of Prediction Dataset")
    print(dim(model_rf_randomSearch$pred))
    [1] "Shape of Prediction Dataset"
    [1] 5376    5
    print("Prediction detailed results - ")
    head(model_rf_randomSearch$pred) #print the first 6 rows
    tail(model_rf_randomSearch$pred) #print the last 6 rows
    [1] "Prediction detailed results - "
    predobsrowIndexmtry Resample
    1  pos pos        1    2    Fold1
    2  neg neg        4    2    Fold1
    3  pos pos        9    2    Fold1
    4  neg pos       10    2    Fold1
    5  neg neg       13    2    Fold1
    6  pos pos       17    2    Fold1
    predobsrowIndexmtry Resample
    5371  neg neg      737    8    Fold5
    5372  neg neg      742    8    Fold5
    5373  neg neg      743    8    Fold5
    5374  neg pos      758    8    Fold5
    5375  neg neg      759    8    Fold5
    5376  neg neg      765    8    Fold5
    print("Best values for all selected Hyperparameters:")
    model_rf_randomSearch$bestTune
    [1] "Best values for all selected Hyperparameters:"
    mtry
    7    8
    ```

5.  Print the overall accuracy (averaged across all folds for each hyperparameter combination):

    ```
    model_rf_randomSearch$results %>% arrange(desc(Accuracy)) %>% head(5)
    ```

    输出如下所示:

    ```
    mtry  Accuracy     Kappa AccuracySDKappaSD
    1    8 0.7838299 0.5190606 0.02262610 0.03707616
    2    7 0.7773194 0.5047353 0.02263485 0.03760842
    3    3 0.7760037 0.4945296 0.02629540 0.05803215
    4    6 0.7734063 0.4964970 0.02451711 0.04409090
    5    5 0.7720907 0.4895592 0.02618707 0.04796626
    ```

6.  Plot the data for the random search optimization of the random forest model:

    ```
    plot(model_rf_randomSearch)
    ```

    输出如下所示:

![Figure 7.11: Visualizing accuracy across values of hyperparameters
](img/C12624_07_11.jpg)

###### 图 7.11:可视化超参数值的准确性

我们将`tuneLength`参数设置为`15`；但由于 R 中的随机森林只针对 1 个参数的超参数调优，即`mtry`，迭代次数在`7`处耗尽。这是因为数据集中只有八个独立变量。在大多数情况下，建议根据数据中要素的数量设置一个较大的数字。我们可以看到`mtry`的最佳值出现在 7。该图展示了各种`mtry`值之间的差异。我们用这个模型达到的最高准确率是 76%。

现在让我们用 XGBoost 尝试同样的实验。这里我们将`tuneLength`设为`35`，这将是计算量很大的，即*15×5(折叠)= 75 次*模型迭代。这将比之前的任何一次迭代花费更长的时间。如果你想更快地看到结果，你可能需要减少使用`tuneLength`的迭代次数。

### 练习 95:随机搜索优化–XGBoost

与随机森林一样，我们将在 XGBoost 模型上执行随机搜索优化。XGBoost 模型有大量的超参数需要优化，因此更适合随机搜索优化。我们将利用与前面练习中相同的数据集来构建 XGBoost 模型，然后执行优化。

这个练习的目的是在 XGBoost 模型上执行随机搜索优化。

执行以下步骤:

1.  使用以下命令将`seed`设置为`2019`:

    ```
    set.seed(2019)
    ```

2.  使用以下命令定义交叉验证方法:

    ```
    train_control = trainControl(method = "cv",  number=5, savePredictions = TRUE)
    ```

3.  用交叉验证和随机搜索优化拟合模型:

    ```
    model_xgb_randomSearch<- train(diabetes~., data=df, trControl=train_control, 
                                   method="xgbTree", metric= "Accuracy",tuneLength = 15)
    ```

4.  打印详细的预测数据集:

    ```
    print("Shape of Prediction Dataset")
    print(dim(model_xgb_randomSearch$pred))
    print("Prediction detailed results - ")
    head(model_xgb_randomSearch$pred) #print the first 6 rows
    tail(model_xgb_randomSearch$pred) #print the last 6 rows
    print("Best values for all selected Hyperparameters:")
    model_xgb_randomSearch$bestTune
    ```

5.  Print the overall accuracy (averaged across all folds for each hyperparameter combination):

    ```
    model_xgb_randomSearch$results %>% arrange(desc(Accuracy)) %>% head(5)
    ```

    输出如下所示:

    ```
    "Shape of Prediction Dataset"
    10368000       11
    "Prediction detailed results - "
    ```

![Figure 7.12: Detailed prediction results
](img/C12624_07_12.jpg)

###### 图 7.12:详细的预测结果

使用*随机搜索优化*，我们可以看到一组不同的参数被选为 XGBoost 模型的最佳组合。这里注意，网格搜索和随机搜索的精度是一样的(差别很小)，但是，参数值完全不同。学习率(`eta`)是 0.4，`max_depth`是 1 而不是 3，`colsample_byTree`是 0.8 而不是 0.7，`nrounds`是 50 而不是 60。我们没有将这些值作为候选值传递给网格搜索。在随机搜索中，如果有更多的选项可供选择，与网格搜索相比，我们可能会有更好的结果。然而，这是以*更长的计算时间*为代价的。

此外，当前使用的数据集很小(约 800 个样本)。随着用例变得更加引人注目(有更多的特性和更多的数据)，随机搜索和网格搜索之间的性能差异可能会更大。

根据经验，强烈建议选择随机搜索而不是网格搜索，尤其是在我们对问题空间的判断无关紧要的情况下。

## 贝叶斯优化

网格搜索和随机搜索的主要权衡之一是，这两种技术都不跟踪用于模型训练的超参数组合的过去评估。理想情况下，如果在该路径中引入一些人工智能，其可以指示具有所选超参数列表上的历史性能的过程以及通过在正确的方向上推进迭代来提高性能的机制，那么这将极大地减少找到超参数的最佳值集所需的迭代次数。然而，网格搜索和随机搜索错过了这一点，并且遍历所有提供的组合，而不考虑来自先前迭代的任何线索。

使用**贝叶斯优化**，我们通过开发一个概率模型，将超参数映射到为机器学习模型选择的损失函数(目标函数)的概率得分，使调整过程能够跟踪以前的迭代及其评估，从而克服了这种权衡。这个概率模型也被称为机器学习模型中主要损失函数的**替代模型**，相比之下，比损失函数更容易优化。

讨论这个过程的数学背景和推导将超出本章的范围。超参数调整的贝叶斯优化的整个过程可以简化如下:

*   定义一个代理模型(超参数到损失函数的概率映射)。
*   找到代理模型的最佳参数。
*   将参数应用于主要损失函数，并基于结果在正确的方向上更新模型。
*   重复进行，直到达到定义的迭代性能。

使用这个简单的框架，贝叶斯优化在大多数情况下以最少的迭代次数提供了理想的超参数集。由于这种方法跟踪过去的迭代和相关的性能，随着数据量的增加，它的实践会更加准确。贝叶斯方法使用起来更有效、更实用，因为它们在许多方面都像人脑一样运作；对于要执行的任何任务，我们都试图理解对世界的最初看法，然后我们基于新的经验来提高我们的理解。贝叶斯超参数优化利用了相同的基本原理，并使用明智的决策实现了调整模型超参数的优化路径。伯格斯特拉等人的一篇论文。(http://proceedings.mlr.press/v28/bergstra13.pdf)巧妙地解释了贝叶斯优化优于随机搜索优化的优势。

贝叶斯优化中有几种技术可以应用于机器学习领域中的超参数调整。贝叶斯方法的一种流行的形式化方法是**基于顺序模型的优化** ( **SMBO** )，其中也有一些变化。SMBO 中的每种方法都基于代理模型的定义方式以及用于评估和更新参数的标准而有所不同。替代模型的几个流行选择是**高斯过程**、**随机森林回归**和**树 Parzen 估计量** ( **TPE** )。类似地，基于每次连续迭代的标准在优化过程中进行评估，并使用 **UCB** ，即 GP **置信上限**，例如**预期改善** (EI)或**改善概率** ( **POI** )。

为了在 R 中实现贝叶斯优化，我们已经有了一些编写良好的库，它们为我们抽象了整个过程。`MlBayesOpt`是由 Yuya Matsumura 在 r 中实现的一个流行的包。它基于 SMBO 中的**高斯过程**进行贝叶斯优化，并允许使用几个函数来更新代理模型。

以下练习对随机森林模型的`mtry`和最小节点大小超参数执行贝叶斯优化。优化过程的输出返回为每个超参数评估的最佳组合。然后，我们需要对超参数值使用相同的组合来实现一个常规模型。

#### 贝叶斯优化:

https://towardsdatascience.com/the-intuitions-behind-bayesian-optimization-with-gaussian-processes-7e00fcc898a0

https://papers.nips.cc/paper/7838-automating-bayesian-optimization-with-bayesian-optimization.pdf

https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f

### 练习 96:对随机森林模型执行贝叶斯优化

对同一数据集执行贝叶斯优化并研究输出。在这种优化技术中，我们将利用贝叶斯优化，通过迭代先前迭代的知识/上下文，直观地为`mtry`超参数选择最佳值。

本练习的目的是为随机森林模型执行贝叶斯优化。

执行以下步骤:

1.  首先，使用下面的命令将`seed`设置为`2019`:

    ```
    set.seed(2019)
    ```

2.  使用以下命令导入`MlBayesOpt`库:

    ```
    library(MlBayesOpt)
    ```

3.  Perform Bayesian optimization for random forest model using the `rf_opt` function from the `MlBayesOpt` package:

    ```
    model_rf_bayesain<- rf_opt(train_data = train,
    train_label = diabetes,
    test_data = test,
    test_label = diabetes,
    mtry_range = c(1L, ncol(df)-1),
    num_tree = 50,
    init_points = 10,
    n_iter = 10,                       
    acq = "poi", eps = 0, 
    optkernel = list(type = "exponential", power =2))
    ```

    输出如下所示:

![Figure 7.13: Output for Bayesian optimization for random forest
](img/C12624_07_13.jpg)

###### 图 7.13:随机森林贝叶斯优化的输出

输出显示为模型评估计算的迭代次数，并在每次迭代结束时返回结果。可以通过增加`n_iter`的值来增加迭代次数。`init_points`变量定义了在贝叶斯优化符合高斯过程之前随机选择的采样目标函数的点的数量。此外，我们将标准函数定义为`eps`参数是一个附加参数，可用于调整 **EI** 和 **POI** ，以平衡开采和勘探，增加ε将使优化的超参数在整个范围内更加分散。

最后，我们还定义了核，即基本高斯过程的相关函数。该参数应该是指定相关函数类型以及平滑度参数的列表。常见的选择是平方指数(默认)或`Matern 5/2`。

贝叶斯优化过程的结果返回了`mtry`的最佳值 6.12(这需要被截断为 6，因为**随机森林**中的`mtry`不接受小数值)和最佳最小节点大小 17。我们可以在随机森林的常规实现中使用这些参数设置，并评估模型结果。

现在让我们在同一个用例上对 XGBoost 模型使用同样的方法。

### 练习 97:使用 XGBoost 执行贝叶斯优化

与前面的练习类似，我们将对同一个数据集执行贝叶斯优化并研究输出，尽管这次是针对 XGBoost 模型。假设 XGBoost 有一个更大的超参数集要优化，我们需要为优化过程中感兴趣的每个超参数提供一系列值。

要对 XGBoost 模型执行贝叶斯优化，请执行以下步骤:

1.  首先，使用下面的命令将`seed`设置为`2019`:

    ```
    set.seed(2019)
    ```

2.  Perform Bayesian optimization for the XGBoost model using the `xgb_opt` function from the `MlBayesOpt` package:

    ```
    model_xgb_bayesian<- xgb_opt(train, diabetes, test, diabetes,objectfun ='binary:logistic',evalmetric='logloss',eta_range = c(0.1, 1L), max_depth_range = c(2L, 8L),nrounds_range = c(70, 160L), bytree_range = c(0.4, 1L), init_points = 4, n_iter = 10, acq = "poi", eps = 0, optkernel = list(type = "exponential", power =2))
    ```

    输出如下所示:

![Figure 7.14: Output for Bayesian Optimization using XGBoost
](img/C12624_07_14.jpg)

###### 图 7.14:使用 XGBoost 的贝叶斯优化的输出

像 random forest 一样，`xgb_opt`函数返回带有候选值的超参数的最优列表。与**随机森林**不同，我们有一个更大的 XGBoost 超参数列表。我们需要为每个超参数定义贝叶斯优化过程的运行范围。

我们可以看到，来自贝叶斯优化的超参数的最佳组合与我们为`nrounds`发现的不同，当我们实现标准 XGBoost 树模型时，需要对`nrounds`进行截断(因为十进制值没有意义)，上面评估的参数即:`nrounds`、`max_depth`、`eta`和`subsample`。

在本章中，我们研究了各种交叉验证技术来执行模型评估。对 k-fold 交叉验证的一个小的修正有助于我们执行模型性能的改进验证。在从 k-fold 转移到 LOOCV 之前，我们可以执行重复的 k-fold 来获得模型的更稳健的评估。该过程重复多次交叉验证，其中折叠在每次重复中以不同的方式分开。执行重复的 k-fold 是比 LOOCV 更好的方法。

### 活动 12:执行重复的 K-Fold 交叉验证和网格搜索优化

在本练习中，我们将利用相同的数据集(如前面练习中所用)，训练一个随机森林模型，重复执行 k-fold 验证 10 次，并研究模型性能。在每次折叠迭代中，我们可以尝试超参数的不同网格值，并对最佳模型进行稳健验证。

活动的目的是在同一个模型上执行重复的 k-fold 交叉验证和网格搜索优化。

执行以下步骤:

1.  加载所需的包(`mlbench`、`caret`和`dplyr`)。
2.  将`PimaIndianDiabetes`数据集从`mlbench`包加载到内存中。
3.  为再现性设置种子值。
4.  使用`caret`包中的`trainControl`函数定义 k 倍验证对象，并将`method`定义为`repeatedcv`而不是`cv`。
5.  在`trainControl`函数中为`repeats = 10`验证中的重复次数定义一个附加结构。
6.  将随机森林模型的`mtry`超参数的网格定义为`(3,4,5)`。
7.  用网格值、交叉验证对象和随机森林分类器来拟合模型。
8.  Study the model performance by plotting the accuracy across different values of the hyperparameter.

    最终输出应该如下所示:

![Figure 7.15: Model performance accuracy across different values of the hyperparameter
](img/C12624_07_15.jpg)

###### 图 7.15:不同超参数值的模型性能精确度

#### 注意

这项活动的解决方案可在第 461 页找到。

## 总结

在本章中，您学习了模型性能改进技术的几个重要方面。我们从**偏差-方差权衡**开始，并理解它会影响模型的性能。我们现在知道，高偏差会导致拟合不足，而高方差会导致模型拟合过度，实现一个会以牺牲另一个为代价。因此，为了建立最好的模型，我们需要在机器学习模型中的偏差和方差之间取得理想的平衡。

接下来，我们探索了 R 中各种类型的交叉验证技术，这些技术提供了实现这些技术的现成函数。我们研究了用于交叉验证的维持、k-fold 和 hold-one-out 验证方法，并了解了如何对机器学习模型的性能进行稳健的评估。然后，我们研究了超参数调整，并详细探讨了网格搜索优化、随机搜索优化和贝叶斯优化技术。机器学习模型的超参数调整帮助我们开发出性能更好的更一般化的模型。

在下一章，我们将探索在云中部署机器学习模型的过程。