

# 三、随机森林装袋

在这一章中，你将熟练建立**随机森林**，XGBoost 的主要竞争对手。像 XGBoost 一样，随机森林是决策树的集合。不同的是，随机森林通过**套袋**结合树木，而 XGBoost 通过**提升**结合树木。随机森林是 XGBoost 的可行替代方案，本章重点介绍了它的优点和局限性。了解随机森林很重要，因为它们提供了对基于树的系综(XGBoost)结构的宝贵见解，并且它们允许在与自己的打包方法的比较和对比中更深入地理解 boosting。

在本章中，您将构建并评估**随机森林分类器**和**随机森林回归器**，掌握随机森林超参数，了解机器学习领域中的装袋，并探索一个案例研究，该案例研究强调了刺激梯度增强(XGBoost)发展的一些随机森林限制。

本章涵盖以下主要主题:

*   打包套装

*   探索随机森林

*   调整随机森林超参数

*   推动随机森林边界–案例研究

# 技术要求

本章的代码可在[https://github . com/packt publishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/chapter 03](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03)获得

# 打包套装

在本节中，您将了解为什么集成方法通常优于单个机器学习模型。此外，你将学习装袋的技术。这两者都是随机森林的基本特征。

## 集成方法

在机器学习中，集成方法是一种机器学习模型，它聚合了个单独模型的预测。由于集成方法结合了多个模型的结果，它们不容易出错，因此往往表现得更好。

假设你的目标是确定一栋房子在上市后的第一个月内能否卖出。你运行了几个机器学习算法，发现**逻辑回归**给出了 80%的准确率，**决策树**给出了 75%的准确率， **K 近邻**给出了 77%的准确率。

一种选择是使用逻辑回归，最准确的模型，作为您的最终模型。一个更有说服力的选择是将每个模型的预测结合起来。

对于分类器，标准选项是采取多数投票。如果三个模型中至少有两个预测房子会在第一个月内卖出，那么预测就是*是*。否则就是*没有*。

总体精度通常更高的整体方法。对于一个错误的预测，一个模型出错是不够的；大多数分类器肯定弄错了。

总体方法通常分为两种类型。第一种结合了不同的机器学习模型，比如 scikit-learn 的`VotingClassifier`，由用户选择。第二种集成方法结合了同一模型的多个版本，XGBoost 和 random forests 就是这种情况。

随机森林是所有集合方法中最流行和最广泛的。随机森林的个体模型是决策树，上一章的重点， [*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) ，*深度决策树*。一个随机森林可能由成百上千的决策树组成，这些决策树的预测被组合在一起以得到最终结果。

尽管随机森林对分类器使用多数规则，对回归器使用所有模型的平均值，但它们也使用一种叫做 bagging 的特殊方法(bootstrap aggregation 的缩写)来选择个体树。

## 引导聚合

**自举**是指带替换的采样。

想象你有一袋 20 颗着色的弹珠。你要选择 10 个弹珠，一次一个。每次你选择一个弹球，你就把它放回袋子里。这意味着有可能，尽管可能性极小，你可以选择同一个弹珠 10 次。

更有可能的是，你会不止一次地挑选一些弹珠，而有些则根本不会。

这是大理石的图像:

![Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg)](img/B15551_03_01.jpg)

图 3.1–打包的视觉演示(重绘自:Siakorn，Wikimedia Commons，[https://Commons . Wikimedia . org/wiki/File:Ensemble _ bagging . SVG](https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg))

从上图可以看出，自举样本是通过替换采样获得的。如果不更换弹珠，就不可能获得比原袋中黑色(*蓝色*在原图中)弹珠更多的样品，如最右边的方框所示。

当谈到随机森林时，bootstrapping 在引擎盖下工作。自举发生在每个决策树生成的时候。如果决策树全部由相同的样本组成，那么这些树将给出相似的预测，使得聚合结果与单个树相似。相反，对于随机森林，使用自举来构建树，通常使用与原始数据集中相同数量的样本。根据数学估计，每棵树的样本中有三分之二是唯一的，三分之一是重复的。

在模型构建的引导阶段之后，每个决策树都会做出自己的预测。结果是一个树的森林，其预测使用分类器的多数规则和回归器的平均值聚合成一个最终预测。

总之，随机森林聚集了自举决策树的预测。这种通用的集成方法在机器学习中被称为 bagging。

# 探索随机森林

为了更好地理解随机森林是如何工作的，让我们使用 scikit-learn 来构建一个。

## 随机森林分类器

让我们使用一个随机森林分类器来预测一个用户的收入是高于还是低于 50，000 美元，使用的是我们在 [*第一章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022) 、*机器学习前景*、以及在 [*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) 、*决策树深度*中重新讨论过的普查数据集。我们将使用`cross_val_score`来确保我们的测试结果具有良好的通用性:

以下步骤使用人口普查数据集构建随机森林分类器并对其进行评分:

1.  静音警告前导入`pandas`、`numpy`、`RandomForestClassifier`和`cross_val_score`:

    ```py
    import pandas as pd
    import numpy as np
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import cross_val_score
    import warnings
    warnings.filterwarnings('ignore')
    ```

2.  加载数据集`census_cleaned.csv`并将其分成`X`(预测列)和`y`(目标列):

    ```py
    df_census = pd.read_csv('census_cleaned.csv')
    X_census = df_census.iloc[:,:-1]
    y_census = df_census.iloc[:,-1]
    ```

    随着我们的导入和数据准备就绪，是时候构建模型了。

3.  接下来，我们初始化随机森林分类器。在实践中，集成算法就像任何其他机器学习算法一样工作。一个模型被初始化，适合训练数据，并且根据测试数据被评分。

    我们通过预先设置以下超参数来初始化随机森林:

    a) `random_state=2`确保您的结果与我们的一致。

    利用并行处理来加速计算。

    c) `n_estimators=10`，先前的 scikit-learn 缺省值，足以加速计算并避免歧义；新的默认值已设置`n_estimators=100`。`n_esmitators`将在下一节详细探讨:

    ```py
    rf = RandomForestClassifier(n_estimators=10, random_state=2, n_jobs=-1)
    ```

4.  现在我们将使用`cross_val_score`。`Cross_val_score`需要模型、预测值列和目标列作为输入。回想一下`cross_val_score`对数据进行拆分、拟合和评分:

    ```py
    scores = cross_val_score(rf, X_census, y_census, cv=5)
    ```

5.  显示结果:

    ```py
    print('Accuracy:', np.round(scores, 3))
    print('Accuracy mean: %0.3f' % (scores.mean()))
    Accuracy: [0.851 0.844 0.851 0.852 0.851]
    Accuracy mean: 0.850
    ```

默认的随机森林分类器为普查数据集提供了比 [*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) 、*深度决策树*中的决策树更好的分数(81%)，但不如 [*第一章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022) 、*机器学习景观* (86%)中的 XGBoost。为什么它比单个决策树表现更好？

性能的提高可能是由于上一节所述的装袋方法。这个森林中有 10 棵树(从`n_estimators=10`开始)，每个预测都基于 10 棵决策树，而不是 1 棵。树是自举的，这增加了多样性，并且是聚集的，这减少了差异。

默认情况下，随机森林分类器在寻找进行分割时，从特征总数的平方根中进行选择。所以，如果有 100 个特征(列)，每个决策树在选择拆分时只会考虑 10 个特征。因此，由于不同的分裂，具有重复样本的两棵树可能给出非常不同的预测。这是随机森林减少方差的另一种方式。

除了分类，随机森林还可以处理回归。

## 随机森林回归变量

在随机森林回归器中，样本是自举的，与随机森林分类器一样，但最大要素数是要素总数，而不是平方根。这个变化是实验结果造成的(见[https://orbi . uli ege . be/bitstream/2268/9357/1/geurts-mlj-advance . pdf](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf))。

此外，通过取所有树的预测的平均值而不是多数规则投票来进行最终预测。

要查看随机森林回归器的运行情况，请完成以下步骤:

1.  上传来自 [*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)*深度决策树*的自行车租赁数据集，并调出前五行进行复习:

    ```py
    df_bikes = pd.read_csv('bike_rentals_cleaned.csv')
    df_bikes.head()
    ```

    前面的代码应该会产生以下输出:

    ![Figure 3.2 – Bike rentals dataset – cleaned](img/B15551_03_02.jpg)

    图 3.2-自行车租赁数据集-已清理

2.  将数据分为`X`和`y`，预测列和目标列:

    ```py
    X_bikes = df_bikes.iloc[:,:-1]
    y_bikes = df_bikes.iloc[:,-1]
    ```

3.  导入回归变量，然后使用相同的默认超参数`n_estimators=10`、`random_state=2`和`n_jobs=-1`对其进行初始化:

    ```py
    from sklearn.ensemble import RandomForestRegressor
    rf = RandomForestRegressor(n_estimators=10, random_state=2, n_jobs=-1)
    ```

4.  现在我们需要使用`cross_val_score`。将回归变量`rf`以及预测值和目标列放在`cross_val_score`中。请注意，负均方误差(`'neg_mean_squared_error'`)应被定义为评分参数。选择 10 个折叠(`cv=10`):

    ```py
    scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)
    ```

5.  找到并显示**均方根误差** ( **RMSE** ):

    ```py
    rmse = np.sqrt(-scores)
    print('RMSE:', np.round(rmse, 3))
    print('RMSE mean: %0.3f' % (rmse.mean()))
    ```

    输出如下所示:

    ```py
    RMSE: [ 801.486  579.987  551.347  846.698  895.05  1097.522   893.738  809.284  833.488 2145.046]
    RMSE mean: 945.365
    ```

随机森林的表现相当不错，尽管不如我们见过的其他模型。我们将在本章后面的案例研究中进一步检查自行车租赁数据集，以了解原因。

接下来，让我们详细检查随机森林超参数。

# 随机森林超参数

随机森林超参数的范围很大，除非已经掌握了决策树超参数的工作知识，如 [*第 2 章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047) 、*决策树深度*中所述。

在这一节中，在对您已经看到的超参数进行分组之前，我们将查看其他随机森林超参数。XGBoost 将使用许多这些超参数。

## oob_score

我们的第一个超参数，也许是最有趣的，是`oob_score`。

随机森林通过装袋选择决策树，这意味着用替换选择样本。在选择了所有的样本之后，应该保留一些没有被选择的样本。

可以保留这些样本作为测试集。模型适合一棵树后，可以立即根据该测试集对模型进行评分。当超参数设置为`oob_score=True`时，就会发生这种情况。

换句话说，`oob_score`提供了一个获取测试分数的捷径。`oob_score`可以在模型拟合后立即打印出来。

让我们在人口普查数据集上使用`oob_score`来看看它在实践中是如何工作的。因为我们使用`oob_score`来测试模型，所以没有必要将数据分成训练集和测试集。

随机森林可以照常用`oob_score=True`初始化:

```py
rf = RandomForestClassifier(oob_score=True, n_estimators=10, random_state=2, n_jobs=-1)
```

接下来，`rf`可能会被拟合上数据:

```py
rf.fit(X_census, y_census)
```

从`oob_score=True`开始，在模型被拟合之后，分数是可用的。可以使用模型属性`.oob_score_`访问它，如下所示(注意`score`后面的下划线):

```py
rf.oob_score_
```

分数如下:

```py
0.8343109855348423
```

如前面的所述，`oob_score`是通过对在训练阶段排除的单棵树上的样本进行评分而创建的。当森林中的树木数量很少时，如 10 个估计者的情况，可能没有足够的测试样本来最大限度地提高准确性。

更多的树意味着更多的样本，通常更准确。

## n _ 估计量

当森林里有很多树的时候，森林是强大的。多少才够？最近，scikit-learn 默认值从 10 更改为 100。虽然 100 棵树可能足以减少方差并获得好的分数，但对于较大的数据集，可能需要 500 或更多的树。

让我们从`n_estimators=50`开始，看看`oob_score`是如何变化的:

```py
rf = RandomForestClassifier(n_estimators=50, oob_score=True, random_state=2, n_jobs=-1)
rf.fit(X_census, y_census)
rf.oob_score_
```

分数如下:

```py
0.8518780135745216
```

明显的进步。100 棵树呢？

```py
rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=2, n_jobs=-1)
rf.fit(X_census, y_census)
rf.oob_score_
```

分数如下:

```py
0.8551334418476091
```

增益较小。随着`n_estimators`继续上升，分数最终会趋于平稳。

## 暖机 _ 启动

`warm_start`超参数用于确定森林中的树木数量(`n_estimators`)。当`warm_start=True`出现时，增加更多的树并不需要从头开始。如果你把`n_estimators`从 100 改成 200，用 200 棵树建造森林可能要花两倍的时间。当`warm_start=True`出现时，拥有 200 棵树的随机森林并不是从零开始，而是从上一个模型停止的地方开始。

`warm_start`可用于绘制范围为`n_estimators`的各种分数。

例如，以下代码以 50 棵树为增量，从 50 棵树开始，到 500 棵树结束，以显示分数范围。这段代码可能需要一些时间来运行，因为它通过每轮添加 50 棵树来构建 10 个随机森林！代码分为以下几个步骤:

1.  导入 matplotlib 和 seaborn，然后用`sns.set()`设置 seaborn 暗格:

    ```py
    import matplotlib.pyplot as plt
    import seaborn as sns
    sns.set()
    ```

2.  初始化一个空的分数列表，并用 50 个估计值初始化一个随机森林分类器，确保`warm_start=True`和`oob_score=True`:

    ```py
    oob_scores = []
    rf = RandomForestClassifier(n_estimators=50, warm_start=True, oob_score=True, n_jobs=-1, random_state=2)
    ```

3.  将`rf`拟合到数据集，然后将`oob_score`追加到`oob_scores`列表:

    ```py
    rf.fit(X_census, y_census)
    oob_scores.append(rf.oob_score_)
    ```

4.  准备一个包含从 50 开始的树的数量的评估者列表:

    ```py
    est = 50
    estimators=[est]
    ```

5.  写一个 for 循环，每轮增加 50 棵树。对于每一轮，给`est`加 50，将`est`追加到`estimators`列表，将`n_estimators`换成`rf.set_params(n_estimators=est)`，将随机森林拟合到数据上，然后追加新的`oob_score_`:

    ```py
    for i in range(9):
        est += 50
        estimators.append(est)
        rf.set_params(n_estimators=est)
        rf.fit(X_census, y_census)
        oob_scores.append(rf.oob_score_)
    ```

6.  为了更好的展示，展示一个大图，然后画出估计值和`oob_scores`。添加适当的标签，然后保存并显示图表:

    ```py
    plt.figure(figsize=(15,7))
    plt.plot(estimators, oob_scores)
    plt.xlabel('Number of Trees')
    plt.ylabel('oob_score_')
    plt.title('Random Forest Warm Start', fontsize=15)
    plt.savefig('Random_Forest_Warm_Start', dpi=325)
    plt.show()
    ```

    此生成如下图形:

![Figure 3.3 – Random forest Warm Start – oob_score per number of trees](img/B15551_03_03.jpg)

图 3.3–随机森林热启动–每棵树数量的 oob 分数

正如你所看到的，树的数量在 300 棵左右达到顶峰。用 300 以上的树成本更高，更费时，收益充其量也是微乎其微。

## 自举

虽然随机森林传统上是自举的，但是`bootstrap`超参数可以设置为`False`。如果`bootstrap=False`，`oob_score`不能包括在内，因为`oob_score`只有在样本被遗漏时才可能出现。

我们不会追求这一选择，虽然它有意义，如果欠拟合发生。

## 啰嗦

建立模型时，`verbose`超参数可能会被更改为更高的数值，以显示更多信息。你可以自己尝试做实验。在构建大型模型时，`verbose=1`可能会沿途提供有用的信息。

## 决策树超参数

剩下的超参数全部来自决策树。事实证明，决策树超参数在随机森林中并不重要，因为随机森林通过设计减少了方差。

以下是根据类别分组的决策树超参数，供您查看。

### 深度

属于这一类别的超参数有:

*   `max_depth`:永远好调音。确定拆分发生的次数。称为树的长度。减少差异的好方法。

### 分裂

属于这一类别的超参数有:

*   `max_features`:限制分割时选择的个特征。

*   `min_samples_split`:增加新分割所需的样本数量。

*   `min_impurity_decrease`:限制分割，减少大于设定阈值的杂质。

### 离开

属于这一类别的超参数有:

*   `min_samples_leaf`:增加节点成为叶子所需的最小样本数。

*   `min_weight_fraction_leaf`:一片叶子所需的总重量的分数。

有关上述超参数的更多信息，请查看官方随机森林回归器文档:[https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . randomforestregressor . html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)

# 推动随机森林边界–案例研究

假设您在一家自行车租赁公司工作，您的目标是根据天气、一天中的时间、一年中的时间以及公司的发展来预测每天的自行车租赁数量。

在本章的前面，您实现了一个带有交叉验证的随机森林回归器，以获得 945 辆自行车的 RMSE。您的目标是修改随机森林以获得尽可能低的错误分数。

## 准备数据集

在这一章的前面，你下载了数据集`df_bikes`并把它分成了`X_bikes`和`y_bikes`。现在您正在进行一些严肃的测试，您决定将`X_bikes`和`y_bikes`分成训练集和测试集，如下所示:

```py
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)
```

## n _ 估计量

首先为`n_estimators`选择一个合理的值。回想一下，`n_estimators`可以增加到以提高精度，代价是计算资源和时间。

以下是 RMSE 使用各种`n_estimators`的`warm_start`方法的图表，使用了之前在*热启动*标题下提供的相同通用代码:

![Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees](img/B15551_03_04.jpg)

图 3.4-随机森林自行车租赁-每棵树的 RMSE

这个图表很有趣。随机森林提供了 50 个估计值的最佳得分。经过 100 次估计后，误差开始逐渐上升，这个概念将在后面重新讨论。

目前，使用`n_estimators=50`作为起点是明智的。

## 交叉值分数

根据前面的图表，自行车租赁的误差范围从 620 到 690，现在是时候看看数据集如何使用`cross_val_score`进行交叉验证了。回想一下，交叉验证的目的是将样本分成 *k* 个不同的折叠，并将所有样本作为不同折叠的测试集。由于所有样本都用于测试模型，`oob_score`将不起作用。

以下代码包含您在本章前面使用的相同步骤:

1.  初始化模型。

2.  使用`cross_val_score`对模型评分，将模型、预测值列、目标列、评分和折叠数作为参数。

3.  计算 RMSE。

4.  显示交叉验证分数和平均值。

代码如下:

```py
rf = RandomForestRegressor(n_estimators=50, warm_start=True, n_jobs=-1, random_state=2)
scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)
rmse = np.sqrt(-scores)
print('RMSE:', np.round(rmse, 3))
print('RMSE mean: %0.3f' % (rmse.mean()))
```

输出如下所示:

```py
RMSE: [ 836.482  541.898  533.086  812.782  894.877  881.117   794.103  828.968  772.517 2128.148]
RMSE mean: 902.398
```

这个分数比本章前面的分数要好。请注意，根据 RMSE 数组中的最后一项，最后一次折叠的误差要高得多。这可能是由于数据中的错误或异常值。

## 微调超参数

是时候创建一个超参数网格来使用`RandomizedSearchCV`微调我们的模型了。下面是一个使用`RandomizedSearchCV`显示沿的 RMSEs 以及平均分数和最佳超参数的函数:

```py
from sklearn.model_selection import RandomizedSearchCV
def randomized_search_reg(params, runs=16, reg=RandomForestRegressor(random_state=2, n_jobs=-1)):
    rand_reg = RandomizedSearchCV(reg, params, n_iter=runs, scoring='neg_mean_squared_error', cv=10, n_jobs=-1, random_state=2)
    rand_reg.fit(X_train, y_train)
    best_model = rand_reg.best_estimator_
    best_params = rand_reg.best_params_
    print("Best params:", best_params)
    best_score = np.sqrt(-rand_reg.best_score_)
    print("Training score: {:.3f}".format(best_score))
    y_pred = best_model.predict(X_test)
    from sklearn.metrics import mean_squared_error as MSE
    rmse_test = MSE(y_test, y_pred)**0.5
    print('Test set score: {:.3f}'.format(rmse_test))
```

下面是放置在新的`randomized_search_reg`函数中的超参数的初始网格，以获得第一批结果:

```py
randomized_search_reg(params={'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05],'min_samples_split':[2, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.1],'min_samples_leaf':[1,2,4,6,8,10,20,30],'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,2,4,6,8,10,20]})
```

输出如下所示:

```py
Best params: {'min_weight_fraction_leaf': 0.0, 'min_samples_split': 0.03, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.05, 'max_leaf_nodes': 25, 'max_features': 0.7, 'max_depth': None}
Training score: 759.076
Test set score: 701.802
```

这是一个重大改进。让我们看看是否可以通过缩小范围做得更好:

```py
randomized_search_reg(params={'min_samples_leaf': [1,2,4,6,8,10,20,30], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4], 'max_depth':[None,2,4,6,8,10,20]})
```

输出如下所示:

```py
Best params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 10}
Training score: 679.052
Test set score: 626.541
```

分数又提高了。

现在让我们增加运行次数，给`max_depth`更多的选项:

```py
randomized_search_reg(params={'min_samples_leaf':[1,2,4,6,8,10,20,30],'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,4,6,8,10,12,15,20]}, runs=20)
```

输出如下所示:

```py
Best params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 12}
Training score: 675.128
Test set score: 619.014
```

分数一直在变好。此时，基于前面的结果，进一步缩小范围可能是值得的:

```py
randomized_search_reg(params={'min_samples_leaf':[1,2,3,4,5,6], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.08, 0.10, 0.12, 0.15], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,8,10,12,14,16,18,20]})
```

输出如下:

```py
Best params: {'min_samples_leaf': 1, 'min_impurity_decrease': 0.05, 'max_features': 0.7, 'max_depth': 18}
Training score: 679.595
Test set score: 630.954
```

考试分数又回升了。此时增加`n_estimators`可能是个好主意。森林里的树越多，就越有可能实现小收益。

我们还可以将运行次数增加到`20`，以寻找更好的超参数组合。请记住，结果是基于随机搜索，而不是全网格搜索:

```py
randomized_search_reg(params={'min_samples_leaf':[1,2,4,6,8,10,20,30], 'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2], 'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],'max_depth':[None,4,6,8,10,12,15,20],'n_estimators':[100]}, runs=20)
```

输出如下所示:

```py
Best params: {'n_estimators': 100, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.1, 'max_features': 0.6, 'max_depth': 12}
Training score: 675.128
Test set score: 619.014
```

这是迄今为止取得的最好成绩。我们可以继续修补。通过足够的实验，测试分数可能会下降到 600 辆以下。但我们似乎也在 600 点左右见顶。

最后，让我们将我们的最佳模型放在`cross_val_score`中，看看结果与原始模型相比如何:

```py
rf = RandomForestRegressor(n_estimators=100,  min_impurity_decrease=0.1, max_features=0.6, max_depth=12, warm_start=True, n_jobs=-1, random_state=2)
scores = cross_val_score(rf, X_bikes, y_bikes, scoring='neg_mean_squared_error', cv=10)
rmse = np.sqrt(-scores)
print('RMSE:', np.round(rmse, 3))
print('RMSE mean: %0.3f' % (rmse.mean()))
```

输出如下所示:

```py
RMSE: [ 818.354  514.173  547.392  814.059  769.54   730.025  831.376  794.634  756.83  1595.237]
RMSE mean: 817.162
```

RMSE 可以追溯到`817`。分数比`903`好很多，但是比`619`差很多。这是怎么回事？

`cross_val_score`中的最后一次拆分可能有问题，因为它的分数是其他的两倍。让我们看看打乱数据是否有用。Scikit-learn 有一个随机播放模块，可从`sklearn.utils`导入，如下所示:

```py
from sklearn.utils import shuffle
```

现在，我们可以将数据混洗如下:

```py
df_shuffle_bikes = shuffle(df_bikes, random_state=2)
```

现在将数据分割成新的`X`和`y`，并再次用`cross_val_score`运行`RandomForestRegressor`:

```py
X_shuffle_bikes = df_shuffle_bikes.iloc[:,:-1]
y_shuffle_bikes = df_shuffle_bikes.iloc[:,-1]
rf = RandomForestRegressor(n_estimators=100,  min_impurity_decrease=0.1, max_features=0.6, max_depth=12, n_jobs=-1, random_state=2)
scores = cross_val_score(rf, X_shuffle_bikes, y_shuffle_bikes, scoring='neg_mean_squared_error', cv=10)
rmse = np.sqrt(-scores)
print('RMSE:', np.round(rmse, 3))
print('RMSE mean: %0.3f' % (rmse.mean()))
```

输出如下所示:

```py
RMSE: [630.093 686.673 468.159 526.676 593.033 724.575 774.402 672.63  760.253  616.797]
RMSE mean: 645.329
```

在混洗的数据中，最后一次拆分没有问题，得分比预期的高很多。

## 随机森林弊端

最终，随机森林受到其个别树木的限制。如果所有的树都犯同样的错误，那么随机森林就会犯这个错误。正如本案例研究中揭示的那样，在数据被打乱之前，随机森林无法显著改善错误，因为数据中的挑战是单个树无法解决的。

能够改进初始缺点的集成方法、将在未来回合中从树的错误中学习的集成方法可能是有利的。Boosting 的设计是为了从树在前几轮的错误中吸取教训。升压，尤其是梯度升压——下一章的重点——解决了这个问题。

最后，下图显示了在自行车租赁数据集中增加树的数量时(如果数据没有被打乱)，优化的随机森林回归变量和默认 XGBoost 回归变量的结果:

![](img/B15551_03_05.jpg)

图 3.5–将 XGBoost 默认模型与优化的随机森林进行比较

如你所见，随着树的数量增加，XGBoost 在学习方面做得更好。而且 XGBoost 型号连调都没调过！

# 总结

在这一章中，你学习了集合方法的重要性。特别是，您了解了 bagging，即引导、替换抽样和聚合的组合，将许多模型组合成一个模型。您构建了随机森林分类器和回归器。您使用`warm_start`超参数调整`n_estimators`，并使用`oob_score_`查找错误。然后，您修改了随机森林超参数来微调模型。最后，您检查了一个案例研究，在这个案例研究中，对数据进行混洗得到了非常好的结果，但是与 XGBoost 相比，向随机森林中添加更多的树并没有给未混洗的数据带来任何好处。

在下一章中，你将学习 boosting 的基本原理，这是一种从错误中学习的集合方法，随着更多树的加入，它将提高精确度。您将实现梯度增强来进行预测，从而为极端梯度增强(更好地称为 XGBoost)奠定基础。