# *第九章*:自然语言处理

从企业系统到社交网络，各种软件的用户每天都在创建万亿字节的文本数据。所有这些未经处理的数据隐藏着改善企业运作方式的惊人机会。

在本章中，我们将学习如何清理和处理我们的数据，以便为创建可用作创建机器学习模型的输入的特征做准备。

我们将在本章中讨论的主题如下:

*   自然语言处理
*   删除不需要的字符串
*   词干化和词汇化
*   `word_tokenizer`
*   文本特征提取

# 技术要求

Optimus 可以与多种后端技术一起处理数据，包括 GPU。对于 GPU，Optimus 用的是 **RAPIDS** ，需要 NVIDIA 卡。关于需求的更多信息，请到 [*第一章*](B17166_01_Final_SB_epub.xhtml#_idTextAnchor015) ，*嗨擎天柱 *GPU 配置*章节！*。

你可以在[https://github . com/packt publishing/Data-Processing-with-Optimus](https://github.com/PacktPublishing/Data-Processing-with-Optimus)找到本章的所有代码。

# 自然语言处理

**自然语言处理** ( **NLP** )是一个介于语言学和计算机科学之间的跨学科领域，主要与人工智能结盟。NLP 处理两种类型的数据——文本和音频——Optimus 更关注文本数据。

非结构化数据大多来自网络(例如，推文或脸书评论)，在进一步分析之前需要预处理。擎天柱可以在这里帮助你，因为它有你完成工作所需的主要功能。让我们先探讨一下如何删除不需要的字符串。

# 删除不需要的字符串

为了执行 NLP，我们需要确保数据没有任何特殊字符、音调符号、HTML 标签或任何其他可能使算法难以正常工作的内容。

为了看看 Optimus 能为你做什么，让我们加载一个带有文本的数据帧来帮助我们演示 Optimus 的特性:

```
df = op.load.csv("text.csv", sep=";")
```

这将加载我们专门从维基百科准备的文本。让我们看看它的内容:

```
text
(object)
"<a>http://google.com</a> <span>Transformers is an American and Japanese media franchise produced by the American toy company Hasbro and Japanese toy company Takara Tomy. It follows the battles of sentient, living autonomous robots, often the Autobots and the Decepticons, who can transform into other forms, such as vehicles and animals. The franchise encompasses toys, animation, comic books,
... 
```

现在我们知道了数据，让我们开始转换它。

## 剥离 HTML

从网站获取数据时，经常会发现 HTML 标签。因为 HTML 标签是在互联网浏览器中显示信息的一种方式，它们很少给大多数常见的 NLP 任务增加任何价值，所以我们倾向于删除它们。

要使用 HTML 标签，您可以用任何可用的标签将一些文本括起来。一些常见的标签是“div”、“a”和“spam”在这种情况下，让我们看看如何删除所有的 HTML 标签:

```
print(df.cols.strip_html("text"))
```

这将打印以下输出:

```
text
(object)
"http://google.com Transformers is an American and Japanese media franchise produced by the American toy company Hasbro and Japanese toy company Takara Tomy. It follows the battles of sentient, living autonomous robots, often the Autobots and the Decepticons, who can transform into other forms, such as vehicles and animals. The franchise encompasses toys, animation, comic books, 
...
```

正如你所看到的，HTML 标签`<a>`和`</a>`已经被移除。这会给我们更干净的价值观。让我们看看如何进一步改变我们的琴弦。

## 删除停用词

除了 URL、HTML 标签和特殊字符之外，还有其他对于情感分析或文本分类来说不重要的单词。像我、我、他、你等等这样的词只是增加了文本的大小而不影响结果，因此，我们应该去掉它们。

对于我们的任务，我们将使用 NTLK 或任何 NLP 库中预制的停用词集合。或者，我们可以根据任务创建我们的集合。

默认情况下，Optimus 会删除英语中的停用词。为此，您可以使用以下代码:

```
print(df.cols.remove_stopwords("name"))
```

这将删除文本中的停用词。例如，您可以看到诸如“is”、“an”、“and”和“by”之类的单词在以下代码中被删除:

```
text
(object)
"<>http://google.com</> <span>transformers american japanese media franchise produced american toy company hasbro japanese toy company takara tomy. Follows battles sentient, living autonomous robots, often autobots decepticons, transform forms, vehicles animals. Franchise encompasses toys, animation, comic books
…
```

如果要使用另一种语言，可以使用`language`参数。让我们看一个路过`"span"sh"`的例子:

```
df.cols.remove_stopwor"s("t"xt", langua"e="span"sh")
```

Optimus 依靠**自然语言工具包** ( **NLTK)** 得到停用词表。正如 NLTK 网站上所说，*是构建 Python 程序来处理人类语言数据的领先平台*。

要获得可用语言的列表，您可以执行以下命令:

```
from nltk.corpus import stopwords
print(stopwords.fileids())
```

这将打印以下输出:

```
['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']
```

如果您想获得特定语言的停用词列表，可以使用以下命令:

```
print(stopwords.words("english"))
```

这将为我们提供英语中每个停用词的列表，如下所示:

```
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', ...
```

我们还将把所有文本转换成小写，因为 Python 中的文本是区分大小写的。

停用词不是唯一可能污染我们数据的东西。在大多数情况下，URL 也应该从我们的数据中删除。让我们看看怎么做。

## 删除 URL

与 HTML 标签一样，几乎可以肯定的是，当从网页中获取数据时，您会发现**统一资源定位符**(**URL**)。URL 只是指向网络上某个位置的一段文本。

因为我们几乎总是想从文本中而不是从 URL 中获得洞察力，我们可以在 Optimus 中使用`remove_urls`函数删除它们，如下所示:

```
print(df.cols.remove_urls())
```

请记住，如果不使用列名，您将对整个数据帧应用该函数:

```
text
(object)
"<a></a><span>Transformers is an American and Japanese media franchise produced by the American toy company Hasbro and Japanese toy company Takara Tomy. It follows the battles of sentient, living autonomous robots, often the Autobots and the Decepticons, who can transform into other forms, such as vehicles and animals. The franchise encompasses toys, animation, comic books
...
```

如您所见，这将从文本中删除子串[http://google.com](http://google.com)。

去掉网址后，我们可以去掉其他字符。让我们看看怎么做。

## 删除特殊字符

特殊的字符如`.`(点号)和`;`(分号)通常不会增加任何值，所以我们可以删除它们。

Optimus 使用 Python 代码库中的符号，我们将使用以下命令导入这些符号:

```
import string
string.punctuation
```

这将打印以下输出:

```
 '!"#$%&\'()*+,-./:;<=>?@[\\]^_'{|}~'
```

如你所见，在这些符号之间是像`$`和`%`这样的符号。在我们的文本中，保留这些来表示钱或百分比可能是重要的，这在处理金融数据或统计数据时可能是有用的。

要删除特殊字符，只需要以下命令:

```
print(df.cols.remove_special_chars("text"))
```

这将打印以下输出:

```
text
(object)
ahttpgooglecoma spanTransformers is an American and Japanese media franchise produced by American toy company Hasbro and Japanese toy company Takara Tomy It follows the battles of sentient living autonomous robots often the Autobots and the Decepticons who can transform into other forms such as vehicles and animals The franchise encompasses toys animation comic books
...
```

如果出于某种原因，你想删除特定的字符，那么你总是可以依靠`replace`功能。要删除连字符，可以使用以下命令:

```
df.cols.replace("text", "-", "", "chars")
```

或者您可以使用一个列表来删除许多字符，如下所示:

```
df.cols.replace("text", ["-", "*"], "", "chars")
```

这将用空白替换`–`和`*`。

现在，我们已经从我们的价值观中删除了无用的文本，让我们通过扩展缩略词来使我们的文本同质化。

## 扩展收缩词

在我们的日常口头和书面交流中，我们很多人倾向于压缩常用词。一个例子是“你是”如何变成“你是”将宫缩转换成它们的自然形式将有助于我们获得进一步的洞察力。

首先，让我们为此示例创建另一个数据集:

```
df2 = op.create.dataframe({"text": ["I'll let you know when, it shouldn't take long. Don't rush."]})
```

然后，我们可以调用`expand_contrated_words`来扩展我们的缩略词，如下所示:

```
df2.cols.expand_contrated_words("text")
```

这将打印以下输出:

```
text
(object)
I will let you know when, it should not take long. Do not rush. 
```

如你所见，单词“我会”、“不应该”和“不要”分别被替换为“我会”、“不应该”和“不要”。

一旦我们扩展了收缩的单词，我们需要使用词干化和词汇化将单词减少到最小表达式。

# 词干化和词汇化

在任何文本中，以多种形式找到一个单词是很常见的。参见这些，例如:

*   卡车
*   卡车
*   卡车的
*   卡车

所有这些单词都有唯一的词根`Truck`。列表中的单词称为屈折变化。

以下引自维基百科:

在语法中，词形变化是对一个词的修饰，以表达不同的语法范畴，如时态、格、语态、体、人称、数、性和语气。词尾变化用前缀、后缀或中缀，或另一种内部修饰如元音变化来表达一个或多个语法范畴。

将一个字从它的屈折形式变为它的根形式称为**字规范化**。

在自然语言处理中，主要有两种技术可以实现这一点:**词干化**和**词汇化**。

## 炮泥

在做词干分析时，我们使用一种算法将单词简化为它的词干。这不是词汇化的情况，在词汇化中，我们使用语言的词根。

求词干有多种算法。Optimus 支持波特、兰卡斯特和雪球算法。让我们看看它是如何工作的。

要对 Optimus 中的列应用词干，可以使用以下命令:

```
df.cols.stem_verbs("text")
```

这将使用 Porter 返回带有词干动词的所有列(这是默认的 param 值):

```
text
(object)
transform american japanes media franchis produc american toy compani hasbro japanes toy compani takara tomi follow battl sentient live autonom robot often autobot decepticon transform form vehicl anim franchis encompass toy anim comic book video game film franchis began 1984 transform toy line compris transform mecha toy takara diaclon microman toylin rebrand western markets1 term gener 1 cover anim televis seri transform comic book seri name divid japanes british canadian spinoff respect sequel follow gener 2 comic book beast war Summary
...
```

一个改进的词干分析器方法是`snowball`词干分析器。与`porter`或`lancaster`词干分析器不同，它支持多种语言。

您可以使用`stemmer`参数来更改您想要使用的词干分析器。例如，要应用`snowball`词干分析器，请使用:

```
print(df.cols.stem_verbs("text", stemmer="snowball", language="english"))
```

这将返回以下输出:

```
text
(object)
transform american japanes media franchis produc american toy compani hasbro japanes toy compani takara tomi follow battl sentient live autonom robot often autobot decepticon transform form vehicl anim franchis encompass toy anim comic book video game film franchis began 1984 transform toy line compris transform mecha toy takara diaclon microman toylin rebrand western markets1 term generat 1 cover anim televis seri transform comic book seri name divid japanes british
...
```

雪球的好处在于它支持多种语言。由于雪球功能是使用`nltk`构建的，您可以通过导入`nltk`获得支持的语言的完整列表:

```
from nltk.stem import SnowballStemmer
print(", ".join(SnowballStemmer.languages)) 
```

我们将得到以下结果:

```
arabic, danish, dutch, english, finish, french, german, hungarian, italian, norwegian, portuguese, romanian, russian, spanish, swedish
```

现在让我们探索一下 Optimus 中三个可用词干分析器之间的区别。

### 转向过度和转向不足

为了更清楚地了解不同词干分析器算法的特性，让我们从三个方面来探究它们:支持的语言、速度和积极性，如下图中的所示:

![Figure 9.1 – Stemmer features
](img/Table_01.jpg)

图 9.1-斯特梅尔特征

一般来说，当你需要 stem 时，Snowball 是最合适的，因为它不太主动，速度更快，并且在许多语言中都可用。用它的创造者 Martin Porter 的话说，它是 Porter stemmer 的改进版本。

## 词汇化

引理化是将一个单词简化为其引理或规范形式的过程。例如，“播放”、“播放”和“已播放”将被简化为“播放”

简化形式是语言学的词根。为了与词干提取过程区分开来，请记住，单词是使用某种算法缩减的。

在 Optimus 中，可以调用`lemmatize_verbs`，如下面的代码所示:

```
df.cols.lemmatize_verbs("text")
```

这将产生以下输出:

```
text
(object)
transformer american japanese medium franchise produce american toy company hasbro japanese toy company takara tomy follow battle sentient living autonomous robot often autobot decepticon transform form vehicle animal franchise encompass toy animation comic book video game film franchise begin 1984 transformer toy line comprise transform mecha toy takara diaclone microman toyline
...
```

正如我们所看到的，单词“变形金刚”、“产生”和“跟随”分别被改成了“变形金刚”、“产生”和“跟随”。

既然我们已经把每个单词都简化成了它的最小表达式，那么让我们来标记每个单词。

# word_tokenizer

单词标记化是将大量文本样本拆分成单词的过程。这是 NLP 任务中的一项要求，其中每个单词都需要被捕获并接受进一步的分析，例如针对特定的情感对它们进行分类和分析。

在 Optimus 中，您只需要在`cols`访问器中调用`word_tokenizer`，如下面的代码所示:

```
print(df.cols.word_tokenize("text","tokens")["text"])
```

然后，您将获得每行的单词列表，如下所示:

```
text
(object)
['transformers', 'american', 'japanese', 'media', 'franchise', 'produced', 'american', 'toy', 'company', 'hasbro', 'japanese', 'toy', 'company', 'takara', 'tomy', 'follows', 'battles', 'sentient', 'living', 'autonomous', 'robots', 'often', 'autobots', 'decepticons', 'transform', 'forms', 'vehicles', 'animals', 'franchise', 'encompasses', 'toys', 'starting', '2019', 'incarnations', 'story', 'based', 'different', 'toy',
...
```

现在我们已经有了列表中的每个单词，让我们来探索如何为每个单词获取额外的信息。

## 词性标注

这是一个过程，在这个过程中，你根据一个词作为词类的角色来标记它。不同的词类包括名词、代词、动词、形容词、副词、介词、连词、感叹词等等。

句子中的每个单词都是一个词类。用这些信息标注这些单词就是所谓的词性标注。这包括名词、代词、动词、形容词、副词、介词、连词、感叹词以及这些部分的子类。

Optimus 将返回一个 Python 元组，根据检测到的单词类型添加以下常量之一。Optimus 使用`nltk`，所以每个单词可能的结果如下:

*   `CC`:并列连词
*   `CD`:基数
*   `DT`:限定词
*   `EX`:存在主义的那里(如“有”；把它想成“存在着”)
*   `FW`:外来词
*   `IN`:介词/从属连词
*   `JJ`:形容词(例如“大”)
*   `JJR`:形容词，比较级(例如，“更大”)
*   `JJS`:形容词，最高级(例如，“最大”)
*   `LS`:列表标记(例如“1”)
*   `MD`:模态(例如，“可能”或“将”)
*   `NN`:名词，单数(例如，“书桌”)
*   `NNS`:名词复数(例如，“书桌”)
*   `NNP`:专有名词，单数(例如，“哈里森”)
*   `NNPS`:专有名词，复数(例如，“美国人”)
*   `PDT`:预定者(例如，“所有的孩子”)
*   `POS`:所有格结尾(例如“父母的”)
*   `PRP`:人称代词(例如，“我”、“他”或“她”)
*   所有格代词(例如，“我的”、“他的”或“她的”)
*   `RB`:副词，非常(比如“默默”)
*   `RBR`:副词，比较级(例如“更好”)
*   `RBS`:副词，最高级(例如，“最好的”)
*   `RP`:分词(例如，“放弃”)
*   例如，去商店
*   `UH`:感叹词(例如“errrrrrrrm”)
*   `VB`:动词，基本形式(例如，“take”)
*   `VBD`:动词，过去式(例如“take”)
*   `VBG`:动词、动名词/现在分词(例如“taking”)
*   `VBN`:动词，过去分词(例如“taken”)
*   `VBP`:动词，唱。当前、非 3D(例如，“拍摄”)
*   `VBZ`:动词，第三人称唱。呈现(例如，“takes”)
*   `WDT`:Wh-限定词(例如，“which”)
*   `WP`:Wh-代词(例如，“谁”或“什么”)
*   `WP` $:所有格 wh-代词(例如，“谁的”)
*   `WRB`:Wh-动词 where，(例如，“when”)

要在 Optimus 中应用词性标注，请使用以下方法:

```
print(df.cols.pos("text"))
```

这将为每行中的每个单词提供一个元组，如下所示:

```
text
(object)
[('transformers', 'NNS'), ('american', 'JJ'), ('japanese', 'JJ'), ('media', 'NNS'), ('franchise', 'NN'), ('produced', 'VBN'), ('american', 'JJ'), ('toy', 'NN'), ('company', 'NN'), ('hasbro', 'VBZ')
...
```

下面总结了结果:

*   `'transformers'`和`'media'`被检测为`'NNS'`，意为“名词复数”
*   `'american'`和`'japanese'`被检测为`'JJ'`，意为“形容词”
*   `'produced'`被检测为`'VBN'`，意为“过去分词”。
*   `'toy'`和`'company'`被检测为`'NN'`，意为“名词，单数。”
*   `'hasbro'`被检测为`'VBZ'`，意为“第三人称单数。”

词性标注本身并不能解决任何特定的自然语言处理问题。然而，它是作为简化许多不同问题(如文本到语音转换和词义消歧)的先决条件而完成的。

## 应用转换

既然我们已经很好地理解了所有的功能，让我们一起应用它们。请记住，您可以链接这些函数，如下例所示:

```
(df 
    .cols.strip_html() 
    .cols.remove_urls() 
    .cols.remove_special_chars() 
    .cols.remove_stopwords() 
    .cols.lemmatize_verbs()
    .cols.num_to_words()
    .cols.word_tokenizer())
```

这将把我们的数据转换成:

```
text
(object)
['transformer', 'american', 'japanese', 'medium', 'franchise', 'produced', 'american', 'toy', 'company', 'hasbro', 'japanese', 'toy', 'company', 'takara', 'tomy', 'follows', 'battle', 'sentient', 'living', 'autonomous', 'robot', 'often', 'autobots', 'decepticons', 'transform', 'form', 'vehicle', 'animal', 'franchise', 'encompasses', 'toy', 'animation', 'comic', 'book', 'video', 'game', 'film', 'franchise', 'began', 'one', 'thousand', ',', 'nine', 'hundred', 'and', 'eighty-four', 'transformer',
...
```

既然我们已经标准化并从我们的字符串中移除了所有可能的噪声，现在是时候将我们的文本转换为可以用作创建我们的机器学习模型的输入的特征了。

# 从文本中提取特征

当在机器学习中使用文本时，我们需要将文本转换为机器学习算法可以理解的特征列表。这意味着我们需要将文本转换成数字。为了实现这一点，Optimus 可以使用两种方法:

*   一袋单词
*   TF-IDF

让我们看看如何在 Optimus 中使用这些方法。

## 包话

在单词包方法中，我们取出所有的单词，然后统计每个单词出现的次数。

统计每个单词的出现次数后，由于一个语料库可能有数百万个单词，因此选择文本中出现频率最高的单词会很有用，如下图所示:

![Figure 9.2 – Bag of words example
](img/B17166_09_1.jpg)

图 9.2–单词袋示例

要在 Optimus 中应用单词包，您可以使用以下代码:

```
_df = df.cols.bag_of_words("text")
```

这将返回一个大的 dataframe，其中所有字符串都是列名，每一行都有字数统计。因为浏览所有数据可能很困难，所以让我们使用以下代码打印前 10 个值:

```
_df.cols.names()[:10]
```

此将返回以下内容:

```
['1983',
 '1984',
 '1991',
 '2001',
 '2005',
 '2007',
 '2010s',
 '2018',
 '2019',
 '20th']
```

为了得到整个计数，我们可以使用下面的代码:

```
len(_df.cols.names())
```

这将返回列的总数:

```
139
```

要打印整个数据帧的样本，可以使用`_df`:

![Figure 9.3 – The displayed result of the dataframe in a notebook
](img/B17166_09_2.jpg)

图 9.3–笔记本中数据框的显示结果

如您所见，在这个新的数据帧中，单词按字母数字顺序排序，每个值有单词数。

我们来看看如何用 n-grams 扩充单词袋的版本。

### 在单字上使用二元模型或三元模型

单词袋方法的扩展版本是 n 元语法袋方法。一个 n 元语法就是任意一系列的 *n 个*记号(单词)。它可以是 1 克、2 克、3 克等等。

让我们看一个使用这段文字的例子:

```
from nltk import ngrams
sentence = 'Transformers is the most amazing TV series.'
```

对于 1-grams，它将输出具有一个值的元组:

```
print(list(ngrams(sentence.split(), 1)))
[('Transformers',), ('is',), ('the',), ('most',), ('amazing',), ('TV',), ('series.',)]
```

对于 2-gram，它将输出具有两个值的元组，其中元组的最后一个值是连续元组中的第一个值:

```
print(list(ngrams(sentence.split(), 2)))
[('Transformers', 'is'), ('is', 'the'), ('the', 'most'), ('most', 'amazing'), ('amazing', 'TV'), ('TV', 'series.')]
```

让我们看看 3 克会发生什么:

```
print(list(ngrams(sentence.split(), 3)))
[('Transformers', 'is', 'the'), ('is', 'the', 'most'), ('the', 'most', 'amazing'), ('most', 'amazing', 'TV'), ('amazing', 'TV', 'series.')
```

需要指出的是，n-grams 包比简单的单词包信息量更大，因为它捕捉到了关于每个单词上下文的更多信息。比如`('most', 'amazing', 'TV')`可以给你比仅仅`('amazing',)`更多的信息。然而，这是有代价的，因为 n 元语法包可以产生比单词包更大更稀疏的特征集(过滤方法有助于最小化这一点)。通常，n-gramss 是我们想要的最高值，因为使用超过 n-grams 的 n-gram 很少会因为稀疏性而提高性能。

要控制想要生成的 n-gram 的种类，只需使用如下代码所示的`ngram_range`参数:

```
df.cols.bag_of_words("text", ngram_range= 2)
```

这将产生一个数据帧，其列代表 2-gram，如下所示:

![Figure 9.4 – HTML representation of the result of the method called
](img/B17166_09_3.jpg)

图 9.4-调用方法的结果的 HTML 表示

正如我们所看到的，使用 2-grams 可以让我们计算短语而不仅仅是单词。这可以提供对数据的复杂洞察。

现在让我们看看 TF-IDF 是如何工作的。

### TF-IDF

另一种衡量文档中单词频率的方法是使用 TF-IDF。

TF-IDF 是一种数字统计，旨在反映一个词对集合或语料库中的文档有多重要。TF-IDF 为每个单词返回 0 到 1 之间的值。出现在许多文档中的单词的值更接近于零，出现在较少文档中的单词的值更接近于 1。

对于这个例子，让我们加载一个包含几行的文件。每一行代表 TF-IDF 中的一个文档:

```
df = op.load.file("text-e.csv")
```

我们来看看数据。以下命令将打印数据集简历:

```
print(df)
```

数据集简历如下所示:

```
text(object)
"<a>http://google.com</a> <span>that'll Transformers is an American and Japanese media franchise produced by American toy company Hasbro and Japanese toy company Takara Tomy. It follows the battles of sentient, living autonomous robots, often the Autobots and the Decepticons, who can transform into other
forms, such as vehicles and animals. The franchise encompasses toys, animation, comic books, video games and films. The franchise began in 1984 with the Transformers toy line, comprising transforming mecha toys from ...
```

在 Optimus 中应用 TF-IDF 就像调用`tf_idf`方法一样简单:

```
df.cols.tf_idf("text").to_dict()
```

这将返回一个包含许多值的字典:

```
{'1980s': [0.0, 0.07302266125566928],
 '1983': [0.028561620660523312, 0.0],
 '1984': [0.02032182833693315, 0.05195622490690069],
 '1991': [0.028561620660523312, 0.0],
 '1993': [0.0, 0.07302266125566928],
 '2001': [0.028561620660523312, 0.0],
 '2005': [0.028561620660523312, 0.0],
 '2007': [0.057123241321046625, 0.0],
 '2010s': [0.028561620660523312, 0.0],
 '2018': [0.028561620660523312, 0.0],
 '2019': [0.028561620660523312, 0.0],
 '20th': [0.028561620660523312, 0.0],
 'able': [0.0, 0.07302266125566928],
 'action': [0.057123241321046625, 0.0],
 'after': [0.057123241321046625, 0.0],
 'afterward': [0.0, 0.07302266125566928],
 'again': [0.057123241321046625, 0.0],
 'aligned': [0.028561620660523312, 0.0],
 'also': [0.0, 0.07302266125566928],
 'alternate': [0.028561620660523312, 0.0],
 ...
```

在这个案例中，我们使用`5to_dict`来更好地洞察数据。对于像这样的大数据帧，记住您可以过滤掉一些特定的列。假设我们想要过滤`toys`和`1984`列:

```
df.cols.tf_idf("text")[["1984","toys"]].print()
```

这将打印以下内容:

```
       1984         toys
  (float64)    (float64)
-----------  -----------
  0.0203218    0.0406437
  0.0519562    0.0519562
```

正如我们所看到的，输出只包括传递给括号的两列。

通过使用 TF-IDF，我们可以从我们的数据中获得有用的信息。例如，我们可以用它来找出一个主题在一个网站的评论区相对于其他主题有多受欢迎。

# 总结

在这一章中，我们介绍了 Optimus 中所有可以轻松清理和准备文本数据的功能，这样您就可以开始您的 NLP 之旅，从简单的操作，如删除停用词和 URL，到更高级的操作，如词干和词汇化。

我们学习了如何对数据集中的文本进行标记和标记，以便有效地从中获取信息。

之后，我们探索了几种从文本中获取特征的方法。我们看到了如何使用单词包和 TF-IDF 将文本转换为可以用作机器学习算法输入的数字。

在下一章，我们将介绍我们认为是 Optimus 最先进的特性，比如实现你的引擎，创建自定义数据转换函数，甚至绘图功能。