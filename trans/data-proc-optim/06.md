# *第 4 章*:合并、整形、聚合数据

当我们必须同时处理多个数据集时，重要的是要有合适的工具，允许我们将所述数据集组合成同质和统一的数据集。正如我们在前面章节中看到的，Optimus 为我们提供了转换操作，允许我们准备一个格式与另一个不一致的数据集，以便我们可以在以后正确地组合它们。一旦转换完成，就有可能以各种方式将它们组合起来，比如通过连接或联合。

在本章中，我们将学习如何使用 Optimus 连接和合并多个数据集，并回顾更复杂的变换，如整形和旋转。最后，我们将学习如何聚合数据，以及如何对一组特定的数据应用聚合。

其中一些概念可能已经为那些来自关系数据库世界的人所熟知。如果你是一个新手，那么不要担心——我们将尝试使用一些图形支持来解释这个概念，让你尽可能快和容易地步入正轨。

我们将在本章中讨论的主题如下:

*   串联数据
*   连接数据
*   重塑和旋转
*   聚集和分组

# 技术要求

Optimus 可以与多种后端技术一起处理数据，包括 GPU。对于 GPU，Optimus 用的是 **RAPIDS** ，需要 NVIDIA 卡。更多关于这方面的要求，请到 [*第一章*](B17166_01_Final_SB_epub.xhtml#_idTextAnchor015) *的 *GPU 配置*部分，嗨擎天柱！*。

你可以在 https://github . com/packt publishing/Data-Processing-with-Optimus 找到本章的所有代码。

# 连接数据

我们称之为连接两个数据集的过程，无论我们是获取它们的列并将其包含在另一个中，还是合并它们的行，结果得到的数据集的行数等于要连接的数据集的行数之和。

让我们看一个行连接的例子:

```
df_a = op.create.dataframe({
    "id": [143, 225, 545],
    "name": ["Alice", "Bob", "Charlie"],
    "city": ["Plymouth", "Bradford", "Norwich"]
})
df_b = op.create.dataframe({
    "id": [765, 329, 152],
    "name": ["Dan", "Erin", "Frank"],
    "city": ["Bath", "Manchester", "Ripon"]
})
```

`df_a`和`df_b`可以连接如下:

```
df_a.rows.append(df_b).print()
```

这会产生以下输出:

```
id        name       city
(int64)   (object)   (object)
---------  ----------  ----------
143       Alice      Plymouth
225       Bob        Bradford
545       Charlie    Norwich
765       Dan        Bath
329       Erin       Manchester
152       Frank      Ripon
```

在前面的示例中，我们可以看到数据集是如何组合的，从而产生第三个数据帧，该数据帧具有更多的行，但列数相同。这是因为输入数据集的列是重合的。

但是如果我们从`df_b`中省略一个专栏会怎么样呢？

```
df_b = df_b.cols.drop("city")
df_a.rows.append(df_b).print()
```

我们将得到以下结果:

```
id        name       city
(int64)   (object)   (object)
--------- ---------- ----------
143       Alice      Plymouth
225       Bob        Bradford
545       Charlie    Norwich
765       Dan        nan
329       Erin       nan
152       Frank      nan
```

可以看到，所有缺失的值都用`nan`填充。如果数据集具有不同的列名，也会出现类似的结果:

```
df_b = df_b.cols.rename("city", "city_b")
df_a.rows.append(df_b).print()
```

我们会得到下面的结果:

```
id        name       city       city_b
(int64)   (object)   (object)   (object)
--------- ---------- ---------- ----------
143       Alice      Plymouth   nan
225       Bob        Bradford   nan
545       Charlie    Norwich    nan
765       Dan        nan        Bath
329       Erin       nan        Manchester
152       Frank      nan        Ripon
```

现在我们已经知道了如何连接两个具有相同列名的数据帧的基础知识，我们将学习如何映射两个不同数据帧的列，以便我们可以连接它们而无需重命名。

## 映射

当我们有两个列名不同的相同数据集时，我们可以传递第二个名为`map`的参数，它允许我们将每个输入数据帧的列映射到一组输出列。

让我们看一个例子:

```
df_a = op.create.dataframe({
    "id": [143, 225, 545],
    "name": ["Alice", "Bob", "Charlie"],
    "city": ["Plymouth", "Bradford", "Norwich"]
}) 
df_b = op.create.dataframe({
    "id_number": [765, 329, 152],
    "name": ["Dan", "Erin", "Frank"],
    "title": ["Bath", "Manchester", "Ripon"]
})
```

`df_a`和`df_b`可以连接如下:

```
names_map = {
    "id_number": ("id", "id_number"),
    "name": ("name", "name"),
    "city": ("city", "title")
}
df_a.rows.append(df_b, names_map=names_map).print()
```

这给了我们以下结果:

```
id_number   name       city
(int64)     (object)   (object)
-----------  ----------  ----------
143         Alice      Plymouth
225         Bob        Bradford
545         Charlie    Norwich
765         Dan        Bath
329         Erin       Manchester
152         Frank      Ripon
```

在前面的示例中，我们在连接之前声明了一个自定义映射，将来自`df_a`的`id`列和`df_b`的`title`列的所有值设置为结果 dataframe 中具有不同名称的列。

但是如果我们想在列轴上连接呢？我们接下来会看这个。

## 连接列

通过连接列，我们可以获得一个数据帧的列，并将它们放入另一个数据帧。这只是将一个数据集中的列放入另一个数据集中，为两个数据集的值保持相同的索引。

要将一个数据集中的列连接到另一个数据集中，只需调用`df.cols.concat(df_other)`，如下所示:

```
df_a = op.create.dataframe({
    "id": [143, 225, 545],
    "name": ["Alice", "Bob", "Charlie"],
    "city": ["Plymouth", "Bradford", "Norwich"]
})
df_b = op.create.dataframe({
    "age": [25, 35, 45],
    "placeholder": ["foo", "bar", "baz"]
})
df_a.cols.concat(df_b).print()
```

这会产生以下输出:

```
id        name       city       age       placeholder
(int64)   (object)   (object)   (int64)   (object) 
--------- ---------- ---------- --------- ------------- 
143       Alice      Plymouth   25        foo 
225       Bob        Bradford   35        bar 
545       Charlie    Norwich    45        baz
```

您可以使用`cols.select`过滤每个数据集的列，如下所示:

```
df_a.cols.select(["id", "name"]).cols.concat(df_b.cols.select([
"age"])) .print()
```

这会产生以下输出:

```
id        name       age       
(int64)   (object)   (int64)   
--------- ---------- --------- 
143       Alice      25        
225       Bob        35        
545       Charlie    45        
```

当我们的数据没有被正确组织时，它会创建错误配对值的行。在这种情况下，我们可以在两个数据集中使用一个公共列，告诉我们什么值对应于什么行。在这种情况下，我们可以使用名为 join 的操作。

# 加入数据

join 操作用于将条目从一个数据源合并到另一个数据源，使用一个公共列作为正确配对数据的键。联接的概念在数据库技术中很常见，在数据库技术中，我们还可以看到不同类型的联接，如内部联接、外部联接、左联接和右联接。下图更好地展示了这些连接:

![Figure 4.1 – Inner, outer, left, and right joins
](img/B17166_04_1.jpg)

图 4.1–内部、外部、左侧和右侧连接

当连接数据时，我们必须识别两个数据帧的关键列。让我们看一个例子:

```
df_a = op.create.dataframe({
    "id": [143, 225, 545, 765, 152],
    "name": ["Alice", "Bob", "Charlie", "Dan", "Frank"] 
}) 
df_b = op.create.dataframe({
    "id": [225, 545, 765, 152, 329],
    "city": ["Bradford", "Norwich", "Bath", "Ripon", "Manchester"],
    "placeholder": ["BRA", "NOR", "BAT", "RIP", "MAN"]
})
```

在两个数据集中，我们都有一个名为`"id"`的列，其值相等，但顺序不同。调用`df.cols.join`需要我们输入带有列标题的`on`参数，如下:

```
df_a.cols.join(df_b, how="outer", on="id").print()
```

我们将得到以下结果:

```
id         name       city        placeholder 
(object)   (object)   (object)    (object) 
---------- ---------- ----------- ------------- 
143        Alice      nan         
225        Bob        Bradford    BRA 
545        Charlie    Norwich     NOR 
765        Dan        Bath        BAT 
152        Frank      Ripon       RIP 
329        nan        Manchester  MAN
```

如我们所见，即使两个数据集都没有排序，结果也包含正确的数据。至于`nan`。正如我们所看到的，我们向`how`提供了“外部”，这意味着它将包括来自两个数据集的所有值。

默认情况下，`join`函数在`how`参数中使用`"left"`。让我们看一个左连接的例子(不包括该参数的值),这样我们可以省略最后一行中对应于`"name"`的`nan`值:

```
df_a.cols.join(df_b, on="id").print()
```

我们将得到以下结果:

```
id         name       city        placeholder 
(object)   (object)   (object)    (object) 
---------- ---------- ----------- ------------- 
143        Alice      nan        
225        Bob        Bradford    BRA 
545        Charlie    Norwich     NOR 
765        Dan        Bath        BAT 
152        Frank      Ripon       RIP
```

但是，`city`和`placeholder`列上仍然有`nan`值。为了省略它们，我们可以在`how`上使用`inner`，这是另一种连接操作，将省略两个数据集中所有缺失的行:

```
df_a.cols.join(df_b, on="id", how="inner").print()
```

我们将得到下面的结果:

```
id         name       city        placeholder 
(object)   (object)   (object)    (object) 
---------- ---------- ----------- ------------- 
225        Bob        Bradford    BRA 
545        Charlie    Norwich     NOR 
765        Dan        Bath        BAT 
152        Frank      Ripon       RIP
```

当每个数据帧的键列有不同的名称时，可以使用`left_on`和`right_on`代替`on`参数，如下所示:

```
df_a.cols.join(df_b, left_on="id_a", right_on="id_b")
```

默认情况下，结果将被排序；也就是说，首先放置键列，然后是左边的列，最后是右边的列，如第一个示例所示。如果想要将键列放在左数据集中的列和右数据集中的列之间，您需要将`True`传递给`key_middle`:

```
df_a.cols.join(df_b, left_on="id_a", key_middle=True).cols.names()
```

在前面的示例中，我们正在连接数据集并获取结果数据集中的列名，这将产生以下结果:

```
["name", "city", "id", "placeholder"]
```

默认情况下，结果数据集中列的顺序将改为`["id", "name", "city", "placeholder"]`。

既然我们已经知道了如何以各种方式合并两个数据集，那么让我们来学习如何对需要这种转换的数据框架进行整形和透视。

# 重塑和旋转

在某些情况下，你会想对你的数据集做一些更彻底的转换。在这一节中，我们将学习如何以各种方式重塑 Optimus 数据框架，包括旋转、锁定和熔化。

## 旋转

旋转是将堆叠的数据改造成具有更简单和更不详细数据的新数据框架的过程。它包括使用所选列的数据，并将其用作列标签。然后，您必须使用一个或多个列对数据进行分组，并使用首选的汇总或聚合对其余数据进行计算。以下是这方面的一个例子:

![Figure 4.2 – How pivoting works
](img/B17166_04_2.jpg)

图 4.2-旋转如何工作

让我们来看一个例子，它是一个在很短的时间内制作的销售数据集:

```
df = op.create.dataframe({
    "date": ["1/1/21", "1/1/21", "1/2/21", "1/2/21", "1/3/21", "1/3/21", "1/3/21", "1/3/21", "1/3/21"], 
    "product": ["Coffee", "Coffee", "Tea", "Coffee", "Tea", "Coffee", "Tea", "Tea", "Coffee"],
    "size": ["big", "big", "big", "big", "big", "small", "small", "small", "small"],
    "price": [1.5, 1.5, 2, 1.5, 2, 1, 1.25, 1.25, 1]
})
```

我们的数据集看起来像这样:

```
date        product     size              price
(object)    (object)    (object)      (float64)
----------  ----------  ----------  -----------
1/1/21      Coffee      big                1.5
1/1/21      Coffee      big                1.5
1/2/21      Tea         big                2
1/2/21      Coffee      big                1.5
1/3/21      Tea         big                2
1/3/21      Coffee      small              1
1/3/21      Tea         small              1.25
1/3/21      Tea         small              1.25
1/3/21      Coffee      small              1
```

现在，让我们统计一下每件产品每天的销售额:

```
df.pivot("date", groupby="product").print()
```

我们将得到以下输出:

```
product        1/1/21     1/2/21     1/3/21
(object)      (int64)    (int64)    (int64)
----------  ---------  ---------  ---------
Coffee              2          1          2
Tea                 0          1          3
```

如我们所见，默认的聚合是计数。我们可以使用`agg`参数显式调用它，这需要一个具有聚合名称和要计算的列的元组，如下所示:

```
df.pivot("date", groupby="product", agg=("count", "date")).print()
```

前面的代码将得到与之前相同的输出。

现在，让我们将`agg`参数用于其他目的:

```
df.pivot("date", groupby="product", agg=("common", "size")).print()
```

我们将得到以下输出:

```
product     1/1/21      1/2/21      1/3/21
(object)    (object)    (object)    (object)
----------  ----------  ----------  ----------------
Coffee      big                 small
Tea         nan         big         ['big', 'small']
```

在前面的示例中，我们从每个日期的销售额中获取每个产品的最常见尺寸。

在下面的例子中，我们看的是每种产品的平均价格:

```
df.pivot("date", groupby="product", agg=("mean", "price")).print()
```

我们将得到以下输出:

```
product          1/1/21       1/2/21       1/3/21
(object)      (float64)    (float64)    (float64)
----------  -----------  -----------  -----------
Coffee              1.5          2            1
Tea                 nan          1.5          1.5
```

也可以按不同的列而不是一个列进行分组，如下所示:

```
df.pivot("date", groupby=["product", "size"]).print()
```

我们将得到以下输出:

```
product     size           1/1/21     1/2/21     1/3/21
(object)    (object)      (int64)    (int64)    (int64)
----------  ----------  ---------  ---------  ---------
Coffee      big                 2          1          0
Coffee      small               0          0          2
Tea         big                 0          1          1
Tea         small               0          0          2
```

旋转是堆叠逆过程的特殊情况。让我们来学习什么是堆叠以及如何使用它。

## 堆叠

堆叠数据集时，列标签将被传递给虚拟列中的值，并被移动以匹配列值，如下所示:

![Figure 4.3 – How stacking works
](img/B17166_04_3.jpg)

图 4.3–堆叠的工作原理

要使用堆栈，我们必须知道哪一列或哪些列代表数据帧中的索引。使用该函数的结果将为我们提供一个带有新索引的数据帧，该索引包含其余列的名称。

让我们看一个类似于上一个示例的库存示例:

```
df = op.create.dataframe({
    "product": ["Coffee", "Coffee", "Tea", "Tea"],
    "size": ["big", "small", "big", "small"],
    "price": [1.5, 1, 2, 1.25],
    "cost": [0.24, 0.2, 0.32, 0.3]
})
```

这就是我们的数据集的样子:

```
product     size              price         cost
(object)    (object)      (float64)    (float64)
----------  ----------  -----------  -----------
Coffee      big                1.5          0.24
Coffee      small              1            0.2
Tea         big                2            0.32
Tea         small              1.25         0.3
```

为了堆叠它，我们需要调用`df.stack`，如下所示:

```
df.stack(index=["product", "size"]).print()
```

这将把前两列标识为数据集的索引。我们将看到以下输出:

```
product     size        variable          value
(object)    (object)    (object)      (float64)
----------  ----------  ----------  -----------
Coffee      big         price              1.5
Coffee      big         cost               0.24
Coffee      small       price              1
Coffee      small       cost               0.2
Tea         big         price              2
Tea         big         cost               0.32
Tea         small       price              1.25
Tea         small       cost               0.3
```

产生的数据集将需要每个新列的名称。默认情况下，它会指定`"variable"`和`"value"`，但是这些名称可以传递给函数，如下所示:

```
df.stack(index=["product", "size"], col_name="foo", value_name="bar").print()
```

这将把前两列标识为数据集的索引。我们将看到以下输出:

```
product     size        foo                 bar
(object)    (object)    (object)      (float64)
----------  ----------  ----------  -----------
Coffee      big         price              1.5
Coffee      big         cost               0.24
Coffee      small       price              1
Coffee      small       cost               0.2
Tea         big         price              2
Tea         big         cost               0.32
Tea         small       price              1.25
Tea         small       cost               0.3
```

到目前为止，我们已经了解了旋转和堆叠，并阐明了旋转是堆叠的逆过程的一种特殊情况，该过程称为拆分。我们接下来会了解这一点。

## 拆垛

拆垛与堆垛相反。因此，在这种情况下，我们必须找出哪个或哪些列代表数据帧中的索引。基于这个和传递给`df.unstack`的级别，索引列将被转换成列:

![Figure 4.4 – How unstacking works
](img/B17166_04_4.jpg)

图 4.4–拆垛的工作原理

让我们来看看拆分之前堆叠的数据帧的结果:

```
df.unstack(index=["product", "size", "variable"]).print()
```

我们将得到以下输出:

```
product     size               cost        price
(object)    (object)      (float64)    (float64)
----------  ----------  -----------  -----------
Coffee      big                0.24         1.5
Coffee      small              0.2          1
Tea         big                0.32         2
Tea         small              0.3          1.25
```

要通过您想要拆分的级别，请使用`level`参数，如下所示:

```
df.unstack(index=["product", "size", "variable"], level=1).print()
```

负数也可以:

```
df.unstack(index=["product", "size", "variable"], level=-2).print()
```

在前面的代码中，我们对多索引中的倒数第二列进行了拆分。

同样，列的名称也可以传递给`level`:

```
df.unstack(index=["product", "size", "variable"], level="size").print()
```

前面的三个例子将分解`"size"`索引列，给出如下结果:

```
product     variable            big        small
(object)    (object)      (float64)    (float64)
----------  ----------  -----------  -----------
Coffee      cost               0.24         0.2
Coffee      price              1.5          1
Tea         cost               0.32         0.3
Tea         price              2            1.25
```

在本节中，我们学习了拆分以及它与旋转和堆叠的关系。现在，让我们深入研究一个类似的转换，它不依赖于显式索引，而是依赖于我们可以设置为标识符的列。

## 融化

熔化用于将数据帧格式从宽变为长，类似于堆栈，但没有索引。在生成的数据帧中，一个或多个列将作为标识符。剩下的将是前一列的值(默认情况下名为`"values"`)和这些值的标识符(名为`"variable"`)。您可以在此函数中排除列。您可以在下图中看到熔化的工作原理:

![Figure 4.5 – How melting works
](img/B17166_04_5.jpg)

图 4.5–熔化的工作原理

要使用它，所需的最少参数只是标识符列:

```
df.melt(id_cols=["product","size"]).print()
```

在前面的代码中，我们只是传递了标识符列，但是您也可以显式地传递您想要获取的值，如下所示:

```
df.melt(id_cols=["product","size"], value_cols=["price", "cost"]).print()
```

上述代码将返回以下输出:

```
product     size        variable          value
(object)    (object)    (object)      (float64)
----------  ----------  ----------  -----------
Coffee      big         price              1.5
Coffee      small       price              1
Tea         big         price              2
Tea         small       price              1.25
Coffee      big         cost               0.24
Coffee      small       cost               0.2
Tea         big         cost               0.32
Tea         small       cost               0.3
```

要定义结果列的名称，可以使用`var_name`和`value_name`:

```
df.melt(id_cols=["product", "size"], var_name="foo", value_name="bar").print()
```

这将导致以下输出:

```
product     size        foo                 bar
(object)    (object)    (object)      (float64)
----------  ----------  ----------  -----------
Coffee      big         price              1.5
Coffee      small       price              1
Tea         big         price              2
Tea         small       price              1.25
Coffee      big         cost               0.24
Coffee      small       cost               0.2
Tea         big         cost               0.32
Tea         small       cost               0.3
```

正如我们所看到的，我们可以通过使用我们刚刚学会如何应用的一些转换来减小数据集的大小，但如果您来自关系数据库世界，那么您已经了解聚合，它可以大幅减小数据集的大小，但会导致它们失去分辨率。

## 聚合

聚合是将多行的值组合成一个值的函数。

Optimus 提供了两种应用聚合的方法。

*   使用`cols`访问器
*   使用`agg`功能

让我们两个都看看。

### cols 访问器

对于例子，让我们说你要计算一列的最小值。在 Optimus 中，您可以这样做:

```
from optimus import Optimus 
op = Optimus("pandas")
df = op.load.file("foo.csv")
df.print()
```

这将打印以下输出:

```
name        job                id
(object)    (object)      (int64)
----------  ----------  ---------
optimus     Leader              1
optimus     Espionage           2
bumblebee   1                   3
bumblebee   3                   4
```

为了得到最小值，我们可以使用下面的代码:

```
df.cols.min("id")
```

您将得到数字`1`作为输出:

```
1
```

您还可以传递一个列列表，如下所示:

```
df.cols.min(["id","name"])
```

您将收到以下输出:

```
{'id': 1, 'name': 'bumblebee'}
```

这里，发生了三件事:

*   相比之下，当在这里传递一个列时，您不仅仅得到值——您还会得到一个 Python 字典，其中包含该列的名称及其值。
*   与返回 dataframe 对象的传统 dataframe 技术不同，Optimus 返回一个 Python 字典。

这对于从我们的数据帧中获取单个聚合非常有用，但是如果您想要获取一组聚合，您可以使用不同的方法，我们将在下面看到。

### agg 方法

现在，让我们探索一下`agg` dataframe 方法。使用此方法，您可以同时计算一个或多个聚合。

例如，如果要计算最小聚合，可以在传递`{"id": "min"}`的同时调用`min`方法，如下所示:

```
df.agg({"id": "min"})
```

这将返回以下输出:

```
1
```

现在，假设您想要应用多个聚合，比如`min`和`max`:

```
df.agg({"name": "min", "id": "max"})
```

这将返回`name`列的`min`值和`id`列的`max`值:

```
{'name': 'bumblebee', 'id': 4}
```

现在您已经知道了如何应用聚合，下面是 Optimus 支持的聚合的完整列表。考虑到有些引擎可以并行进行一些聚合，这一点非常重要。例如，您不能使用`cols`计算两个聚合，如下所示:

```
print(df.cols.std("id"))
print(df.cols.min("id"))
```

但是，您可以使用`agg`函数同时传递两个参数，如下所示:

```
df.agg({"id":"std","name":min})
```

Dataframe 引擎每天都在进化。如果您想深入了解哪些功能可以并行化，请查阅相关文档。

为了结束这一节，下面是 Optimus 支持的所有聚合的完整列表:

![Table 4.1 – Aggregation supported by Optimus
](img/B17166_04_table_4.1.jpg)

表 4.1–Optimus 支持的聚合

这些聚合将在整个数据集上执行，不考虑任何组。现在，让我们了解一下，如果我们想将结果限制在某些组中，应该怎么做。

# 聚集和分组

在考虑另一列中的一组公共值时，希望计算数据集中的最小值、最大值或任何其他聚合，这是一种常见的情况。在这里，我们可以使用一种称为分组的做法。让我们尝试使用下图来解释这个概念:

![Figure 4.6 – Sum values applying grouping
](img/B17166_04_6.jpg)

图 4.6–应用分组对值求和

在前面的示例中，我们对来自相同物种的`sepal_lenght`值求和。现在，让我们学习如何在 Optimus 中使用分组和聚合。

例如，我们有以下数据框架:

```
df = op.load.file("foo.csv")
df.print()
```

数据帧包含以下数据:

```
name        job                id
(object)    (object)      (int64)
----------  ----------  ---------
optimus     Leader              1
optimus     Espionage           2
bumblebee   1                   3
bumblebee   3                   4
```

也许您想计算列`name`中`optimus`和`bumblebee`值的最小值`id`。为此，`agg()`函数支持`groupby`参数，该参数将使用`name`列中的公共值来计算`id`列中的最小值。让我们看看它是如何工作的:

```
A = df.agg({"id":min}, groupby="name")
```

这将返回一个 Python 字典，其中包含组名的键和组的最小值的值:

```
{'bumblebee': 3, 'optimus': 1}
```

获得 Python 字典后，只需使用以下命令就可以访问 Bumblebee 值:

```
A["bumblebee"]
```

这将返回一个表示`"bumblebee"`组最小值的整数:

```
3
```

如您所见，结果在一个 Python 字典中。根据数据集的大小，此结果的大小可能会有所不同，因此在获得任何结果之前请注意这一点。您可能希望在 dataframe 中请求该调用的输出，这可以通过将`dataframe`传递到输出参数中来实现。

# 总结

在本章中，我们学习了很多可以用来连接、合并、连接、聚合和分组数据的函数。所有这些函数都将为您提供强大的功能，让您可以根据需要来处理数据和塑造数据，以便从数据中获得洞察力，针对不同的用途重新格式化数据集，以及组合多个数据集以在同一数据集中获得更高的数据分辨率。

在下一章中，我们将学习 Optimus 提供的分析数据的功能，这样我们可以更好地了解它。