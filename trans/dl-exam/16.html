<html><head/><body>
<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Implementing Fish Recognition</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">实施鱼类识别</h1>
                
            
            
                
<p class="calibre2">以下是<a href="c6be0d67-2ba9-45ac-b6dd-116518853f42.xhtml" target="_blank" class="calibre11">第一章</a>、<em class="calibre19">数据科学——鸟瞰图</em>中<em class="calibre19">鱼类识别</em>部分的整段代码。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Code for fish recognition</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">鱼类识别代码</h1>
                
            
            
                
<p class="calibre2">在解释了我们的鱼识别示例的主要构建块之后，我们可以看到所有的代码片段连接在一起，并看到我们如何设法用几行代码构建这样一个复杂的系统:</p>
<pre class="calibre21">#Loading the required libraries along with the deep learning platform Keras with TensorFlow as backend<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>np.random.seed(2017)<br class="title-page-name"/>import os<br class="title-page-name"/>import glob<br class="title-page-name"/>import cv2<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import time<br class="title-page-name"/>import warnings<br class="title-page-name"/>from sklearn.cross_validation import KFold<br class="title-page-name"/>from keras.models import Sequential<br class="title-page-name"/>from keras.layers.core import Dense, Dropout, Flatten<br class="title-page-name"/>from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D<br class="title-page-name"/>from keras.optimizers import SGD<br class="title-page-name"/>from keras.callbacks import EarlyStopping<br class="title-page-name"/>from keras.utils import np_utils<br class="title-page-name"/>from sklearn.metrics import log_loss<br class="title-page-name"/>from keras import __version__ as keras_version<br class="title-page-name"/># Parameters<br class="title-page-name"/># ----------<br class="title-page-name"/># x : type<br class="title-page-name"/>#    Description of parameter `x`.<br class="title-page-name"/>def rezize_image(img_path):<br class="title-page-name"/>  img = cv2.imread(img_path)<br class="title-page-name"/>  img_resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)<br class="title-page-name"/>  return img_resized<br class="title-page-name"/>#Loading the training samples from their corresponding folder names, where we have a folder for each type<br class="title-page-name"/>def load_training_samples():<br class="title-page-name"/>  #Variables to hold the training input and output variables<br class="title-page-name"/>  train_input_variables = []<br class="title-page-name"/>  train_input_variables_id = []<br class="title-page-name"/>  train_label = []<br class="title-page-name"/>  # Scanning all images in each folder of a fish type<br class="title-page-name"/>  print('Start Reading Train Images')<br class="title-page-name"/>  folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']<br class="title-page-name"/>  for fld in folders:<br class="title-page-name"/>      folder_index = folders.index(fld)<br class="title-page-name"/>      print('Load folder {} (Index: {})'.format(fld, folder_index))<br class="title-page-name"/>      imgs_path = os.path.join('..', 'input', 'train', fld, '*.jpg')<br class="title-page-name"/>      files = glob.glob(imgs_path)<br class="title-page-name"/>      for file in files:<br class="title-page-name"/>          file_base = os.path.basename(file)<br class="title-page-name"/>          # Resize the image<br class="title-page-name"/>          resized_img = rezize_image(file)<br class="title-page-name"/>          # Appending the processed image to the input/output variables of the classifier<br class="title-page-name"/>          train_input_variables.append(resized_img)<br class="title-page-name"/>          train_input_variables_id.append(file_base)<br class="title-page-name"/>          train_label.append(folder_index)<br class="title-page-name"/>  return train_input_variables, train_input_variables_id, train_label<br class="title-page-name"/>#Loading the testing samples which will be used to testing how well the model was trained<br class="title-page-name"/>def load_testing_samples():<br class="title-page-name"/>  # Scanning images from the test folder<br class="title-page-name"/>  imgs_path = os.path.join('..', 'input', 'test_stg1', '*.jpg')<br class="title-page-name"/>  files = sorted(glob.glob(imgs_path))<br class="title-page-name"/>  # Variables to hold the testing samples<br class="title-page-name"/>  testing_samples = []<br class="title-page-name"/>  testing_samples_id = []<br class="title-page-name"/>  #Processing the images and appending them to the array that we have<br class="title-page-name"/>  for file in files:<br class="title-page-name"/>      file_base = os.path.basename(file)<br class="title-page-name"/>      # Image resizing<br class="title-page-name"/>      resized_img = rezize_image(file)<br class="title-page-name"/>      testing_samples.append(resized_img)<br class="title-page-name"/>      testing_samples_id.append(file_base)<br class="title-page-name"/>  return testing_samples, testing_samples_id<br class="title-page-name"/># formatting the images to fit our model<br class="title-page-name"/>def format_results_for_types(predictions, test_id, info):<br class="title-page-name"/>  model_results = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER',<br class="title-page-name"/>    'SHARK', 'YFT'])<br class="title-page-name"/>  model_results.loc[:, 'image'] = pd.Series(test_id, index=model_results.index)<br class="title-page-name"/>  sub_file = 'testOutput_' + info + '.csv'<br class="title-page-name"/>  model_results.to_csv(sub_file, index=False)<br class="title-page-name"/>def load_normalize_training_samples():<br class="title-page-name"/>  # Calling the load function in order to load and resize the training samples<br class="title-page-name"/>  training_samples, training_label, training_samples_id = load_training_samples()<br class="title-page-name"/>  # Converting the loaded and resized data into Numpy format<br class="title-page-name"/>  training_samples = np.array(training_samples, dtype=np.uint8)<br class="title-page-name"/>  training_label = np.array(training_label, dtype=np.uint8)<br class="title-page-name"/>  # Reshaping the training samples<br class="title-page-name"/>  training_samples = training_samples.transpose((0, 3, 1, 2))<br class="title-page-name"/>  # Converting the training samples and training labels into float format<br class="title-page-name"/>  training_samples = training_samples.astype('float32')<br class="title-page-name"/>  training_samples = training_samples / 255<br class="title-page-name"/>  training_label = np_utils.to_categorical(training_label, 8)<br class="title-page-name"/>  return training_samples, training_label, training_samples_id<br class="title-page-name"/>#Loading and normalizing the testing sample to fit into our model<br class="title-page-name"/>def load_normalize_testing_samples():<br class="title-page-name"/>  # Calling the load function in order to load and resize the testing samples<br class="title-page-name"/>  testing_samples, testing_samples_id = load_testing_samples()<br class="title-page-name"/>  # Converting the loaded and resized data into Numpy format<br class="title-page-name"/>  testing_samples = np.array(testing_samples, dtype=np.uint8)<br class="title-page-name"/>  # Reshaping the testing samples<br class="title-page-name"/>  testing_samples = testing_samples.transpose((0, 3, 1, 2))<br class="title-page-name"/>  # Converting the testing samples into float format<br class="title-page-name"/>  testing_samples = testing_samples.astype('float32')<br class="title-page-name"/>  testing_samples = testing_samples / 255<br class="title-page-name"/>  return testing_samples, testing_samples_id<br class="title-page-name"/>def merge_several_folds_mean(data, num_folds):<br class="title-page-name"/>  a = np.array(data[0])<br class="title-page-name"/>  for i in range(1, num_folds):<br class="title-page-name"/>      a += np.array(data[i])<br class="title-page-name"/>  a /= num_folds<br class="title-page-name"/>  return a.tolist()<br class="title-page-name"/># Create CNN model architecture<br class="title-page-name"/>def create_cnn_model_arch():<br class="title-page-name"/>  pool_size = 2 # we will use 2x2 pooling throughout<br class="title-page-name"/>  conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...<br class="title-page-name"/>  conv_depth_2 = 64 # ...switching to 64 after the first pooling layer<br class="title-page-name"/>  kernel_size = 3 # we will use 3x3 kernels throughout<br class="title-page-name"/>  drop_prob = 0.5 # dropout in the FC layer with probability 0.5<br class="title-page-name"/>  hidden_size = 32 # the FC layer will have 512 neurons<br class="title-page-name"/>  num_classes = 8 # there are 8 fish types<br class="title-page-name"/>  # Conv [32] -&gt; Conv [32] -&gt; Pool<br class="title-page-name"/>  cnn_model = Sequential()<br class="title-page-name"/>  cnn_model.add(ZeroPadding2D((1, 1), input_shape=(3, 32, 32), dim_ordering='th'))<br class="title-page-name"/>  cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu', dim_ordering='th'))<br class="title-page-name"/>  cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th')<br class="title-page-name"/>  cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu', dim_ordering='th'))<br class="title-page-name"/>  cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2), dim_ordering='th'))<br class="title-page-name"/>  # Conv [64] -&gt; Conv [64] -&gt; Pool<br class="title-page-name"/>  cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))<br class="title-page-name"/>  cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu', dim_ordering='th'))<br class="title-page-name"/>  cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))<br class="title-page-name"/>  cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu', dim_ordering='th'))<br class="title-page-name"/>  cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2), dim_ordering='th'))<br class="title-page-name"/>  # Now flatten to 1D, apply FC then ReLU (with dropout) and finally softmax(output layer)<br class="title-page-name"/>  cnn_model.add(Flatten())<br class="title-page-name"/>  cnn_model.add(Dense(hidden_size, activation='relu'))<br class="title-page-name"/>  cnn_model.add(Dropout(drop_prob))<br class="title-page-name"/>  cnn_model.add(Dense(hidden_size, activation='relu'))<br class="title-page-name"/>  cnn_model.add(Dropout(drop_prob))<br class="title-page-name"/>  cnn_model.add(Dense(num_classes, activation='softmax'))<br class="title-page-name"/>  # initiating the stochastic gradient descent optimiser<br class="title-page-name"/>  stochastic_gradient_descent = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)<br class="title-page-name"/>  cnn_model.compile(optimizer=stochastic_gradient_descent,  # using the stochastic gradient descent optimiser<br class="title-page-name"/>                    loss='categorical_crossentropy')  # using the cross-entropy loss function<br class="title-page-name"/>  return cnn_model<br class="title-page-name"/>#Model using with kfold cross validation as a validation method<br class="title-page-name"/>def create_model_with_kfold_cross_validation(nfolds=10):<br class="title-page-name"/>  batch_size = 16 # in each iteration, we consider 32 training examples at once<br class="title-page-name"/>  num_epochs = 30 # we iterate 200 times over the entire training set<br class="title-page-name"/>  random_state = 51 # control the randomness for reproducibility of the results on the same platform<br class="title-page-name"/>  # Loading and normalizing the training samples prior to feeding it to the created CNN model<br class="title-page-name"/>  training_samples, training_samples_target, training_samples_id = load_normalize_training_samples()<br class="title-page-name"/>  yfull_train = dict()<br class="title-page-name"/>  # Providing Training/Testing indices to split data in the training samples<br class="title-page-name"/>  # which is splitting data into 10 consecutive folds with shuffling<br class="title-page-name"/>  kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)<br class="title-page-name"/>  fold_number = 0 # Initial value for fold number<br class="title-page-name"/>  sum_score = 0 # overall score (will be incremented at each iteration)<br class="title-page-name"/>  trained_models = [] # storing the modeling of each iteration over the folds<br class="title-page-name"/>  # Getting the training/testing samples based on the generated training/testing indices by Kfold<br class="title-page-name"/>  for train_index, test_index in kf:<br class="title-page-name"/>      cnn_model = create_cnn_model_arch()<br class="title-page-name"/>      training_samples_X = training_samples[train_index] # Getting the training input variables<br class="title-page-name"/>      training_samples_Y = training_samples_target[train_index] # Getting the training output/label variable<br class="title-page-name"/>      validation_samples_X = training_samples[test_index] # Getting the validation input variables<br class="title-page-name"/>      validation_samples_Y = training_samples_target[test_index] # Getting the validation output/label variabl<br class="title-page-name"/>      fold_number += 1<br class="title-page-name"/>      print('Fold number {} out of {}'.format(fold_number, nfolds))<br class="title-page-name"/>      callbacks = [<br class="title-page-name"/>          EarlyStopping(monitor='val_loss', patience=3, verbose=0),<br class="title-page-name"/>      ]<br class="title-page-name"/>      # Fitting the CNN model giving the defined settings<br class="title-page-name"/>      cnn_model.fit(training_samples_X, training_samples_Y, batch_size=batch_size,<br class="title-page-name"/>        nb_epoch=num_epochs,<br class="title-page-name"/>            shuffle=True, verbose=2, validation_data=(validation_samples_X,<br class="title-page-name"/>              validation_samples_Y),<br class="title-page-name"/>            callbacks=callbacks)<br class="title-page-name"/>      # measuring the generalization ability of the trained model based on the validation set<br class="title-page-name"/>      predictions_of_validation_samples = <br class="title-page-name"/>        cnn_model.predict(validation_samples_X.astype('float32'), batch_size=batch_size, <br class="title-page-name"/>          verbose=2)<br class="title-page-name"/>      current_model_score = log_loss(Y_valid, predictions_of_validation_samples)<br class="title-page-name"/>      print('Current model score log_loss: ', current_model_score)<br class="title-page-name"/>      sum_score += current_model_score*len(test_index)<br class="title-page-name"/>      # Store valid predictions<br class="title-page-name"/>      for i in range(len(test_index)):<br class="title-page-name"/>          yfull_train[test_index[i]] = predictions_of_validation_samples[i]<br class="title-page-name"/>      # Store the trained model<br class="title-page-name"/>      trained_models.append(cnn_model)<br class="title-page-name"/>  # incrementing the sum_score value by the current model calculated score<br class="title-page-name"/>  overall_score = sum_score/len(training_samples)<br class="title-page-name"/>  print("Log_loss train independent avg: ", overall_score)<br class="title-page-name"/>  #Reporting the model loss at this stage<br class="title-page-name"/>  overall_settings_output_string = 'loss_' + str(overall_score) + '_folds_' + str(nfolds) + '_ep_' + str(num_epochs)<br class="title-page-name"/>  return overall_settings_output_string, trained_models<br class="title-page-name"/>#Testing how well the model is trained<br class="title-page-name"/>def test_generality_crossValidation_over_test_set(overall_settings_output_string, cnn_models):<br class="title-page-name"/>  batch_size = 16 # in each iteration, we consider 32 training examples at once<br class="title-page-name"/>  fold_number = 0 # fold iterator<br class="title-page-name"/>  number_of_folds = len(cnn_models) # Creating number of folds based on the value used in the training step<br class="title-page-name"/>  yfull_test = [] # variable to hold overall predictions for the test set<br class="title-page-name"/>  #executing the actual cross validation test process over the test set<br class="title-page-name"/>  for j in range(number_of_folds):<br class="title-page-name"/>      model = cnn_models[j]<br class="title-page-name"/>      fold_number += 1<br class="title-page-name"/>      print('Fold number {} out of {}'.format(fold_number, number_of_folds))<br class="title-page-name"/>      #Loading and normalizing testing samples<br class="title-page-name"/>      testing_samples, testing_samples_id = load_normalize_testing_samples()<br class="title-page-name"/>      #Calling the current model over the current test fold<br class="title-page-name"/>      test_prediction = model.predict(testing_samples, batch_size=batch_size, verbose=2)<br class="title-page-name"/>      yfull_test.append(test_prediction)<br class="title-page-name"/>  test_result = merge_several_folds_mean(yfull_test, number_of_folds)<br class="title-page-name"/>  overall_settings_output_string = 'loss_' + overall_settings_output_string \<br class="title-page-name"/>              + '_folds_' + str(number_of_folds)<br class="title-page-name"/>  format_results_for_types(test_result, testing_samples_id, overall_settings_output_string)<br class="title-page-name"/># Start the model training and testing<br class="title-page-name"/>if __name__ == '__main__':<br class="title-page-name"/>  info_string, models = create_model_with_kfold_cross_validation()<br class="title-page-name"/>  test_generality_crossValidation_over_test_set(info_string, models)</pre>


            

            
        
    </body>

</html>
</body></html>