<html><head/><body>
<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>TensorFlow in Action - Some Basic Examples</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">张量流的作用-一些基本的例子</h1>
                
            
            
                
<p class="calibre2">在本章中，我们将解释 TensorFlow 背后的主要计算概念，即计算图模型，并演示如何通过实现线性回归和逻辑回归来让您步入正轨。</p>
<p class="calibre2">本章将涵盖以下主题:</p>
<ul class="calibre7">
<li class="calibre8">单个神经元的容量和激活函数</li>
<li class="calibre8">激活功能</li>
<li class="calibre8">前馈神经网络</li>
<li class="calibre8">对多层网络的需求</li>
<li class="calibre8">张量流术语—概述</li>
<li class="calibre8">线性回归模型—建立和训练</li>
<li class="calibre8">逻辑回归模型——建立和训练</li>
</ul>
<p class="calibre2">我们将从解释单个神经元实际上可以做什么/建模开始，基于此，对多层网络的需求将会出现。接下来，我们将详细阐述 TensorFlow 中使用/可用的主要概念和工具，以及如何使用这些工具构建简单的示例，如线性回归和逻辑回归。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Capacity of a single neuron</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">单个神经元的容量</h1>
                
            
            
                
<p class="calibre2"><strong class="calibre13">神经网络</strong>是一种计算模型，主要受人脑的生物神经网络处理输入信息的方式的启发。神经网络在机器学习研究(特别是深度学习)和工业应用方面取得了巨大突破，如在计算机视觉、语音识别和文本处理方面的突破性成果。在这一章中，我们将试图发展对一种叫做<strong class="calibre13">多层感知器</strong>的特殊类型的神经网络的理解。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Biological motivation and connections</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">生物动机和联系</h1>
                
            
            
                
<p class="calibre2">我们大脑的基本计算单元被称为<strong class="calibre13">神经元</strong>，我们的神经系统中大约有 860 亿个神经元，它们通过大约<img class="fm-editor-equation8" src="img/d63a8baf-bbe2-4bae-ad11-1657839aa8bf.png"/>到<img class="fm-editor-equation8" src="img/c889ca1f-940a-4aa4-b33d-d9631a8aa4c0.png"/>突触连接在一起。</p>
<p class="calibre2"><em class="calibre19">图 1 </em>显示了一个生物神经元。<em class="calibre19">图 2 </em>显示了相应的数学模型。在生物神经元的图中，每个神经元从其树突接收输入信号，然后沿着其轴突产生输出信号，轴突在轴突上分裂并通过突触连接到其他神经元。</p>
<p class="calibre2">在神经元的相应数学计算模型中，沿着轴突<img class="fm-editor-equation9" src="img/613ebbfe-8820-436c-914e-9fcf1212b25f.png"/>传播的信号基于突触处的突触强度与系统中另一个神经元的树突的乘法运算<img class="fm-editor-equation10" src="img/12164cb0-d15a-430c-94a1-ba0adabcfa27.png"/>相互作用，由<img class="fm-editor-equation11" src="img/b060c83c-50ec-4aa1-9eec-425f11521da3.png"/>表示。这个想法是突触权重/强度<img class="fm-editor-equation12" src="img/9ed5a5b6-6d41-4348-8719-8ee0f4812468.png"/>被网络学习，它们是控制特定神经元对另一个神经元的影响的那些。</p>
<p class="calibre2">此外，在图 2 的<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre19">的基本计算模型中，树突将信号传送到主细胞体，并在那里将所有信号相加。如果最终结果高于某个阈值，神经元可以在计算模型中触发。</em></p>
<p class="calibre2">此外，值得一提的是，我们需要控制沿着轴突的输出尖峰的频率，所以我们使用一种叫做<strong class="calibre13">激活函数</strong>的东西。实际上，激活函数的常见选择是 sigmoid 函数σ，因为它接受实值输入(求和后的信号强度),并将其压缩到 0 和 1 之间。我们将在下一节中详细介绍这些激活功能:</p>
<div><img src="img/903ae559-7cca-47b8-a502-97797b4a994c.png" class="calibre66"/></div>
<p>图 1:大脑的计算单位(http://cs231n.github.io/assets/nn1/neuron.png)</p>
<p class="calibre2">生物模型有相应的基本数学模型:<br class="calibre20"/></p>
<div><img src="img/c94c95ce-1f72-4a7b-9f55-c17a2f00ad10.jpeg" class="calibre67"/></div>
<p>图 2:大脑计算单元的数学模型(http://cs231n.github.io/assets/nn1/neuron_model.jpeg)</p>
<div><p class="calibre2">神经网络中的基本计算单元是神经元，通常称为<strong class="calibre13">节点</strong>或<strong class="calibre13">单元</strong>。它从一些其他节点或外部源接收输入，并计算输出。每个输入都有一个相关的<strong class="calibre13">权重</strong> ( <strong class="calibre13"> w </strong>)，根据其相对于其他输入的重要性来分配。该节点将函数<em class="calibre19"> f </em>(我们稍后定义)应用于其输入的加权和。</p>
</div>
<p class="calibre2">所以，神经网络的基本计算单元一般称为<strong class="calibre13">神经元</strong> / <strong class="calibre13">节点</strong> / <strong class="calibre13">单元。</strong></p>
<p class="calibre2">这个神经元从先前的神经元甚至外部来源接收输入，然后对这个输入进行一些处理，产生所谓的激活。该神经元的每个输入都与其自身的权重<img class="fm-editor-equation12" src="img/c0332e36-48b3-4716-b43a-177ce42e409a.png"/>相关联，该权重代表了这种连接的强度以及该输入的重要性。</p>
<p class="calibre2">因此，神经网络的这一基本构建模块的最终输出是通过重要性加权的输入的总和版本<em class="calibre19"> w </em>，然后神经元通过激活函数传递总和输出。</p>
<div><img src="img/7cb94f65-1a2f-497f-9668-800a94d2ae6a.png" class="calibre68"/></div>
<p>图 3:单个神经元</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Activation functions</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">激活功能</h1>
                
            
            
                
<p class="calibre2">如图<em class="calibre19">图 3 </em>所示计算神经元的输出，并通过一个激活函数将非线性引入输出。这个<em class="calibre19"> f </em>被称为<strong class="calibre13">激活功能</strong>。激活功能的主要目的是:</p>
<ul class="calibre7">
<li class="calibre8">在神经元的输出中引入非线性。这很重要，因为大多数真实世界的数据是非线性的，我们希望神经元学习这些非线性表示。</li>
<li class="calibre8">将输出压缩到特定范围内。</li>
</ul>
<p class="calibre2">每个激活函数(或非线性)都取一个数字，并对其执行某个固定的数学运算。在实践中，您可能会遇到几种激活功能。</p>
<p class="calibre2">因此，我们将简要介绍最常见的激活功能。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Sigmoid</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">乙状结肠的</h1>
                
            
            
                
<p class="calibre2">历史上，sigmoid 激活函数在研究人员中被广泛使用。该函数接受实值输入，并将其压缩到 0 到 1 之间的范围，如下图所示:</p>
<div><em class="calibre25">σ(x) = 1 / (1 + exp(−x))</em></div>
<div><img src="img/cb589e92-9a77-4fa2-b417-d69f20ea718a.png" class="calibre69"/></div>
<p>图 4:乙状结肠激活功能</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Tanh</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">双曲正切</h1>
                
            
            
                
<p class="calibre2">Tanh 是另一个允许一些负值的激活函数。Tanh 接受一个实值输入并将它们压缩到[-1，1]:</p>
<div><em class="calibre25">tanh(x) = 2σ(2x) − 1</em></div>
<div><img src="img/feef9554-16ff-4f6b-9047-cdc9b679729f.png" class="calibre70"/></div>
<p>图 5: Tanh 激活函数</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>ReLU</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">热卢</h1>
                
            
            
                
<p class="calibre2"><strong class="calibre13">整流线性单元</strong> ( <strong class="calibre13"> ReLU </strong>)不容许负值，因为它接受实值输入并将其阈值设为零(用零代替负值):</p>
<div><em class="calibre25">f(x) = max(0, x)</em></div>
<div><img src="img/f1c98eda-2e60-4fb1-b43d-094110178204.jpg" class="calibre71"/></div>
<p>图 6: Relu 激活功能</p>
<p class="calibre2"><strong class="calibre13">偏置的重要性</strong>:偏置的主要功能是为每个节点提供一个可训练的常数值(除了节点接收的正常输入)。请参见<a href="https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks" target="_blank" class="calibre11">https://stack overflow . com/questions/2480650/role-of-bias-in-neural-networks</a>中的链接，以了解更多关于神经元中偏差的作用。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Feed-forward neural network</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">前馈神经网络</h1>
                
            
            
                
<p class="calibre2">前馈神经网络是设计的第一种也是最简单的人工神经网络。它包含多个分层排列的神经元(节点)。相邻层的节点之间有连接或边。所有这些连接都有与之相关的权重。</p>
<p class="calibre2">图 7 显示了一个前馈神经网络的例子:</p>
<div><img src="img/6b47331f-bfc0-4af2-8fd1-f08ddf2ec754.png" class="calibre72"/></div>
<p>图 7:一个前馈神经网络的例子</p>
<p class="calibre2">在前馈网络中，信息仅在一个方向上移动，即从输入节点向前移动，通过隐藏节点(如果有)到达输出节点。网络中没有循环或回路(前馈网络的这一属性不同于递归神经网络，在递归神经网络中，节点之间的连接形成一个循环)。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>The need for multilayer networks</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">对多层网络的需求</h1>
                
            
            
                
<p class="calibre2">一个<strong class="calibre13">多层感知器</strong> ( <strong class="calibre13"> MLP </strong>)包含一个或多个隐藏层(除了一个输入和一个输出层)。虽然单层感知器只能学习线性函数，但 MLP 也可以学习非线性函数。</p>
<p class="calibre2"><em class="calibre19">图 7 </em>显示了一个单一隐藏层的 MLP。注意，所有连接都有与之相关的权重，但是图中只显示了三个权重(<em class="calibre19"> w0 </em>、<em class="calibre19"> w1 </em>和<em class="calibre19"> w2 </em>)。</p>
<p class="calibre2"><strong class="calibre13">输入层</strong>:输入层有三个节点。偏置节点的值为 1。其他两个节点将 X1 和 X2 作为外部输入(它们是取决于输入数据集的数值)。如前所述，<strong class="calibre13">输入层</strong>不进行任何计算，因此<strong class="calibre13">输入层</strong>节点的输出分别为<strong class="calibre13"> 1 </strong>、<strong class="calibre13"> X1 </strong>和<strong class="calibre13"> X2 </strong>，并送入<strong class="calibre13">隐藏层</strong>。</p>
<p class="calibre2"><strong class="calibre13">隐层</strong>:<strong class="calibre13">隐层</strong>也有三个节点，偏置节点输出为 1。<strong class="calibre13">隐藏层</strong>中其他两个节点的输出取决于<strong class="calibre13">输入层</strong> ( <strong class="calibre13"> 1 </strong>、<strong class="calibre13"> X1 </strong>和<strong class="calibre13"> X2 </strong>)的输出以及与连接(边)相关的权重。记住<em class="calibre19"> f </em>是指激活功能。这些输出然后被馈送到<strong class="calibre13">输出层</strong>中的节点。</p>
<div><img src="img/38718db0-2454-42d0-be89-545151286aba.png" class="calibre73"/></div>
<p>图 8:具有一个隐藏层的多层感知器</p>
<p class="calibre2"><strong class="calibre13">输出层:</strong><strong class="calibre13">输出层</strong>有两个节点；它们从<strong class="calibre13">隐藏层</strong>获取输入，并对高亮显示的隐藏节点执行类似的计算。作为这些计算结果的计算值(<strong class="calibre13"> Y1 </strong>和<strong class="calibre13"> Y2 </strong>)充当多层感知器的输出。</p>
<p class="calibre2">给定一组特征<em class="calibre19"> X = (x1，x2，…) </em>和一个目标<em class="calibre19"> y </em>，多层感知器可以学习特征和目标之间的关系，用于分类或回归。</p>
<p class="calibre2">让我们举个例子来更好地理解多层感知器。假设我们有以下学生成绩数据集:</p>
<p class="calibre2"><strong class="calibre13">表 1–样本学生成绩数据集</strong></p>
<table class="calibre35">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre38"><div> <strong class="calibre1">学时学了</strong> </div></td>
<td class="calibre38"><div> <strong class="calibre1">中期成绩</strong> </div></td>
<td class="calibre38"><div> <strong class="calibre1">期末成绩</strong> </div></td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p>35</p>
</td>
<td class="calibre38">
<p>67</p>
</td>
<td class="calibre38">
<p>及格</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p>12</p>
</td>
<td class="calibre38">
<p>75</p>
</td>
<td class="calibre38">
<p>失败</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p>16</p>
</td>
<td class="calibre38">
<p>89</p>
</td>
<td class="calibre38">
<p>及格</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p>45</p>
</td>
<td class="calibre38">
<p>56</p>
</td>
<td class="calibre38">
<p>及格</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p>10</p>
</td>
<td class="calibre38">
<p>90</p>
</td>
<td class="calibre38">
<p>失败</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">两个输入栏显示学生学习的小时数和学生获得的期中成绩。<strong class="calibre13">期末成绩</strong>栏可以有两个值，<strong class="calibre13"> 1 </strong>或<strong class="calibre13"> 0 </strong>，表示学生在期末是否通过。例如，我们可以看到，如果学生学习了 35 个小时，并在期中获得了 67 分，他/她最终通过了期末考试。</p>
<p class="calibre2">现在，假设我们想预测一个学了 25 个小时，期中得了 70 分的学生是否能通过期末考试:</p>
<div><p class="calibre2"><strong class="calibre13">表 2–期末成绩未知的样本学生</strong></p>
</div>
<table class="calibre35">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre38"><strong class="calibre1">学习时间</strong></td>
<td class="calibre38"><strong class="calibre1">中期标志</strong></td>
<td class="calibre38"><strong class="calibre1">期末成绩</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre38">26</td>
<td class="calibre38">70</td>
<td class="calibre38">？</td>
</tr>
</tbody>
</table>
<p class="calibre2">这是一个二元分类问题，其中 MLP 可以从给定的示例(训练数据)中学习，并在给定新数据点的情况下做出明智的预测。我们很快就会看到 MLP 是如何学习这种关系的。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Training our MLP – the backpropagation algorithm</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">训练我们的 MLP——反向传播算法</h1>
                
            
            
                
<p class="calibre2">多层感知器学习的过程称为<strong class="calibre13">反向传播</strong>算法。我推荐读一下赫曼思·库马尔的这篇 Quora 回答，<a href="https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri" target="_blank" class="calibre11">https://www . Quora . com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/赫曼思-库马尔-曼特里</a>(后面引用)，里面对反向传播解释得很清楚。</p>
<p><strong class="calibre1">误差反向传播</strong>，通常缩写为反向传播，是人工神经网络(ANN)训练的几种方式之一。这是一个有监督的训练方案，这意味着，它从标记的训练数据中学习(有一个监督器，来指导它的学习)。<br class="title-page-name"/>简单来说，BackProp 就像“<strong class="calibre1">从错误中学习”</strong>。每当人工神经网络出错时，主管就进行纠正。一个人工神经网络由不同层的节点组成；输入层、中间隐藏层和输出层。相邻层的节点之间的连接具有与其相关联的“权重”。学习的目标是为这些边分配正确的权重。给定一个输入向量，这些权重决定了输出向量是什么。<br class="title-page-name"/>在监督学习中，对训练集进行标注。这意味着，对于某些给定的输入，我们知道期望的/预期的输出(标签)。<br class="title-page-name"/>反向投影算法:<br class="title-page-name"/>最初所有的边缘权重都是随机分配的。对于训练数据集中的每个输入，激活人工神经网络并观察其输出。将该输出与我们已知的期望输出进行比较，误差被“传播”回前一层。此错误会被记录下来，并相应地“调整”权重。重复该过程，直到输出误差低于预定阈值。一旦上述算法终止，我们就有了一个“学习过的”ANN，我们认为它已经准备好处理“新的”输入。据说这个 ANN 已经从几个例子(标记的数据)和它的错误(错误传播)中学习了。”<br class="title-page-name"/>——赫曼思·库马尔。</p>
<p class="calibre2">现在我们已经知道了反向传播是如何工作的，让我们回到我们的学生成绩数据集。</p>
<p class="calibre2">图 8 中<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre19">所示的 MLP 在输入层有两个节点，用于输入学习小时数和中期分数。它还有一个带有两个节点的隐藏层。输出层也有两个节点；上层节点输出<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre19">通过</em>的概率，而下层节点输出<em xmlns:epub="http://www.idpf.org/2007/ops" class="calibre19">失败</em>的概率。</em></p>
<p class="calibre2">在分类应用中，我们广泛使用 softmax 函数(<a href="http://cs231n.github.io/linear-classify/#softmax" target="_blank" class="calibre11">http://cs231n.github.io/linear-classify/#softmax</a>)作为 MLP 输出层的激活函数，以确保输出是概率，并且它们加起来等于 1。softmax 函数获取一个任意实值分数的向量，并将其压缩为一个介于 0 和 1 之间的值的向量，这些值的总和为 1。所以，在这种情况下:</p>
<div><img class="fm-editor-equation13" src="img/e46f34df-4448-4998-aba9-9ce537e465ee.png"/></div>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Step 1 – forward propagation</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">步骤 1-正向传播</h1>
                
            
            
                
<p class="calibre2">网络中的所有权重都是随机初始化的。让我们考虑一个具体的隐藏层节点，称之为<em class="calibre19"> V </em>。假设从输入到该节点的连接的权重是<strong class="calibre13"> w1 </strong>、<strong class="calibre13"> w2 </strong>和<strong class="calibre13"> w3 </strong>(如图所示)。</p>
<p class="calibre2">然后，网络将第一个训练样本作为输入(我们知道，对于输入 35 和 67，通过的概率是 1):</p>
<ul class="calibre7">
<li class="calibre8">网络的输入= [35，67]</li>
<li class="calibre8">网络的期望输出(目标)= [1，0]</li>
</ul>
<p class="calibre2">然后，从所考虑的节点输出<em class="calibre19"> V </em>，可以计算如下(f 是激活函数，例如 sigmoid):</p>
<p class="CDPAlignCenter3"><em class="calibre19"> V = f (1*w1 + 35*w2 + 67*w3) </em></p>
<div><p class="calibre2">类似地，也计算隐藏层中另一个节点的输出。隐藏层中两个节点的输出充当输出层中两个节点的输入。这使我们能够计算输出层中两个节点的输出概率。</p>
<p class="calibre2">假设输出层中两个节点的输出概率分别为 0.4 和 0.6(因为权重是随机分配的，所以输出也将是随机的)。我们可以看到，计算出的概率(0.4 和 0.6)与期望的概率(分别为 1 和 0)相差甚远；因此，网络被称为具有<em class="calibre19">不正确的输出</em>。</p>
</div>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Step 2 – backpropagation and weight updation</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">步骤 2–反向传播和权重更新</h1>
                
            
            
                
<p class="calibre2">我们计算输出节点处的总误差，并使用反向传播通过网络传播这些误差，以计算梯度。然后，我们使用诸如梯度下降的优化方法来调整网络中的所有权重，目的是减少输出层的误差。</p>
<p class="calibre2">假设与考虑中的节点相关联的新权重是<em class="calibre19"> w4 </em>、<em class="calibre19"> w5 </em>和<em class="calibre19"> w6 </em>(在反向传播和调整权重之后)。</p>
<p class="calibre2">如果我们现在将相同的样本作为输入馈送到网络，网络应该比初始运行表现得更好，因为权重现在已经被优化以最小化预测中的误差。与之前的[0.6，-0.4]相比，输出节点的误差现在减少到[0.2，-0.2]。这意味着我们的网络已经学会正确分类我们的第一个训练样本。</p>
<p class="calibre2">我们对数据集中的所有其他训练样本重复这个过程。然后，我们的网络据说学习了那些例子。</p>
<p class="calibre2">如果我们现在想预测一个学习了 25 个小时、期中成绩为 70 分的学生是否能通过期末考试，我们就要通过正向传播步骤，找到通过和失败的输出概率。</p>
<p class="calibre2">我在这里避免了数学方程和梯度下降等概念的解释，而是试图发展算法的直觉。关于反向传播算法的更多数学上的讨论，请参考这个链接:<a href="http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html" target="_blank" class="calibre11">http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html</a>。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>TensorFlow terminologies – recap</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">TensorFlow 术语——概述</h1>
                
            
            
                
<p class="calibre2">在本节中，我们将概述 TensorFlow 库以及基本 TensorFlow 应用程序的结构。TensorFlow 是一个开源库，用于创建大规模的机器学习应用；它可以在各种硬件上模拟计算，从 android 设备到异构多 gpu 系统。</p>
<p class="calibre2">TensorFlow 使用一种特殊的结构，以便在不同的设备(如 CPU 和 GPU)上执行代码。计算被定义为一个图，每个图由操作组成，也称为<strong class="calibre13"> ops </strong>，所以每当我们使用 TensorFlow 时，我们都在图中定义一系列操作。</p>
<p class="calibre2">要运行这些操作，我们需要将图形启动到一个会话中。会话翻译操作，并将它们传递给设备执行。</p>
<p class="calibre2">例如，下图显示了 TensorFlow 中的一个图表。<em class="calibre19"> W </em>、<em class="calibre19"> x </em>和<em class="calibre19"> b </em>是该图边上的张量。<em class="calibre19"> MatMul </em>是对张量<em class="calibre19"> W </em>和<em class="calibre19"> x </em>的运算；之后，调用<em class="calibre19"> Add </em>，我们用<em class="calibre19"> b </em>将前一个运算符的结果相加。每个操作的合成张量与下一个交叉，直到最后，在那里有可能得到想要的结果。</p>
<div><img src="img/e2f35a77-296a-4058-832f-d70cfdfbea53.png" class="calibre74"/></div>
<p>图 9:示例张量流计算图</p>
<p class="calibre2">为了使用 TensorFlow，我们需要导入库；我们将把它命名为<kbd class="calibre12">tf</kbd>，这样我们就可以通过写<kbd class="calibre12">tf</kbd>点然后写模块名来访问模块:</p>
<pre class="calibre21">import tensorflow as tf</pre>
<p class="calibre2">为了创建我们的第一个图，我们将从使用源操作开始，它不需要任何输入。这些源操作将把它们的信息传递给其他操作，这些操作将实际运行计算。</p>
<p class="calibre2">让我们创建两个将输出数字的源操作。我们将它们定义为<kbd class="calibre12">A</kbd>和<kbd class="calibre12">B</kbd>，您可以在下面的代码中看到:</p>
<pre class="calibre21">A = tf.constant([2])</pre>
<pre class="calibre21">B = tf.constant([3])</pre>
<p class="calibre2">之后，我们将定义一个简单的计算操作<kbd class="calibre12">tf.add()</kbd>，用于对两个元素求和。您也可以使用<kbd class="calibre12">C = A + B</kbd>，如下代码所示:</p>
<pre class="calibre21">C = tf.add(A,B)</pre>
<pre class="calibre21">#C = A + B is also a way to define the sum of the terms</pre>
<p class="calibre2">因为图形需要在会话的上下文中执行，所以我们需要创建一个会话对象:</p>
<pre class="calibre21">session = tf.Session()</pre>
<p class="calibre2">为了观察图表，让我们运行会话，从先前定义的<kbd class="calibre12">C</kbd>操作中获得结果:</p>
<pre class="calibre21">result = session.run(C)<br class="title-page-name"/>print(result)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>[5]</pre>
<p class="calibre2">你可能会想，仅仅把两个数字相加就要做很多工作，但是理解张量流的基本结构是非常重要的。一旦你这样做了，你可以定义任何你想要的计算；同样，TensorFlow 的结构允许它在不同的设备(CPU 或 GPU)上处理计算，甚至在集群中处理。如果你想了解更多，你可以运行方法<kbd class="calibre12">tf.device()</kbd>。</p>
<p class="calibre2">也可以随意试验 TensorFlow 的结构，以便更好地了解它的工作原理。如果您想要 TensorFlow 支持的所有数学运算的列表，可以查看文档。</p>
<p class="calibre2">现在，您应该了解 TensorFlow 的结构以及如何创建基本的应用程序。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Defining multidimensional arrays using TensorFlow</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">使用张量流定义多维数组</h1>
                
            
            
                
<p class="calibre2">现在我们将尝试使用 TensorFlow 来定义这样的数组:</p>
<pre class="calibre21">salar_var = tf.constant([4])<br class="title-page-name"/>vector_var = tf.constant([5,4,2])<br class="title-page-name"/>matrix_var = tf.constant([[1,2,3],[2,2,4],[3,5,5]])<br class="title-page-name"/>tensor = tf.constant( [ [[1,2,3],[2,3,4],[3,4,5]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ] )<br class="title-page-name"/>with tf.Session() as session:<br class="title-page-name"/>    result = session.run(salar_var)<br class="title-page-name"/>    print "Scalar (1 entry):\n %s \n" % result<br class="title-page-name"/>    result = session.run(vector_var)<br class="title-page-name"/>    print "Vector (3 entries) :\n %s \n" % result<br class="title-page-name"/>    result = session.run(matrix_var)<br class="title-page-name"/>    print "Matrix (3x3 entries):\n %s \n" % result<br class="title-page-name"/>    result = session.run(tensor)<br class="title-page-name"/>    print "Tensor (3x3x3 entries) :\n %s \n" % result</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Scalar (1 entry):<br class="title-page-name"/> [2] <br class="title-page-name"/><br class="title-page-name"/>Vector (3 entries) :<br class="title-page-name"/> [5 6 2] <br class="title-page-name"/><br class="title-page-name"/>Matrix (3x3 entries):<br class="title-page-name"/> [[1 2 3]<br class="title-page-name"/> [2 3 4]<br class="title-page-name"/> [3 4 5]] <br class="title-page-name"/><br class="title-page-name"/>Tensor (3x3x3 entries) :<br class="title-page-name"/> [[[ 1  2  3]<br class="title-page-name"/>  [ 2  3  4]<br class="title-page-name"/>  [ 3  4  5]]<br class="title-page-name"/><br class="title-page-name"/> [[ 4  5  6]<br class="title-page-name"/>  [ 5  6  7]<br class="title-page-name"/>  [ 6  7  8]]<br class="title-page-name"/><br class="title-page-name"/> [[ 7  8  9]<br class="title-page-name"/>  [ 8  9 10]<br class="title-page-name"/>  [ 9 10 11]]]</pre>
<p class="calibre2">现在，您已经理解了这些数据结构，我鼓励您使用前面的一些函数来试验它们，看看它们将如何根据它们的结构类型进行操作:</p>
<pre class="calibre21">Matrix_one = tf.constant([[1,2,3],[2,3,4],[3,4,5]])<br class="title-page-name"/>Matrix_two = tf.constant([[2,2,2],[2,2,2],[2,2,2]])<br class="title-page-name"/>first_operation = tf.add(Matrix_one, Matrix_two)<br class="title-page-name"/>second_operation = Matrix_one + Matrix_two<br class="title-page-name"/>with tf.Session() as session:<br class="title-page-name"/>    result = session.run(first_operation)<br class="title-page-name"/>    print "Defined using tensorflow function :"<br class="title-page-name"/>    print(result)<br class="title-page-name"/>    result = session.run(second_operation)<br class="title-page-name"/>    print "Defined using normal expressions :"<br class="title-page-name"/>    print(result)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Defined using tensorflow function :<br class="title-page-name"/>[[3 4 5]<br class="title-page-name"/> [4 5 6]<br class="title-page-name"/> [5 6 7]]<br class="title-page-name"/>Defined using normal expressions :<br class="title-page-name"/>[[3 4 5]<br class="title-page-name"/> [4 5 6]<br class="title-page-name"/> [5 6 7]]</pre>
<p class="calibre2">使用常规符号定义和<kbd class="calibre12">tensorflow</kbd>函数，我们能够得到一个元素级乘法，也称为<strong class="calibre13">哈达玛乘积</strong>。但是如果我们想要正规的矩阵乘积呢？我们需要使用另一个名为<kbd class="calibre12">tf.matmul()</kbd>的张量流函数:</p>
<pre class="calibre21">Matrix_one = tf.constant([[2,3],[3,4]])<br class="title-page-name"/>Matrix_two = tf.constant([[2,3],[3,4]])<br class="title-page-name"/>first_operation = tf.matmul(Matrix_one, Matrix_two)<br class="title-page-name"/>with tf.Session() as session:<br class="title-page-name"/>    result = session.run(first_operation)<br class="title-page-name"/>    print "Defined using tensorflow function :"<br class="title-page-name"/>    print(result)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Defined using tensorflow function :<br class="title-page-name"/>[[13 18]<br class="title-page-name"/> [18 25]]</pre>
<p class="calibre2">我们也可以自己定义这个乘法，但是已经有一个函数可以做到这一点，所以没有必要重新发明轮子！</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Why tensors?</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">为什么是张量？</h1>
                
            
            
                
<p class="calibre2">张量结构通过给我们以我们想要的方式塑造数据集的自由来帮助我们。</p>
<p class="calibre2">由于图像中的信息是如何编码的，这在处理图像时特别有用。</p>
<p class="calibre2">想想图像，很容易理解它有高度和宽度，因此用二维结构(矩阵)表示其中包含的信息是有意义的...直到你记得图像是有颜色的。为了增加关于颜色的信息，我们需要另一个维度，这时张量变得特别有用。</p>
<p class="calibre2">图像被编码到颜色通道中；图像数据在给定点以颜色通道中每种颜色的强度表示，最常见的是 RGB(即红色、蓝色和绿色)。图像中包含的信息是图像的宽度和高度中每个通道颜色的强度，就像这样:</p>
<div><img src="img/37850946-f7a2-47ba-8780-ae325f94f654.png" class="calibre75"/></div>
<p>图 10:特定图像的不同颜色通道</p>
<p class="calibre2">所以，红色通道在每个点的强度与宽度和高度可以用一个矩阵来表示；蓝色和绿色通道也是如此。所以，我们最终有三个矩阵，当它们结合在一起，就形成了一个张量。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Variables</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">变量</h1>
                
            
            
                
<p class="calibre2">现在我们更熟悉数据的结构，我们将看看 TensorFlow 如何处理变量。</p>
<p class="calibre2">为了定义变量，我们使用命令<kbd class="calibre12">tf.variable()</kbd>。为了能够在计算图形中使用变量，有必要在会话中运行图形之前初始化它们。这是通过运行<kbd class="calibre12">tf.global_variables_initializer()</kbd>来完成的。</p>
<p class="calibre2">要更新变量的值，我们只需运行一个赋值操作，为变量赋值:</p>
<pre class="calibre21">state = tf.Variable(0)</pre>
<p class="calibre2">让我们首先创建一个简单的计数器，一个每次增加一个单位的变量:</p>
<pre class="calibre21">one = tf.constant(1)<br class="title-page-name"/>new_value = tf.add(state, one)<br class="title-page-name"/>update = tf.assign(state, new_value)</pre>
<p class="calibre2">启动图形后，必须通过运行初始化操作来初始化变量。我们首先必须将初始化操作添加到图形中:</p>
<pre class="calibre21">init_op = tf.global_variables_initializer()</pre>
<p class="calibre2">然后，我们启动一个会话来运行图表。</p>
<p class="calibre2">我们首先初始化变量，然后打印状态变量的初始值，最后运行更新状态变量的操作，并在每次更新后打印结果:</p>
<pre class="calibre21">with tf.Session() as session:<br class="title-page-name"/> session.run(init_op)<br class="title-page-name"/> print(session.run(state))<br class="title-page-name"/> for _ in range(3):<br class="title-page-name"/>    session.run(update)<br class="title-page-name"/>    print(session.run(state))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>0<br class="title-page-name"/>1<br class="title-page-name"/>2<br class="title-page-name"/>3</pre>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Placeholders</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">占位符</h1>
                
            
            
                
<p class="calibre2">现在，我们知道了如何在 TensorFlow 内部操纵变量，但是如何在 TensorFlow 模型之外提供数据呢？</p>
<p class="calibre2">如果要从模型外部向 TensorFlow 模型提供数据，需要使用占位符。</p>
<p class="calibre2">那么，这些占位符是什么，它们有什么作用？占位符可以被看作是模型中的<em class="calibre19">孔</em>，数据将被传递到的<em class="calibre19">孔</em>。您可以使用<kbd class="calibre12">tf.placeholder(datatype)</kbd>来创建它们，其中<kbd class="calibre12">datatype</kbd>指定数据的类型(整数、浮点、字符串和布尔值)及其精度(8、16、32 和 64)位。</p>
<p class="calibre2">具有各自 Python 语法的每种数据类型的定义如下:</p>
<p class="packtfigref1"><strong class="calibre1">表 3–不同张量流数据类型的定义</strong></p>
<div><table class="calibre35">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><strong class="calibre13">数据类型</strong></p>
</td>
<td class="calibre38">
<p class="calibre2"><strong class="calibre13"> Python 类型</strong></p>
</td>
<td class="calibre38">
<p class="calibre2"><strong class="calibre13">描述</strong></p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_FLOAT</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.float32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">32 位浮点。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_DOUBLE</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.float64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">64 位浮点</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_INT8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.int8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">8 位有符号整数。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_INT16</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.int16</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">16 位有符号整数。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_INT32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.int32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">32 位有符号整数。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_INT64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.int64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">64 位有符号整数。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_UINT8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.uint8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">8 位无符号整数。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_STRING</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.string</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">可变长度字节数组。张量的每个元素都是一个字节数组。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_BOOL</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.bool</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">布尔型。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_COMPLEX64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.complex64</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">由两个 32 位浮点组成的复数:实部和虚部。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_COMPLEX128</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.complex128</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">由两个 64 位浮点组成的复数:实部和虚部。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_QINT8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.qint8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">量化运算中使用的 8 位有符号整数。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_QINT32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.qint32</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">量化运算中使用的 32 位有符号整数。</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">DT_QUINT8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2"><kbd class="calibre12">tf.quint8</kbd></p>
</td>
<td class="calibre38">
<p class="calibre2">量化运算中使用的 8 位无符号整数。</p>
</td>
</tr>
</tbody>
</table>
</div>
<div><p class="calibre2">让我们创建一个占位符:</p>
<pre class="calibre21">a=tf.placeholder(tf.float32)</pre>
<p class="calibre2">并定义一个简单乘法运算:</p>
<pre class="calibre21">b=a*2</pre></div>
<p class="calibre2">现在，我们需要定义并运行会话，但是因为我们在模型中创建了一个<em class="calibre19">孔</em>来传递数据，所以当我们初始化会话时。我们不得不用数据来传递一个论点；否则我们会得到一个错误。</p>
<p class="calibre2">为了将数据传递给模型，我们使用一个额外的参数<kbd class="calibre12">feed_dict</kbd>调用会话，其中我们应该传递一个字典，每个占位符名称后面跟有其各自的数据，就像这样:</p>
<pre class="calibre21">with tf.Session() as sess:<br class="title-page-name"/>    result = sess.run(b,feed_dict={a:3.5})<br class="title-page-name"/>    print result</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>7.0</pre>
<p class="calibre2">由于 TensorFlow 中的数据是以多维数组的形式传递的，因此我们可以通过占位符传递任何类型的张量，以获得简单乘法运算的答案:</p>
<pre class="calibre21">dictionary={a: [ [ [1,2,3],[4,5,6],[7,8,9],[10,11,12] ] , [ [13,14,15],[16,17,18],[19,20,21],[22,23,24] ] ] }<br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/>    result = sess.run(b,feed_dict=dictionary)<br class="title-page-name"/>    print result</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>[[[  2.   4.   6.]<br class="title-page-name"/>  [  8.  10.  12.]<br class="title-page-name"/>  [ 14.  16.  18.]<br class="title-page-name"/>  [ 20.  22.  24.]]<br class="title-page-name"/><br class="title-page-name"/> [[ 26.  28.  30.]<br class="title-page-name"/>  [ 32.  34.  36.]<br class="title-page-name"/>  [ 38.  40.  42.]<br class="title-page-name"/>  [ 44.  46.  48.]]]</pre>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Operations</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">操作</h1>
                
            
            
                
<p class="calibre2">运算是表示图上张量的数学运算的节点。这些操作可以是任何类型的函数，比如增加和减少张量，或者可能是激活函数。</p>
<p class="calibre2"><kbd class="calibre12">tf.matmul</kbd>、<kbd class="calibre12">tf.add</kbd>、<kbd class="calibre12">tf.nn.sigmoid</kbd>是 TensorFlow 中的一些操作。这些类似于 Python 中的函数，但是直接在张量上操作，每个函数做一件特定的事情。</p>
<p class="calibre2">其他操作可在<a href="https://www.tensorflow.org/api_guides/python/math_ops" target="_blank" class="calibre11">https://www.tensorflow.org/api_guides/python/math_ops</a>轻松找到。</p>
<p class="calibre2">让我们尝试一下这些操作:</p>
<pre class="calibre21">a = tf.constant([5])<br class="title-page-name"/>b = tf.constant([2])<br class="title-page-name"/>c = tf.add(a,b)<br class="title-page-name"/>d = tf.subtract(a,b)<br class="title-page-name"/>with tf.Session() as session:<br class="title-page-name"/>    result = session.run(c)<br class="title-page-name"/>    print 'c =: %s' % result<br class="title-page-name"/>    result = session.run(d)<br class="title-page-name"/>    print 'd =: %s' % result</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>c =: [7]<br class="title-page-name"/>d =: [3]</pre>
<p class="calibre2"><kbd class="calibre12">tf.nn.sigmoid</kbd>是一个激活函数:稍微复杂一点，但是这个函数帮助学习模型评估什么样的信息是好的还是不好的。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Linear regression model – building and training</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">线性回归模型–构建和培训</h1>
                
            
            
                
<p class="calibre2">根据我们在<a href="6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml" target="_blank" class="calibre11">第 2 章</a>、<em class="calibre19">数据建模中对线性回归的解释——泰坦尼克号的例子</em>我们将依靠这个定义建立一个简单的线性回归模型。</p>
<p class="calibre2">让我们从导入实现所需的包开始:</p>
<pre class="calibre21">import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import matplotlib.patches as mpatches<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>plt.rcParams['figure.figsize'] = (10, 6)</pre>
<p class="calibre2">让我们定义一个独立变量:</p>
<pre class="calibre21">input_values = np.arange(0.0, 5.0, 0.1)<br class="title-page-name"/>input_values</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,<br class="title-page-name"/>        1.1,  1.2,  1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,<br class="title-page-name"/>        2.2,  2.3,  2.4,  2.5,  2.6,  2.7,  2.8,  2.9,  3. ,  3.1,  3.2,<br class="title-page-name"/>        3.3,  3.4,  3.5,  3.6,  3.7,  3.8,  3.9,  4. ,  4.1,  4.2,  4.3,<br class="title-page-name"/>        4.4,  4.5,  4.6,  4.7,  4.8,  4.9])</pre>
<pre class="calibre21">##You can adjust the slope and intercept to verify the changes in the graph<br class="title-page-name"/>weight=1<br class="title-page-name"/>bias=0<br class="title-page-name"/>output = weight*input_values + bias<br class="title-page-name"/>plt.plot(input_values,output)<br class="title-page-name"/>plt.ylabel('Dependent Variable')<br class="title-page-name"/>plt.xlabel('Indepdendent Variable')<br class="title-page-name"/>plt.show()<br class="title-page-name"/>Output:</pre>
<div><img src="img/8e33a510-0612-4242-8cc9-7229a1ae9265.png" class="calibre76"/></div>
<p>图 11:因变量和自变量的可视化</p>
<p class="calibre2"> </p>
<p class="calibre2">现在，让我们看看这是如何被解释成张量流代码的。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Linear regression with TensorFlow</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">张量流线性回归</h1>
                
            
            
                
<p class="calibre2">对于第一部分，我们将生成随机数据点，并定义一个线性关系；我们将使用 TensorFlow 来调整并获得正确的参数:</p>
<pre class="calibre21">input_values = np.random.rand(100).astype(np.float32)</pre>
<p class="calibre2">本例中使用的模型方程为:</p>
<div><img class="fm-editor-equation14" src="img/9bc580b1-9609-40b2-9c9c-f3cffa70bd0f.png"/></div>
<p class="calibre2">这个等式没有什么特别的，它只是一个我们用来生成数据点的模型。事实上，您可以将参数更改为您想要的任何值，稍后您将会这样做。我们在这些点上添加了一些高斯噪声，让它变得更有趣一些:</p>
<pre class="calibre21">output_values = input_values * 2 + 3<br class="title-page-name"/>output_values = np.vectorize(lambda y: y + np.random.normal(loc=0.0, scale=0.1))(output_values)</pre>
<p class="calibre2">下面是一个数据示例:</p>
<pre class="calibre21">list(zip(input_values,output_values))[5:10]</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>[(0.25240293, 3.474361759429548), <br class="title-page-name"/>(0.946697, 4.980617375175061), <br class="title-page-name"/>(0.37582186, 3.650345806087635), <br class="title-page-name"/>(0.64025956, 4.271037640404975), <br class="title-page-name"/>(0.62555283, 4.37001850440196)]</pre>
<p class="calibre2">首先，我们用任何随机猜测初始化变量<img class="fm-editor-equation15" src="img/7beac794-06f0-4f0c-95d2-8f5344ad94e2.png"/>和<img class="fm-editor-equation16" src="img/323cad53-6532-4e69-8234-50b2b8511bc7.png"/>，然后我们定义线性函数:</p>
<pre class="calibre21">weight = tf.Variable(1.0)<br class="title-page-name"/>bias = tf.Variable(0.2)<br class="title-page-name"/>predicted_vals = weight * input_values + bias</pre>
<p class="calibre2">在典型的线性回归模型中，我们最小化要调整的方程的平方误差减去目标值(我们拥有的数据)，因此我们将要最小化的方程定义为损失。</p>
<p class="calibre2">为了找到损失的价值，我们使用<kbd class="calibre12">tf.reduce_mean()</kbd>。该函数求多维张量的平均值，结果可以有不同的维数:</p>
<pre class="calibre21">model_loss = tf.reduce_mean(tf.square(predicted_vals - output_values))</pre>
<p class="calibre2">然后，我们定义优化器方法。这里，我们将使用一个简单的梯度下降，学习率为 0.5。</p>
<p class="calibre2">现在，我们将定义我们的图的训练方法，但是我们将使用什么方法来最小化损失？是<kbd class="calibre12">tf.train.GradientDescentOptimizer</kbd>。</p>
<p class="calibre2"><kbd class="calibre12">.minimize()</kbd>函数将最小化我们的优化器的误差函数，从而产生一个更好的模型:</p>
<pre class="calibre21">model_optimizer = tf.train.GradientDescentOptimizer(0.5)<br class="title-page-name"/>train = model_optimizer.minimize(model_loss)</pre>
<p class="calibre2">在执行图形之前，不要忘记初始化变量:</p>
<pre class="calibre21">init = tf.global_variables_initializer()<br class="title-page-name"/>sess = tf.Session()<br class="title-page-name"/>sess.run(init)</pre>
<p class="calibre2">现在，我们准备开始优化并运行图表:</p>
<pre class="calibre21">train_data = []<br class="title-page-name"/>for step in range(100):<br class="title-page-name"/>    evals = sess.run([train,weight,bias])[1:]<br class="title-page-name"/>    if step % 5 == 0:<br class="title-page-name"/>       print(step, evals)<br class="title-page-name"/>       train_data.append(evals)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>(0, [2.5176678, 2.9857566])<br class="title-page-name"/>(5, [2.4192538, 2.3015416])<br class="title-page-name"/>(10, [2.5731843, 2.221911])<br class="title-page-name"/>(15, [2.6890132, 2.1613526])<br class="title-page-name"/>(20, [2.7763696, 2.1156814])<br class="title-page-name"/>(25, [2.8422525, 2.0812368])<br class="title-page-name"/>(30, [2.8919399, 2.0552595])<br class="title-page-name"/>(35, [2.9294133, 2.0356679])<br class="title-page-name"/>(40, [2.957675, 2.0208921])<br class="title-page-name"/>(45, [2.9789894, 2.0097487])<br class="title-page-name"/>(50, [2.9950645, 2.0013444])<br class="title-page-name"/>(55, [3.0071881, 1.995006])<br class="title-page-name"/>(60, [3.0163314, 1.9902257])<br class="title-page-name"/>(65, [3.0232272, 1.9866205])<br class="title-page-name"/>(70, [3.0284278, 1.9839015])<br class="title-page-name"/>(75, [3.0323503, 1.9818509])<br class="title-page-name"/>(80, [3.0353084, 1.9803041])<br class="title-page-name"/>(85, [3.0375392, 1.9791379])<br class="title-page-name"/>(90, [3.039222, 1.9782581])<br class="title-page-name"/>(95, [3.0404909, 1.9775947])</pre>
<p class="calibre2">让我们将训练过程形象化，以符合数据点:</p>
<pre class="calibre21">print('Plotting the data points with their corresponding fitted line...')<br class="title-page-name"/>converter = plt.colors<br class="title-page-name"/>cr, cg, cb = (1.0, 1.0, 0.0)<br class="title-page-name"/><br class="title-page-name"/>for f in train_data:<br class="title-page-name"/><br class="title-page-name"/>    cb += 1.0 / len(train_data)<br class="title-page-name"/>    cg -= 1.0 / len(train_data)<br class="title-page-name"/><br class="title-page-name"/>    if cb &gt; 1.0: cb = 1.0<br class="title-page-name"/><br class="title-page-name"/>    if cg &lt; 0.0: cg = 0.0<br class="title-page-name"/><br class="title-page-name"/>    [a, b] = f<br class="title-page-name"/>    f_y = np.vectorize(lambda x: a*x + b)(input_values)<br class="title-page-name"/>    line = plt.plot(input_values, f_y)<br class="title-page-name"/>    plt.setp(line, color=(cr,cg,cb))<br class="title-page-name"/><br class="title-page-name"/>plt.plot(input_values, output_values, 'ro')<br class="title-page-name"/>green_line = mpatches.Patch(color='red', label='Data Points')<br class="title-page-name"/>plt.legend(handles=[green_line])<br class="title-page-name"/>plt.show()</pre>
<pre class="calibre21">Output:</pre>
<div><img src="img/1840b114-b57e-4f87-b90b-6a8082c02b13.png" class="calibre77"/></div>
<p>图 12:由回归线拟合的数据点的可视化</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Logistic regression model – building and training</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">逻辑回归模型——构建和培训</h1>
                
            
            
                
<p class="calibre2">同样基于我们在<a href="6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml" target="_blank" class="calibre11">第 2 章</a>、<em class="calibre19">数据建模在行动-泰坦尼克号示例</em>中对逻辑回归的解释，我们将在 TensorFlow 中实现逻辑回归算法。简而言之，逻辑回归通过逻辑/sigmoid 传递输入，然后将结果视为概率:</p>
<div><img src="img/06e797f4-9584-46f6-8915-d1177426c2bb.png" class="calibre78"/></div>
<p>图 13:区分两个线性可分的类，0 和 1</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Utilizing logistic regression in TensorFlow</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">在 TensorFlow 中使用逻辑回归</h1>
                
            
            
                
<p class="calibre2">为了在 TensorFlow 中使用逻辑回归，我们首先需要导入将要使用的库。为此，您可以运行以下代码单元格:</p>
<pre class="calibre21">import tensorflow as tf<br class="title-page-name"/><br class="title-page-name"/>import pandas as pd<br class="title-page-name"/><br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import time<br class="title-page-name"/>from sklearn.datasets import load_iris<br class="title-page-name"/>from sklearn.cross_validation import train_test_split<br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<p class="calibre2">接下来，我们将加载我们将要使用的数据集。在这种情况下，我们使用的是内置的虹膜数据集。因此，没有必要做任何预处理，我们可以直接操作它。我们将数据集分成<em class="calibre19"> x </em>和<em class="calibre19"> y </em>，然后随机地(伪)训练<em class="calibre19"> x </em>和<em class="calibre19"> y </em>并测试<em class="calibre19"> x </em>和<em class="calibre19"> y </em>:</p>
<pre class="calibre21">iris_dataset = load_iris()<br class="title-page-name"/>iris_input_values, iris_output_values = iris_dataset.data[:-1,:], iris_dataset.target[:-1]<br class="title-page-name"/>iris_output_values= pd.get_dummies(iris_output_values).values<br class="title-page-name"/>train_input_values, test_input_values, train_target_values, test_target_values = train_test_split(iris_input_values, iris_output_values, test_size=0.33, random_state=42)</pre>
<p class="calibre2">现在，我们定义<em class="calibre19"> x </em>和<em class="calibre19"> y </em>。这些占位符将保存我们的虹膜数据(包括特征和标签矩阵)，并帮助将它们传递给算法的不同部分。您可以将占位符视为我们用来插入数据的空壳。我们还需要赋予它们与数据形状相对应的形状。稍后，我们将通过一个<kbd class="calibre12">feed_dict</kbd>(输入字典)向占位符输入数据，从而将数据插入到这些占位符中:</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Why use placeholders?</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">为什么要使用占位符？</h1>
                
            
            
                
<p class="calibre2">TensorFlow 的这个功能允许我们创建一个算法，它接受数据，并在不知道数据量的情况下了解数据的形状。当我们在训练中插入<em class="calibre19">批</em>数据时，我们可以在不改变整个算法的情况下，轻松调整单步训练的样本数量:</p>
<pre class="calibre21"># numFeatures is the number of features in our input data.<br class="title-page-name"/># In the iris dataset, this number is '4'.<br class="title-page-name"/>num_explanatory_features = train_input_values.shape[1]<br class="title-page-name"/><br class="title-page-name"/># numLabels is the number of classes our data points can be in.<br class="title-page-name"/># In the iris dataset, this number is '3'.<br class="title-page-name"/>num_target_values = train_target_values.shape[1]<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Placeholders<br class="title-page-name"/># 'None' means TensorFlow shouldn't expect a fixed number in that dimension<br class="title-page-name"/>input_values = tf.placeholder(tf.float32, [None, num_explanatory_features]) # Iris has 4 features, so X is a tensor to hold our data.<br class="title-page-name"/>output_values = tf.placeholder(tf.float32, [None, num_target_values]) # This will be our correct answers matrix for 3 classes.<br class="title-page-name"/></pre>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Set model weights and bias</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">设置模型权重和偏差</h1>
                
            
            
                
<p class="calibre2">与线性回归非常相似，我们需要一个共享的可变权重矩阵用于逻辑回归。我们将<em class="calibre19"> W </em>和<em class="calibre19"> b </em>都初始化为充满零的张量。既然要学习<em class="calibre19"> W </em>和<em class="calibre19"> b </em>，那么它们的初始值就没有太大关系了。这些变量是定义我们的回归模型结构的对象，我们可以在它们被训练后保存它们，以便我们以后可以重用它们。</p>
<p class="calibre2">我们定义两个张量流变量作为我们的参数。这些变量将包含我们的逻辑回归的权重和偏差，它们将在训练期间不断更新。</p>
<p class="calibre2">请注意，<em class="calibre19"> W </em>的形状为[4，3],因为我们希望将 4 维输入向量乘以它，以生成差异类的 3 维证据向量。<em class="calibre19"> b </em>有一个【3】的形状，我们可以把它加到输出上。此外，与我们的占位符(本质上是等待输入数据的空壳)不同，TensorFlow 变量需要用值初始化，比如用零初始化:</p>
<pre class="calibre21">#Randomly sample from a normal distribution with standard deviation .01<br class="title-page-name"/><br class="title-page-name"/>weights = tf.Variable(tf.random_normal([num_explanatory_features,num_target_values],<br class="title-page-name"/>                                      mean=0,<br class="title-page-name"/>                                      stddev=0.01,<br class="title-page-name"/>                                      name="weights"))<br class="title-page-name"/><br class="title-page-name"/>biases = tf.Variable(tf.random_normal([1,num_target_values],<br class="title-page-name"/>                                   mean=0,<br class="title-page-name"/>                                   stddev=0.01,<br class="title-page-name"/>                                   name="biases"))</pre>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Logistic regression model</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">逻辑回归模型</h1>
                
            
            
                
<p class="calibre2">我们现在定义我们的操作，以便正确地运行逻辑回归。逻辑回归通常被认为是一个单一的方程:</p>
<div><img class="fm-editor-equation17" src="img/c5e9a90e-75b4-4675-878e-4be59ea25db9.png"/></div>
<p class="calibre2">然而，为了清楚起见，我们可以将其分为三个主要部分:</p>
<ul class="calibre7">
<li class="calibre8">一种加权乘特征矩阵乘法运算</li>
<li class="calibre8">加权特征和偏差项的总和</li>
<li class="calibre8">最后，应用一个 sigmoid 函数</li>
</ul>
<p class="calibre2">因此，您会发现这些组件被定义为三个独立的操作:</p>
<pre class="calibre21"># Three-component breakdown of the Logistic Regression equation.<br class="title-page-name"/># Note that these feed into each other.<br class="title-page-name"/>apply_weights = tf.matmul(input_values, weights, name="apply_weights")<br class="title-page-name"/>add_bias = tf.add(apply_weights, biases, name="add_bias")<br class="title-page-name"/>activation_output = tf.nn.sigmoid(add_bias, name="activation")</pre>
<p class="calibre2">正如我们之前看到的，我们要使用的函数是逻辑函数，它在应用权重和偏差后被输入数据。在 TensorFlow 中，这个函数被实现为<kbd class="calibre12">nn.sigmoid</kbd>函数。实际上，它将有偏差的加权输入拟合成 0-100%的曲线，这就是我们想要的概率函数。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Training</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">培养</h1>
                
            
            
                
<p class="calibre2">学习算法就是我们如何搜索最佳权重向量(<em class="calibre19"> w </em>)。这种搜索是寻找优化误差/成本测量的假设的优化问题。</p>
<p class="calibre2">所以，模型的成本或损失函数会告诉我们，我们的模型不好，我们需要最小化这个函数。您可以遵循不同的损失或成本标准。在这个实现中，我们将使用<strong class="calibre13">均值</strong> <strong class="calibre13">平方误差</strong> ( <strong class="calibre13"> MSE </strong>)作为损失函数。</p>
<p class="calibre2">为了完成最小化损失函数的任务，我们将使用梯度下降算法。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Cost function</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">价值函数</h1>
                
            
            
                
<p class="calibre2">在定义我们的成本函数之前，我们需要定义我们要训练多长时间，以及我们应该如何定义学习率:</p>
<pre class="calibre21">#Number of training epochs<br class="title-page-name"/>num_epochs = 700<br class="title-page-name"/># Defining our learning rate iterations (decay)<br class="title-page-name"/>learning_rate = tf.train.exponential_decay(learning_rate=0.0008,<br class="title-page-name"/>                                          global_step=1,<br class="title-page-name"/>                                          decay_steps=train_input_values.shape[0],<br class="title-page-name"/>                                          decay_rate=0.95,<br class="title-page-name"/>                                          staircase=True)<br class="title-page-name"/><br class="title-page-name"/># Defining our cost function - Squared Mean Error<br class="title-page-name"/>model_cost = tf.nn.l2_loss(activation_output - output_values, name="squared_error_cost")<br class="title-page-name"/># Defining our Gradient Descent<br class="title-page-name"/>model_train = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_cost)</pre>
<p class="calibre2">现在，是时候通过会话变量来执行我们的计算图了。</p>
<p class="calibre2">首先，我们需要使用<kbd class="calibre12">tf.initialize_all_variables()</kbd>用零或随机值初始化我们的权重和偏差。这个初始化步骤将成为我们计算图形中的一个节点，当我们将图形放入会话时，操作将运行并创建变量:</p>
<pre class="calibre21"># tensorflow session<br class="title-page-name"/>sess = tf.Session()<br class="title-page-name"/><br class="title-page-name"/># Initialize our variables.<br class="title-page-name"/>init = tf.global_variables_initializer()<br class="title-page-name"/>sess.run(init)<br class="title-page-name"/><br class="title-page-name"/>#We also want some additional operations to keep track of our model's efficiency over time. We can do this like so:<br class="title-page-name"/># argmax(activation_output, 1) returns the label with the most probability<br class="title-page-name"/># argmax(output_values, 1) is the correct label<br class="title-page-name"/>correct_predictions = tf.equal(tf.argmax(activation_output,1),tf.argmax(output_values,1))<br class="title-page-name"/><br class="title-page-name"/># If every false prediction is 0 and every true prediction is 1, the average returns us the accuracy<br class="title-page-name"/>model_accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"))<br class="title-page-name"/><br class="title-page-name"/># Summary op for regression output<br class="title-page-name"/>activation_summary = tf.summary.histogram("output", activation_output)<br class="title-page-name"/><br class="title-page-name"/># Summary op for accuracy<br class="title-page-name"/>accuracy_summary = tf.summary.scalar("accuracy", model_accuracy)<br class="title-page-name"/><br class="title-page-name"/># Summary op for cost<br class="title-page-name"/>cost_summary = tf.summary.scalar("cost", model_cost)<br class="title-page-name"/><br class="title-page-name"/># Summary ops to check how variables weights and biases are updating after each iteration to be visualized in TensorBoard<br class="title-page-name"/>weight_summary = tf.summary.histogram("weights", weights.eval(session=sess))<br class="title-page-name"/>bias_summary = tf.summary.histogram("biases", biases.eval(session=sess))<br class="title-page-name"/><br class="title-page-name"/>merged = tf.summary.merge([activation_summary, accuracy_summary, cost_summary, weight_summary, bias_summary])<br class="title-page-name"/>writer = tf.summary.FileWriter("summary_logs", sess.graph)<br class="title-page-name"/><br class="title-page-name"/>#Now we can define and run the actual training loop, like this:<br class="title-page-name"/># Initialize reporting variables<br class="title-page-name"/><br class="title-page-name"/>inital_cost = 0<br class="title-page-name"/>diff = 1<br class="title-page-name"/>epoch_vals = []<br class="title-page-name"/>accuracy_vals = []<br class="title-page-name"/>costs = []<br class="title-page-name"/><br class="title-page-name"/># Training epochs<br class="title-page-name"/>for i in range(num_epochs):<br class="title-page-name"/>    if i &gt; 1 and diff &lt; .0001:<br class="title-page-name"/>       print("change in cost %g; convergence."%diff)<br class="title-page-name"/>       break<br class="title-page-name"/><br class="title-page-name"/>    else:<br class="title-page-name"/>       # Run training step<br class="title-page-name"/>       step = sess.run(model_train, feed_dict={input_values: train_input_values, output_values: train_target_values})<br class="title-page-name"/><br class="title-page-name"/>       # Report some stats evert 10 epochs<br class="title-page-name"/>       if i % 10 == 0:<br class="title-page-name"/>           # Add epoch to epoch_values<br class="title-page-name"/>           epoch_vals.append(i)<br class="title-page-name"/><br class="title-page-name"/>           # Generate the accuracy stats of the model<br class="title-page-name"/>           train_accuracy, new_cost = sess.run([model_accuracy, model_cost], feed_dict={input_values: train_input_values, output_values: train_target_values})<br class="title-page-name"/><br class="title-page-name"/>           # Add accuracy to live graphing variable<br class="title-page-name"/>           accuracy_vals.append(train_accuracy)<br class="title-page-name"/><br class="title-page-name"/>           # Add cost to live graphing variable<br class="title-page-name"/>           costs.append(new_cost)<br class="title-page-name"/>&gt;<br class="title-page-name"/>           # Re-assign values for variables<br class="title-page-name"/>           diff = abs(new_cost - inital_cost)<br class="title-page-name"/>           cost = new_cost<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>           print("Training step %d, accuracy %g, cost %g, cost change %g"%(i, train_accuracy, new_cost, diff))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Training step 0, accuracy 0.343434, cost 34.6022, cost change 34.6022<br class="title-page-name"/>Training step 10, accuracy 0.434343, cost 30.3272, cost change 30.3272<br class="title-page-name"/>Training step 20, accuracy 0.646465, cost 28.3478, cost change 28.3478<br class="title-page-name"/>Training step 30, accuracy 0.646465, cost 26.6752, cost change 26.6752<br class="title-page-name"/>Training step 40, accuracy 0.646465, cost 25.2844, cost change 25.2844<br class="title-page-name"/>Training step 50, accuracy 0.646465, cost 24.1349, cost change 24.1349<br class="title-page-name"/>Training step 60, accuracy 0.646465, cost 23.1835, cost change 23.1835<br class="title-page-name"/>Training step 70, accuracy 0.646465, cost 22.3911, cost change 22.3911<br class="title-page-name"/>Training step 80, accuracy 0.646465, cost 21.7254, cost change 21.7254<br class="title-page-name"/>Training step 90, accuracy 0.646465, cost 21.1607, cost change 21.1607<br class="title-page-name"/>Training step 100, accuracy 0.666667, cost 20.677, cost change 20.677<br class="title-page-name"/>Training step 110, accuracy 0.666667, cost 20.2583, cost change 20.2583<br class="title-page-name"/>Training step 120, accuracy 0.666667, cost 19.8927, cost change 19.8927<br class="title-page-name"/>Training step 130, accuracy 0.666667, cost 19.5705, cost change 19.5705<br class="title-page-name"/>Training step 140, accuracy 0.666667, cost 19.2842, cost change 19.2842<br class="title-page-name"/>Training step 150, accuracy 0.666667, cost 19.0278, cost change 19.0278<br class="title-page-name"/>Training step 160, accuracy 0.676768, cost 18.7966, cost change 18.7966<br class="title-page-name"/>Training step 170, accuracy 0.69697, cost 18.5867, cost change 18.5867<br class="title-page-name"/>Training step 180, accuracy 0.69697, cost 18.3951, cost change 18.3951<br class="title-page-name"/>Training step 190, accuracy 0.717172, cost 18.2191, cost change 18.2191<br class="title-page-name"/>Training step 200, accuracy 0.717172, cost 18.0567, cost change 18.0567<br class="title-page-name"/>Training step 210, accuracy 0.737374, cost 17.906, cost change 17.906<br class="title-page-name"/>Training step 220, accuracy 0.747475, cost 17.7657, cost change 17.7657<br class="title-page-name"/>Training step 230, accuracy 0.747475, cost 17.6345, cost change 17.6345<br class="title-page-name"/>Training step 240, accuracy 0.757576, cost 17.5113, cost change 17.5113<br class="title-page-name"/>Training step 250, accuracy 0.787879, cost 17.3954, cost change 17.3954<br class="title-page-name"/>Training step 260, accuracy 0.787879, cost 17.2858, cost change 17.2858<br class="title-page-name"/>Training step 270, accuracy 0.787879, cost 17.182, cost change 17.182<br class="title-page-name"/>Training step 280, accuracy 0.787879, cost 17.0834, cost change 17.0834<br class="title-page-name"/>Training step 290, accuracy 0.787879, cost 16.9895, cost change 16.9895<br class="title-page-name"/>Training step 300, accuracy 0.79798, cost 16.8999, cost change 16.8999<br class="title-page-name"/>Training step 310, accuracy 0.79798, cost 16.8141, cost change 16.8141<br class="title-page-name"/>Training step 320, accuracy 0.79798, cost 16.732, cost change 16.732<br class="title-page-name"/>Training step 330, accuracy 0.79798, cost 16.6531, cost change 16.6531<br class="title-page-name"/>Training step 340, accuracy 0.808081, cost 16.5772, cost change 16.5772<br class="title-page-name"/>Training step 350, accuracy 0.818182, cost 16.5041, cost change 16.5041<br class="title-page-name"/>Training step 360, accuracy 0.838384, cost 16.4336, cost change 16.4336<br class="title-page-name"/>Training step 370, accuracy 0.838384, cost 16.3655, cost change 16.3655<br class="title-page-name"/>Training step 380, accuracy 0.838384, cost 16.2997, cost change 16.2997<br class="title-page-name"/>Training step 390, accuracy 0.838384, cost 16.2359, cost change 16.2359<br class="title-page-name"/>Training step 400, accuracy 0.848485, cost 16.1741, cost change 16.1741<br class="title-page-name"/>Training step 410, accuracy 0.848485, cost 16.1141, cost change 16.1141<br class="title-page-name"/>Training step 420, accuracy 0.848485, cost 16.0558, cost change 16.0558<br class="title-page-name"/>Training step 430, accuracy 0.858586, cost 15.9991, cost change 15.9991<br class="title-page-name"/>Training step 440, accuracy 0.858586, cost 15.944, cost change 15.944<br class="title-page-name"/>Training step 450, accuracy 0.858586, cost 15.8903, cost change 15.8903<br class="title-page-name"/>Training step 460, accuracy 0.868687, cost 15.8379, cost change 15.8379<br class="title-page-name"/>Training step 470, accuracy 0.878788, cost 15.7869, cost change 15.7869<br class="title-page-name"/>Training step 480, accuracy 0.878788, cost 15.7371, cost change 15.7371<br class="title-page-name"/>Training step 490, accuracy 0.878788, cost 15.6884, cost change 15.6884<br class="title-page-name"/>Training step 500, accuracy 0.878788, cost 15.6409, cost change 15.6409<br class="title-page-name"/>Training step 510, accuracy 0.878788, cost 15.5944, cost change 15.5944<br class="title-page-name"/>Training step 520, accuracy 0.878788, cost 15.549, cost change 15.549<br class="title-page-name"/>Training step 530, accuracy 0.888889, cost 15.5045, cost change 15.5045<br class="title-page-name"/>Training step 540, accuracy 0.888889, cost 15.4609, cost change 15.4609<br class="title-page-name"/>Training step 550, accuracy 0.89899, cost 15.4182, cost change 15.4182<br class="title-page-name"/>Training step 560, accuracy 0.89899, cost 15.3764, cost change 15.3764<br class="title-page-name"/>Training step 570, accuracy 0.89899, cost 15.3354, cost change 15.3354<br class="title-page-name"/>Training step 580, accuracy 0.89899, cost 15.2952, cost change 15.2952<br class="title-page-name"/>Training step 590, accuracy 0.909091, cost 15.2558, cost change 15.2558<br class="title-page-name"/>Training step 600, accuracy 0.909091, cost 15.217, cost change 15.217<br class="title-page-name"/>Training step 610, accuracy 0.909091, cost 15.179, cost change 15.179<br class="title-page-name"/>Training step 620, accuracy 0.909091, cost 15.1417, cost change 15.1417<br class="title-page-name"/>Training step 630, accuracy 0.909091, cost 15.105, cost change 15.105<br class="title-page-name"/>Training step 640, accuracy 0.909091, cost 15.0689, cost change 15.0689<br class="title-page-name"/>Training step 650, accuracy 0.909091, cost 15.0335, cost change 15.0335<br class="title-page-name"/>Training step 660, accuracy 0.909091, cost 14.9987, cost change 14.9987<br class="title-page-name"/>Training step 670, accuracy 0.909091, cost 14.9644, cost change 14.9644<br class="title-page-name"/>Training step 680, accuracy 0.909091, cost 14.9307, cost change 14.9307<br class="title-page-name"/>Training step 690, accuracy 0.909091, cost 14.8975, cost change 14.8975<br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">现在，是时候看看我们的训练模型在<kbd class="calibre12">iris</kbd>数据集上的表现了，所以让我们针对测试集测试我们的训练模型:</p>
<pre class="calibre21"># test the model against the test set<br class="title-page-name"/>print("final accuracy on test set: %s" %str(sess.run(model_accuracy,<br class="title-page-name"/>                                                    feed_dict={input_values: test_input_values,<br class="title-page-name"/>                                                               output_values: test_target_values}))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>final accuracy on test set: 0.9</pre>
<div><p class="calibre2">在测试集上获得 0.9 的准确度真的很好，你可以通过改变历元的数量来尝试获得更好的结果。</p>
</div>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Summary</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p class="calibre2">在本章中，我们对神经网络和多层神经网络的必要性进行了基本解释。我们还用一些基本的例子介绍了 TensorFlow 计算图模型，比如线性回归和逻辑回归。</p>
<p class="calibre2">接下来，我们将通过更高级的示例，演示如何使用 TensorFlow 来构建类似手写字符识别的东西。我们还将处理架构工程的核心思想，它已经取代了传统机器学习中的特征工程。</p>


            

            
        
    </body>

</html>
</body></html>