<html><head/><body>
<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Autoencoders – Feature Extraction and Denoising</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">自动编码器–特征提取和去噪</h1>
                
            
            
                
<p class="calibre2">自动编码器网络是目前广泛使用的深度学习架构之一。它主要用于高效解码任务的无监督学习。通过学习特定数据集的编码或表示，它还可以用于降维。使用本章中的自动编码器，我们将展示如何通过构建另一个具有相同维度但噪声更少的数据集来对数据集进行降噪。为了在实践中使用这一概念，我们将从 MNIST 数据集中提取重要的特征，并尝试了解性能将如何因此得到显著提高。</p>
<p class="calibre2">本章将涵盖以下主题:</p>
<ul class="calibre7">
<li class="calibre8">自动编码器简介</li>
<li class="calibre8">自动编码器的例子</li>
<li class="calibre8">自动编码器架构</li>
<li class="calibre8">压缩 MNIST 数据集</li>
<li class="calibre8">卷积自动编码器</li>
<li class="calibre8">降噪自动编码器</li>
<li class="calibre8">自动编码器的应用</li>
</ul>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Introduction to autoencoders</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">自动编码器简介</h1>
                
            
            
                
<p class="calibre2">自动编码器是另一种深度学习架构，可用于许多有趣的任务，但它也可以被视为普通前馈神经网络的变体，其中输出与输入具有相同的维度。如图<em class="calibre19">图 1 </em>所示，自动编码器的工作方式是通过输入数据样本<em class="calibre19"> (x <sub class="calibre28"> 1 </sub>，...，x <sub class="calibre28"> 6 </sub> ) </em>到网络。它将尝试在层<em class="calibre19"> L2 </em>中学习该数据的较低表示，这可以称为以较低表示对数据集进行编码的一种方式。然后，网络的第二部分，你可以称之为解码器，负责从这个表示构造一个输出<img class="fm-editor-equation32" src="img/f1bff437-7017-445b-93d4-7886dd3f1623.png"/>。您可以将网络从输入数据中学习到的中间较低表示视为其压缩版本。</p>
<p class="calibre2">与我们迄今为止看到的所有其他深度学习架构没有太大不同，自动编码器使用反向传播算法。</p>
<p class="calibre2">自动编码器神经网络是一种无监督学习算法，它应用反向传播，将目标值设置为等于输入值:</p>
<div><img src="img/e447a3cc-4dd7-4ae1-bba0-fd6c845ebbe5.png" class="calibre153"/></div>
<p>图 1:通用自动编码器架构</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Examples of autoencoders</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">自动编码器的例子</h1>
                
            
            
                
<p class="calibre2">在这一章中，我们将使用 MNIST 数据集演示自动编码器的不同变体的一些例子。作为具体的例子，假设输入<em class="calibre19"> x </em>是来自 28×28 图像(784 像素)的像素强度值；因此输入数据样本的数量为<em class="calibre19"> n=784 </em>。<em class="calibre19"> L2 </em>层有<em class="calibre19"> s2=392 </em>个隐藏单元。并且由于输出将具有与输入数据样本相同的维数，<em class="calibre19"> y ∈ R784 </em>。输入层神经元数量将为<em class="calibre19"> 784 </em>，中间层<em class="calibre19"> 392 </em>神经元<em class="calibre19">L2</em>；因此，网络将是一个较低的表示，这是输出的压缩版本。然后，网络将把输入<em class="calibre19"> a(L2) ∈ R392 </em>的这个压缩的下表示馈送到网络的第二部分，该第二部分将尽力从这个压缩的版本中重构输入像素<em class="calibre19"> 784 </em>。</p>
<p class="calibre2">自动编码器依赖于由图像像素表示的输入样本将以某种方式相关的事实，然后它将使用该事实来重建它们。因此，自动编码器有点类似于降维技术，因为它们也学习输入数据的较低表示。</p>
<p class="calibre2">总而言之，典型的自动编码器将由三部分组成:</p>
<ol class="calibre16">
<li class="calibre8">编码器部分，负责将输入压缩成较低的表示形式</li>
<li class="calibre8">该代码是编码器的中间结果</li>
<li class="calibre8">解码器，负责使用该代码重建原始输入</li>
</ol>
<p class="calibre2">下图显示了典型自动编码器的三个主要组件:</p>
<div><img src="img/3f678096-4330-4dfb-b907-c4f22b8c3971.png" class="calibre154"/></div>
<p>图 2:编码器如何作用于图像</p>
<p class="calibre2">正如我们提到的，自动编码器部分学习输入的压缩表示，然后馈送给第三部分，第三部分试图重建输入。重新构建的输入将类似于输出，但不会与原始输出完全相同，因此自动编码器不能用于压缩任务。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Autoencoder architectures</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">自动编码器架构</h1>
                
            
            
                
<p class="calibre2">正如我们提到的，典型的自动编码器由三部分组成。让我们更详细地探讨这三个部分。为了激励你，我们不打算在这一章中重新发明轮子。编码器-解码器部分只是一个完全连接的神经网络，而代码部分是另一个神经网络，但不是完全连接的。该代码部分的维数是可控的，我们可以将其视为超参数:</p>
<div><img src="img/552fb5f6-0459-4e13-887b-d55d288e0f72.png" class="calibre155"/></div>
<p>图 3:自动编码器的通用编码器-解码器架构</p>
<p class="calibre2">在开始使用自动编码器压缩 MNIST 数据集之前，我们将列出一组可以用来微调自动编码器模型的超参数。主要有四个超参数:</p>
<ol class="calibre16">
<li class="calibre8"><strong class="calibre1">代码部分大小</strong>:这是中间层的单元数。这一层中的单元数量越少，我们得到的输入表示就越压缩。</li>
<li class="calibre8"><strong class="calibre1">编码器和解码器的层数</strong>:正如我们提到的，编码器和解码器只不过是一个完全连接的神经网络，我们可以通过增加更多的层来尽可能地深入。</li>
<li class="calibre8"><strong class="calibre1">每层的单位数量</strong>:我们也可以在每层使用不同的单位数量。编码器和解码器的形状非常类似于 DeconvNets，其中编码器的层数随着接近代码部分而减少，然后随着接近解码器的最后一层而开始增加。</li>
<li class="calibre8"><strong class="calibre1">模型损失函数</strong>:我们也可以使用不同的损失函数，比如 MSE 或者交叉熵。</li>
</ol>
<p class="calibre2">在定义这些超参数并给它们初始值之后，我们可以使用反向传播算法来训练网络。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Compressing the MNIST dataset</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">压缩 MNIST 数据集</h1>
                
            
            
                
<p class="calibre2">在这一部分，我们将构建一个简单的自动编码器，可用于压缩 MNIST 数据集。因此，我们将把这个数据集的图像馈送到编码器部分，编码器部分将尝试为它们学习较低的压缩表示；然后，我们将尝试在解码器部分再次构建输入图像。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>The MNIST dataset</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">MNIST 数据集</h1>
                
            
            
                
<p class="calibre2">我们将通过使用 TensorFlow 的帮助函数获取 MNIST 数据集来开始实现。</p>
<p class="calibre2">让我们为这个实现导入必要的包:</p>
<pre class="calibre21">%matplotlib inline<br class="title-page-name"/><br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<pre class="calibre21"><br class="title-page-name"/>from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/>mnist_dataset = input_data.read_data_sets('MNIST_data', validation_size=0)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>Extracting MNIST_data/train-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/train-labels-idx1-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</pre>
<p class="calibre2">让我们从绘制 MNIST 数据集中的一些示例开始:</p>
<pre class="calibre21"># Plotting one image from the training set.<br class="title-page-name"/>image = mnist_dataset.train.images[2]<br class="title-page-name"/>plt.imshow(image.reshape((28, 28)), cmap='Greys_r')</pre>
<pre class="calibre21">Output:</pre>
<div><img src="img/9ca739a8-c53b-48ee-b8b3-18f455abca36.png" class="calibre156"/></div>
<p>图 4:来自 MNIST 数据集的示例图像</p>
<div><pre class="calibre21"># Plotting one image from the training set.<br class="title-page-name"/>image = mnist_dataset.train.images[2]<br class="title-page-name"/>plt.imshow(image.reshape((28, 28)), cmap='Greys_r')<br class="title-page-name"/><br class="title-page-name"/>Output:</pre></div>
<div><img src="img/25c86e3f-00cd-4eda-a9de-ed311292e709.png" class="calibre157"/></div>
<div><p>图 5:来自 MNIST 数据集的示例图像</p>
</div>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Building the model</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">构建模型</h1>
                
            
            
                
<p class="calibre2">为了构建编码器，我们需要计算出每个 MNIST 图像有多少像素，这样我们就可以计算出编码器输入层的大小。MNIST 数据集中的每幅图像都是 28 x 28 像素，因此我们将把这个矩阵整形为 28 x 28 = 784 像素值的向量。我们不需要标准化 MNIST 的图像，因为它们已经被标准化了。</p>
<p class="calibre2">让我们开始构建模型的三个组件。在这个实现中，我们将使用一个非常简单的单一隐藏层架构，然后是 ReLU 激活，如下图所示:</p>
<div><img src="img/a887c40b-4430-4ce7-95c3-8ebf56ad2b69.png" class="calibre158"/></div>
<p>图 6:MNIST 实现的编码器-解码器架构</p>
<p class="calibre2">让我们根据前面的解释来实现这个简单的编码器-解码器架构:</p>
<pre class="calibre21"># The size of the encoding layer or the hidden layer.<br class="title-page-name"/>encoding_layer_dim = 32 <br class="title-page-name"/><br class="title-page-name"/>img_size = mnist_dataset.train.images.shape[1]<br class="title-page-name"/><br class="title-page-name"/># defining placeholder variables of the input and target values<br class="title-page-name"/>inputs_values = tf.placeholder(tf.float32, (None, img_size), name="inputs_values")<br class="title-page-name"/>targets_values = tf.placeholder(tf.float32, (None, img_size), name="targets_values")<br class="title-page-name"/><br class="title-page-name"/># Defining an encoding layer which takes the input values and incode them.<br class="title-page-name"/>encoding_layer = tf.layers.dense(inputs_values, encoding_layer_dim, activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># Defining the logit layer, which is a fully-connected layer but without any activation applied to its output<br class="title-page-name"/>logits_layer = tf.layers.dense(encoding_layer, img_size, activation=None)<br class="title-page-name"/><br class="title-page-name"/># Adding a sigmoid layer after the logit layer<br class="title-page-name"/>decoding_layer = tf.sigmoid(logits_layer, name = "decoding_layer")<br class="title-page-name"/><br class="title-page-name"/># use the sigmoid cross entropy as a loss function<br class="title-page-name"/>model_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_layer, labels=targets_values)<br class="title-page-name"/><br class="title-page-name"/># Averaging the loss values accross the input data<br class="title-page-name"/>model_cost = tf.reduce_mean(model_loss)<br class="title-page-name"/><br class="title-page-name"/># Now we have a cost functiont that we need to optimize using Adam Optimizer<br class="title-page-name"/>model_optimizier = tf.train.AdamOptimizer().minimize(model_cost)</pre>
<p class="calibre2">现在，我们已经定义了我们的模型，并使用了二元交叉熵，因为图像，像素已经标准化。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Model training</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">模特培训</h1>
                
            
            
                
<p class="calibre2">在这一部分，我们将开始培训过程。我们将使用<kbd class="calibre12">mnist_dataset</kbd>对象的 helper 函数，以便从数据集中随机获取一批特定大小的数据；然后，我们将对这批图像运行优化程序。</p>
<p class="calibre2">让我们从创建会话变量开始这一部分，它将负责执行我们前面定义的计算图:</p>
<pre class="calibre21"># creating the session<br class="title-page-name"/> sess = tf.Session()</pre>
<p class="calibre2">接下来，让我们开始培训流程:</p>
<pre class="calibre21">num_epochs = 20<br class="title-page-name"/>train_batch_size = 200<br class="title-page-name"/><br class="title-page-name"/>sess.run(tf.global_variables_initializer())<br class="title-page-name"/>for e in range(num_epochs):<br class="title-page-name"/>    for ii in range(mnist_dataset.train.num_examples//train_batch_size):<br class="title-page-name"/>        input_batch = mnist_dataset.train.next_batch(train_batch_size)<br class="title-page-name"/>        feed_dict = {inputs_values: input_batch[0], targets_values: input_batch[0]}<br class="title-page-name"/>        input_batch_cost, _ = sess.run([model_cost, model_optimizier], feed_dict=feed_dict)<br class="title-page-name"/><br class="title-page-name"/>        print("Epoch: {}/{}...".format(e+1, num_epochs),<br class="title-page-name"/>              "Training loss: {:.3f}".format(input_batch_cost))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.089<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.096<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.096<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.092<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.096<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.089<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.090<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.088<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.090<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.092<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.092<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093</pre>
<p class="calibre2">在运行前面的代码片段 20 个时期后，我们将获得一个经过训练的模型，它能够从 MNIST 数据的测试集中生成或重建图像。请记住，如果我们提供的图像与模型被训练的图像不相似，那么重建过程就不会工作，因为自动编码器是特定于数据的。</p>
<p class="calibre2">让我们通过输入来自测试集的一些图像来测试已训练的模型，并查看模型如何能够在解码器部分重建它们:</p>
<pre class="calibre21">fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))<br class="title-page-name"/><br class="title-page-name"/>input_images = mnist_dataset.test.images[:10]<br class="title-page-name"/>reconstructed_images, compressed_images = sess.run([decoding_layer, encoding_layer], feed_dict={inputs_values: input_images})<br class="title-page-name"/><br class="title-page-name"/>for imgs, row in zip([input_images, reconstructed_images], axes):<br class="title-page-name"/>    for img, ax in zip(imgs, row):<br class="title-page-name"/>        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')<br class="title-page-name"/>        ax.get_xaxis().set_visible(False)<br class="title-page-name"/>        ax.get_yaxis().set_visible(False)<br class="title-page-name"/><br class="title-page-name"/>fig.tight_layout(pad=0.1)</pre>
<p class="calibre2">输出:</p>
<div><img src="img/7167052e-0b8f-4049-9738-4cca4e2c0e0f.png" class="calibre159"/></div>
<p>图 7:原始测试图像(第一行)和它们的构造(第二行)的例子</p>
<p class="calibre2">如您所见，重建图像非常接近输入图像，但我们可能会在编码器-解码器部分使用卷积层来获得更好的图像。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Convolutional autoencoder</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">卷积自动编码器</h1>
                
            
            
                
<p class="calibre2">之前的简单实现在尝试从 MNIST 数据集重建输入图像时做得很好，但我们可以通过自动编码器的编码器和解码器部分中的卷积层获得更好的性能。这种替换的结果网络被称为<strong class="calibre13">卷积自动编码器</strong> ( <strong class="calibre13"> CAE </strong>)。这种能够替换层的灵活性是自动编码器的一大优势，并使它们适用于不同的领域。</p>
<p class="calibre2">我们将用于 CAE 的架构将在网络的解码器部分包含上采样层，以获得图像的重建版本。<br class="calibre20"/></p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Dataset</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">资料组</h1>
                
            
            
                
<p class="calibre2">在这个实现中，我们可以使用任何类型的成像数据集，看看卷积版本的自动编码器会有什么不同。为此，我们仍将使用 MNIST 数据集，因此让我们从使用 TensorFlow 辅助工具获取数据集开始:</p>
<pre class="calibre21">%matplotlib inline<br class="title-page-name"/><br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<pre class="calibre21">from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/>mnist_dataset = input_data.read_data_sets('MNIST_data', validation_size=0)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/><br class="title-page-name"/>mnist_dataset = input_data.read_data_sets('MNIST_data', validation_size=0)<br class="title-page-name"/><br class="title-page-name"/>Extracting MNIST_data/train-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/train-labels-idx1-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</pre>
<p class="calibre2">让我们展示数据集中的一个数字:</p>
<pre class="calibre21"># Plotting one image from the training set.<br class="title-page-name"/>image = mnist_dataset.train.images[2]<br class="title-page-name"/>plt.imshow(image.reshape((28, 28)), cmap='Greys_r')</pre>
<p class="calibre2">输出:</p>
<div><img src="img/2105032e-0770-41d3-af6a-85e6f825f43f.png" class="calibre160"/></div>
<p>图 8:来自 MNIST 数据集的示例图像</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Building the model</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">构建模型</h1>
                
            
            
                
<div><p class="calibre2">在此实现中，我们将使用步长为 1 的卷积层，填充参数设置为相同。这样，我们不会改变图像的高度或宽度。此外，我们正在使用一套最大池层，以减少图像的宽度和高度，从而建立一个压缩的图像较低的代表。</p>
<p class="calibre2">因此，让我们继续构建我们网络的核心:</p>
</div>
<pre class="calibre21">learning_rate = 0.001<br class="title-page-name"/><br class="title-page-name"/># Define the placeholder variable sfor the input and target values<br class="title-page-name"/>inputs_values = tf.placeholder(tf.float32, (None, 28,28,1), name="inputs_values")<br class="title-page-name"/>targets_values = tf.placeholder(tf.float32, (None, 28,28,1), name="targets_values")<br class="title-page-name"/><br class="title-page-name"/># Defining the Encoder part of the netowrk<br class="title-page-name"/># Defining the first convolution layer in the encoder parrt<br class="title-page-name"/># The output tenosor will be in the shape of 28x28x16<br class="title-page-name"/>conv_layer_1 = tf.layers.conv2d(inputs=inputs_values, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x16<br class="title-page-name"/>maxpool_layer_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x8<br class="title-page-name"/>conv_layer_2 = tf.layers.conv2d(inputs=maxpool_layer_1, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x8<br class="title-page-name"/>maxpool_layer_2 = tf.layers.max_pooling2d(conv_layer_2, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x8<br class="title-page-name"/>conv_layer_3 = tf.layers.conv2d(inputs=maxpool_layer_2, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 4x4x8<br class="title-page-name"/>encoded_layer = tf.layers.max_pooling2d(conv_layer_3, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># Defining the Decoder part of the netowrk<br class="title-page-name"/># Defining the first upsampling layer in the decoder part<br class="title-page-name"/># The output tenosor will be in the shape of 7x7x8<br class="title-page-name"/>upsample_layer_1 = tf.image.resize_images(encoded_layer, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x8<br class="title-page-name"/>conv_layer_4 = tf.layers.conv2d(inputs=upsample_layer_1, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x8<br class="title-page-name"/>upsample_layer_2 = tf.image.resize_images(conv_layer_4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x8<br class="title-page-name"/>conv_layer_5 = tf.layers.conv2d(inputs=upsample_layer_2, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x8<br class="title-page-name"/>upsample_layer_3 = tf.image.resize_images(conv_layer_5, size=(28,28), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x16<br class="title-page-name"/>conv6 = tf.layers.conv2d(inputs=upsample_layer_3, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x1<br class="title-page-name"/>logits_layer = tf.layers.conv2d(inputs=conv6, filters=1, kernel_size=(3,3), padding='same', activation=None)<br class="title-page-name"/><br class="title-page-name"/># feeding the logits values to the sigmoid activation function to get the reconstructed images<br class="title-page-name"/>decoded_layer = tf.nn.sigmoid(logits_layer)<br class="title-page-name"/><br class="title-page-name"/># feeding the logits to sigmoid while calculating the cross entropy<br class="title-page-name"/>model_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_values, logits=logits_layer)<br class="title-page-name"/><br class="title-page-name"/># Getting the model cost and defining the optimizer to minimize it<br class="title-page-name"/>model_cost = tf.reduce_mean(model_loss)<br class="title-page-name"/>model_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_cost)</pre>
<p class="calibre2">现在我们可以走了。我们构建了卷积神经网络的解码器-解码器部分，同时展示了如何在解码器部分重建输入图像的维度。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Model training</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">模特培训</h1>
                
            
            
                
<p class="calibre2">现在我们已经建立了模型，我们可以通过从 MNIST 数据集生成随机批次并将其提供给前面定义的优化器来开始学习过程。</p>
<p class="calibre2">让我们从创建会话变量开始；它将负责执行我们之前定义的计算图:</p>
<pre class="calibre21">sess = tf.Session()<br class="title-page-name"/>num_epochs = 20<br class="title-page-name"/>train_batch_size = 200<br class="title-page-name"/>sess.run(tf.global_variables_initializer())<br class="title-page-name"/><br class="title-page-name"/>for e in range(num_epochs):<br class="title-page-name"/>    for ii in range(mnist_dataset.train.num_examples//train_batch_size):<br class="title-page-name"/>        input_batch = mnist_dataset.train.next_batch(train_batch_size)<br class="title-page-name"/>        input_images = input_batch[0].reshape((-1, 28, 28, 1))<br class="title-page-name"/>        input_batch_cost, _ = sess.run([model_cost, model_optimizer], feed_dict={inputs_values: input_images,targets_values: input_images})<br class="title-page-name"/><br class="title-page-name"/>        print("Epoch: {}/{}...".format(e+1, num_epochs),<br class="title-page-name"/>              "Training loss: {:.3f}".format(input_batch_cost))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.105<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.096<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.097<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102</pre>
<p class="calibre2">在运行前面的代码片段 20 个时期后，我们将得到一个训练有素的 CAE，所以让我们继续通过输入来自 MNIST 数据集的类似图像来测试这个模型:</p>
<div><pre class="calibre21">fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))<br class="title-page-name"/>input_images = mnist_dataset.test.images[:10]<br class="title-page-name"/>reconstructed_images = sess.run(decoded_layer, feed_dict={inputs_values: input_images.reshape((10, 28, 28, 1))})<br class="title-page-name"/><br class="title-page-name"/>for imgs, row in zip([input_images, reconstructed_images], axes):<br class="title-page-name"/>    for img, ax in zip(imgs, row):<br class="title-page-name"/>        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')<br class="title-page-name"/>        ax.get_xaxis().set_visible(False)<br class="title-page-name"/>        ax.get_yaxis().set_visible(False)<br class="title-page-name"/><br class="title-page-name"/>fig.tight_layout(pad=0.1)<br class="title-page-name"/><br class="title-page-name"/>Output:</pre>
<div><img src="img/38895773-80a4-4f5d-b572-2f728dac20dd.png" class="calibre161"/></div>
<p>图 9:使用卷积自动编码器的原始测试图像(第一行)及其构造(第二行)的示例</p>
</div>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Denoising autoencoders</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">降噪自动编码器</h1>
                
            
            
                
<p class="calibre2">我们可以通过迫使自动编码器学习关于输入数据的更重要的特征来进一步发展它的架构。通过向输入图像添加噪声并将原始图像作为目标，该模型将尝试去除该噪声并学习关于它们的重要特征，以便在输出中得出有意义的重建图像。这种 CAE 架构可用于从输入图像中去除噪声。自动编码器的这种特殊变化称为<strong class="calibre13">去噪自动编码器</strong>:</p>
<div><img src="img/5100d26b-63c5-4c69-93cd-4fb8df5ddcb2.png" class="calibre162"/></div>
<p>图 10:原始图像和添加一点高斯噪声后的相同图像的示例</p>
<p class="calibre2">因此，让我们从实现下图中的架构开始。我们在去噪自动编码器架构中增加的唯一额外内容是原始输入图像中的一些噪声:</p>
<div><img src="img/5a04b580-1641-457a-8609-cf2392f35c28.png" class="calibre163"/></div>
<p>图 11:自动编码器的一般去噪架构</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Building the model</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">构建模型</h1>
                
            
            
                
<div><p class="calibre2">在这种实现中，我们将在编码器和解码器部分使用更多层，原因是我们给输入增加了新的复杂性。</p>
<p class="calibre2">下一个模型与之前的 CAE 完全相同，但增加了额外的层，这将帮助我们从有噪声的图像中重建无噪声的图像。</p>
</div>
<p class="calibre2">让我们继续构建这个架构:</p>
<pre class="calibre21">learning_rate = 0.001<br class="title-page-name"/><br class="title-page-name"/># Define the placeholder variable sfor the input and target values<br class="title-page-name"/>inputs_values = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs_values')<br class="title-page-name"/>targets_values = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets_values')<br class="title-page-name"/><br class="title-page-name"/># Defining the Encoder part of the netowrk<br class="title-page-name"/># Defining the first convolution layer in the encoder parrt<br class="title-page-name"/># The output tenosor will be in the shape of 28x28x32<br class="title-page-name"/>conv_layer_1 = tf.layers.conv2d(inputs=inputs_values, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x32<br class="title-page-name"/>maxpool_layer_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x32<br class="title-page-name"/>conv_layer_2 = tf.layers.conv2d(inputs=maxpool_layer_1, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x32<br class="title-page-name"/>maxpool_layer_2 = tf.layers.max_pooling2d(conv_layer_2, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x16<br class="title-page-name"/>conv_layer_3 = tf.layers.conv2d(inputs=maxpool_layer_2, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 4x4x16<br class="title-page-name"/>encoding_layer = tf.layers.max_pooling2d(conv_layer_3, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Defining the Decoder part of the netowrk<br class="title-page-name"/># Defining the first upsampling layer in the decoder part<br class="title-page-name"/># The output tenosor will be in the shape of 7x7x16<br class="title-page-name"/>upsample_layer_1 = tf.image.resize_images(encoding_layer, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x16<br class="title-page-name"/>conv_layer_4 = tf.layers.conv2d(inputs=upsample_layer_1, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x16<br class="title-page-name"/>upsample_layer_2 = tf.image.resize_images(conv_layer_4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x32<br class="title-page-name"/>conv_layer_5 = tf.layers.conv2d(inputs=upsample_layer_2, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x32<br class="title-page-name"/>upsample_layer_3 = tf.image.resize_images(conv_layer_5, size=(28,28), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x32<br class="title-page-name"/>conv_layer_6 = tf.layers.conv2d(inputs=upsample_layer_3, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x1<br class="title-page-name"/>logits_layer = tf.layers.conv2d(inputs=conv_layer_6, filters=1, kernel_size=(3,3), padding='same', activation=None)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># feeding the logits values to the sigmoid activation function to get the reconstructed images<br class="title-page-name"/>decoding_layer = tf.nn.sigmoid(logits_layer)<br class="title-page-name"/><br class="title-page-name"/># feeding the logits to sigmoid while calculating the cross entropy<br class="title-page-name"/>model_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_values, logits=logits_layer)<br class="title-page-name"/><br class="title-page-name"/># Getting the model cost and defining the optimizer to minimize it<br class="title-page-name"/>model_cost = tf.reduce_mean(model_loss)<br class="title-page-name"/>model_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_cost)</pre>
<p class="calibre2">现在我们有了一个更复杂或更深入的卷积模型。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Model training</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">模特培训</h1>
                
            
            
                
<p class="calibre2">是时候开始训练这个更深层次的网络了，这反过来将需要更多的时间通过从有噪输入中重建无噪图像来收敛。</p>
<p class="calibre2">因此，让我们从创建会话变量开始:</p>
<pre class="calibre21">sess = tf.Session()</pre>
<p class="calibre2">接下来，我们将开始培训过程，但更多的时代:</p>
<pre class="calibre21">num_epochs = 100<br class="title-page-name"/>train_batch_size = 200<br class="title-page-name"/><br class="title-page-name"/># Defining a noise factor to be added to MNIST dataset<br class="title-page-name"/>mnist_noise_factor = 0.5<br class="title-page-name"/>sess.run(tf.global_variables_initializer())<br class="title-page-name"/><br class="title-page-name"/>for e in range(num_epochs):<br class="title-page-name"/>    for ii in range(mnist_dataset.train.num_examples//train_batch_size):<br class="title-page-name"/>        input_batch = mnist_dataset.train.next_batch(train_batch_size)<br class="title-page-name"/>        <br class="title-page-name"/>        # Getting and reshape the images from the corresponding batch<br class="title-page-name"/>        batch_images = input_batch[0].reshape((-1, 28, 28, 1))<br class="title-page-name"/>        <br class="title-page-name"/>        # Add random noise to the input images<br class="title-page-name"/>        noisy_images = batch_images + mnist_noise_factor * np.random.randn(*batch_images.shape)<br class="title-page-name"/>        <br class="title-page-name"/>        # Clipping all the values that are above 0 or above 1<br class="title-page-name"/>        noisy_images = np.clip(noisy_images, 0., 1.)<br class="title-page-name"/>        <br class="title-page-name"/>        # Set the input images to be the noisy ones and the original images to be the target<br class="title-page-name"/>        input_batch_cost, _ = sess.run([model_cost, model_optimizer], feed_dict={inputs_values: noisy_images,<br class="title-page-name"/>                                                         targets_values: batch_images})<br class="title-page-name"/><br class="title-page-name"/>        print("Epoch: {}/{}...".format(e+1, num_epochs),<br class="title-page-name"/>              "Training loss: {:.3f}".format(input_batch_cost))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.103<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.103<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.096<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.103<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.096<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.103<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.097<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101</pre>
<p class="calibre2">现在，我们已经训练该模型能够产生无噪声的图像，这使得自动编码器适用于许多领域。</p>
<p class="calibre2">在下一段代码中，我们不会将 MNIST 测试集的行图像提供给模型，因为我们需要首先向这些图像添加噪声，以查看经过训练的模型如何能够生成无噪声的图像。</p>
<p class="calibre2">在这里，我将噪声添加到测试图像中，并让它们通过自动编码器。它在消除噪音方面做得非常出色，尽管有时很难辨别原始数字是多少:</p>
<pre class="calibre21">#Defining some figures<br class="title-page-name"/>fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))<br class="title-page-name"/><br class="title-page-name"/>#Visualizing some images<br class="title-page-name"/>input_images = mnist_dataset.test.images[:10]<br class="title-page-name"/>noisy_imgs = input_images + mnist_noise_factor * np.random.randn(*input_images.shape)<br class="title-page-name"/><br class="title-page-name"/>#Clipping and reshaping the noisy images<br class="title-page-name"/>noisy_images = np.clip(noisy_images, 0., 1.).reshape((10, 28, 28, 1))<br class="title-page-name"/><br class="title-page-name"/>#Getting the reconstructed images<br class="title-page-name"/>reconstructed_images = sess.run(decoding_layer, feed_dict={inputs_values: noisy_images})<br class="title-page-name"/><br class="title-page-name"/>#Visualizing the input images and the noisy ones<br class="title-page-name"/>for imgs, row in zip([noisy_images, reconstructed_images], axes):<br class="title-page-name"/>    for img, ax in zip(imgs, row):<br class="title-page-name"/>        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')<br class="title-page-name"/>        ax.get_xaxis().set_visible(False)<br class="title-page-name"/>        ax.get_yaxis().set_visible(False)<br class="title-page-name"/><br class="title-page-name"/>fig.tight_layout(pad=0.1)</pre>
<pre class="calibre21"><br class="title-page-name"/>Output:</pre>
<div><img src="img/14c7f119-bf36-4b37-aa0a-777b02b5703a.png" class="calibre164"/></div>
<p>图 12:具有一些高斯噪声的原始测试图像的例子(顶行)以及它们基于训练的去噪自动编码器的构造</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Applications of autoencoders</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">自动编码器的应用</h1>
                
            
            
                
<p class="calibre2">在前面从较低表示构建图像的示例中，我们看到它与原始输入非常相似，我们还看到了 CANs 在对噪声数据集进行去噪时的优势。我们在上面实现的这种示例对于图像构建应用程序和数据集去噪非常有用。因此，您可以将上述实现推广到您感兴趣的任何其他示例。</p>
<p class="calibre2">此外，在本章中，我们已经看到了 autoencoder 架构的灵活性，以及我们如何对其进行不同的更改。我们甚至测试了它来解决从输入图像中去除噪声的困难问题。这种灵活性为 auoencoders 非常适合的更多应用打开了大门。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Image colorization</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">图像彩色化</h1>
                
            
            
                
<p class="calibre2">自动编码器——尤其是卷积版本——可以用于更困难的任务，如图像着色。在下面的示例中，我们为模型提供了一个没有任何颜色的输入图像，该图像的重建版本将由自动编码器模型进行着色:</p>
<div><img src="img/4d095b47-f875-4eed-89ec-378f9c56067d.jpeg" class="calibre165"/></div>
<p>图 13:CAE 被训练来给图像着色</p>
<div><img src="img/8e56cfc3-2c87-4af8-b844-081f5549e6ff.jpg" class="calibre31"/></div>
<p>图 14:彩色纸张架构</p>
<p class="calibre2">既然我们的 autoencoder 已经训练好了，我们就可以用它来给我们从未见过的图片着色了！</p>
<p class="calibre2">这种应用程序可以用来为早期相机拍摄的非常旧的图像着色。</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>More applications</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">更多应用</h1>
                
            
            
                
<p class="calibre2">另一个有趣的应用是生成更高分辨率的图像，或神经图像增强，如下图所示。</p>
<p class="calibre2">这些图显示了张曦轲的图像彩色化的更真实的版本:</p>
<div><img src="img/737b6da4-0bd7-437c-8a6b-e79adbe19b63.png" class="calibre166"/></div>
<p>图 15:张曦轲、菲利普·伊索拉和阿列克谢·埃夫罗斯的彩色图像</p>
<p class="calibre2">此图显示了自动编码器增强图像的另一个应用:</p>
<div><img src="img/1eae3e42-4da8-4cea-b961-30bbe5229382.png" class="calibre31"/></div>
<p>图 16:Alex JC(https://github.com/alexjc/neural-enhance)的神经增强</p>


            

            
        
    </body>

</html>


<html xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <title>Summary</title>
    <meta content="urn:uuid:b6d6a510-8f2b-40e4-8a30-e199afd87745" name="Adept.expected.resource"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  

</head>
  <body class="calibre">
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p class="calibre2">在这一章中，我们介绍了一个全新的架构，可以用于许多有趣的应用。自动编码器非常灵活，所以在图像增强、着色或构造方面，您可以随意提出自己的问题。此外，还有更多自动编码器的变体，称为<strong class="calibre13">变体自动编码器</strong>。它们也用于非常有趣的应用，例如图像生成。</p>


            

            
        
    </body>

</html>
</body></html>