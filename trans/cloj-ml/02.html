<html><head/><body>



<title>Chapter 2. Understanding Linear Regression</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch02"/>第二章。了解线性回归</h1></div></div></div><p>在这一章，我们开始探索机器学习模型和技术。机器学习的最终目标是<em>从一些经验样本数据中归纳出</em>事实。这被称为<strong>概括</strong>、<a id="id114" class="indexterm"/>，本质上是使用这些推断的事实以准确的速度对新的、看不见的数据进行准确处理的能力。机器学习的两大类是<strong>监督</strong>学习和<strong>非监督</strong>学习。术语<strong>监督学习</strong> <a id="id115" class="indexterm"/>用于描述机器学习的任务，其中从一些标记的数据中形成理解或模型。通过标记，我们意味着样本数据与一些观察值相关联。从基本意义上说，模型是对数据以及数据如何随不同参数变化的统计描述。监督机器学习技术用来创建模型的初始数据被称为模型的<strong>训练数据</strong> <a id="id116" class="indexterm"/>。另一方面，无监督学习<a id="id117" class="indexterm"/>技术通过在未标记数据中寻找模式来估计模型。由于无监督学习技术使用的数据是未标记的，因此通常没有明确的基于是或否的奖励系统来确定估计的模型是否准确和正确。</p><p>我们现在将检查<em>线性回归</em>，这是一个可以用于预测的有趣模型。作为一种监督学习，回归模型是从一些数据中创建的，其中许多参数以某种方式组合在一起，以产生几个目标值。该模型实际上描述了目标值和模型参数之间的关系，并且当提供有模型参数的值时，可以用于预测目标值。</p><p>我们将首先研究单变量和多变量的线性回归，然后描述可用于从一些给定数据制定机器学习模型的算法。我们将研究这些模型背后的推理，同时演示如何在 Clojure 中实现创建这些模型的算法。</p><div><div><div><div><h1 class="title"><a id="ch02lvl1sec16"/>了解单变量线性回归</h1></div></div></div><p>我们经常遇到需要从一些样本数据中创建一个近似模型的情况。当提供了所需参数时，该模型可用于预测更多此类数据。例如，我们可能想要研究某个特定<a id="id118" class="indexterm"/>城市某一天的降雨频率，我们将假设降雨频率随当天的湿度而变化。如果我们知道某一天的湿度，一个公式化的模型可以用来预测该天降雨的可能性。我们从一些数据开始公式化模型，首先在这些数据上拟合一条带有一些参数和系数的直线(即方程)。这类模型被称为<strong>线性回归</strong>模型<a id="id119" class="indexterm"/>。如果我们假设样本数据只有一个维度，我们可以将线性回归视为在样本数据上拟合直线<img src="img/4351OS_02_01.jpg" alt="Understanding single-variable linear regression"/>的一种方式。</p><p>线性回归模型简单描述为表示模型的<strong>回归和</strong>或<strong>因变量</strong>的线性方程。公式化的回归模型可以有一个到几个参数，取决于可用的数据，模型的这些参数也被称为模型的<strong>回归变量</strong>、<strong>特征</strong>或<strong>自变量</strong>。我们将首先探索单个独立变量的线性回归模型。</p><p>使用单变量线性回归<a id="id121" class="indexterm"/>的一个示例问题<a id="id120" class="indexterm"/>是预测特定一天的降雨概率，这取决于当天的湿度。该训练数据可以用下面的表格形式表示:</p><div><img src="img/4351OS_02_85.jpg" alt="Understanding single-variable linear regression"/></div><p>对于单变量线性模型，因变量必须根据单个参数而变化。因此，我们的样本数据本质上由两个向量组成，即，一个是因变量<em> Y </em>的值，另一个是自变量<em> X </em>的值。两个<a id="id122" class="indexterm"/>向量长度相同。该数据可以正式表示为两个向量或单列矩阵，如下所示:</p><div><img src="img/4351OS_02_03.jpg" alt="Understanding single-variable linear regression"/></div><p>让我们快速定义 Clojure 中两个矩阵后的<a id="id123" class="indexterm"/>，<em> X </em>和<em> Y </em>，来表示一些样本数据:</p><div><pre class="programlisting">(def X (cl/matrix [8.401 14.475 13.396 12.127 5.044
                      8.339 15.692 17.108 9.253 12.029]))

(def Y (cl/matrix [-1.57 2.32  0.424  0.814 -2.3
           0.01 1.954 2.296 -0.635 0.328]))</pre></div><p>这里，我们定义 10 个点的数据；使用下面的<a id="id124" class="indexterm"/>咒语<code class="literal">scatter-plot</code>功能，这些点可以很容易地绘制在散点图上:</p><div><pre class="programlisting">(def linear-samp-scatter
  (scatter-plot X Y))

(defn plot-scatter []
  (view linear-samp-scatter))

(plot-scatter)</pre></div><p>前面的代码显示了我们数据的散点图<a id="id125" class="indexterm"/>:</p><div><img src="img/4351OS_02_04.jpg" alt="Understanding single-variable linear regression"/></div><p>之前的散点图是我们在<code class="literal">X</code>和<code class="literal">Y</code>中定义的 10 个数据点的<a id="id126" class="indexterm"/>简单表示。</p><div><div><h3 class="title"><a id="note11"/>注意</h3><p><code class="literal">scatter-plot</code>函数可以在咒语库的<code class="literal">charts</code>命名空间中找到。使用此函数的文件的命名空间声明应类似于以下声明:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter.charts :only [scatter-plot]]))</pre></div></div></div><p>现在我们有了数据的可视化，让我们在给定的数据点上估计一个线性模型。我们可以使用咒语库中的<code class="literal">linear-model</code>函数<a id="id128" class="indexterm"/>生成任意数据的线性模型。这个函数返回一个描述公式化模型的映射，以及许多关于这个模型的有用数据。首先，我们可以通过使用该图中的<code class="literal">:fitted</code>键值对，在之前的散点图上绘制线性模型。我们首先从返回的地图中获取<code class="literal">:fitted</code>键的值，并使用<code class="literal">add-lines</code>函数<a id="id129" class="indexterm"/>将其添加到散点图中；这显示在以下代码中:</p><div><pre class="programlisting">(def samp-linear-model
  (linear-model Y X))
(defn plot-model []
  (view (add-lines samp-scatter-plot 
          X (:fitted linear-samp-scatter))))

(plot-model)</pre></div><p>这段代码在我们之前定义的散点图上生成了线性模型的如下不言自明的图:</p><div><img src="img/4351OS_02_06.jpg" alt="Understanding single-variable linear regression"/></div><p>前面的图<a id="id130" class="indexterm"/>将线性模型<code class="literal">samp-linear-model</code>描绘为在我们在<code class="literal">X</code>和<code class="literal">Y</code>中定义的 10 个数据点上绘制的直线。</p><div><div><h3 class="title"><a id="note12"/>注</h3><p><code class="literal">linear-model</code>函数可以在咒语库的<code class="literal">stats</code>命名空间中找到。使用<code class="literal">linear-model</code>的文件的名称空间声明应该类似于下面的声明:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter.stats :only [linear-model]]))</pre></div></div></div><p>嗯，看起来<a id="id131" class="indexterm"/>好像咒语的<code class="literal">linear-model</code>功能<a id="id132" class="indexterm"/>为我们做了大部分工作。本质上，该函数通过使用<strong>普通最小二乘法</strong> ( <strong> OLS </strong>)曲线拟合算法<a id="id133" class="indexterm"/>来创建我们数据的线性模型。我们将很快深入这个算法的细节，但是让我们首先理解一条曲线如何精确地拟合到一些给定的数据上。</p><p>我们先来定义一条直线是如何表示的。在坐标几何中，直线只是独立变量<em> x </em>的函数，它具有给定的斜率<em> m </em>和截距<em> c </em>。线<em> y </em>的作用可以正式写成<img src="img/4351OS_02_01.jpg" alt="Understanding single-variable linear regression"/>。线条的斜率代表当<em> x </em>的值变化时<em> y </em>的值变化多少。该方程的截距正好是直线与图中的<em> y </em>轴相交的地方。请注意，等式<em> y </em>与<em> Y </em>不同，后者实际上代表我们已经获得的等式的值。</p><p>类似于坐标几何中直线的定义，我们使用矩阵<em> X </em>和<em> Y </em>的定义正式定义了单变量线性回归模型<a id="id134" class="indexterm"/>，如下所示:</p><div><img src="img/4351OS_02_09.jpg" alt="Understanding single-variable linear regression"/></div><p>单变量线性模型的这种定义实际上非常通用，因为我们可以使用同一个方程来定义多变量线性模型；我们将在本章后面看到这一点。在前面的定义中，术语<img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>是表示<em> y </em>相对于<em> x </em>的线性<a id="id135" class="indexterm"/>比例的系数。就几何学而言，它只是符合矩阵<em> X </em>和<em> Y </em>中给定数据的直线的斜率。由于<em> X </em>是一个矩阵或向量，<img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>也可以被认为是矩阵<em> X </em>的缩放因子。</p><p>另外，术语<img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>是解释当<em> x </em>为零时<em> y </em>的值的另一个系数。换句话说，它是等式的<em> y </em>截距。公式化模型的系数<img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>称为线性模型的<strong>回归系数</strong>或<strong>效应</strong>，系数<img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>称为模型的<strong>误差项</strong>或<strong>偏差</strong>。一个模型甚至可能有几个回归系数，我们将在本章后面看到。原来，误差<img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>实际上只是另一个回归系数，可以按照惯例与模型的其他影响一起提及。有趣的是，这种误差通常决定了数据的分散性或方差。</p><p>使用前面例子中的<code class="literal">linear-model</code>函数返回的映射，我们可以很容易地检查生成的模型的系数。返回的映射有一个<code class="literal">:coefs</code>键，它映射到一个包含模型系数的向量。按照惯例，误差项也包含在这个向量中，只是作为另一个系数:</p><div><pre class="programlisting">user&gt; (:coefs samp-linear-model)
[-4.1707801647266045 0.39139682427040384]</pre></div><p>现在我们已经定义了数据的线性模型。很明显，并不是所有的点都在一条代表公式化模型的直线上。在<em> y </em>轴上，每个数据点都与线性模型图有一些偏差，这种偏差可以是正的，也可以是负的。为了表示模型与给定数据的总体偏差，我们使用<em>残差平方和</em>、<em>均方误差</em>和<em>均方根误差</em>函数。这三个函数的值表示公式化模型中误差量的标量测量。</p><p>术语<em>误差</em>和<em>残差</em>之间的区别在于，误差是对观察值与其期望值<a id="id136" class="indexterm"/>的差异量的度量，而残差是对不可观察统计误差的估计，我们使用的统计模型无法对其建模或理解。我们可以说，在一组观察值中，一个观察值<a id="id137" class="indexterm"/>与所有值的平均值之间的差是一个残差。公式化模型中残差的数量必须等于样本数据中因变量的观察值的数量。</p><p>我们可以使用<code class="literal">:residuals</code>关键字从由<code class="literal">linear-model</code>函数生成的线性模型中获取残差，如下面的代码所示:</p><div><pre class="programlisting">user&gt; (:residuals samp-linear-model)
[-0.6873445559690581 0.8253111334125092 -0.6483716931997257 0.2383108767994172 -0.10342541689331242 0.9169220471357067 -0.01701880172457293 -0.22923670489146497 -0.08581465024744239 -0.20933223442208365]</pre></div><p><strong>预测的误差平方和</strong> ( <strong> SSE </strong> ) <a id="id138" class="indexterm"/>就是公式化模型中的误差之和。<a id="id139" class="indexterm"/>注意，在下面的等式中，误差项<img src="img/4351OS_02_12.jpg" alt="Understanding single-variable linear regression"/>的符号并不重要，因为我们对该差值求平方；因此，它总是产生正值。SSE <a id="id140" class="indexterm"/>也被称为<strong>残差平方和</strong> ( <strong> RSS </strong>)。</p><div><img src="img/4351OS_02_13.jpg" alt="Understanding single-variable linear regression"/></div><p><code class="literal">linear-model</code>函数也计算公式化模型的 SSE，这个值可以使用<code class="literal">:sse</code>关键字检索；下面几行代码说明了这一点:</p><div><pre class="programlisting">user&gt; (:sse samp-linear-model)
2.5862250345284887</pre></div><p><strong>均方误差</strong> ( <strong> MSE </strong>)测量公式化模型中误差的平均大小，而不考虑<a id="id141" class="indexterm"/>误差的方向。我们可以通过对因变量的所有给定值与其在公式化线性模型上的相应预测值的差值求平方，并计算这些平方误差的平均值来计算该值。MSE 也称为模型的<strong>均方预测误差</strong>。如果公式化模型的 MSE 为零，那么我们可以说该模型完全符合给定的数据。当然，这对于真实数据实际上是不可能的，尽管我们可以找到一组理论上 MSE 为零的值。</p><p>对于因变量<img src="img/4351OS_02_14.jpg" alt="Understanding single-variable linear regression"/>的一组给定的<em> N </em>值和从公式化模型计算的一组估计值<img src="img/4351OS_02_15.jpg" alt="Understanding single-variable linear regression"/>，我们可以<a id="id142" class="indexterm"/>正式表示公式化模型<img src="img/4351OS_02_16.jpg" alt="Understanding single-variable linear regression"/>的 MSE 函数如下:</p><div><img src="img/4351OS_02_17.jpg" alt="Understanding single-variable linear regression"/></div><p><strong>均方根误差</strong> ( <strong> RMSE </strong> ) <a id="id143" class="indexterm"/>或<strong>均方根偏差</strong>就是 MSE 的平方根，而<a id="id144" class="indexterm"/>通常用于测量公式化线性模型的偏差。RMSE 偏向于<a id="id145" class="indexterm"/>较大的误差，因此与尺度相关。这意味着当不希望出现大误差时，RMSE 特别有用。</p><p>我们可以正式定义<a id="id146" class="indexterm"/>公式化模型的 RMSE 如下:</p><div><img src="img/4351OS_02_18.jpg" alt="Understanding single-variable linear regression"/></div><p>公式化线性模型精度的另一个度量是<strong>决定系数</strong> <a id="id147" class="indexterm"/>，写为<img src="img/4351OS_02_19.jpg" alt="Understanding single-variable linear regression"/>。决定系数表示公式化模型与给定样本数据的拟合程度，定义如下。该系数根据样本数据<img src="img/4351OS_02_20.jpg" alt="Understanding single-variable linear regression"/>中观察值的平均值、SSE 和误差总和<img src="img/4351OS_02_21.jpg" alt="Understanding single-variable linear regression"/>来定义。</p><div><img src="img/4351OS_02_22.jpg" alt="Understanding single-variable linear regression"/></div><p>我们可以使用<code class="literal">:r-square</code>关键字从<code class="literal">linear-model</code>函数生成的模型中检索出<img src="img/4351OS_02_19.jpg" alt="Understanding single-variable linear regression"/>的计算值，如下所示:</p><div><pre class="programlisting">user&gt; (:r-square samp-linear-model)
0.8837893226172282</pre></div><p>为了建立一个最适合样本数据的模型，我们应该努力使前面描述的值最小化。对于一些给定的数据，我们可以制定几个模型，并计算每个模型的总误差。该计算误差可用于确定哪个公式化<a id="id148" class="indexterm"/>模型最适合数据，从而为给定数据选择最佳线性模型。</p><p>基于公式化模型的 MSE，该模型被称为具有<a id="id149" class="indexterm"/> <strong>成本函数</strong> <a id="id150" class="indexterm"/>。在一些数据上拟合线性模型的问题等价于最小化公式化线性模型的成本函数的问题。表示为<img src="img/4351OS_02_23.jpg" alt="Understanding single-variable linear regression"/>的<a id="id151" class="indexterm"/>的成本函数可以简单地认为是公式化模型的参数的函数。通常，这个成本函数转化为模型的 MSE。因为 RMSE 随着模型的公式化参数而变化，所以模型的以下成本函数是这些参数的函数:</p><div><img src="img/4351OS_02_24.jpg" alt="Understanding single-variable linear regression"/></div><p>这给我们带来了对线性模型的估计效应<img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>和<img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>的一些数据拟合线性回归模型的问题的以下正式定义:</p><div><img src="img/4351OS_02_25.jpg" alt="Understanding single-variable linear regression"/></div><p>该定义规定，我们可以通过确定这些参数的值来估计由参数<img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>和<img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>表示的线性模型，对于这些参数，成本函数<img src="img/4351OS_02_23.jpg" alt="Understanding single-variable linear regression"/>采用最小可能值，理想地为零。</p><div><div><h3 class="title"><a id="note13"/>注意</h3><p>在上式中，<img src="img/4351OS_02_26.jpg" alt="Understanding single-variable linear regression"/>表达式表示成本函数的标准范数<em> N </em>维欧几里德空间。术语<em>范数</em>，我们指的是在 N<em>维空间中只有正值的函数。</em></p></div></div><p>让我们想象公式化的<a id="id154" class="indexterm"/>模型的成本函数的<a id="id152" class="indexterm"/>欧几里德空间<a id="id153" class="indexterm"/>如何相对于模型的参数变化。为此，我们假设代表常值误差的<img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>参数为零。在参数<img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>上的线性模型的成本函数<img src="img/4351OS_02_27.jpg" alt="Understanding single-variable linear regression"/>的曲线将理想地表现为抛物线，类似于下面的曲线:</p><div><img src="img/4351OS_02_28.jpg" alt="Understanding single-variable linear regression"/></div><p>对于单个参数<img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>，我们可以绘制前面的二维图表。类似地，对于公式化模型的两个参数<img src="img/4351OS_02_10.jpg" alt="Understanding single-variable linear regression"/>和<img src="img/4351OS_02_11.jpg" alt="Understanding single-variable linear regression"/>，产生三维图。该<a id="id155" class="indexterm"/>图呈碗状或具有<a id="id156" class="indexterm"/>凸面，如下图所示。此外，我们可以将其推广到<a id="id157" class="indexterm"/>公式化模型的<em> N </em>个参数，并绘制出<img src="img/4351OS_02_30.jpg" alt="Understanding single-variable linear regression"/>个维度的图。</p><div><img src="img/4351OS_02_31.jpg" alt="Understanding single-variable linear regression"/></div></div></div>





<title>Understanding gradient descent</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch02lvl1sec17"/>了解梯度下降</h1></div></div></div><p>梯度下降算法<a id="id158" class="indexterm"/>是最简单的技术之一，虽然不是最有效的技术，但可以形成成本函数或模型误差可能值最小的线性模型。该算法本质上是为公式化的线性模型寻找成本函数的局部最小值。</p><p>如前所述，单变量线性回归模型的成本函数的三维图将显示为具有<em>全局最小值</em>的凸面或碗状表面。所谓最小值，我们是指成本函数在图面上的这一点上具有最小的可能值。梯度下降算法本质上是从曲面上的任意点开始，并执行一系列步骤来逼近曲面的局部最小值。</p><p>这个过程可以想象成把一个球扔进山谷或两座相邻的小山之间，结果球慢慢地滚向海拔最低的地方。重复该算法，直到表面上当前点的表观成本函数值收敛到零，这象征性地意味着滚下山的球停止，如我们前面所述。</p><p>当然，如果图面上有多个局部最小值，梯度下降可能不会真正起作用。然而，对于一个适当缩放的单变量线性回归模型，图的表面总是有一个单一的全局最小值，正如我们前面所说明的。因此，在这种情况下，我们仍然可以使用梯度下降算法来找到绘图表面的全局最小值。</p><p>这个算法的要点是，我们从曲面上的某个点开始，然后向最低点走几步。我们可以用下面的等式来正式表示这一点:</p><div><img src="img/4351OS_02_32.jpg" alt="Understanding gradient descent"/></div><p>这里，我们从成本函数<em> J </em>的图上由<img src="img/4351OS_02_33.jpg" alt="Understanding gradient descent"/>表示的点开始，并递增地减去成本函数<img src="img/4351OS_02_34.jpg" alt="Understanding gradient descent"/>的一阶偏导数的乘积，该乘积是相对于公式化模型的参数导出的。这意味着我们在曲面上朝着局部最小值缓慢向下移动，直到我们在曲面上找不到更低的点。术语<img src="img/4351OS_02_35.jpg" alt="Understanding gradient descent"/>决定了我们走向局部最小值的步长有多大，被称为<a id="id161" class="indexterm"/>梯度下降算法的<a id="id160" class="indexterm"/>步<em>步</em>。我们重复这个迭代，直到<img src="img/4351OS_02_36.jpg" alt="Understanding gradient descent"/>和<img src="img/4351OS_02_33.jpg" alt="Understanding gradient descent"/>之间的差收敛到零，或者至少减小到接近零的阈值。</p><p>下图显示了<a id="id162" class="indexterm"/>向成本函数图表面的局部最小值下降的过程:</p><div><img src="img/4351OS_02_37.jpg" alt="Understanding gradient descent"/></div><p>上图<a id="id163" class="indexterm"/>是图面的等高线图，其中圆形线连接高度相等的点。我们从点<img src="img/4351OS_02_39.jpg" alt="Understanding gradient descent"/>开始，执行梯度下降算法的单次迭代，将表面向下步进到点<img src="img/4351OS_02_40.jpg" alt="Understanding gradient descent"/>。我们重复这个过程，直到我们到达相对于初始起点<img src="img/4351OS_02_39.jpg" alt="Understanding gradient descent"/>的表面的局部最小值。注意，通过每次迭代，步长减小，因为当我们接近局部最小值时，该表面的切线斜率也趋于零。</p><p>对于误差常数<img src="img/4351OS_02_11.jpg" alt="Understanding gradient descent"/>等于零的单变量线性回归模型，我们可以简化梯度下降算法的<a id="id164" class="indexterm"/>偏导数分量<img src="img/4351OS_02_34.jpg" alt="Understanding gradient descent"/>。当模型只有一个参数<img src="img/4351OS_02_10.jpg" alt="Understanding gradient descent"/>时，一阶偏导数就是图面上该点处切线的斜率。因此，我们计算这条切线的斜率，并在这个斜率的方向上前进一步，这样我们就到达了<em> y </em>轴上方的一个高程点。这显示在下面的公式中:</p><div><img src="img/4351OS_02_41.jpg" alt="Understanding gradient descent"/></div><p>我们可以<a id="id165" class="indexterm"/>实现梯度下降算法的简化版本<a id="id166" class="indexterm"/>，如下所示:</p><div><pre class="programlisting">(def gradient-descent-precision 0.001)

(defn gradient-descent
  "Find the local minimum of the cost function's plot"
  [F' x-start step]
  (loop [x-old x-start]
    (let [x-new (- x-old
                   (* step (F' x-old)))
          dx (- x-new x-old)]
      (if (&lt; dx gradient-descent-precision)
        x-new
        (recur x-new)))))</pre></div><p>在前面的函数中，我们从点<code class="literal">x-start</code>开始，递归应用梯度下降算法，直到值<code class="literal">x-new</code>收敛。注意，这个过程是使用<code class="literal">loop</code>形式作为尾部递归函数实现的。</p><p>使用偏导数，我们可以正式表示如何使用梯度下降算法计算参数<img src="img/4351OS_02_10.jpg" alt="Understanding gradient descent"/>和<img src="img/4351OS_02_11.jpg" alt="Understanding gradient descent"/>，如下所示:</p><div><img src="img/4351OS_02_42.jpg" alt="Understanding gradient descent"/></div></div>





<title>Understanding multivariable linear regression</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch02lvl1sec18"/>了解多变量线性回归</h1></div></div></div><p><a id="id167" class="indexterm"/>多变量线性回归模型可以有多个变量<a id="id168" class="indexterm"/>或特征，这与我们之前研究的只有一个变量的线性回归模型相反。有趣的是，<a id="id169" class="indexterm"/>单变量线性模型的定义本身可以通过矩阵扩展到多个变量。</p><p>我们可以通过在样本数据中包含更多独立变量，如最低和最高温度，将我们之前预测特定日期降雨概率的<a id="id170" class="indexterm"/>示例扩展到一个多变量模型。因此，多变量线性回归模型的定型数据将类似于下图:</p><div><img src="img/4351OS_02_43.jpg" alt="Understanding multivariable linear regression"/></div><p>对于多变量线性回归模型，<a id="id171" class="indexterm"/>训练数据由两个矩阵定义，<em> X </em>和<em> Y </em>。这里，<em> X </em>是一个<img src="img/4351OS_02_44.jpg" alt="Understanding multivariable linear regression"/>矩阵，其中<em> P </em>是模型中自变量的个数。矩阵<em> Y </em>是一个长度为<em> N </em>的向量，就像单变量的线性模型一样。该模型如下所示:</p><div><img src="img/4351OS_02_45.jpg" alt="Understanding multivariable linear regression"/></div><p>对于下面这个 Clojure 中多变量线性回归的例子<a id="id172" class="indexterm"/>,我们不会通过代码生成样本数据，而是使用来自 Incanter 库中的样本数据。我们可以使用咒语库的<a id="id173" class="indexterm"/> <code class="literal">get-dataset</code>函数获取任何数据集。</p><div><div><h3 class="title"><a id="note14"/>注意</h3><p>在接下来的例子中，来自咒语库中的<code class="literal">sel</code>、<code class="literal">to-matrix</code>和<code class="literal">get-dataset</code>函数可以导入到我们的名称空间中，如下所示:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter.datasets :only [get-dataset]]
        [incanter.core :only [sel to-matrix]]))</pre></div></div></div><p>我们可以通过使用<code class="literal">:iris</code>关键字参数调用<code class="literal">get-dataset</code>函数<a id="id175" class="indexterm"/>来获取<strong>虹膜</strong>数据集<a id="id174" class="indexterm"/>；这显示如下:</p><div><pre class="programlisting">(def iris
  (to-matrix (get-dataset :iris)))

(def X (sel iris :cols (range 1 5)))
(def Y (sel iris :cols 0))</pre></div><p>我们首先使用<code class="literal">to-matrix</code>和<code class="literal">get-dataset</code>函数将变量<code class="literal">iris</code>定义为一个矩阵，然后定义两个矩阵<code class="literal">X</code>和<code class="literal">Y</code>。这里，<code class="literal">Y</code>实际上是一个 150 值的向量，或者说是一个大小为<img src="img/4351OS_02_46.jpg" alt="Understanding multivariable linear regression"/>的矩阵，而<code class="literal">X</code>是一个大小为<img src="img/4351OS_02_47.jpg" alt="Understanding multivariable linear regression"/>的矩阵。因此，<code class="literal">X</code>可以用来表示四个自变量的值，<code class="literal">Y</code>表示因变量的值。注意，<code class="literal">sel</code>函数用于从<code class="literal">iris</code>矩阵中选择一组列。事实上，我们可以<a id="id176" class="indexterm"/>从<code class="literal">iris</code>数据矩阵中选择更多这样的列，但是为了简单起见，我们在下面的例子中只使用四列。</p><div><div><h3 class="title"><a id="note15"/>注</h3><p>我们在前面的代码示例中使用的数据集是<em>虹膜</em>数据集，它可以在咒语库中找到。这个数据集具有相当多的历史意义，因为它被罗纳德·费雪爵士用来首次开发用于分类的<strong>线性判别分析</strong> ( <strong> LDA </strong>)方法<a id="id177" class="indexterm"/>(有关更多信息，请参考“Iris 中的物种问题”)。该数据集包含鸢尾属植物三个不同物种的 50 个样本，即<em> Setosa </em>、<em> Versicolor </em>和<em> Virginica </em>。在每个样品中测量这些物种的花的四个特征，即花瓣宽度、花瓣长度、萼片宽度和萼片长度。请注意，在本书的过程中，我们会多次遇到这个数据集。</p></div></div><p>有趣的是，<code class="literal">linear-model</code>函数接受具有多列的矩阵，因此我们可以使用该函数拟合一个针对单变量和多变量数据的<a id="id178" class="indexterm"/>线性回归模型，如下所示:</p><div><pre class="programlisting">(def iris-linear-model
  (linear-model Y X))
(defn plot-iris-linear-model []
  (let [x (range -100 100)
        y (:fitted iris-linear-model)]
    (view (xy-plot x y :x-label "X" :y-label "Y"))))

(plot-iris-linear-model)</pre></div><p>在前面的代码示例中，我们使用<code class="literal">xy-plot</code>函数<a id="id179" class="indexterm"/>绘制线性模型，同时提供可选参数来指定定义绘图中的轴标签。此外，我们通过使用<code class="literal">range</code>函数生成一个矢量来指定<em> x </em>轴的范围。<code class="literal">plot-iris-linear-model</code>功能<a id="id180" class="indexterm"/>生成以下图形:</p><div><img src="img/4351OS_02_48.jpg" alt="Understanding multivariable linear regression"/></div><p>尽管从上一个示例中生成的曲线图中的曲线似乎没有任何确定的形状，我们<a id="id181" class="indexterm"/>仍然可以通过向公式化模型提供自变量的值，使用这个生成的模型来估计或预测因变量的值。为了<a id="id182" class="indexterm"/>做到这一点，我们必须首先定义具有多个特征的线性回归模型的因变量和自变量之间的关系。</p><p>独立变量<em> P </em>的线性回归模型产生<img src="img/4351OS_02_50.jpg" alt="Understanding multivariable linear regression"/>回归系数，因为我们包括了误差常数以及模型的其他系数，并且还定义了一个额外的变量<img src="img/4351OS_02_39.jpg" alt="Understanding multivariable linear regression"/>，它总是<em> 1 </em>。</p><p><code class="literal">linear-model</code>函数符合公式模型中系数<em> P </em>的个数总是比样本数据中自变量总数<em> N </em>多 1 的命题；这显示在以下代码中:</p><div><pre class="programlisting">user&gt; (= (count (:coefs iris-linear-model)) 
         (+ 1 (column-count X)))
true</pre></div><p>我们将多变量回归模型的因变量和自变量之间的关系正式表示如下:</p><div><img src="img/4351OS_02_52.jpg" alt="Understanding multivariable linear regression"/></div><p>由于变量<img src="img/4351OS_02_39.jpg" alt="Understanding multivariable linear regression"/>在上式中始终为<em> 1 </em>，因此<img src="img/4351OS_02_53.jpg" alt="Understanding multivariable linear regression"/>的值类似于单变量线性模型定义的误差常数<img src="img/4351OS_02_11.jpg" alt="Understanding multivariable linear regression"/>。</p><p>我们可以定义一个向量来表示前面方程的所有系数为<img src="img/4351OS_02_10.jpg" alt="Understanding multivariable linear regression"/>。这个向量被称为公式化回归模型的<a id="id183" class="indexterm"/> <strong>参数向量</strong> <a id="id184" class="indexterm"/>。此外，模型的独立<a id="id185" class="indexterm"/>变量可以用向量表示。因此，我们可以将回归变量<em> Y </em>定义为参数向量转置和模型自变量向量的乘积:</p><div><img src="img/4351OS_02_54.jpg" alt="Understanding multivariable linear regression"/></div><p>多项式函数<a id="id186" class="indexterm"/>也可以通过用单个变量代替多项式方程中的每个高阶变量来简化为标准形式。例如，考虑下面的多项式方程:</p><div><img src="img/4351OS_02_55.jpg" alt="Understanding multivariable linear regression"/></div><p>我们可以用变量<img src="img/4351OS_02_56.jpg" alt="Understanding multivariable linear regression"/>代替<img src="img/4351OS_02_57.jpg" alt="Understanding multivariable linear regression"/>，从而将方程简化为多变量线性回归模型的标准形式。</p><p>这将我们带到具有多个变量的线性模型的成本函数的以下形式定义，其仅仅是具有单个变量的线性模型的成本函数的定义的扩展:</p><div><img src="img/4351OS_02_58.jpg" alt="Understanding multivariable linear regression"/></div><p>注意，在前面的定义中，我们可以将模型的各个系数与参数向量<img src="img/4351OS_02_10.jpg" alt="Understanding multivariable linear regression"/>互换使用。</p><p>类似于我们的问题<a id="id187" class="indexterm"/>在某些给定数据上用单个变量拟合模型的定义，我们可以将制定<a id="id188" class="indexterm"/>多变量线性模型的问题定义为最小化前面的成本函数的问题:</p><div><img src="img/4351OS_02_59.jpg" alt="Understanding multivariable linear regression"/></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec08"/>多变量梯度下降</h2></div></div></div><p>我们可以应用梯度下降算法来寻找具有多个变量的模型的局部最小值。当然，由于<a id="id189" class="indexterm"/>模型中有多个系数，我们必须对所有这些系数应用该算法，而不是在只有一个变量的回归模型中只有两个系数。</p><p>因此，梯度下降算法可用于寻找多变量线性回归模型的参数向量<img src="img/4351OS_02_10.jpg" alt="Gradient descent with multiple variables"/>中所有系数的值，并且形式上定义如下:</p><div><img src="img/4351OS_02_60.jpg" alt="Gradient descent with multiple variables"/></div><p>在前面的定义中，术语<img src="img/4351OS_02_61.jpg" alt="Gradient descent with multiple variables"/>仅指公式化模型中<img src="img/4351OS_02_62.jpg" alt="Gradient descent with multiple variables"/>自变量的样本值。同样，变量<img src="img/4351OS_02_63.jpg" alt="Gradient descent with multiple variables"/>总是<em> 1 </em>。因此，该定义可以仅应用于两个系数，这两个系数对应于我们先前对具有单个变量的线性回归模型的梯度下降算法的定义。</p><p>正如我们前面所看到的，<a id="id190" class="indexterm"/>梯度下降算法可以应用于具有单变量和多变量的线性回归模型。然而，对于某些模型，梯度下降算法实际上可能需要大量迭代，或者更确切地说，需要大量时间来收敛模型系数的估计值。有时，算法也会发散，因此在这种情况下我们将无法计算模型的系数。让我们来看看影响该算法的行为和性能的一些因素:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">样本数据的所有特征必须相对于彼此进行缩放。通过缩放，我们的意思是样本数据中所有独立变量的值都具有相似的取值范围。理想情况下，所有独立变量的观测值必须在<em> -1 </em>和<em> 1 </em>之间。这可以正式表述为:<div> <img src="img/4351OS_02_64.jpg" alt="Gradient descent with multiple variables"/> </div></li><li class="listitem" style="list-style-type: disc">我们可以将独立变量的观察值标准化到这些值的平均值。我们可以通过使用观察值的标准偏差来进一步标准化该数据。总之，我们将这些值替换为减去这些值的平均值<img src="img/4351OS_02_65.jpg" alt="Gradient descent with multiple variables"/>，然后将结果表达式除以标准偏差<img src="img/4351OS_02_66.jpg" alt="Gradient descent with multiple variables"/>所得的值。这在下面的公式中显示:<div> <img src="img/4351OS_02_67.jpg" alt="Gradient descent with multiple variables"/> </div></li><li class="listitem" style="list-style-type: disc">步进率或学习率<img src="img/4351OS_02_35.jpg" alt="Gradient descent with multiple variables"/>是另一个重要因素，它决定了算法收敛到公式化模型参数值的速度。理想情况下，应选择步进率，以使模型参数的新旧迭代值之间的差异在每次迭代中具有<a id="id191" class="indexterm"/>最佳变化量。一方面，如果这个值太大，算法甚至会在每次迭代后产生模型参数的发散值。因此，在这种情况下，算法永远不会找到全局最小值。另一方面，该速率的较小值可能会导致算法因不必要的大量迭代而变慢。</li></ul></div></div></div>





<title>Understanding Ordinary Least Squares</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch02lvl1sec19"/>理解普通最小二乘法</h1></div></div></div><p>另一种估计线性回归模型参数向量的技术是<strong>普通最小二乘法</strong> ( <strong> OLS </strong>)方法。OLS 方法<a id="id192" class="indexterm"/>本质上是通过最小化线性回归模型中的误差平方和来工作的。</p><p>线性回归模型的预测误差平方和(SSE)可以根据模型的实际值和期望值定义如下:</p><div><img src="img/4351OS_02_68.jpg" alt="Understanding Ordinary Least Squares"/></div><p>SSE 的上述定义可以使用矩阵乘积进行分解，如下所示:</p><div><img src="img/4351OS_02_69.jpg" alt="Understanding Ordinary Least Squares"/></div><p>我们可以通过使用全局最小值的定义来求解用于估计参数向量<img src="img/4351OS_02_10.jpg" alt="Understanding Ordinary Least Squares"/>的前述方程。由于该方程是二次方程的形式，并且项<img src="img/4351OS_02_70.jpg" alt="Understanding Ordinary Least Squares"/>总是大于零，因此成本函数的表面的全局最小值可以被定义为<a id="id193" class="indexterm"/>在该点处表面的切线斜率的变化率为零的点。此外，该图是线性模型参数的函数，因此表面图的方程应通过估计的参数向量<img src="img/4351OS_02_10.jpg" alt="Understanding Ordinary Least Squares"/>进行微分。因此，我们可以为公式化模型的最佳参数向量<img src="img/4351OS_02_10.jpg" alt="Understanding Ordinary Least Squares"/>求解该方程，如下所示:</p><div><img src="img/4351OS_02_71.jpg" alt="Understanding Ordinary Least Squares"/></div><p>上述推导中的最后一个等式给出了最佳参数向量<img src="img/4351OS_02_10.jpg" alt="Understanding Ordinary Least Squares"/>的定义，其形式表示如下:</p><div><img src="img/4351OS_02_72.jpg" alt="Understanding Ordinary Least Squares"/></div><p>我们可以使用 core.matrix 库的<code class="literal">transpose</code>和<code class="literal">inverse</code>函数以及 Incanter 库的<code class="literal">bind-columns</code>函数，通过 OLS 方法实现参数向量的上述定义:</p><div><pre class="programlisting">(defn linear-model-ols
  "Estimates the coefficients of a multi-var linear
  regression model using Ordinary Least Squares (OLS) method"
  [MX MY]
  (let [X (bind-columns (repeat (row-count MX) 1) MX)
        Xt (cl/matrix (transpose X))
        Xt-X (cl/* Xt X)]
    (cl/* (inverse Xt-X) Xt MY)))

(def ols-linear-model
  (linear-model-ols X Y))

(def ols-linear-model-coefs
  (cl/as-vec ols-linear-model))</pre></div><p>这里，我们首先添加一列，其中每个元素都是<code class="literal">1</code>，因为矩阵<code class="literal">MX</code>的第一列使用了<code class="literal">bind-columns</code>函数<a id="id194" class="indexterm"/>。我们添加的额外一列代表<a id="id195" class="indexterm"/>自变量<img src="img/4351OS_02_39.jpg" alt="Understanding Ordinary Least Squares"/>，其值始终为<code class="literal">1</code>。然后，我们使用<code class="literal">transpose</code>和<code class="literal">inverse</code>函数为矩阵<code class="literal">MX</code>和<code class="literal">MY</code>中的数据计算线性回归模型的估计系数。</p><div><div><h3 class="title"><a id="note16"/>注意</h3><p>对于当前的例子，来自 Incanter 库的<code class="literal">bind-columns</code>函数可以导入到我们的名称空间中，如下所示:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter.core :only [bind-columns]]))</pre></div></div></div><p>先前定义的函数可应用于我们先前定义的矩阵(<em> X </em>和<em> Y </em>)，如下所示:</p><div><pre class="programlisting">(def ols-linear-model
  (linear-model-ols X Y))

(def ols-linear-model-coefs
  (cl/as-vec ols-linear-model))</pre></div><p>在前面的代码中，<code class="literal">ols-linear-model-coefs</code>只是一个变量，<code class="literal">ols-linear-model</code>是一个只有一列的矩阵，用向量表示。我们使用 clatrix 库中的<code class="literal">as-vec</code>函数来执行这种转换。</p><p>我们可以实际验证由<code class="literal">ols-linear-model</code>函数<a id="id196" class="indexterm"/>估计的系数实际上等于由咒语库的<code class="literal">linear-model</code>函数生成的系数，如下图所示:</p><div><pre class="programlisting">user&gt; (cl/as-vec (ols-linear-model X Y))
[1.851198344985435 0.6252788163253274 0.7429244752213087 -0.4044785456588674 -0.22635635488532463]
user&gt; (:coefs iris-linear-model)
[1.851198344985515 0.6252788163253129 0.7429244752213329 -0.40447854565877606 -0.22635635488543926]
user&gt; (every? #(&lt; % 0.0001) 
                      (map - 
                         ols-linear-model-coefs 
                         (:coefs iris-linear-model)))
true</pre></div><p>在前面代码示例的最后一个表达式中，我们找到了由<code class="literal">ols-linear-model</code>函数<a id="id197" class="indexterm"/>产生的系数之间的差值，由<code class="literal">linear-model</code>函数产生的差值<a id="id198" class="indexterm"/>，并检查这些差值是否都小于<code class="literal">0.0001</code>。</p></div>





<title>Using linear regression for prediction</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch02lvl1sec20"/>使用线性回归进行预测</h1></div></div></div><p>一旦我们确定了线性回归模型的系数，我们就可以使用这些系数来预测模型的因变量的值。线性回归模型将预测值定义为每个系数与其对应自变量的值的乘积之和<a id="id199" class="indexterm"/>。</p><p>我们可以很容易地定义以下<a id="id200" class="indexterm"/>通用函数，当提供自变量的系数和值时，该函数预测给定公式化线性回归模型的因变量的值:</p><div><pre class="programlisting">(defn predict [coefs X]
  {:pre [(= (count coefs)
            (+ 1 (count X)))]}
  (let [X-with-1 (conj X 1)
        products (map * coefs X-with-1)]
    (reduce + products)))</pre></div><p>在前面的函数中，我们使用一个前提条件来断言系数的数量和独立变量的值。该函数期望自变量的值的数量比模型的系数的数量少一个，因为我们添加了一个额外的参数来表示自变量，该自变量的值总是<em> 1 </em>。然后，该函数使用<code class="literal">map</code>函数计算相应系数和独立变量值的乘积，然后使用<code class="literal">reduce</code>函数计算这些乘积项的总和。</p></div>





<title>Understanding regularization</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch02lvl1sec21"/>了解正规化</h1></div></div></div><p>线性回归使用线性方程估计一些给定的训练数据；这种解决方案可能并不总是最适合给定的数据。当然，这在很大程度上取决于我们试图建模的问题。<strong>正则化</strong> <a id="id201" class="indexterm"/>是为数据提供更好拟合的常用技术。通常，通过减少模型的一些独立变量的影响来正则化给定的模型。或者，我们可以将其建模为高阶多项式。正则化并不排斥线性回归，大多数机器学习算法都使用某种形式的正则化，以便从给定的训练数据中创建更准确的模型。</p><p>当模型没有将因变量估计为接近训练数据中因变量观察值的值时，称该模型为<strong>欠拟合</strong>或<strong>高偏差</strong>。另一方面，当估计的模型完全符合数据，但不够普遍，无法用于预测时，模型也可以被称为<strong>过度拟合</strong>，或者说具有<strong>高方差</strong>。过度拟合模型通常描述训练数据中的随机误差或噪声，而不是模型的因变量和自变量之间的潜在关系。最佳拟合回归模型通常位于由欠拟合和过拟合模型创建的模型之间，并且可以通过正则化过程获得。</p><p>欠拟合或过拟合模型正则化的常用方法<a id="id202" class="indexterm"/>是<strong> Tikhnov 正则化</strong> <a id="id203" class="indexterm"/>。在统计学上，这种方法也叫<strong>岭回归</strong>。我们可以将 Tikhnov 正则化的一般形式描述如下:</p><div><img src="img/4351OS_02_73.jpg" alt="Understanding regularization"/></div><p>假设<em> A </em>表示从<a id="id204" class="indexterm"/>自变量向量<em> x </em>到因变量<em> y </em>的映射。值<em> A </em>类似于回归模型的参数向量。向量<em> x </em>与因变量观测值的关系，记为<em> b </em>，可表示如下。</p><p>欠拟合模型相对于实际数据有很大的误差，或者说是偏差。我们应该努力减少这种错误。这可以正式表示如下，并且基于估计模型的残差的总和:</p><div><img src="img/4351OS_02_74.jpg" alt="Understanding regularization"/></div><p>Tikhnov 正则化将一个惩罚最小平方项添加到前面的方程中，以防止过拟合，其正式定义如下:</p><div><img src="img/4351OS_02_75.jpg" alt="Understanding regularization"/></div><p>前面等式中的项<img src="img/4351OS_02_76.jpg" alt="Understanding regularization"/>被称为正则化矩阵<a id="id205" class="indexterm"/>。在最简单的 Tikhnov 正则化形式中，<a id="id206" class="indexterm"/>这个矩阵取值<img src="img/4351OS_02_77.jpg" alt="Understanding regularization"/>，其中<img src="img/4351OS_02_78.jpg" alt="Understanding regularization"/>是一个常数。尽管将该方程应用于回归模型超出了本书的范围，但我们可以使用 Tikhnov 正则化生成一个线性回归模型<a id="id207" class="indexterm"/>，其成本函数如下:</p><div><img src="img/4351OS_02_79.jpg" alt="Understanding regularization"/></div><p>在前面的等式中，<img src="img/4351OS_02_80.jpg" alt="Understanding regularization"/>项被称为模型的正则化参数。必须适当选择该值，因为该参数的较大值可能会产生欠拟合模型。</p><p>使用之前定义的成本函数，我们可以应用梯度下降来确定参数向量，如下所示:</p><div><img src="img/4351OS_02_81.jpg" alt="Understanding regularization"/></div><p>我们还可以将正则化应用于确定参数向量的 OLS 方法，如下所示:</p><div><img src="img/4351OS_02_82.jpg" alt="Understanding regularization"/></div><p>在上式中，<em> L </em>被称为平滑矩阵<a id="id208" class="indexterm"/>，并且可以采用以下形式。注意，我们在第 1 章<a class="link" href="ch01.html" title="Chapter 1. Working with Matrices">、<em>使用矩阵</em>中使用了<em> L </em>的后一种定义形式。</a></p><div><img src="img/4351OS_01_0100.jpg" alt="Understanding regularization"/></div><p>有趣的是，当前面等式中的正则化参数<a id="id209" class="indexterm"/>为<em> 0 </em>时，正则化解使用 OLS 方法还原为原始解。</p></div>





<title>Summary</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch02lvl1sec22"/>总结</h1></div></div></div><p>在这一章中，我们学习了线性回归和一些算法，这些算法可用于从一些样本数据中建立最佳的线性回归模型。以下是我们讨论的一些其他要点:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">我们讨论了单变量和多变量的线性回归</li><li class="listitem" style="list-style-type: disc">我们实现了梯度下降算法来制定一个一元线性回归模型</li><li class="listitem" style="list-style-type: disc">我们实现了<strong>普通最小二乘法</strong> ( <strong> OLS </strong>)方法来寻找最优线性回归模型的系数</li><li class="listitem" style="list-style-type: disc">我们介绍了正则化以及如何将其应用于线性回归</li></ul></div><p>在下一章中，我们将研究机器学习的一个不同领域，即分类。分类也是回归的一种形式，用于将数据归类到不同的类或组中。</p></div>
</body></html>