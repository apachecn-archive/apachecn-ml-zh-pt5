<html><head/><body>



<title>Chapter 4. Building Neural Networks</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch04"/>第四章。构建神经网络</h1></div></div></div><p>在本章中，我们将介绍<strong>人工神经网络</strong> ( <strong> ANNs </strong>)。<a id="id297" class="indexterm"/>我们将研究人工神经网络的基本表示，然后讨论几种可用于监督和非监督机器学习问题的人工神经网络模型。我们还引入了<strong> Enclog </strong> Clojure 库<a id="id298" class="indexterm"/>来构建 ann。</p><p>神经网络非常适合在一些给定的数据中寻找模式，并在计算中有一些实际应用，如手写识别和机器视觉。人工神经网络通常被组合或相互连接来模拟一个给定的问题。有趣的是，它们可以应用于几个机器学习问题，如回归和分类。人工神经网络在计算的几个领域都有应用，并且不局限于机器学习的范围。</p><p><strong>无监督学习</strong> <a id="id299" class="indexterm"/>是一种机器学习形式，其中给定的训练数据不包含任何关于给定输入样本属于哪个类的信息。由于训练数据是<em>未标记的</em>，无监督学习算法必须完全独立地确定给定数据中的各种类别。一般来说，这是通过找出不同数据之间的相似之处，然后将数据分成几个类别来完成的。这种技术被称为<strong>聚类分析，</strong> <a id="id300" class="indexterm"/>，我们将在接下来的章节中详细研究这种方法。人工神经网络被用于无监督的机器学习技术，主要是因为它们能够快速识别一些未标记数据中的模式。ann 展示的这种专门形式的无监督学习<a id="id301" class="indexterm"/>被称为<a id="id302" class="indexterm"/> <strong>竞争学习</strong>。</p><p>关于人工神经网络的一个有趣的事实是，它们是根据显示学习能力的高阶动物的中枢神经系统的结构和行为来建模的。</p><div><div><div><div><h1 class="title"><a id="ch04lvl1sec28"/>理解非线性回归</h1></div></div></div><p>至此，读者必须意识到梯度下降算法可用于估计回归和分类问题的线性和逻辑回归模型。一个显而易见的问题是:当我们可以使用<a id="id303" class="indexterm"/>梯度下降从训练数据中估计线性回归和逻辑回归模型时，神经网络的需求是什么？要理解人工神经网络的必要性，我们必须先了解非线性回归。</p><p>假设我们有一个单一特征变量<em> X </em>和一个随<em> X </em>变化的因变量<em> Y </em>，如下图所示:</p><div><img src="img/4351OS_04_01.jpg" alt="Understanding nonlinear regression"/></div><p>如上图所示，很难(如果不是不可能的话)将因变量<em> Y </em>建模为自变量<em> X </em>的线性方程。我们可以将因变量<em> Y </em>建模为因变量<em> X </em>的高阶多项式方程，从而将问题转化为标准形式的线性回归。因此，因变量<em> Y </em>被称为随自变量<em> X </em>非线性变化。当然，也很有可能无法使用多项式函数对数据进行建模。</p><p>还可以看出，使用梯度下降计算多项式函数中所有项的权重或系数具有<img src="img/4351OS_04_02.jpg" alt="Understanding nonlinear regression"/>的时间复杂度，其中<em> n </em>是训练数据中特征的数量。同样，计算一个三阶多项式方程中所有项的系数的算法复杂度是<img src="img/4351OS_04_03.jpg" alt="Understanding nonlinear regression"/>。很明显，梯度下降的时间复杂度<a id="id304" class="indexterm"/>随着模型特征的数量呈几何级数增加。因此，梯度下降本身不足以有效地模拟具有大量特征的非线性回归模型。</p><p>另一方面，人工神经网络在建立具有大量特征的数据的非线性回归模型方面非常有效。我们现在将学习人工神经网络的基本思想和几种可用于监督和非监督学习问题的人工神经网络模型。</p></div></div>





<title>Representing neural networks</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch04lvl1sec29"/>代表神经网络</h1></div></div></div><p>人工神经网络是从具有学习能力的有机体(如哺乳动物和爬行动物)的中枢神经系统的行为中模仿而来的。这些生物体的中枢神经系统包括生物体的大脑、脊髓和支持神经组织的网络。大脑处理信息并产生电信号，这些电信号通过神经纤维网络传输到机体的各个器官。虽然有机体的大脑执行大量复杂的处理和控制，但它实际上是神经元的集合。然而，感觉信号的实际处理是由这些神经元的几种复杂组合完成的。当然，每个神经元只能处理大脑所处理的极小一部分信息。大脑实际上是通过这个复杂的神经元网络将电信号从身体的各种感觉器官发送到运动器官来发挥作用的。单个神经元具有如下图所示的细胞结构:</p><div><img src="img/4351OS_04_04.jpg" alt="Representing neural networks"/></div><p>一个神经元有几个靠近细胞核的树突和一个单一的<em>轴突</em>，它传输来自细胞核的信号。树突用于接收来自其他神经元的信号，并可以被视为神经元的输入。类似地，神经元的轴突类似于神经元的输出。因此，神经元在数学上可以表示为一个处理几个输入并产生一个输出的<a id="id306" class="indexterm"/>函数。</p><p>这些神经元中的几个相互连接，这个网络被称为<strong>神经网络</strong>。一个神经元本质上是通过向其他神经元传递微弱的电信号来实现其功能的。两个神经元之间的互连空间被称为突触。</p><p>人工神经网络由几个相互连接的神经元组成。每个神经元都可以用一个数学函数来表示，该函数消耗几个输入值并产生一个输出值，如下图所示:</p><div><img src="img/4351OS_04_05.jpg" alt="Representing neural networks"/></div><p>上图可以说明一个神经元。用数学术语来说，它只是一个将一组输入值<img src="img/4351OS_04_07.jpg" alt="Representing neural networks"/>映射到输出值<img src="img/4351OS_04_08.jpg" alt="Representing neural networks"/>的函数<img src="img/4351OS_04_06.jpg" alt="Representing neural networks"/>。函数<img src="img/4351OS_04_06.jpg" alt="Representing neural networks"/>称为神经元的<strong>激活函数</strong> <a id="id308" class="indexterm"/>，其输出值<img src="img/4351OS_04_08.jpg" alt="Representing neural networks"/>称为神经元的<strong>激活</strong>。神经元的这种表现被称为<strong>感知器</strong>。感知器可以单独使用<a id="id310" class="indexterm"/>，并且足够有效地估计监督机器学习模型，如线性回归和逻辑回归。然而，复杂的非线性数据可以用几个相互连接的感知器更好地建模。</p><p>通常，偏置输入被加到提供给感知器的一组输入值上。对于输入值<img src="img/4351OS_04_07.jpg" alt="Representing neural networks"/>，我们添加术语<img src="img/4351OS_04_09.jpg" alt="Representing neural networks"/>作为偏置输入，因此<img src="img/4351OS_04_10.jpg" alt="Representing neural networks"/>。下图显示了添加了此偏置值的神经元:</p><div><img src="img/4351OS_04_11.jpg" alt="Representing neural networks"/></div><p>提供给感知器的每个输入值<img src="img/4351OS_04_12.jpg" alt="Representing neural networks"/>都有一个相关的权重<img src="img/4351OS_04_13.jpg" alt="Representing neural networks"/>。该权重类似于<a id="id311" class="indexterm"/>线性回归模型的特征系数。激活函数应用于这些权重及其相应的输入值。我们可以根据输入值、它们的权重和感知器的激活函数来正式定义感知器的估计输出值<img src="img/4351OS_04_06.jpg" alt="Representing neural networks"/>,如下所示:</p><div><img src="img/4351OS_04_14.jpg" alt="Representing neural networks"/></div><p>人工神经网络节点使用的激活函数很大程度上取决于要建模的样本数据。通常使用<strong> sigmoid </strong>或<strong>双曲正切</strong>函数作为分类问题的激活函数(更多信息，请参考<em>基于汽油近红外(NIR)光谱建立校正模型的小波神经网络(WNN)方法</em>)。对于给定的阈值输入，sigmoid 功能被称为<em>激活</em>。</p><p>我们可以绘制 sigmoid 函数的方差来描述这种行为，如下图所示:</p><div><img src="img/4351OS_04_15.jpg" alt="Representing neural networks"/></div><p>人工神经网络可以大致分为<em>前馈神经网络</em>和<em>递归神经网络</em>(更多信息，请参考<em>双向递归神经网络</em>)。这两种人工神经网络的区别在于，在前馈神经网络中，人工神经网络节点之间的连接不会形成有向循环，而在递归神经网络<a id="id312" class="indexterm"/>中，节点互连会形成有向循环。因此，在前馈神经网络中，神经网络的给定层中的每个节点仅从神经网络中紧邻的前一层中的节点接收输入。</p><p>有几个人工神经网络模型有实际应用，我们将在本章中探讨其中的几个。</p></div>





<title>Understanding multilayer perceptron ANNs</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch04lvl1sec30"/>理解多层感知器神经网络</h1></div></div></div><p>我们现在介绍一个简单的前馈神经网络模型——<strong>多层感知器</strong>模型。这个模型<a id="id313" class="indexterm"/>代表了一个基本的前馈神经网络，并且足够通用于在监督学习领域中建模回归和分类问题。所有的输入都通过一个单向的前馈神经网络。这是前馈神经网络中任何层都没有反馈<em>的直接结果。</em></p><p>所谓反馈，是指给定层的输出作为输入反馈到人工神经网络中前一层的感知器。此外，使用单层感知器将意味着仅使用单个激活函数，这相当于使用<em>逻辑回归</em>来模拟给定的训练数据。这意味着该模型不能用于拟合非线性数据，而非线性数据是人工神经网络的主要动机。我们必须注意，我们在第三章、<em>中讨论了逻辑回归对数据进行分类</em>。</p><p>多层感知器人工神经网络可以用下图来说明<a id="id314" class="indexterm"/>:</p><div><img src="img/4351OS_04_16.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>多层感知器 ANN 包括几层感知器节点<a id="id315" class="indexterm"/>。它展示了一个单一的输入层，一个单一的输出层，和几个隐藏层的感知器。输入层只是将输入值传递给人工神经网络的第一个隐藏层。然后，这些值通过其他隐藏层传播到输出层，在那里使用激活函数对它们进行加权和求和，最终产生输出值。</p><p>训练数据中的每个样本由<img src="img/4351OS_04_17.jpg" alt="Understanding multilayer perceptron ANNs"/>元组表示，其中<img src="img/4351OS_04_18.jpg" alt="Understanding multilayer perceptron ANNs"/>是期望输出，<img src="img/4351OS_04_19.jpg" alt="Understanding multilayer perceptron ANNs"/>是<img src="img/4351OS_04_20.jpg" alt="Understanding multilayer perceptron ANNs"/>训练样本的输入值。<img src="img/4351OS_04_19.jpg" alt="Understanding multilayer perceptron ANNs"/>输入向量包括与训练数据中特征数量相等的多个值。</p><p>每个节点的输出被称为该节点的<strong>激活</strong>，并由层<img src="img/4351OS_04_22.jpg" alt="Understanding multilayer perceptron ANNs"/>中的<img src="img/4351OS_04_20.jpg" alt="Understanding multilayer perceptron ANNs"/>节点的术语<img src="img/4351OS_04_21.jpg" alt="Understanding multilayer perceptron ANNs"/>表示。如前所述，用于产生该值的激活函数是 sigmoid 函数或双曲正切函数。当然，任何其他的<a id="id316" class="indexterm"/>数学函数都可以用来拟合样本数据。多层感知器网络的输入层简单地将偏置输入添加到输入值，并且提供给 ANN 的输入集被中继到下一层。我们可以将这种等式正式表示如下:</p><div><img src="img/4351OS_04_23.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>人工神经网络中每一对层之间的突触都有一个相关的权重矩阵。这些矩阵中的行数等于输入值的数量，即，更靠近 ANN 输入层的层中的节点数量和等于更靠近 ANN 输出层的 synapse 层中的节点数量的列数。对于层<em> l </em>，权重矩阵由项<img src="img/4351OS_04_24.jpg" alt="Understanding multilayer perceptron ANNs"/>表示。</p><p>层<em> l </em>的激活值可以使用人工神经网络的激活函数来确定。激活函数应用于权重矩阵和由 ANN 中的前一层产生的激活值的乘积。通常，用于多层感知器的激活函数是 sigmoid 函数。这种等式可以正式表示如下:</p><div><img src="img/4351OS_04_25.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>通常，用于多层感知器的激活函数是 sigmoid 函数。请注意，我们没有在人工神经网络的输出层添加偏差值。此外，输出图层可以生成任意数量的输出值。为了对一个<em> k-class </em>分类问题建模，我们需要一个产生<em> k </em>输出值的人工神经网络。</p><p>为了执行二元分类，我们最多只能对两类输入数据进行建模。用于二元分类的人工神经网络生成的输出值总是 0 或 1。由此可见，对于<img src="img/4351OS_04_26.jpg" alt="Understanding multilayer perceptron ANNs"/>类，<img src="img/4351OS_04_27.jpg" alt="Understanding multilayer perceptron ANNs"/>。</p><p>我们还可以使用<em> k </em>二进制输出值对多类分类建模，因此，人工神经网络的输出是一个<img src="img/4351OS_04_28.jpg" alt="Understanding multilayer perceptron ANNs"/>矩阵。这可以正式表示如下:</p><div><img src="img/4351OS_04_29.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>因此，我们可以使用多层感知器人工神经网络来执行二进制和多类分类。多层感知器 ANN 可以使用<strong>反向传播</strong> <strong>算法</strong>、<a id="id317" class="indexterm"/>进行训练，我们将在本章稍后研究和实现。</p><p>让我们假设我们想要模拟逻辑异或门的行为。XOR 门可以被认为是一个二进制分类器，它需要两个输入并产生一个输出。模拟 XOR 门的人工神经网络的结构如下图所示。有趣的是，线性回归可用于模拟 AND 和 OR 逻辑门，但不能用于模拟 XOR 门。这是由于异或门输出的非线性性质，因此，人工神经网络被用来克服这一限制。</p><div><img src="img/4351OS_04_30.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>上图所示的多层感知器<a id="id318" class="indexterm"/>在输入层有三个节点，在隐藏层有四个节点，在输出层有一个节点。请注意，除了输出层之外的每一层都将偏差输入添加到下一层中节点的输入值集合中。如上图所示，人工神经网络中有两个突触，它们与权重矩阵<img src="img/4351OS_04_31.jpg" alt="Understanding multilayer perceptron ANNs"/>和<img src="img/4351OS_04_32.jpg" alt="Understanding multilayer perceptron ANNs"/>相关联。注意，第一个突触在输入层和隐藏层之间，第二个突触在隐藏层和输出层之间。权重矩阵<img src="img/4351OS_04_31.jpg" alt="Understanding multilayer perceptron ANNs"/>的大小为<img src="img/4351OS_04_33.jpg" alt="Understanding multilayer perceptron ANNs"/>，权重矩阵<img src="img/4351OS_04_32.jpg" alt="Understanding multilayer perceptron ANNs"/>的大小为<img src="img/4351OS_04_34.jpg" alt="Understanding multilayer perceptron ANNs"/>。此外，术语<img src="img/4351OS_04_35.jpg" alt="Understanding multilayer perceptron ANNs"/>用于表示人工神经网络中的所有权重矩阵。</p><p>由于多层感知器 ANN 中每个节点的激活函数是 sigmoid 函数，我们可以定义类似于逻辑回归模型的成本函数的 ANN 节点权重的成本函数。人工神经网络的成本函数可以根据权重矩阵定义如下:</p><div><img src="img/4351OS_04_36.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>前面的成本函数实质上是人工神经网络输出层中每个节点的成本函数的平均值(更多信息，参见材料科学中的<em>神经网络</em>)。对于具有<em> K </em>输出值的多层感知器 ANN，我们在<em> K </em>项上执行平均。注意，<img src="img/4351OS_04_37.jpg" alt="Understanding multilayer perceptron ANNs"/>表示人工神经网络的<img src="img/4351OS_04_38.jpg" alt="Understanding multilayer perceptron ANNs"/>输出值，<img src="img/4351OS_04_39.jpg" alt="Understanding multilayer perceptron ANNs"/>表示人工神经网络的输入变量，<em> N </em>是训练数据中样本值的个数。成本函数本质上是逻辑回归的函数，但此处应用于<em> K </em>输出值。我们可以将正则化参数添加到前面的成本函数中，并使用下面的等式来表达正则化的成本函数:</p><div><img src="img/4351OS_04_40.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>上式中定义的成本函数<a id="id319" class="indexterm"/>增加了一个类似于逻辑回归的正则项。正则项实质上是几层人工神经网络的所有输入值的所有权重的平方和，不包括增加的偏差输入的权重。此外，术语<img src="img/4351OS_04_41.jpg" alt="Understanding multilayer perceptron ANNs"/>指的是 ANN 的层<em> l </em>中的节点数。值得注意的一点是，在前面的正则化成本函数中，只有正则化项取决于人工神经网络的层数。因此，估计模型的<em>推广</em>是基于 ANN 中的层数。</p></div>





<title>Understanding the backpropagation algorithm</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch04lvl1sec31"/>了解反向传播算法</h1></div></div></div><p><strong>反向传播学习</strong>算法<a id="id320" class="indexterm"/>用于从一组给定的样本值中训练一个多层感知器 ANN。简而言之，该算法首先计算一组给定的<a id="id321" class="indexterm"/>输入值的输出值，并计算人工神经网络输出的误差量。通过将 ANN 的预测输出值与来自提供给 ANN 的训练数据的给定输入值的预期输出值进行比较，来确定 ANN 中的误差量。然后，计算出的误差用于修改人工神经网络的权重。因此，在用合理数量的样本训练 ANN 之后，ANN 将能够预测一组输入值的输出值。该算法包括三个不同的阶段。它们如下:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">正向传播阶段</li><li class="listitem" style="list-style-type: disc">反向传播阶段</li><li class="listitem" style="list-style-type: disc">权重更新阶段</li></ul></div><p>ANN 中突触的权重首先被初始化为范围<img src="img/4351OS_04_42.jpg" alt="Understanding the backpropagation algorithm"/>和<img src="img/4351OS_04_43.jpg" alt="Understanding the backpropagation algorithm"/>内的随机值。我们将权重初始化为该范围内的值，以避免权重矩阵中的对称性。这种对称的避免被称为<strong>对称破坏，</strong> <a id="id322" class="indexterm"/>，它的执行使得反向传播算法的每次迭代在人工神经网络中的突触权重中产生显著的变化。这在人工神经网络中是可取的，因为它的每个节点应该独立于人工神经网络中的其他节点进行学习。如果所有节点都具有相同的权重，估计的<a id="id323" class="indexterm"/>学习模型将会过拟合或欠拟合。</p><p>此外，反向传播学习<a id="id324" class="indexterm"/>算法需要两个附加参数，即学习速率<img src="img/4351OS_04_44.jpg" alt="Understanding the backpropagation algorithm"/>和学习动量<img src="img/4351OS_04_45.jpg" alt="Understanding the backpropagation algorithm"/>。我们将在本节后面的示例中看到这些参数的效果。</p><p>该算法的前向传播阶段简单地计算人工神经网络各层中所有节点的激活值。如前所述，输入层中节点的激活值是人工神经网络的输入值和偏差输入。这可以通过下式正式定义:</p><div><img src="img/4351OS_04_23.jpg" alt="Understanding the backpropagation algorithm"/></div><p>使用来自 ANN 输入层的这些激活值，确定 ANN 其它层中节点的激活。这是通过将激活函数应用于给定层的权重矩阵和 ANN 中前一层的激活值的乘积来实现的。这可以正式表示如下:</p><div><img src="img/4351OS_04_46.jpg" alt="Understanding the backpropagation algorithm"/></div><p>前面的等式解释了层<em> l </em>的激活值等于应用于前一层的输出(或激活)值和给定层的权重矩阵的激活函数。接下来，输出层的激活值被<em>反向传播</em>。这样，我们的意思是激活值从输出层通过隐藏层遍历到人工神经网络的输入层。在此阶段，我们确定人工神经网络中每个节点的误差量或增量。通过<a id="id325" class="indexterm"/>计算预期输出值<img src="img/4351OS_04_18.jpg" alt="Understanding the backpropagation algorithm"/>和输出层激活值<img src="img/4351OS_04_47.jpg" alt="Understanding the backpropagation algorithm"/>之间的差值，确定输出层的增量值。这种差异计算可以用下面的等式来总结:</p><div><img src="img/4351OS_04_48.jpg" alt="Understanding the backpropagation algorithm"/></div><p>层<em> l </em>的术语<img src="img/4351OS_04_49.jpg" alt="Understanding the backpropagation algorithm"/>是大小为<img src="img/4351OS_04_50.jpg" alt="Understanding the backpropagation algorithm"/>的矩阵，其中<em> j </em>是层<em> l </em>中的节点数。该术语可以正式定义如下:</p><div><img src="img/4351OS_04_51.jpg" alt="Understanding the backpropagation algorithm"/></div><p>除了 ANN 的输出层之外的层<a id="id326" class="indexterm"/>的增量项由以下等式确定:</p><div><img src="img/4351OS_04_52.jpg" alt="Understanding the backpropagation algorithm"/></div><p>在前面的等式中，二元运算<img src="img/4351OS_04_53.jpg" alt="Understanding the backpropagation algorithm"/>用于表示两个大小相等的矩阵的逐元素乘法。请注意，该操作不同于矩阵乘法，按元素的乘法将返回由两个大小相等的矩阵中位置相同的元素的乘积组成的矩阵。术语<img src="img/4351OS_04_54.jpg" alt="Understanding the backpropagation algorithm"/>代表人工神经网络中使用的激活函数的导数。当我们使用 sigmoid 函数作为激活函数时，术语<img src="img/4351OS_04_55.jpg" alt="Understanding the backpropagation algorithm"/>的值为<img src="img/4351OS_04_56.jpg" alt="Understanding the backpropagation algorithm"/>。</p><p>因此，我们可以计算神经网络中所有节点的 delta 值。我们可以使用这些δ值来确定人工神经网络突触的梯度。我们现在进入反向传播算法的最终权重更新阶段。</p><p>各种突触的梯度首先被初始化为所有元素都为 0 的矩阵。给定突触的梯度矩阵的大小与突触的权重矩阵的大小相同。梯度项<img src="img/4351OS_04_57.jpg" alt="Understanding the backpropagation algorithm"/>表示神经网络中紧接在层<em> l </em>之后的突触层的梯度。ANN 中突触梯度的初始化正式表示如下:</p><div><img src="img/4351OS_04_58.jpg" alt="Understanding the backpropagation algorithm"/></div><p>对于<a id="id327" class="indexterm"/>训练数据中的每个样本值，我们计算神经网络中所有节点的增量和激活值。使用以下等式将这些值添加到突触的梯度中:</p><div><img src="img/4351OS_04_59.jpg" alt="Understanding the backpropagation algorithm"/></div><p>然后，我们计算所有样本值的梯度平均值<a id="id328" class="indexterm"/>，并使用给定层的增量和梯度值来更新权重矩阵，如下所示:</p><div><img src="img/4351OS_04_60.jpg" alt="Understanding the backpropagation algorithm"/></div><p>因此，算法的学习速率和学习动量参数仅在权重更新阶段起作用。前面的三个方程代表反向传播算法的一次迭代。必须进行大量的迭代，直到 ANN 中的总误差收敛到一个小值。我们现在可以使用以下步骤总结反向传播学习算法<a id="id329" class="indexterm"/>:</p><div><ol class="orderedlist arabic"><li class="listitem">将人工神经网络突触的权重初始化为随机值。</li><li class="listitem">选择一个样本值，并通过人工神经网络的几个层向前传播该样本值，以生成人工神经网络中每个节点的激活。</li><li class="listitem">将人工神经网络最后一层产生的激活通过隐藏层反向传播到人工神经网络的输入层。通过这一步，我们计算神经网络中每个节点的误差或增量。</li><li class="listitem">计算步骤 3 产生的误差与神经网络中所有节点的突触权重或输入激活的乘积。这一步产生网络中每个节点的权重梯度。每个梯度由一个比率或百分比表示。</li><li class="listitem">使用人工神经网络中给定层的梯度和增量，计算人工神经网络中突触层的权重变化。然后从人工神经网络中突触的权重中减去这些变化。这实质上是反向传播算法的权重更新步骤。</li><li class="listitem">对训练数据中的<a id="id330" class="indexterm"/>其余样本重复步骤 2 至 5。</li></ol></div><p>反向传播学习算法中有几个不同的<a id="id331" class="indexterm"/>部分，我们现在将实现每个部分并将其组合成一个完整的实现。由于神经网络中突触和<a id="id332" class="indexterm"/>激活的增量和权重可以用矩阵表示，我们可以编写该算法的矢量化实现。</p><div><div><h3 class="title"><a id="note19"/>注意</h3><p>注意，对于下面的例子，我们需要来自 Incanter 库中的名称空间<code class="literal">incanter.core</code>的函数。这个名称空间中的函数实际上使用 Clatrix 库来表示矩阵及其操作。</p></div></div><p>让我们假设我们需要实现一个 ANN 来模拟一个逻辑 XOR 门。样本数据只是 XOR 门的真值表，可以表示为向量，如下所示:</p><div><pre class="programlisting">;; truth table for XOR logic gate
(def sample-data [[[0 0] [0]]
                  [[0 1] [1]]
                  [[1 0] [1]]
                  [[1 1] [0]]])</pre></div><p>在前面的向量<code class="literal">sample-data</code>中定义的每个元素本身是一个向量，包括用于 XOR 门的输入和输出值的其他向量。我们将使用这个向量作为构建人工神经网络的训练数据。这本质上是一个分类问题，我们会用 ann 来建模。抽象地说，人工神经网络应该能够执行二元和多元分类。我们可以将人工神经网络的协议定义如下:</p><div><pre class="programlisting">(defprotocol NeuralNetwork
  (run        [network inputs])
  (run-binary [network inputs])
  (train-ann  [network samples]))</pre></div><p>前面代码中定义的<code class="literal">NeuralNetwork</code>协议有三个功能。<code class="literal">train-ann</code>函数<a id="id333" class="indexterm"/>可用于训练人工神经网络，需要一些样本数据。可以在这个人工神经网络上使用<code class="literal">run</code>和<code class="literal">run-binary</code>函数来分别执行多类和二元分类。<code class="literal">run</code>和<code class="literal">run-binary</code>功能都需要一组输入值。</p><p>反向传播算法<a id="id334" class="indexterm"/>的第一步是初始化人工神经网络突触的权重。我们可以使用<code class="literal">rand</code>和<code class="literal">matrix</code>函数将这些权重生成一个矩阵，如下所示:</p><div><pre class="programlisting">(defn rand-list
  "Create a list of random doubles between 
  -epsilon and +epsilon."
  [len epsilon]
  (map (fn [x] (- (rand (* 2 epsilon)) epsilon))
         (range 0 len)))

(defn random-initial-weights
  "Generate random initial weight matrices for given layers.
  layers must be a vector of the sizes of the layers."
  [layers epsilon]
  (for [i (range 0 (dec (length layers)))]
    (let [cols (inc (get layers i))
          rows (get layers (inc i))]
      (matrix (rand-list (* rows cols) epsilon) cols))))</pre></div><p>前面代码中的<code class="literal">rand-list</code>函数<a id="id335" class="indexterm"/>在<code class="literal">epsilon</code>的正负范围内创建一个随机元素列表。如前所述，我们选择这个范围来打破权重矩阵的对称性。</p><p><code class="literal">random-initial-weights</code>函数<a id="id337" class="indexterm"/>为人工神经网络的不同层生成几个权重矩阵。如前面代码中所定义的，<code class="literal">layers</code>参数必须是 ANN 各层大小的向量。对于输入层有两个节点、隐藏层有三个节点、输出层有一个节点的 ANN，我们将<code class="literal">layers</code>作为<code class="literal">[2 3 1]</code>传递给<code class="literal">random-initial-weights</code>函数。每个权重矩阵的列数等于输入数，行数等于 ANN 下一层的节点数。我们将给定层的权重矩阵中的列数设置为输入数，加上神经层偏差的额外输入。注意，我们使用了稍微不同形式的<code class="literal">matrix</code>函数。这种形式接受一个向量，并将这个向量分割成一个矩阵，这个矩阵有许多列，这些列由这个函数的第二个参数指定。因此，传递给这种形式的<code class="literal">matrix</code>函数的向量必须有<code class="literal">(* rows cols)</code>元素，其中<code class="literal">rows</code>和<code class="literal">cols</code>分别是权重矩阵中的行数和列数。</p><p>由于我们需要将<a id="id338" class="indexterm"/> sigmoid 函数应用于人工神经网络中某层的所有激活，我们必须定义一个函数，将 sigmoid 函数应用于给定矩阵中的所有元素。我们可以使用<code class="literal">incanter.core</code>名称空间中的<code class="literal">div</code>、<code class="literal">plus</code>、<code class="literal">exp</code>和<code class="literal">minus</code>函数来实现这样一个函数，如下面的代码所示:</p><div><pre class="programlisting">(defn sigmoid
  "Apply the sigmoid function 1/(1+exp(-z)) to all 
  elements in the matrix z."
  [z]
  (div 1 (plus 1 (exp (minus z)))))</pre></div><div><div><h3 class="title"><a id="note20"/>注意</h3><p>请注意，前面定义的所有函数都对给定矩阵中的所有元素应用相应的算术运算，并返回一个新矩阵。</p></div></div><p>我们还需要<a id="id339" class="indexterm"/>在人工神经网络的每一层添加一个偏置节点。这可以通过包装<code class="literal">bind-rows</code>函数<a id="id340" class="indexterm"/>来完成，它将一行元素添加到一个矩阵中，如下面的代码所示:</p><div><pre class="programlisting">(defn bind-bias
  "Add the bias input to a vector of inputs."
  [v]
  (bind-rows [1] v))</pre></div><p>因为偏移值总是 1，所以我们将元素行指定为<code class="literal">[1]</code>到<code class="literal">bind-rows</code>函数。</p><p>使用前面定义的函数，我们可以实现前向传播。实际上，我们必须将人工神经网络中两层之间给定突触的权重相乘，然后对每个生成的激活值应用 sigmoid 函数，如以下代码所示:</p><div><pre class="programlisting">(defn matrix-mult
  "Multiply two matrices and ensure the result is also a matrix."
  [a b]
  (let [result (mmult a b)]
    (if (matrix? result)
      result
      (matrix [result]))))

(defn forward-propagate-layer
  "Calculate activations for layer l+1 given weight matrix 
  of the synapse between layer l and l+1 and layer l activations."
  [weights activations]
  (sigmoid (matrix-mult weights activations)))

(defn forward-propagate
  "Propagate activation values through a network's
  weight matrix and return output layer activation values."
  [weights input-activations]
  (reduce #(forward-propagate-layer %2 (bind-bias %1))
          input-activations weights))</pre></div><p>在前面的代码中，我们首先定义了一个<code class="literal">matrix-mult</code>函数<a id="id341" class="indexterm"/>，它执行矩阵乘法，并确保结果是一个矩阵。注意，为了定义<code class="literal">matrix-mult</code>，我们使用了<code class="literal">mmult</code>函数<a id="id342" class="indexterm"/>，而不是将两个相同大小的矩阵中的相应元素相乘的<code class="literal">mult</code>函数。</p><p>使用<code class="literal">matrix-mult</code>和<code class="literal">sigmoid</code>函数，我们可以在人工神经网络的两层之间实现前向传播步骤。这是在<code class="literal">forward-propagate-layer</code>函数中完成的，函数<a id="id343" class="indexterm"/>只是将代表 ANN 中两层之间突触权重的矩阵与输入激活值相乘，同时确保返回的<a id="id344" class="indexterm"/>值始终是一个矩阵。要将一组给定的值传播到人工神经网络的所有层，我们必须添加一个偏差输入，并对每一层应用<code class="literal">forward-propagate-layer</code>函数。这可以通过在<code class="literal">forward-propagate-layer</code>函数的闭包上使用<code class="literal">reduce</code>函数来简洁地完成，如前面代码中定义的<code class="literal">forward-propagate</code>函数所示。</p><p>虽然<code class="literal">forward-propagate</code>函数可以确定人工神经网络的输出激活，但我们实际上需要人工神经网络中所有节点的激活来使用反向传播。我们可以通过将<code class="literal">reduce</code>函数转换为递归函数并引入一个累加器变量来存储人工神经网络中每一层的激活来做到这一点。下面代码中定义的<code class="literal">forward-propagate-all-activations</code>函数<a id="id345" class="indexterm"/>实现了这一思想，并使用<code class="literal">loop</code>形式递归应用<code class="literal">forward-propagate-layer</code>函数:</p><div><pre class="programlisting">(defn forward-propagate-all-activations
  "Propagate activation values through the network 
  and return all activation values for all nodes."
  [weights input-activations]
  (loop [all-weights     weights
         activations     (bind-bias input-activations)
         all-activations [activations]]
    (let [[weights
           &amp; all-weights']  all-weights
           last-iter?       (empty? all-weights')
           out-activations  (forward-propagate-layer
                             weights activations)
           activations'     (if last-iter? out-activations
                                (bind-bias out-activations))
           all-activations' (conj all-activations activations')]
      (if last-iter? all-activations'
          (recur all-weights' activations' all-activations')))))</pre></div><p>前面代码中定义的<code class="literal">forward-propagate-all-activations</code>函数要求 ANN 中所有节点的权重和输入值作为激活值通过 ANN。我们首先使用<code class="literal">bind-bias</code>功能将偏置输入添加到人工神经网络的输入激活中。然后，我们将这个值存储在一个累加器中，即变量<code class="literal">all-activations</code>，作为 ANN 中所有激活的<a id="id347" class="indexterm"/>向量。然后将<code class="literal">forward-propagate-layer</code>函数<a id="id348" class="indexterm"/>应用于 ANN 各层的权重矩阵，并且每次迭代将偏置输入添加到 ANN 中相应层的输入激活中。</p><div><div><h3 class="title"><a id="note21"/>注意</h3><p>注意，我们没有在最后一次迭代中添加偏差输入，因为它计算 ANN 的输出层。因此，<code class="literal">forward-propagate-all-activations</code>函数通过人工神经网络应用输入值的前向传播，并返回人工神经网络中每个节点的激活。注意，这个向量中的激活值是按照人工神经网络的层的顺序排列的。</p></div></div><p>我们现在将实现反向传播学习算法的反向传播阶段。首先，我们必须实现一个函数，根据公式<img src="img/4351OS_04_61.jpg" alt="Understanding the backpropagation algorithm"/>计算误差项<img src="img/4351OS_04_49.jpg" alt="Understanding the backpropagation algorithm"/>。我们将借助以下代码来实现这一点:</p><div><pre class="programlisting">(defn back-propagate-layer
  "Back propagate deltas (from layer l+1) and 
  return layer l deltas."
  [deltas weights layer-activations]
  (mult (matrix-mult (trans weights) deltas)
        (mult layer-activations (minus 1 layer-activations))))</pre></div><p>前面代码中定义的<code class="literal">back-propagate-layer</code>函数<a id="id349" class="indexterm"/>根据层的权重和人工神经网络中下一层的差值，计算人工神经网络中突触层<em> l </em>的误差或差值。</p><div><div><h3 class="title"><a id="note22"/>注意</h3><p>请注意，我们仅使用矩阵乘法通过<code class="literal">matrix-mult</code>函数计算<img src="img/4351OS_04_62.jpg" alt="Understanding the backpropagation algorithm"/>项。所有其他乘法操作都是矩阵的元素乘法，使用<code class="literal">mult</code>函数完成。</p></div></div><p>本质上，我们必须通过人工神经网络的各种隐藏层将该函数从输出层应用到输入层，以产生人工神经网络中每个节点的增量值。然后，可以将这些增量值添加到节点的激活中，从而产生梯度值，我们必须通过该梯度值来调整人工神经网络中节点的权重。我们可以用类似于<code class="literal">forward-propagate-all-activations</code>函数的方式<a id="id350" class="indexterm"/>来实现，也就是说，通过递归地将<code class="literal">back-propagate-layer</code>函数应用于人工神经网络的各个层。当然，我们必须以相反的顺序遍历 ANN 的各个层，即<a id="id351" class="indexterm"/>从输出层开始，通过隐藏层，到输入层。我们将借助以下代码来实现这一点:</p><div><pre class="programlisting">(defn calc-deltas
  "Calculate hidden deltas for back propagation.
  Returns all deltas including output-deltas."
  [weights activations output-deltas]
  (let [hidden-weights     (reverse (rest weights))
        hidden-activations (rest (reverse (rest activations)))]
    (loop [deltas          output-deltas
           all-weights     hidden-weights
           all-activations hidden-activations
           all-deltas      (list output-deltas)]
      (if (empty? all-weights) all-deltas
        (let [[weights
               &amp; all-weights']      all-weights
               [activations
                &amp; all-activations'] all-activations
              deltas'        (back-propagate-layer
                               deltas weights activations)
              all-deltas'    (cons (rest deltas') 
                                    all-deltas)]
          (recur deltas' all-weights' 
                 all-activations' all-deltas'))))))</pre></div><p><code class="literal">calc-deltas</code>函数决定了人工神经网络中所有感知器节点的增量值。对于这个计算，不需要输入和输出激活。计算增量值只需要绑定到<code class="literal">hidden-activations</code>变量<a id="id352" class="indexterm"/>的隐藏激活。此外，输入层的权重被跳过，因为它们被绑定到<code class="literal">hidden-weights</code>变量<a id="id353" class="indexterm"/>。<code class="literal">calc-deltas</code>函数<a id="id354" class="indexterm"/>然后将<code class="literal">back-propagate-layer</code>函数<a id="id355" class="indexterm"/>应用于 ANN 中每个突触层的所有权重矩阵，从而确定矩阵中所有节点的增量。请注意，我们没有将偏差节点的增量添加到一组计算的增量中。这是使用<code class="literal">rest</code>函数<a id="id356" class="indexterm"/>、<code class="literal">(rest deltas')</code>对给定突触层的计算增量完成的，因为第一个增量是给定层中的偏置输入的增量。</p><p>根据定义，给定突触层<img src="img/4351OS_04_57.jpg" alt="Understanding the backpropagation algorithm"/>的梯度向量项通过将矩阵<img src="img/4351OS_04_63.jpg" alt="Understanding the backpropagation algorithm"/>和<img src="img/4351OS_04_64.jpg" alt="Understanding the backpropagation algorithm"/>相乘来确定<a id="id357" class="indexterm"/>，矩阵<img src="img/4351OS_04_63.jpg" alt="Understanding the backpropagation algorithm"/>和<img src="img/4351OS_04_64.jpg" alt="Understanding the backpropagation algorithm"/>分别表示下一层的增量和给定层的激活。我们将借助以下代码来实现这一点:</p><div><pre class="programlisting">(defn calc-gradients
  "Calculate gradients from deltas and activations."
  [deltas activations]
  (map #(mmult %1 (trans %2)) deltas activations))</pre></div><p>前面代码中显示的<code class="literal">calc-gradients</code>函数<a id="id358" class="indexterm"/>是术语<img src="img/4351OS_04_65.jpg" alt="Understanding the backpropagation algorithm"/>的简明实现。由于我们将处理一系列增量和激活<a id="id359" class="indexterm"/>项，我们使用<code class="literal">map</code>函数将前面的等式应用于 ANN 中相应的增量和激活。使用<code class="literal">calc-deltas</code>和<code class="literal">calc-gradient</code>函数，我们可以确定给定训练样本的人工神经网络中所有节点权重的总误差。我们将借助以下代码来实现这一点:</p><div><pre class="programlisting">(defn calc-error
  "Calculate deltas and squared error for given weights."
  [weights [input expected-output]]
  (let [activations    (forward-propagate-all-activations 
                        weights (matrix input))
        output         (last activations)
        output-deltas  (minus output expected-output)
        all-deltas     (calc-deltas 
                        weights activations output-deltas)
        gradients      (calc-gradients all-deltas activations)]
    (list gradients
          (sum (pow output-deltas 2)))))</pre></div><p>前面代码中定义的<code class="literal">calc-error</code>函数需要两个参数——人工神经网络中突触层的权重矩阵和一个样本训练值，如<code class="literal">[input expected-output]</code>所示。首先使用<code class="literal">forward-propagate-all-activations</code>函数<a id="id360" class="indexterm"/>计算人工神经网络中所有节点的激活，最后一层的 delta 值计算为人工神经网络产生的预期输出值和实际输出值之差。人工神经网络计算的输出值就是人工神经网络产生的最后一个激活值，如前面代码中的<code class="literal">(last activations)</code>所示。使用计算的激活，通过<code class="literal">calc-deltas</code>功能<a id="id361" class="indexterm"/>确定所有感知器节点的增量。使用<code class="literal">calc-gradients</code>函数，这些增量值依次用于确定人工神经网络各层的权重梯度。对于给定的<a id="id363" class="indexterm"/>样本值，人工神经网络的<strong>均方误差</strong> ( <strong> MSE </strong> ) <a id="id362" class="indexterm"/>也通过将人工神经网络输出层的增量值的平方相加来计算。</p><p>对于人工神经网络中某一层的给定权重矩阵，我们必须将该层的梯度初始化为与权重矩阵具有相同维数的矩阵，并且梯度矩阵中的所有元素必须设置为<code class="literal">0</code>。这可以通过组合使用<code class="literal">dim</code>函数和<code class="literal">matrix</code>函数来实现，函数以向量的形式返回矩阵的大小，函数的变体形式如下面的代码所示:</p><div><pre class="programlisting">(defn new-gradient-matrix
  "Create accumulator matrix of gradients with the
  same structure as the given weight matrix
  with all elements set to 0."
  [weight-matrix]
  (let [[rows cols] (dim weight-matrix)]
    (matrix 0 rows cols)))</pre></div><p>在前面代码中定义的<code class="literal">new-gradient-matrix</code>函数<a id="id364" class="indexterm"/>中，<code class="literal">matrix</code>函数需要一个值、<a id="id365" class="indexterm"/>行数和列数来初始化一个矩阵。该函数产生一个初始化的梯度矩阵，其结构与提供的权重矩阵相同。</p><p>我们现在实现<code class="literal">calc-gradients-and-error</code>函数<a id="id366" class="indexterm"/>来对一组权重矩阵和样本值应用<code class="literal">calc-error</code>函数。我们必须对每个样本应用<code class="literal">calc-error</code>函数，并累加梯度和 MSE 值的总和。然后，我们计算这些累积值的平均值，以返回给定样本值和权重矩阵的梯度矩阵和总 MSE。我们将借助以下代码来实现这一点:</p><div><pre class="programlisting">(defn calc-gradients-and-error' [weights samples]
  (loop [gradients   (map new-gradient-matrix weights)
         total-error 1
         samples     samples]
    (let [[sample
           &amp; samples']     samples
           [new-gradients
            squared-error] (calc-error weights sample)
            gradients'     (map plus new-gradients gradients)
            total-error'   (+ total-error squared-error)]
      (if (empty? samples')
        (list gradients' total-error')
        (recur gradients' total-error' samples')))))

(defn calc-gradients-and-error
  "Calculate gradients and MSE for sample
  set and weight matrix."
  [weights samples]
  (let [num-samples   (length samples)
        [gradients
         total-error] (calc-gradients-and-error'
                       weights samples)]
    (list
      (map #(div % num-samples) gradients)    ; gradients
      (/ total-error num-samples))))          ; MSE</pre></div><p>前面代码中定义的<code class="literal">calc-gradients-and-error</code>函数<a id="id367" class="indexterm"/>依赖于<code class="literal">calc-gradients-and-error'</code>辅助函数。<code class="literal">The calc-gradients-and-error'</code>函数初始化梯度矩阵，执行<code class="literal">calc-error</code>函数的应用<a id="id368" class="indexterm"/>，并累加计算的梯度值和 MSE。<code class="literal">calc-gradients-and-error</code>函数<a id="id369" class="indexterm"/>简单地计算从<code class="literal">calc-gradients-and-error'</code>函数返回的累积梯度矩阵和 MSE 的平均值。</p><p>现在，在我们的实现中唯一缺少的部分<a id="id370" class="indexterm"/>是使用计算的梯度修改人工神经网络中节点的权重。简而言之，我们必须反复更新权重，直到观察到 MSE 收敛。这实际上是一种应用于人工神经网络节点的梯度下降形式。我们现在将实现梯度下降的这种变体，以便通过重复修改 ANN 中节点的权重来训练 ANN，如下面的代码所示:</p><div><pre class="programlisting">(defn gradient-descent-complete?
  "Returns true if gradient descent is complete."
  [network iter mse]
  (let [options (:options network)]
    (or (&gt;= iter (:max-iters options))
        (&lt; mse (:desired-error options)))))</pre></div><p>前面代码中定义的<code class="literal">gradient-descent-complete?</code>函数<a id="id371" class="indexterm"/>只是检查梯度下降的终止条件。该函数假设表示为网络的 ANN 是包含<code class="literal">:options</code>关键字的映射或记录。这个键的值又是包含 ANN 的各种配置选项的另一个映射。<code class="literal">gradient-descent-complete?</code>功能检查人工神经网络的总均方误差是否小于由<code class="literal">:desired-error</code>选项指定的期望均方误差。此外，我们添加了另一个条件来检查执行的迭代次数是否超过了由<code class="literal">:max-iters</code>选项指定的最大迭代次数。</p><p>现在，我们将为多层感知器神经网络实现一个<code class="literal">gradient-descent</code>函数。在该实现中，<a id="id372" class="indexterm"/>权重的变化由梯度下降算法提供的<code class="literal">step</code>函数来计算。然后，这些计算出的变化被简单地添加到人工神经网络突触层的现有<a id="id373" class="indexterm"/>权重中。我们将借助以下代码实现多层感知器神经网络的<code class="literal">gradient-descent</code>功能:</p><div><pre class="programlisting">(defn apply-weight-changes
  "Applies changes to corresponding weights."
  [weights changes]
  (map plus weights changes))

(defn gradient-descent
  "Perform gradient descent to adjust network weights."
  [step-fn init-state network samples]
  (loop [network network
         state init-state
         iter 0]
    (let [iter     (inc iter)
          weights  (:weights network)
          [gradients
           mse]    (calc-gradients-and-error weights samples)]
      (if (gradient-descent-complete? network iter mse)
        network
        (let [[changes state] (step-fn network gradients state)
              new-weights     (apply-weight-changes 
                               weights changes)
              network         (assoc network 
                              :weights new-weights)]
          (recur network state iter))))))</pre></div><p>前面代码中定义的<code class="literal">apply-weight-changes</code>函数<a id="id374" class="indexterm"/>简单地将权重和人工神经网络权重的计算变化相加。<code class="literal">gradient-descent</code>函数<a id="id375" class="indexterm"/>需要一个<code class="literal">step</code>函数(指定为<code class="literal">step-fn</code>)、人工神经网络的初始状态、人工神经网络本身以及训练人工神经网络的样本数据。该函数必须根据人工神经网络、初始梯度矩阵和人工神经网络的初始状态计算权重变化。<code class="literal">step-fn</code>函数也返回人工神经网络改变后的状态。然后使用<code class="literal">apply-weight-changes</code>函数更新 ANN 的权重，并且重复执行该迭代，直到<code class="literal">gradient-descent-complete?</code>函数返回<code class="literal">true</code>。ANN 的权重由<code class="literal">network</code>图中的<code class="literal">:weights</code>关键字指定。然后通过简单地覆盖由关键字<code class="literal">:weights</code>指定的<code class="literal">network</code>上的值来更新这些权重。</p><p>在<a id="id376" class="indexterm"/>反向传播算法的背景下，我们需要指定人工神经网络必须训练的学习速率和学习动量。需要这些参数来确定 ANN 中节点权重的变化。然后，必须将实现该计算的函数指定为<code class="literal">gradient-descent</code>函数的<code class="literal">step-fn</code>参数，如以下代码所示:</p><div><pre class="programlisting">(defn calc-weight-changes
  "Calculate weight changes:
  changes = learning rate * gradients + 
            learning momentum * deltas."
  [gradients deltas learning-rate learning-momentum]
  (map #(plus (mult learning-rate %1)
              (mult learning-momentum %2))
       gradients deltas))

(defn bprop-step-fn [network gradients deltas]
  (let [options             (:options network)
        learning-rate       (:learning-rate options)
        learning-momentum   (:learning-momentum options)
        changes             (calc-weight-changes
                             gradients deltas
                             learning-rate learning-momentum)]
    [(map minus changes) changes]))

(defn gradient-descent-bprop [network samples]
  (let [gradients (map new-gradient-matrix (:weights network))]
    (gradient-descent bprop-step-fn gradients
                      network samples)))</pre></div><p>前面代码中定义的<code class="literal">calc-weight-changes</code>函数根据人工神经网络中给定层的梯度值和增量计算权重的变化，称为<img src="img/4351OS_04_66.jpg" alt="Understanding the backpropagation algorithm"/>。<code class="literal">bprop-step-fn</code>函数从由<code class="literal">network</code>表示的 ANN 中提取学习速率<a id="id377" class="indexterm"/>和学习动量参数，并使用<code class="literal">calc-weight-changes</code>函数。由于权重将通过<code class="literal">gradient-descent</code>函数与变化相加，我们使用<code class="literal">minus</code>函数将权重的变化作为负值返回。</p><p><code class="literal">gradient-descent-bprop</code>函数<a id="id378" class="indexterm"/>简单地初始化给定 ANN 权重的梯度矩阵，并通过<a id="id379" class="indexterm"/>指定<code class="literal">bprop-step-fn</code>为要使用的<code class="literal">step</code>函数来调用<code class="literal">gradient-descent</code>函数。使用<code class="literal">gradient-descent-bprop</code>函数，我们可以实现之前定义的抽象<code class="literal">NeuralNetwork</code>协议，如下所示:</p><div><pre class="programlisting">(defn round-output
  "Round outputs to nearest integer."
  [output]
  (mapv #(Math/round ^Double %) output))

(defrecord MultiLayerPerceptron [options]
  NeuralNetwork

  ;; Calculates the output values for the given inputs.
  (run [network inputs]
    (let [weights (:weights network)
          input-activations (matrix inputs)]
      (forward-propagate weights input-activations)))

  ;; Rounds the output values to binary values for
  ;; the given inputs.
  (run-binary [network inputs]
    (round-output (run network inputs)))

  ;; Trains a multilayer perceptron ANN from sample data.
  (train-ann [network samples]
    (let [options         (:options network)
          hidden-neurons  (:hidden-neurons options)
          epsilon         (:weight-epsilon options)
          [first-in
           first-out]     (first samples)
          num-inputs      (length first-in)
          num-outputs     (length first-out)
          sample-matrix   (map #(list (matrix (first %)) 
                                      (matrix (second %)))
                               samples)
          layer-sizes     (conj (vec (cons num-inputs 
                                           hidden-neurons))
                                num-outputs)
          new-weights     (random-initial-weights 
                           layer-sizes epsilon)
          network         (assoc network :weights new-weights)]
      (gradient-descent-bprop network sample-matrix))))</pre></div><p>前面代码中定义的<a id="id380" class="indexterm"/>记录使用<code class="literal">gradient-descent-bprop</code>函数训练多层感知器 ANN。<code class="literal">train-ann</code>函数首先从指定给 ANN 的选项映射中提取隐藏神经元的数量和常数<img src="img/4351OS_04_43.jpg" alt="Understanding the backpropagation algorithm"/>的值。<a id="id381" class="indexterm"/> ANN 中各种突触层的大小首先从样本数据中确定，并绑定到<code class="literal">layer-sizes</code>变量。然后使用<code class="literal">random-initial-weights</code>函数<a id="id382" class="indexterm"/>初始化人工神经网络的权重，并使用<code class="literal">assoc</code>函数在记录<code class="literal">network</code>中更新。最后，调用<code class="literal">gradient-descent-bprop</code>函数<a id="id383" class="indexterm"/>使用反向传播学习算法训练 ANN。</p><p>由<code class="literal">MultiLayerPerceptron</code>记录定义的 ANN 还实现了来自<code class="literal">NeuralNetwork</code>协议的另外两个功能<code class="literal">run</code>和<code class="literal">run-binary</code>。<code class="literal">run</code>功能使用<code class="literal">forward-propagate</code>功能<a id="id384" class="indexterm"/>来确定训练过的<code class="literal">MultiLayerPerceptron</code>人工神经网络的输出值。对于给定的一组输入值，<a id="id385" class="indexterm"/> <code class="literal">run-binary</code>函数只是对<code class="literal">run</code>函数返回的输出值进行舍入。</p><p>使用<code class="literal">MultiLayerPerceptron</code>记录创建的 ANN 需要一个单独的<code class="literal">options</code>参数，其中包含我们可以为 ANN 指定的各种选项。我们可以为这种人工神经网络定义如下默认选项:</p><div><pre class="programlisting">(def default-options
  {:max-iters 100
   :desired-error 0.20
   :hidden-neurons [3]
   :learning-rate 0.3
   :learning-momentum 0.01
   :weight-epsilon 50})

(defn train [samples]
  (let [network (MultiLayerPerceptron. default-options)]
    (train-ann network samples)))</pre></div><p>由<code class="literal">default-options</code>变量<a id="id386" class="indexterm"/>定义的映射包含以下键，这些键指定了<code class="literal">MultiLayerPerceptron</code> ANN 的选项:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">:max-iter</code>:该键指定运行<code class="literal">gradient-descent</code>功能的最大迭代次数。</li><li class="listitem" style="list-style-type: disc"><code class="literal">:desired-error</code>:该变量指定人工神经网络中预期的或可接受的 MSE。</li><li class="listitem" style="list-style-type: disc"><code class="literal">:hidden-neurons</code>:该变量指定网络中隐藏神经节点的数量。值<code class="literal">[3]</code>表示具有三个神经元的单个隐藏层。</li><li class="listitem" style="list-style-type: disc"><code class="literal">:learning-rate </code>和<code class="literal">:learning-momentum</code>:这些键指定反向传播学习算法的权重更新阶段的学习速率和学习动量。</li><li class="listitem" style="list-style-type: disc"><code class="literal">:epsilon</code>:该变量指定<code class="literal">random-initial-weights</code>函数<a id="id387" class="indexterm"/>用来初始化人工神经网络权重的常数。</li></ul></div><p>我们还定义了一个简单的<a id="id388" class="indexterm"/>辅助函数<code class="literal">train</code>来创建一个<code class="literal">MultiLayerPerceptron</code>类型的 ANN，并使用<code class="literal">train-ann</code>函数<a id="id389" class="indexterm"/>和由<code class="literal">samples</code>参数指定的样本数据来训练 ANN。我们现在可以从由<code class="literal">sample-data</code>变量指定的训练数据中创建一个经过训练的人工神经网络，如下所示:</p><div><pre class="programlisting">user&gt; (def MLP (train sample-data))
#'user/MLP</pre></div><p>然后，我们可以使用经过训练的<a id="id390" class="indexterm"/> ANN 来预测一些输入值的输出。由<code class="literal">MLP</code>定义的人工神经网络产生的输出与异或门的输出非常匹配，如下所示:</p><div><pre class="programlisting">user&gt; (run-binary MLP  [0 1])
[1]
user&gt; (run-binary MLP  [1 0])
[1]</pre></div><p>然而，经过训练的人工神经网络对某些输入集产生不正确的输出，如下所示:</p><div><pre class="programlisting">user&gt; (run-binary MLP  [0 0])
[0]
user&gt; (run-binary MLP  [1 1]) ;; incorrect output generated
[1]</pre></div><p>为了提高经过训练的人工神经网络的准确性，我们可以采取几种措施。首先，我们可以使用人工神经网络的权重矩阵来正则化计算的梯度。这一修改将对前面的实现产生显著的改进。我们还可以增加要执行的最大迭代次数。我们还可以通过调整学习速率、学习动力、<a id="id391" class="indexterm"/>和人工神经网络中隐藏节点的数量来调整算法，使其性能更好。这些修改被跳过，因为它们必须由读者来完成。</p><p><strong> Enclog </strong>库<a id="id392" class="indexterm"/>(<a class="ulink" href="http://github.com/jimpil/enclog">http://github.com/jimpil/enclog</a>)是用于机器学习算法和人工神经网络的<strong> Encog </strong>库的 Clojure 包装库。Encog 库(<a class="ulink" href="http://github.com/encog">http://github.com/encog</a>)有两个主要实现:一个在 Java 中，一个在. NET 中。我们可以使用 Enclog 库轻松生成定制的 ann 来建模监督和非监督的机器学习问题。</p><div><div><h3 class="title"><a id="note23"/>注</h3><p>通过向<code class="literal">project.clj</code>文件添加以下依赖关系，可将 Enclog 库添加到 Leiningen 项目中:</p><div><pre class="programlisting">[org.encog/encog-core "3.1.0"]
[enclog "0.6.3"]</pre></div><p>注意，Enclog 库需要 Encog Java 库作为依赖项。</p><p>对于下面的示例，命名空间声明应该类似于下面的声明:</p><div><pre class="programlisting">(ns my-namespace
  (:use [enclog nnets training]))</pre></div></div></div><p>我们可以使用来自<code class="literal">enclog.nnets</code>名称空间的<code class="literal">neural-pattern</code>和<code class="literal">network</code>函数从 Enclog 库中创建一个 ANN。<code class="literal">neural-pattern</code>功能<a id="id393" class="indexterm"/>用于指定人工神经网络的神经网络模型。<code class="literal">network</code>函数接受从<code class="literal">neural-pattern</code>函数返回的神经网络模型，并创建一个新的人工神经网络。我们可以根据指定的神经网络模型向<code class="literal">network</code>函数提供几个<a id="id394" class="indexterm"/>选项。前馈多层感知器网络定义如下:</p><div><pre class="programlisting">(def mlp (network (neural-pattern :feed-forward)
                  :activation :sigmoid
                  :input      2
                  :output     1
                  :hidden     [3]))</pre></div><p>对于一个前馈神经网络，我们可以用<code class="literal">:activation</code>键给<code class="literal">network</code>函数指定激活函数。对于我们的例子，我们使用 sigmoid 函数，它被指定为<code class="literal">:sigmoid</code>，作为神经网络节点的激活函数。我们还使用<code class="literal">:input</code>、<code class="literal">:output</code>和<code class="literal">:hidden</code>键指定了人工神经网络的输入、输出和隐藏层中的节点数量。</p><p>为了用一些样本数据训练由<code class="literal">network</code>函数创建的 ANN，我们使用了来自<code class="literal">enclog.training</code>名称空间的<code class="literal">trainer</code>和<code class="literal">train</code>函数。用于训练人工神经网络的学习算法必须指定为<code class="literal">trainer</code>函数的第一个参数。对于反向传播算法，该参数是<code class="literal">:back-prop</code>关键字。训练器函数返回的值<a id="id395" class="indexterm"/>代表一个人工神经网络以及用于训练人工神经网络的学习算法。然后使用<code class="literal">train</code>函数在人工神经网络上实际运行指定的训练算法。我们将借助以下代码来实现这一点:</p><div><pre class="programlisting">(defn train-network [network data trainer-algo]
  (let [trainer (trainer trainer-algo
                         :network network
                         :training-set data)]
    (train trainer 0.01 1000 []))) ;; 0.01 is the expected error</pre></div><p>前面代码中定义的<code class="literal">train-network</code>函数有三个参数。第一个参数是由网络函数创建的 ANN，第二个参数是用于训练 ANN 的训练数据，第三个参数指定必须训练 ANN 的学习算法。如前面的代码所示，我们可以使用关键参数<code class="literal">:network</code>和<code class="literal">:training-set</code>将 ANN 和训练数据指定给<code class="literal">trainer</code>函数<a id="id396" class="indexterm"/>。然后，使用样本数据，使用<code class="literal">train</code>函数在人工神经网络上运行训练算法。我们可以将 ANN 中的预期误差和运行训练算法的最大迭代次数指定为<code class="literal">train</code>函数<a id="id397" class="indexterm"/>的第一个和第二个参数。在前面的例子中，期望误差是<code class="literal">0.01</code>，最大迭代次数是 1000。传递给<code class="literal">train</code>函数的最后一个参数是一个指定 ANN 行为的向量<a id="id398" class="indexterm"/>，我们通过将它作为一个空向量传递来忽略它。</p><p>用于在人工神经网络上运行训练算法的训练数据可以使用 Enclog 的<code class="literal">data</code>函数创建。例如，我们可以使用<code class="literal">data</code>函数为逻辑异或门创建一个训练数据，如下所示:</p><div><pre class="programlisting">(def dataset
  (let [xor-input [[0.0 0.0] [1.0 0.0] [0.0 1.0] [1.0 1.0]]
        xor-ideal [[0.0]     [1.0]     [1.0]     [0.0]]]
        (data :basic-dataset xor-input xor-ideal)))</pre></div><p><code class="literal">data</code>函数要求数据类型作为函数的第一个参数，后面是训练数据的输入和输出值作为向量。对于我们的例子，我们将使用<code class="literal">:basic-dataset</code>和<code class="literal">:basic </code>参数。<code class="literal">:basic-dataset</code>关键字可用于创建训练<a id="id399" class="indexterm"/>数据，而<code class="literal">:basic </code>关键字可用于指定一组输入值。</p><p>使用由<code class="literal">dataset</code>变量和<code class="literal">train-network</code>函数<a id="id400" class="indexterm"/>定义的数据，我们可以训练人工神经网络的<code class="literal">MLP</code>来模拟异或门的输出，如下所示:</p><div><pre class="programlisting">user&gt; (def MLP (train-network mlp dataset :back-prop))
Iteration # 1 Error: 26.461526% Target-Error: 1.000000%
Iteration # 2 Error: 25.198031% Target-Error: 1.000000%
Iteration # 3 Error: 25.122343% Target-Error: 1.000000%
Iteration # 4 Error: 25.179218% Target-Error: 1.000000%
...
...
Iteration # 999 Error: 3.182540% Target-Error: 1.000000%
Iteration # 1,000 Error: 3.166906% Target-Error: 1.000000%
#'user/MLP</pre></div><p>如前面的输出所示，经过训练的人工神经网络的误差约为 3.16%。现在，我们可以使用经过训练的人工神经网络来预测一组输入值的输出。为此，我们使用 Java 的<code class="literal">compute</code>和<code class="literal">getData</code>方法，它们分别由<code class="literal">.compute</code>和<code class="literal">.getData</code>指定。我们可以定义一个简单的助手函数来调用输入值向量的<code class="literal">.compute</code>方法，并将输出舍入为二进制值，如下所示:</p><div><pre class="programlisting">(defn run-network [network input]
  (let [input-data (data :basic input)
        output     (.compute network input-data)
        output-vec (.getData output)]
    (round-output output-vec)))</pre></div><p>我们现在可以使用<code class="literal">run-network</code>函数<a id="id401" class="indexterm"/>使用输入值向量来测试训练好的人工神经网络，如下所示:</p><div><pre class="programlisting">user&gt; (run-network MLP [1 1])
[0]
user&gt; (run-network MLP [1 0])
[1]
user&gt; (run-network MLP [0 1])
[1]
user&gt; (run-network MLP [0 0])
[0]</pre></div><p>如前面的<a id="id402" class="indexterm"/>代码所示，由<code class="literal">MLP</code>表示的经过训练的人工神经网络完全符合 XOR 门的行为。</p><p>总之，Enclog 库为我们提供了一组功能强大的函数，可用于构建人工神经网络。在前面的例子中，我们探索了前馈多层感知器模型。该库提供了其他几种 ANN 模型，如<strong>自适应共振理论</strong> ( <strong> ART </strong> ) <a id="id403" class="indexterm"/>、<strong>自组织映射</strong> ( <strong> SOM </strong>)、Elman 网络等。Enclog 库还允许我们定制特定神经网络模型中节点的激活函数。对于我们示例中的前馈网络，我们使用了 sigmoid 函数。<a id="id404" class="indexterm"/>该库还支持多种数学函数，如正弦、双曲正切、对数和线性函数。Enclog 库还支持几种机器学习算法，可用于训练人工神经网络。</p></div>





<title>Understanding recurrent neural networks</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch04lvl1sec32"/>了解递归神经网络</h1></div></div></div><p>我们现在将把注意力转移到递归神经网络<a id="id405" class="indexterm"/>上，并研究一个简单的递归神经网络模型。一个<a id="id406" class="indexterm"/> <strong> Elman 神经网络</strong> <a id="id407" class="indexterm"/>是一个简单的递归 ANN，具有单个输入、输出和隐藏层。还有一个额外的神经节点<em>上下文层</em>。Elman 神经网络用于模拟监督和非监督机器学习问题中的短期记忆。Enclog 支持 Elman 神经网络，我们将演示如何使用 Enclog 库构建 Elman 神经网络。</p><p>Elman 神经网络的上下文层<a id="id408" class="indexterm"/>从 ANN 的隐藏层接收未加权的输入。这样，人工神经网络可以记住我们使用隐藏层生成的先前值，并使用这些值来影响预测值。因此，上下文层充当人工神经网络的一种短期记忆。Elman 神经网络可以通过下图来说明:</p><div><img src="img/4351OS_04_67.jpg" alt="Understanding recurrent neural networks"/></div><p>如前图所示，Elman 网络的结构<a id="id409" class="indexterm"/>类似于前馈多层感知器 ANN。Elman 网络为 ANN 增加了一个额外的神经节点上下文层。上图所示的 Elman 网络有两个输入，产生两个输出。Elman 网络的输入和隐藏层增加了额外的偏置输入，类似于多层感知器。隐藏层神经元的激活被直接馈送到两个上下文节点<img src="img/4351OS_04_68.jpg" alt="Understanding recurrent neural networks"/>和<img src="img/4351OS_04_69.jpg" alt="Understanding recurrent neural networks"/>。存储在这些上下文节点中的值随后被 ANN 的隐藏层中的节点用来回忆先前的激活以确定新的激活值。</p><p>我们可以创建一个 Elman 网络<a id="id410" class="indexterm"/>，它将<code class="literal">:elman</code>关键字指定给 Enclog 库中的<code class="literal">neural-pattern</code>函数，如下所示:</p><div><pre class="programlisting">(def elman-network (network (neural-pattern :elman)
                             :activation :sigmoid
                             :input      2
                             :output     1
                             :hidden     [3]))</pre></div><p>为了训练<a id="id411" class="indexterm"/> Elman 网络，我们可以使用弹性传播算法(更多信息，请参考<em>改进的 Rprop 学习算法的经验评估</em>)。该算法也可用于训练 Enclog 支持的其他递归网络。有趣的是，弹性传播算法也可以用来训练前馈网络。该算法的性能也明显优于反向传播学习算法。尽管对这种算法的完整描述超出了本书的范围，但鼓励读者了解更多关于这种学习算法的知识。弹性传播算法被指定为<code class="literal">train-network</code>函数的<code class="literal">:resilient-prop</code>关键字，我们之前已经定义过了。我们可以使用<code class="literal">train-network</code>函数<a id="id412" class="indexterm"/>和<code class="literal">dataset</code>变量训练 Elman 神经网络，如下所示:</p><div><pre class="programlisting">user&gt; (def EN (train-network elman-network dataset 
                             :resilient-prop))
Iteration # 1 Error: 26.461526% Target-Error: 1.000000%
Iteration # 2 Error: 25.198031% Target-Error: 1.000000%
Iteration # 3 Error: 25.122343% Target-Error: 1.000000%
Iteration # 4 Error: 25.179218% Target-Error: 1.000000%
...
...
Iteration # 99 Error: 0.979165% Target-Error: 1.000000%
#'user/EN</pre></div><p>如前面的代码所示，与反向传播算法相比，弹性传播算法需要的迭代次数相对较少。我们现在可以使用这个经过训练的人工神经网络来模拟 XOR 门，就像我们在前面的例子中所做的那样。</p><p>总之，<a id="id413" class="indexterm"/>递归神经网络模型和训练算法是可用于使用 ann 对分类或回归问题建模的其他有用模型。</p></div>





<title>Building SOMs</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch04lvl1sec33"/>构建 SOMs</h1></div></div></div><p>SOM(读作<strong> ess-o-em </strong>)是另一个有趣的人工神经网络模型，对无监督学习很有用。SOMs】用于几个实际应用中，如手写和图像识别。当我们在第 7 章<a class="link" href="ch07.html" title="Chapter 7. Clustering Data">、<em>聚类数据</em>中讨论聚类时，我们还将再次讨论 SOMs。</a></p><p>在无监督学习中，样本数据不包含预期的输出值，人工神经网络必须完全依靠自己识别和匹配输入数据中的模式。som 用于<em>竞争学习</em>，这是一种特殊的无监督学习，其中人工神经网络输出层中的神经元相互竞争激活。被激活的神经元决定了 ANN 的最终输出值，因此，被激活的神经元也被称为<a id="id415" class="indexterm"/> <strong>获胜神经元</strong>。</p><p>神经生物学研究表明，发送到大脑的不同感觉输入以有序的模式映射到大脑<em>大脑皮层</em>的相应区域。因此，处理密切相关操作的神经元紧密地聚集在一起。这被称为地形形成 <a id="id416" class="indexterm"/>的<strong>原理，事实上，som 是以这种行为为模型的。</strong></p><p>SOM 本质上是将具有大量维度的输入数据转换成低维离散映射。通过将神经元放置在该图的节点来训练 SOM。SOM 的内部图通常有一个或两个维度。SOM 中的神经元变得<em>选择性地调谐</em>到输入值中的模式。当 SOM 中的特定神经元针对特定的输入模式被激活时，其相邻的神经元往往会变得更加兴奋，并且更加适应输入值中的模式。这种行为被称为一组神经元的<strong>横向相互作用</strong> <a id="id417" class="indexterm"/>。因此，SOM 在输入数据中寻找模式。当在一组输入中发现相似的模式时，SOM 识别该模式。SOM 中的神经节点层可以如下所示:</p><div><img src="img/4351OS_04_70.jpg" alt="Building SOMs"/></div><p>SOM 有一个输入层和一个计算层，如上图所示。计算层也被称为 SOM 的<strong>特征图</strong> <a id="id418" class="indexterm"/>。输入节点将输入值映射到计算层的几个神经元。计算层中的每个节点都将其输出连接到其相邻节点，并且这些连接中的每一个都具有与之相关联的权重。这些权重被称为特征图的<a id="id420" class="indexterm"/> <strong>连接权重</strong> <a id="id421" class="indexterm"/>。SOM 通过调整其计算层中节点的连接权重来记住输入值中的模式。</p><p>SOM 的自组织过程<a id="id422" class="indexterm"/>可以描述如下:</p><div><ol class="orderedlist arabic"><li class="listitem">连接权重首先被初始化为随机值。</li><li class="listitem">对于每个输入模式，计算层中的神经节点使用判别函数计算一个值。这些值然后被用来决定获胜的神经元。</li><li class="listitem">选择具有最小判别函数值的神经元，并修改与其周围神经元的连接权重，以针对输入数据中的相似模式激活。</li></ol></div><p>必须修改权重，使得对于输入中的给定模式，由相邻节点的判别函数产生的值减少。因此，对于输入数据中的类似模式，获胜节点及其周围节点产生更高的输出或激活值。调整权重的变化量取决于为训练算法指定的学习速率。</p><p>对于输入数据中给定数量的维度<a id="id423" class="indexterm"/>D，判别函数可正式定义如下:</p><div><img src="img/4351OS_04_71.jpg" alt="Building SOMs"/></div><p>在上式中，<img src="img/4351OS_04_72.jpg" alt="Building SOMs"/>项是 SOM 中<img src="img/4351OS_04_73.jpg" alt="Building SOMs"/>神经元的权重向量。向量<img src="img/4351OS_04_72.jpg" alt="Building SOMs"/>的长度等于连接到<img src="img/4351OS_04_73.jpg" alt="Building SOMs"/>神经元的神经元数量。</p><p>一旦我们在 SOM 中选择了获胜的神经元，我们必须选择获胜神经元的相邻神经元。我们必须调整这些相邻神经元的权重以及获胜神经元的权重。可以使用多种方案来选择获胜神经元的相邻节点。在最简单的情况下，我们可以选择单个相邻的神经元。</p><p>我们也可以使用<code class="literal">bubble</code>函数或<code class="literal">radial bias</code>函数<a id="id424" class="indexterm"/>来选择获胜神经元周围的一组相邻神经元(更多信息，请参考<em>多变量函数插值和自适应网络</em>)。</p><p>为了训练一个 SOM，我们必须执行以下步骤作为训练算法的一部分:</p><div><ol class="orderedlist arabic"><li class="listitem">将计算层中节点的权重设置为随机值。</li><li class="listitem">从训练数据中选择一个样本输入模式。</li><li class="listitem">为所选的一组输入模式找到获胜的神经元。</li><li class="listitem">更新获胜神经元及其周围节点的权重。</li><li class="listitem">对训练数据中的所有样本重复步骤 2 到 4。</li></ol></div><p>Enclog 库支持 SOM 神经网络模型和训练算法。我们可以从 Enclog 库中创建并训练一个 SOM，如下所示:</p><div><pre class="programlisting">(def som (network (neural-pattern :som) :input 4 :output 2))

(defn train-som [data]
  (let [trainer (trainer :basic-som :network som
                         :training-set data
                         :learning-rate 0.7
                         :neighborhood-fn 
          (neighborhood-F :single))]
    (train trainer Double/NEGATIVE_INFINITY 10 [])))</pre></div><p>出现在前面代码中的<code class="literal">som</code>变量<a id="id426" class="indexterm"/>代表一个 SOM。<code class="literal">train-som</code>功能可用于训练 SOM。SOM 训练算法被指定为<code class="literal">:basic-som</code>。注意，我们使用<code class="literal">:learning-rate</code>键将学习率指定为<code class="literal">0.7</code>。</p><p>在前面的代码中，传递给<code class="literal">trainer</code>函数<a id="id427" class="indexterm"/>的<code class="literal">:neighborhood-fn</code>键指定了我们如何为一组给定的输入值选择 SOM 中获胜节点的邻居。我们指定<a id="id428" class="indexterm"/>获胜节点的单个相邻节点必须在<code class="literal">(neighborhood-F :single)</code>的帮助下被选择。我们也可以指定不同的邻域函数。例如，我们可以将<code class="literal">bubble</code>函数指定为<code class="literal">:bubble</code>，或者将<code class="literal">radial basis</code>函数指定为<code class="literal">:rbf</code>。</p><p>我们可以使用<code class="literal">train-som</code>函数<a id="id429" class="indexterm"/>用一些输入模式来训练 SOM。注意，用于训练 SOM 的训练数据将不具有任何输出值。SOM 必须自己识别输入数据中的模式。一旦 SOM 被训练，我们就可以使用 Java <code class="literal">classify</code>方法<a id="id430" class="indexterm"/>来检测输入中的模式。对于以下示例，我们仅提供两种输入模式来训练 SOM:</p><div><pre class="programlisting">(defn train-and-run-som []
  (let [input [[-1.0, -1.0, 1.0, 1.0 ]
               [1.0, 1.0, -1.0, -1.0]]
        input-data (data :basic-dataset input nil) ;no ideal data
        SOM        (train-som input-data)
        d1         (data :basic (first input))
        d2         (data :basic (second input))]
    (println "Pattern 1 class:" (.classify SOM d1))
    (println "Pattern 2 class:" (.classify SOM d2))
    SOM))</pre></div><p>我们可以运行前面代码中定义的<code class="literal">train-and-run-som</code>函数<a id="id431" class="indexterm"/>，并观察到 SOM 将训练数据中的两种输入模式识别为两个不同的类，如下所示:</p><div><pre class="programlisting">user&gt; (train-and-run-som)
Iteration # 1 Error: 2.137686% Target-Error: NaN
Iteration # 2 Error: 0.641306% Target-Error: NaN
Iteration # 3 Error: 0.192392% Target-Error: NaN
...
...
Iteration # 9 Error: 0.000140% Target-Error: NaN
Iteration # 10 Error: 0.000042% Target-Error: NaN
Pattern 1 class: 1
Pattern 2 class: 0
#&lt;SOM org.encog.neural.som.SOM@19a0818&gt;</pre></div><p>总之，SOMs 是处理无监督学习问题的一个很好的模型。此外，我们可以使用 Enclog 库轻松构建 som 来模拟这类问题。</p></div>





<title>Summary</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch04lvl1sec34"/>总结</h1></div></div></div><p>在这一章中，我们探讨了一些有趣的人工神经网络模型。这些模型可以应用于解决监督和非监督机器学习问题。以下是我们讨论的一些其他要点:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">我们已经探讨了人工神经网络的必要性及其主要类型，即前馈和递归人工神经网络。</li><li class="listitem" style="list-style-type: disc">我们研究了多层感知器神经网络和用于训练这种神经网络的反向传播算法。我们还使用矩阵和矩阵运算在 Clojure 中提供了反向传播算法的简单实现。</li><li class="listitem" style="list-style-type: disc">我们已经介绍了可以用来建立人工神经网络的 Enclog 库。这个库可以用来建模监督和非监督的机器学习问题。</li><li class="listitem" style="list-style-type: disc">我们已经探索了递归 Elman 神经网络，其可用于在相对较少的迭代次数中产生具有小误差的 ann。我们还描述了如何使用 Enclog 库创建和训练这样一个 ANN。</li><li class="listitem" style="list-style-type: disc">我们介绍了 SOMs，这是一种可以应用于无监督学习领域的神经网络。我们还描述了如何使用 Enclog 库创建和训练 SOM。</li></ul></div></div>
</body></html>