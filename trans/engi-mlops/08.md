<title>B16572_06_Final_JM_ePub</title>

# 第 6 章:部署您的 ML 系统的关键原则

在本章中，您将学习在生产中部署**机器学习** ( **ML** )模型的基本原则，并针对我们一直在处理的业务问题实际部署 ML 模型。为了获得全面的理解和第一手经验，我们将使用 Azure ML 服务在两个不同的部署目标上部署之前(在 [*第 4 章*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074) 、*机器学习管道*和 [*第 5 章*](B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093) 、*模型评估和打包*)训练和打包的 ML 模型:一个 Azure 容器实例和一个 Kubernetes 集群。

我们还将学习如何使用我们已经使用过的名为 MLflow 的开源框架来部署 ML 模型。这将使您能够理解如何使用两种不同的工具(Azure ML 服务和 MLflow)在不同的部署目标上部署 ML 模型作为 REST API 端点。这将使您具备在云上为任何给定场景部署 ML 模型所需的技能。

在这一章中，我们首先来看看 ML 在研究和生产中的不同之处，然后继续探讨以下主题:

*   研究与生产中的 ML
*   理解生产中最大似然推理的类型
*   浏览您的解决方案的制图基础设施
*   实践部署(针对业务问题)
*   了解持续集成和持续部署的需求

# 研究与生产中的 ML

研究中的 ML的实施具有特定的目标和优先级，以提高该领域的技术水平，而生产中的 ML 的目标是优化、自动化或增强场景或业务。

为了理解 ML 模型的部署，让我们从比较 ML 在研究和生产(在行业中)中是如何实现的开始。在*表 6.1* 中列出的多个因素，如性能、优先级、数据、公平性和可解释性，描述了部署和 ML 在研究和生产中的不同工作方式:

![Table 6.1 – ML in research and production
](img/Table_6.1.jpg)

表 6.1-研究和生产中的 ML

## 数据

总的来说，研究项目中的数据是静态的，因为数据科学家或统计学家正在处理一组数据集，并试图击败当前最先进的模型。例如，最近，自然语言处理模型的许多突破被见证，例如，来自 Google 的 BERT 或来自百度的 XLNet。为了训练这些模型，数据被收集并编译成静态数据集。在研究领域，为了评估或基准测试模型的性能，静态数据集用于评估性能，如*表 6.2* (来源:[https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237))所示:

![Table 6.2 – BERT versus XLNet performance (in research)
](img/Table_6.2.jpg)

表 6.2–BERT 与 XLNet 性能对比(研究中)

对于实例，我们可以通过在一个名为 SQUAD (10，000+ QnA) version 1.1 的流行数据集上比较两个模型的性能来比较它们的性能，在该数据集上，BERT 的准确率为 92.8%，XLNET 的准确率为 94.0%。同样，研究中用于培训和评估模型的数据是静态的，而生产或工业用例中的数据是动态的，并且随着环境、运营、业务或用户不断变化。

## 公平

在现实生活中，有偏见的模型可能代价高昂。不公平或有偏见的决策将导致业务和运营的糟糕选择。对于生产中的 ML 模型，重要的是做出的决策尽可能公平。如果生产中的模型不公平，对企业来说成本会很高。例如，最近，亚马逊制作了人力资源筛选软件，根据申请人对他们申请的工作的适合性来筛选申请人。亚马逊的 ML 专家发现男性候选人比女性候选人更受青睐(来源:[https://www . business insider . com/Amazon-ai-biased-against-women-no-surprise-Sandra-wachter-2018-10](https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10))。这种系统偏见可能代价高昂，因为在亚马逊的案例中，你可能会因为偏见而错过一些令人惊叹的人才。因此，在生产中拥有公平的模型是至关重要的，并且应该不断地被监控。在研究中，公平的模型也很重要，但不像在生产或现实生活中那样重要，公平也不像在生产中那样受到严格监控。研究的目标是击败最先进的技术，模型的公平性是次要目标。

## 可解释性

模型可解释性在生产中至关重要，以便了解 ML 模型决策及其对运营或业务的影响之间的相关性或因果关系，从而优化、增强或自动化手头的业务或任务。在研究中，情况并非如此，研究的目标是挑战或击败最先进的结果，这里优先考虑的是更好的性能(如准确性或其他指标)。在研究的情况下，ML 模型的可解释性是好的，但不是强制性的。通常，ML 项目更关心预测结果，而不是理解因果关系。ML 模型擅长发现数据中的相关性，但不是因果关系。在我们的事业中，我们努力不陷入将关联等同于事业的陷阱。这个问题严重影响了我们依赖 ML 的能力。这个问题严重限制了我们使用 ML 做决策的能力。我们需要能够理解数据之间因果关系的资源，并构建能够从商业角度很好地概括的 ML 解决方案。拥有正确的模型可解释性机制可以增强我们对因果关系的理解，并使我们能够精心设计 ML 解决方案，这些解决方案可以很好地概括并能够处理以前看不到的数据。因此，我们可以使用 ML 做出更加可靠和透明的决策。

在生产的情况下(在业务用例中)，根本不建议缺乏可解释性。让我们看一个假设的例子。让我们假设你得了癌症，必须选择一名外科医生为你做手术。有两个外科医生可用，一个是人类(治愈率为 80%)，另一个是 AI 黑盒模型(治愈率为 90%)，无法解读或解释其工作原理，但治愈率很高。你会选择什么？AI 还是外科医生治愈癌症？如果模型不是黑箱模型，用 AI 代替外科医生会更容易。虽然人工智能比外科医生更好，但如果不理解模型，决策、信任和遵从是一个问题。模型的可解释性对于做出法律决策至关重要。因此，在生产中具有 ML 的模型可解释性是至关重要的。我们将在后面的章节中了解更多。

## 性能

当谈到 ML 模型的性能时，研究的重点是改进最先进的模型，而生产的重点是建立更好的模型，而不是服务于业务需求的简单模型(**最先进的**模型不是重点)。

## 优先级

在研究中，优先考虑更快更好地训练模型，而在生产中，优先考虑更快的推理，因为重点是实时做出决策并满足业务需求。

# 了解生产中 ML 推理的类型

在前面的部分，我们看到了 ML 在研究和生产中的优先顺序。为了满足生产中的业务需求，根据需要，使用各种部署目标来推断 ML 模型。使用 ML 模型预测或做出决策称为 ML 模型推理。让我们探索在不同的部署目标上部署 ML 模型的方法，以根据业务需求促进 ML 推断。

## 部署目标

在这一节中，我们将看看不同类型的部署目标，以及我们为什么以及如何在这些部署目标中为推理提供 ML 模型。让我们先看一下虚拟机或本地服务器。

### 虚拟机

虚拟机可以是云上的或内部的，这取决于企业或组织的 IT 设置。在虚拟机上提供 ML 模型是很常见的。ML 模型以 web 服务的形式在虚拟机上提供服务。运行在虚拟机上的 web 服务接收包含输入数据的用户请求(作为 HTTP 请求)。web 服务在接收到输入数据后，以所需的格式对其进行预处理，以推断 ML 模型，这是 web 服务的一部分。在 ML 模型做出预测或执行任务之后，输出被转换并以用户可读的格式呈现。常见的分为 **JavaScript 对象符号** ( **JSON** )或**可扩展标记语言字符串** ( **XML** )。通常，web 服务是以 REST API 的形式提供的。REST API web 服务可以使用多种工具开发；例如，根据需要，FLASK 或 FAST API web 应用程序工具可用于使用 Python 或 Java 中的 Spring Boot 或 R 中的 Plumber 来开发 REST API web 服务。并行使用虚拟机的组合来扩展和维护 web 服务的健壮性。

为了协调流量和扩展机器，负载均衡器用于将传入的请求分派给虚拟机，以进行 ML 模型推理。通过这种方式，ML 模型被部署在云中或内部的虚拟机上，以满足业务需求，如下图所示:

![Figure 6.1 – Deployment on virtual machines
](img/B16572_06_01.jpg)

图 6.1–虚拟机上的部署

### 容器

容器是使用定制设置的 Linux 操作系统运行应用程序的可靠方式。容器是一个应用程序，使用开发人员编排的自定义设置运行。与虚拟机相比，容器是一种替代的、资源效率更高的模型服务方式。它们像虚拟机一样运行，因为它们有自己的运行时环境，该环境被隔离并局限于内存、文件系统和进程。

开发人员可以定制容器，将它们限制在所需的资源范围内，如内存、文件系统和进程，而虚拟机仅限于这种定制。它们更加灵活，以模块化方式运行，因此提供了更高的资源效率和优化。它们允许扩展到零的可能性，因为容器可以减少到零个副本，并根据请求运行备份。这样，与在虚拟机上运行 web 服务相比，更低的计算功耗是可能的。由于这种较低的计算功耗，在云上节省成本是可能的。

容器有很多优点；然而，一个缺点可能是使用容器所需的复杂性，因为它需要专业知识。

容器和虚拟机的运行方式有一些不同。例如，可以有多个运行在虚拟机内部的容器，它们与虚拟机共享操作系统和资源，但是虚拟机运行自己的资源和操作系统。容器可以模块化操作，但是虚拟机作为单个单元操作。Docker 用于构建和部署容器；不过也有替代品，比如 Mesos 和 CoreOS rkt。容器通常与 ML 模型和 web 服务打包在一起，以便于 ML 推断，类似于我们如何在虚拟机中提供包装在 web 服务中的 ML 模型。容器需要被编排以供用户使用。容器的编排意味着容器的部署、管理、扩展和联网的自动化。使用一个容器编排系统(如 Kubernetes)来编排容器。在下图中，我们可以看到具有自动伸缩功能的容器编排(基于请求流量):

![Figure 6.2 – Deployment on containers
](img/B16572_06_02.jpg)

图 6.2–集装箱上的部署

### 无服务器

顾名思义，无服务器计算不涉及虚拟机或容器。它消除了基础架构管理任务，如操作系统管理、服务器管理、容量配置和磁盘管理。无服务器计算使开发人员和组织能够专注于他们的核心产品，而不是诸如管理和操作云上或内部的服务器之类的世俗任务。无服务器计算通过使用云本地服务来实现。

例如，微软 Azure 使用 Azure 函数，AWS 使用 Lambda 函数来部署无服务器应用。无服务器应用程序的部署包括提交一组文件(以`.zip`文件的形式)来运行 ML 应用程序。的。zip 存档通常有一个文件，其中包含要执行的特定函数或方法。使用云服务将 zip 存档上传到云平台，并部署为无服务器应用程序。部署的应用程序充当 API 端点，向服务于 ML 模型的无服务器应用程序提交输入。

使用无服务器应用程序部署 ML 模型有很多优点:不需要安装或升级依赖项，也不需要维护或升级系统。无服务器应用程序可按需自动扩展，整体性能稳定。同步(在单个系列中一个接一个地执行，A->B->C->D)和异步(并行或按优先级执行，而不是按顺序:A->C->D->B 或 A 和 B 一起并行，C 和 D 并行)操作都受无服务器功能支持。然而，也有一些缺点，例如云资源可用性(如 RAM 或磁盘空间)或 GPU 不可用，这些对于运行深度学习或强化学习模型等繁重模型来说可能是至关重要的要求。例如，如果我们部署了一个没有使用无服务器操作的模型，我们可能会遇到资源限制。部署的模型或应用程序不会自动缩放，因此会限制可用的计算能力。如果更多的用户推断模型或应用程序超过限制，我们将击中资源不可用拦截器。在下图中，我们可以看到传统应用程序和无服务器应用程序是如何开发的:

![Figure 6.3: Traditional versus serverless deployments
](img/B16572_06_03.jpg)

图 6.3:传统部署与无服务器部署

要开发无服务器应用程序，开发人员只需关注应用程序的逻辑，而不必担心后端或安全代码，这些由云服务在部署无服务器应用程序时负责。

### 模型流

模型流是服务于模型的一种方法，用于处理流数据。流数据没有开始或结束。每一秒钟都有数以千计的来源产生数据，必须尽快处理和分析这些数据。例如，谷歌搜索结果必须实时处理。模型流是部署 ML 模型的另一种方式。与其他模型服务技术(如 REST APIs 或批处理方法)相比，它有两个主要优势。第一个优势是异步性(一次服务多个请求)。REST API ML 应用程序是健壮的和可伸缩的，但是有同步性的限制(它们基于先到先服务的原则处理来自客户端的请求，这可能导致高延迟和资源利用率。为了应对这种限制，流处理是可用的。它本质上是异步的，因为用户或客户端不必协调或等待系统处理请求。

流处理能够异步处理并随时为用户服务。为此，流处理使用消息代理从用户或客户端接收消息。message 代理允许数据的到来，并随着时间的推移分散处理。如图 5.4 所示，消息代理对传入的请求进行解耦，并促进用户或客户端与服务之间的通信，而无需了解彼此的操作。消息流代理有几个选择，比如 Apace Storm、Apache Kafka、Apache Spark、Apache Flint、Amazon Kinesis 和 StreamSQL。您选择的工具取决于 IT 设置和架构。

![Figure 6.4 – Model streaming process 
](img/B16572_06_04.jpg)

图 6.4–模型流流程

流处理的第二个优势是当在 ML 系统中推断多个模型时。REST APIs 非常适合单模型或双模型处理，但是当需要推断多个模型时，它们会设法产生延迟并使用大量计算资源，此外，它们仅限于同步推断。

在多模型的情况下，流处理是一个很好的选择，因为运行 ML 系统所需的所有模型和工件(代码和文件)可以打包在一起，并部署在流处理引擎上(它在自己的机器集群上运行，并管理分布式数据处理的资源分配)。

例如，让我们看看一个智能电子邮件助理的用例，其任务是自动化客户服务，如图*图 5.4* 所示。为了自动回复以服务于其用户，电子邮件助理系统使用多个模型执行多个预测:

*   预测电子邮件的类别，如垃圾邮件、帐户或续订
*   意图识别
*   情感预测
*   答案/文本生成

部署在 REST API 端点上的这四个模型将产生很高的延迟和维护成本，而流服务是一个很好的替代方案，因为它可以将多个模型打包并作为一个流程提供服务，并以流的形式持续地为用户请求提供服务。因此，在这种情况下，推荐在 REST API 端点上使用流。

## 为我们的解决方案规划基础设施

在本节中，我们绘制了基础设施需求和部署目标，以满足不同的业务需求，如*表 6.3* 所示:

![Table 6.3 – Mapping the infrastructure for ML solutions
](img/Table_6.3.jpg)

表 6.3–映射 ML 解决方案的基础设施

根据您的使用案例，建议选择合适的基础设施和部署目标来服务于 ML 模型，以产生业务或运营影响。

# 动手部署(针对业务问题)

在这一部分，我们将学习如何为我们一直在处理的业务问题部署解决方案。到目前为止，我们已经完成了数据处理，ML 模型训练，序列化模型，并注册到 Azure ML 工作区。在本节中，我们将探索如何在容器和自动伸缩集群上的序列化模型上执行推理。这些部署将让你有一个广泛的了解，并为你未来的任务做好准备。

我们将使用 Python 作为主要编程语言，Docker 和 Kubernetes 用于构建和部署容器。我们将从使用 Azure ML 在 Azure 容器实例上部署 REST API 服务开始。接下来，我们将使用 Azure ML 在使用 Kubernetes(用于容器编排)的自动扩展集群上部署 REST API 服务，最后，我们将使用 MLflow 和开源 ML 框架在 Azure 容器实例上部署；这样，我们将学习如何在云(Azure)上使用多种工具和部署 ML 模型。让我们开始在 **Azure 容器实例** ( **ACI** )上进行部署。

## 在 ACI 上部署模型

要让开始部署，请转到之前在 Azure DevOps 上克隆的的 GitHub 存储库(在 [*第 3 章*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053) 、*代码满足数据*)，访问名为`06_ModelDeployment`的文件夹，并按照`01_Deploy_model_ACI.ipynb`笔记本中的实现步骤操作:

1.  We start by importing the required packages and check for the version of the Azure ML SDK, as shown in the following code:

    ```
    %matplotlib inline
    import numpy as np
    import matplotlib.pyplot as plt
    import azureml.core
    from azureml.core import Workspace
    from azureml.core.model import Model
    # display the core SDK version number
    print("Azure ML SDK Version: ", azureml.core.VERSION)
    ```

    前面的代码将打印 Azure ML SDK 版本(例如，`1.10.0`；您的版本可能有所不同)。

2.  接下来，使用来自 Azure ML SDK 的`workspace`函数，我们连接到 ML 工作区，并从工作区下载所需的序列化文件和之前使用`Model`函数训练的模型。序列化的`scaler`和`model`用于执行推理或预测。`Scaler`将用于将输入数据缩小到与用于模型训练的数据相同的规模，而`model`文件用于对输入数据进行预测:

    ```
    ws = Workspace.from_config() print(ws.name, ws.resource_group, ws.location, sep = '\n') scaler = Model(ws,'scaler').download(exist_ok=True) model = Model(ws,'support-vector-classifier').download(exist_ok=True)
    ```

3.  After the `scaler` and the `model` files are downloaded, the next step is to prepare the `scoring` file. The `scoring` file is used to infer the ML models in the containers deployed with the ML service in the Azure container instance and Kubernetes cluster. The `scoring` script takes input passed by the user and infers the ML model for prediction and then serves the output with the prediction to the user. It contains two primary functions, `init()` and `run()`. We start by importing the required libraries and then define the `init()` and `run()` functions:

    ```
    %%writefile score.py
    import json
    import numpy as np
    import os
    import pickle
    import joblib
    import onnxruntime
    import time
    from azureml.core.model import Model
    ```

    `%%writefile score.py`将这段代码写到一个名为`score.py`的文件中，这个文件随后作为 ML 服务的一部分打包到容器中，用于执行 ML 模型推理。

4.  我们定义了`init()`函数；它下载所需的模型，并将它们反序列化为用于预测的变量:

    ```
    def onnxruntime we can deserialize the support vector classifier model. The InferenceSession() function is used for deserializing and serving the model for inference, and the input_name and label_name variables are loaded from the deserialized model.  
    ```

5.  简而言之，`init()`函数加载文件(`model`和`scaler`)并反序列化和提供做出预测所需的模型和工件文件，这些文件由`run()`函数使用，如下:

    ```
    def run() function takes raw incoming data as the argument, performs ML model inference, and returns the predicted result as the output. When called, the run() function receives the incoming data, which is sanitized and loaded into a variable for scaling. The incoming data is scaled using the scaler loaded previously in the init() function. Next, the model inference step, which is the key step, is performed by inferencing scaled data to the model, as shown previously. The prediction inferred from the model is then returned as the output. This way, the scoring file is written into score.py to be used for deployment.
    ```

6.  接下来，我们将继续在 Azure 容器实例上部署服务的关键部分。为此，我们通过创建一个环境`myenv.yml`来定义一个部署环境，如下面的代码所示。使用`CondaDependencies()`函数，我们提到所有需要安装在 Docker 容器中的`pip`包，这些包将被部署为 ML 服务。触发环境文件

    ```
    from azureml.core.conda_dependencies import CondaDependencies  myenv = CondaDependencies.create(pip_packages=["numpy", "onnxruntime", "joblib", "azureml-core", "azureml-defaults", "scikit-learn==0.20.3"]) with open("myenv.yml","w") as f:     f.write(myenv.serialize_to_string())
    ```

    后，容器内安装`numpy`、`onnxruntime`、`joblib`、`azureml-core`、`azureml-defaults`、`scikit-learn`等包
7.  接下来，使用`InferenceConfig()`函数定义推理配置，调用时将`score.py`和环境文件作为参数。接下来，我们调用`AciWebservice()`函数来初始化`aciconfig`变量中的计算配置(`cpu_cores`和`memory`)，如下:

    ```
    from azureml.core.model import InferenceConfig from azureml.core.environment import Environment myenv = Environment.from_conda_specification(name="myenv", file_path="myenv.yml") inference_config = InferenceConfig(entry_script="score.py", environment=myenv) from azureml.core.webservice import AciWebservice aciconfig = AciWebservice.deploy_configuration(cpu_cores=1,  memory_gb=1,  tags={"data": "weather"},  description='weather-prediction')
    ```

8.  现在我们已经准备好在 ACI 上部署 ML 或 web 服务了。我们将使用`score.py`、环境文件(`myenv.yml`)、`inference_config`和`aci_config`来部署 ML 或 web 服务。我们将需要指向要部署的模型或工件。为此，我们使用`Model()`函数从工作区加载`scaler`和`model`文件，并为部署做好准备:

    ```
    %%time from azureml.core.webservice import Webservice from azureml.core.model import InferenceConfig from azureml.core.environment import Environment from azureml.core import Workspace from azureml.core.model import Model ws = Workspace.from_config() model1 = Model(ws, 'support-vector-classifier') model2 = Model(ws, 'scaler') service = Model.deploy(workspace=ws,                         name='weatherprediction',                         models=[model1, model2],                         inference_config=inference_config,                         deployment_config=aciconfig) service.wait_for_deployment(show_output=True)
    ```

9.  After the models are mounted into variables, `model1` and `model2`, we proceed with deploying them as a web service. We use the `deploy()` function to deploy the mounted models as a web service on the ACI, as shown in the preceding code. This process will take around 8 minutes, so grab your popcorn and enjoy the service being deployed. You will see a message like this:

    ```
    Running..............................................................................
    Succeeded
    ACI service creation operation finished, operation "Succeeded"
    CPU times: user 610 ms, sys: 103 ms, total: 713 ms
    Wall time: 7min 57s
    ```

    恭喜你！您已经使用 MLOps 成功部署了您的第一个 ML 服务。

10.  让我们来看看已部署服务的工作方式和健壮性。检查服务 URL 和 Swagger URL，如下面的代码所示。您可以使用这些 URL 为您选择的输入数据实时执行 ML 模型推断:

    ```
    print(service.scoring_uri) print(service.swagger_uri)
    ```

11.  在 Azure ML 工作区中检查已部署的服务。
12.  现在，我们可以使用 Azure ML SDK `service.run()`函数通过传递一些输入数据来测试服务，如下所示:

    ```
    import json test_sample = json.dumps({'data': [[34.927778, 0.24, 7.3899, 83, 16.1000, 1016.51, 1]]}) test_sample = bytes(test_sample,encoding = 'utf8') prediction = Temperature_C, Humidity, Wind_speed_kmph, Wind_bearing_degrees, Visibility_km, Pressure_millibars, and Current_weather_condition. Encode the input data in UTF-8 for smooth inference. Upon inferring the model using service.run(), the model returns a prediction of 0 or 1. 0 means a clear sky and 1 means it will rain. Using this service, we can make weather predictions at the port of Turku as tasked in the business problem.
    ```

13.  The service we have deployed is a REST API web service that we can infer with an HTTP request as follows:

    ```
    import requests
    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}
    if service.auth_enabled:
        headers['Authorization'] = 'Bearer '+ service.get_keys()[0]
    elif service.token_auth_enabled:
        headers['Authorization'] = 'Bearer '+ service.get_token()[0]
    scoring_uri = service.scoring_uri
    print(scoring_uri)
    response = requests.post(scoring_uri, data=test_sample, headers=headers)
    print(response.status_code)
    print(response.elapsed)
    print(response.json())
    ```

    当通过传递输入数据发出`POST`请求时，服务以`0`或`1`的形式返回模型预测。当您得到这样的预测时，您的服务正在工作，并且足够健壮以满足生产需求。

    接下来，我们将在自动扩展集群上部署服务；这是生产场景的理想选择，因为部署的服务可以自动伸缩并满足用户需求。

## 在 Azure Kubernetes 服务(AKS)上部署模型

为了让开始部署，在 [*第 3 章*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053) ，*代码满足数据*中，进入`06_ModelDeployment`文件夹，并遵循`02_Deploy_model_AKS.ipynb`笔记本中的实现步骤:

1.  正如我们在上一节中所做的，首先从`azureml.core`导入所需的包，如`matplotlib`、`numpy`和`azureml.core`，以及所需的函数，如`Workspace`和`Model`，如下面的代码块所示:

    ```
    %matplotlib inline import numpy as np import matplotlib.pyplot as plt   import azureml.core from azureml.core import Workspace from azureml.core.model import Model # display the core SDK version number print("Azure ML SDK Version: ", azureml.core.VERSION)
    ```

2.  打印 Azure ML SDK 的版本并检查版本(例如，它将打印`1.10.0`；您的版本可能有所不同)。使用配置文件和`Workspace`函数连接到您的工作区，如下面的代码块所示:

    ```
    ws = Workspace.from_config() print(ws.name, ws.resource_group, ws.location, sep = '\n') scaler = Model(ws,'scaler').download(exist_ok=True) model = Model(ws,'support-vector-classifier').download(exist_ok=True)
    ```

3.  像之前一样下载`model`和`scaler`文件。下载完`model`和`scaler`文件后，下一步是准备`scoring`文件，用于推断部署了 ML 服务的容器中的 ML 模型。`scoring`脚本接受用户传递的输入，推断 ML 模型进行预测，然后将带有预测的输出提供给用户。我们将从导入所需的库开始，如下面的代码块所示:

    ```
    %%writefile score.py import json import numpy as np import os import pickle import joblib import onnxruntime import time from azureml.core.model import Model
    ```

4.  由于我们之前为 ACI 部署制作了`score.py`，我们将使用相同的文件。它包含两个主要功能，`init()`和`run()`。我们定义了`init()`函数；它下载所需的模型，并将它们反序列化为用于预测的变量:

    ```
    def init():     global model, scaler, input_name, label_name     scaler_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'scaler/2/scaler.pkl')     # deserialize the model file back into a sklearn model     scaler = joblib.load(scaler_path)          model_onnx = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'support-vector-classifier/2/svc.onnx')     model = onnxruntime.InferenceSession(model_onnx, None)     input_name = model.get_inputs()[0].name     label_name = model.get_outputs()[0].name
    ```

5.  As we did in the previous section on ACI deployment, by using `onnxruntime` package functions we can deserialize the support vector classifier model.The `InferenceSession()` function is used to deserialize and serve the model for inference, and the `input_name` and `label_name` variables are loaded from the deserialized model. In a nutshell, the `init()` function loads files (`model` and `scaler`), and deserializes and serves the model and artifact files needed for making predictions that are used by the `run()` function:

    ```
    def run(raw_data):
                    try: 
                        data = np.array(json.loads(raw_data)['data']).astype('float32')
                        data = scaler.fit_transform(data.reshape(1, 7))
                        # make prediction
                        model_prediction = model.run([label_name], {input_name: data.astype(np.float32)})[0]
                        # you can return any data type as long as it is JSON-serializable

                    except Exception as e:   
                        model_prediction = 'error'

                    return model_prediction
    ```

    对于 AKS 部署，我们将使用之前在*部分中使用的相同的`run()`函数在 ACI* 上部署模型。这样，我们就可以在 AKS 上部署服务了。

6.  接下来，我们将进入在`CondaDependencies()`功能上部署服务的关键部分。我们将提到所有需要安装在 Docker 容器中的`pip`和`conda`包，这些包将作为 ML 服务部署。触发`environment`文件后，`numpy`、`onnxruntime`、`joblib`、`azureml-core`、`azureml-defaults`、`scikit-learn`等包被安装在容器内。接下来，在没有任何认证的情况下，使用微软容器注册中心中公开可用的容器。这个容器将安装您的环境，并将被配置为部署到您的目标 AKS:

    ```
    from azureml.core import Environment from azureml.core.conda_dependencies import CondaDependencies  conda_deps = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.19.1','scipy'], pip_packages=["numpy", "onnxruntime", "joblib", "azureml-core", "azureml-defaults", "scikit-learn==0.20.3"]) myenv = Environment(name='myenv') myenv.python.conda_dependencies = conda_deps # use an image available in public Container Registry without authentication myenv.docker.base_image = "mcr.microsoft.com/azureml/o16n-sample-user-base/ubuntu-miniconda"
    ```

7.  现在，使用`InferenceConfig()`函数定义推理配置，该函数在被调用时将`score.py`和环境变量作为参数:

    ```
    from azureml.core.model import InferenceConfig inf_config = InferenceConfig(entry_script='score.py', environment=myenv)
    ```

8.  现在我们已经准备好在 Azure Kubernetes 服务(自动扩展集群)上部署 ML 或 web 服务。为此，我们需要创建一个 AKS 集群，并将其附加到 Azure ML 工作区。为您的集群选择一个名称，并使用`ComputeTarget()`功能检查它是否存在。否则，将使用`ComputeTarget.create()`功能创建或配置集群。它采用一个 workspace 对象，`ws`；服务名称；和创建集群的供应配置。我们使用配置配置的默认参数来创建默认集群:

    ```
    %%time from azureml.core.compute import ComputeTarget from azureml.core.compute_target import ComputeTargetException from azureml.core.compute import AksCompute, ComputeTarget # Choose a name for your AKS cluster aks_name = 'port-aks'  # Verify that cluster does not exist already try:     aks_target = aks_name = port-aks) already exists, a new cluster will not be created. Rather, the existing cluster (named port-aks here) will be attached to the workspace for further deployments.
    ```

9.  接下来，我们继续在 Kubernetes 集群中部署 ML 服务的关键任务。为了进行部署，我们需要一些先决条件，比如挂载要部署的模型。我们使用`Model()`函数挂载模型，从工作区加载`scaler`和`model`文件，并让它们准备好进行部署，如下面代码中的所示:

    ```
    from azureml.core.webservice import Webservice,  AksWebservice # Set the web service configuration (using default here) aks_config = AksWebservice.deploy_configuration() %%time from azureml.core.webservice import Webservice from azureml.core.model import InferenceConfig from azureml.core.environment import Environment from azureml.core import Workspace from azureml.core.model import Model ws = Workspace.from_config() model1 = Model(ws, 'support-vector-classifier') model2 = Model(ws, 'scaler')
    ```

10.  Now we are all set to deploy the service on AKS. We deploy the service with the help of the `Model.deploy()` function from the Azure ML SDK, which takes the workspace object, `ws`; `service_name`; `models`; `inference_config`; `deployment_config`; and `deployment_target` as arguments upon being called:

    ```
    %%time
    aks_service_name ='weatherpred-aks'
    aks_service = Model.deploy (workspace=ws,
                               name=aks_service_name,
                               models=[model1, model2],
                               inference_config=inf_config,
                               deployment_config=aks_config,
                               deployment_target=aks_target)
    aks_service.wait_for_deployment(show_output = True)
    print(aks_service.state)
    ```

    部署服务大约需要10 分钟。部署 ML 服务后，您将收到如下消息:

    ```
    Running........................ Succeeded AKS service creation operation finished, operation "Succeeded"
    ```

    恭喜你！现在您已经在 AKS 上部署了一个 ML 服务。让我们使用 Azure ML SDK 来测试它。

11.  我们使用`service.run()`函数将数据传递给服务并获得预测，如下所示:

    ```
    import json test_sample = json.dumps({'data': [[34.927778, 0.24, 7.3899, 83, 16.1000, 1016.51, 1]]}) test_sample = bytes(test_sample,encoding = 'utf8') prediction = service.run(input_data=test_sample)
    ```

12.  The deployed service is a REST API web service that can be accessed with an HTTP request as follows:

    ```
    import requests
    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}
    if service.auth_enabled:
        headers['Authorization'] = 'Bearer '+ service.get_keys()[0]
    elif service.token_auth_enabled:
        headers['Authorization'] = 'Bearer '+ service.get_token()[0]
    scoring_uri = service.scoring_uri
    print(scoring_uri)
    response = requests.post(scoring_uri, data=test_sample, headers=headers)
    print(response.status_code)
    print(response.elapsed)
    print(response.json())
    ```

    当通过传递输入数据发出`POST`请求时，服务以`0`或`1`的形式返回模型预测。当您得到这样的预测时，您的服务正在工作，并且健壮地服务于生产需求。该服务根据用户的请求流量从`0`扩展到所需数量的容器副本。

## 使用 MLflow 部署服务

最后，让我们使用 MLflow 在部署目标(ACI)上部署一个 ML 服务，获得使用开源框架的实践经验。要开始，先去之前在 Azure DevOps 上克隆的 Git 库(在 [*第三章*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053) ，*代码遇见数据*)，访问名为`06_ModelDeployment`的文件夹，按照`02_Deploy_model_MLflow.ipynb`笔记本中的实现步骤。在实现之前，建议阅读本文档以理解`mlflow.azureml`SDK:[https://docs . Microsoft . com/en-us/azure/machine-learning/how-to-use-ml flow # deploy-and-register-ml flow-models](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models)背后的概念。

1.  我们首先导入所需的包，并检查 Azure ML SDK 的版本，如下面的代码块所示:

    ```
    import numpy as np import mlflow.azureml  import azureml.core # display the core SDK version number print("Azure ML SDK Version: ", azureml.core.VERSION)
    ```

2.  接下来，使用来自 Azure ML SDK 的`workspace`函数，我们连接到 ML 工作区，并使用`set_tracking_uri` :

    ```
    from azureml.core import Workspace from azureml.core.model import Model ws = Workspace.from_config() print(ws.name, ws.resource_group, ws.location, sep = '\n') mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())
    ```

    为工作区设置跟踪 URI
3.  现在进入工作区，从`models`或`experiments`部分获取`mlflow`模型的路径，并设置路径:

    ```
    from azureml.core.webservice import AciWebservice, Webservice # Set the model path to the model folder created by your run  model_path = "model path"
    ```

4.  现在我们已经准备好使用`mlflow`和`azureml` SDK 部署到 ACI。使用`deploy_configuration`功能配置 ACI 部署目标，并使用`mlflow.azureml.deploy`功能部署到 ACI。`deploy`函数被调用时以`model_uri`、`workspace`、`model_name`、`service_name`、`deployment_config`和自定义标签为参数:

    ```
    # Configure  aci_config = AciWebservice.deploy_configuration (cpu_cores=1,  memory_gb=1,  tags={'method' : 'mlflow'},  description='weather pred model', location='eastus2') # Deploy on ACI (webservice,model) = mlflow.azureml.deploy(model_uri= 'runs:/{}/{}'.format(run.id, model_path), workspace=ws,  model_name='svc-mlflow', service_name='port-weather-pred', deployment_config=aci_config, tags=None, mlflow_home=None, synchronous=True) webservice.wait_for_deployment(show_output=True)
    ```

在部署成功后，您将得到部署成功的消息。为了更清楚地了解 MLflow 部署，请遵循以下示例:[https://docs . Microsoft . com/en-us/azure/machine-learning/how-to-use-ml flow # deploy-and-register-ml flow-models](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow#deploy-and-register-mlflow-models)。

恭喜你！您已经使用`azureml`和`mlflow`在不同的部署目标上部署了 ML 模型，例如 ACI 和 AKS。

接下来，我们将重点关注使用持续集成和持续部署将 MLOps 的全部功能带到桌面上，以便在生产中拥有一个健壮的动态开发系统。

# 了解持续集成和持续部署的需求

**持续集成** ( **CI** )和**持续部署** ( **CD** )能够持续交付给 ML 服务。目标是维护和版本化用于模型训练的源代码，使触发器能够并行执行必要的工作，构建工件，并发布它们以部署到 ML 服务。一些云供应商支持 DevOps 服务，这些服务可用于监控 ML 服务、生产中的 ML 模型以及与云中其他服务的协调。

使用 CI 和 CD，我们可以实现持续学习，这对于 ML 系统的成功至关重要。没有持续的学习，ML 系统注定会以失败的 **PoC** ( **概念验证**)告终。我们将深入研究 CI/CD 的概念，并实现实际操作的 CI 和 CD 管道，以便在下一章中看到 MLOps 的作用。

# 总结

在这一章中，我们学习了在生产中部署 ML 模型的关键原则。我们探讨了各种部署方法和目标及其需求。为了获得全面的理解和实践经验，我们实施了部署，以了解 ML 模型如何部署在各种部署目标上，如虚拟机、容器和自动扩展集群中。这样，您就可以准备好应对任何类型的部署挑战。

在下一章中，我们将深入研究构建、部署和维护由 CI 和 CD 支持的健壮 ML 服务的秘密。这将发挥 MLOps 的潜力！我们来深入探讨一下。