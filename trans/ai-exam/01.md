

# 一、通过强化学习开始使用下一代人工智能

下一代人工智能迫使我们认识到机器确实会思考。虽然机器不像我们一样思考，但它们的思维过程已经在许多领域证明了它的效率。过去，人们认为人工智能会复制人类的思维过程。只有神经形态计算(见*第 18 章*、*神经形态计算*)仍然坚持这个目标。正如我们将在本章中看到的，大多数人工智能现在已经超越了人类的思维方式。

**马尔可夫决策过程** ( **MDP** )，一种**强化学习** ( **RL** )算法，完美地阐释了机器是如何以自己独特的方式变得智能的。人类根据经验建立决策过程。MDP 是无记忆的。人类使用逻辑和推理来思考问题。MDP 100%的时间都在应用随机决策。人类用语言思考，标记他们感知的一切。MDP 有一种无监督的方法，不使用标签或训练数据。MDP 推动了自动驾驶汽车(SDC)、翻译工具、调度软件等的机器思维过程。这种无记忆的、随机的、无标签的机器思维过程标志着以前人类问题解决方式的历史性变化。

随着这种认识而来的是一个更令人兴奋的事实。例如，基于物联网的人工智能算法和混合解决方案已经开始在战略领域超越人类。虽然人工智能不能在每个领域取代人类，但人工智能结合经典自动化现在占据了关键领域:银行、市场营销、供应链管理、调度和许多其他关键领域。

正如你将看到的，从这一章开始，你可以作为一个适应性思考者在这个新世界中占据核心地位。你可以设计 AI 解决方案并实现它们。没有时间可以浪费了。在这一章中，我们将通过 MDP 快速直接地进入强化学习。

今天，AI 本质上是翻译成源代码的数学，这使得传统开发者很难学习。然而，我们将务实地处理这种方法。

这里的目标不是走捷径。我们正努力将复杂性分解成可理解的部分，并用现实来面对它们。你将从一开始就发现如何应用适应性思考者的过程，这将引导你从一个想法到强化学习的解决方案，并进入下一代人工智能的重心。

# 强化学习概念

AI 在不断进化。经典方法指出:

*   人工智能涵盖所有领域
*   机器学习是人工智能的一个子集，具有聚类、分类、回归和强化学习
*   深度学习是涉及神经网络的机器学习的子集

然而，这些领域经常重叠，很难将神经形态计算，例如，其子符号方法，归入这些类别(见*第 18 章*、*神经形态计算*)。

在这一章中，RL 显然适合机器学习。让我们简单了解一下 MDP 的科学基础，也就是我们将要探索的 RL 算法。需要记住的主要概念如下:

*   **最优运输**:1781 年，加斯帕德蒙赫定义了使用最短和最具成本效益的路径从一个地点到另一个地点的运输优化；比如挖煤，然后用最划算的路径到工厂。这后来被扩展到从 A 点到 b 点的任何形式的路径。
*   **Boltzmann equation and constant**: In the late 19th century, Ludwig Boltzmann changed our vision of the world with his probabilistic distribution of particles beautifully summed up in his entropy formula:

    *S*=*k** log*W*

    *S* 代表所表达的一个系统的熵(能量，无序度)。 *k* 是玻尔兹曼常数， *W* 代表微观状态的数量。我们将在*第 14 章*、*使用受限玻尔兹曼机器(RBM)和主成分分析(PCA)准备聊天机器人的输入*中进一步探讨玻尔兹曼的思想。

*   概率分布更进一步:乔赛亚·威拉德·吉布斯将大量粒子的概率分布更进一步。在那个时候，概率信息理论正在快速发展。在 19 世纪初，安德烈·马尔科夫将概率算法应用于语言和其他领域。一个信息论的现代时代诞生了。
*   **当玻尔兹曼和最优运输相遇** : 2011 年菲尔兹奖获得者塞德里克·维拉尼将玻尔兹曼的方程式带到了另一个高度。维拉尼接着统一了最优运输和玻尔兹曼。塞德里克·维拉尼证明了一些对 19 世纪的数学家来说有些直观但需要证明的东西。

让我们将前面的所有概念具体化为一个真实世界的例子，这个例子将解释为什么使用 MDP 的强化学习是如此创新。

分析下面这杯茶将带你进入下一代人工智能:

![](img/B15438_01_01.png)

图 1.1:考虑一杯茶

你可以用两种不同的方式来看待这杯茶:

1.  **宏观**:你看杯子和内容。你可以看到杯子里茶的体积，当你拿着杯子的时候，你可以感觉到温度。
2.  **微观状态**:但是你能说出茶叶中有多少分子，哪些是热的、温的、冷的，它们的速度和方向吗？不可能吧？

现在，想象一下，这款茶包含 2，000，000，000+个脸书账户，或 1 亿+个亚马逊 Prime 用户，每年有数百万次交付。在这个层次上，我们干脆放弃控制每一个项目的想法。我们研究趋势和概率。

玻尔兹曼提供了一种评估现实世界特征的概率方法。通过最优运输在物流中实现玻尔兹曼意味着温度可以是产品的等级，速度可以与交付的距离相关联，方向可以是我们将在本章中学习的路线。

Markov 获得了微观状态概率描述的成熟成果，并将其应用于他的 MDP。强化学习采用大量的元素(一杯茶中的颗粒、送货地点、社交网络账户)并定义它们可能采取的路径。

当我们根本无法分析我们全球化世界所面临的海量数据的状态和路径时，人类思想的转折点就发生了，这些数据生成的图像、声音、文字和数字超过了传统的软件方法。

考虑到这一点，我们可以开始探索 MDP。

# 如何适应机器思维，成为适应性思考者

强化学习是机器学习的基础之一，它假设通过与环境交互，通过反复试验来学习。这听起来很熟悉，不是吗？这就是我们人类一生所做的——在痛苦中！尝试事物，评估，然后继续；或者试试别的。

在现实生活中，你是你思维过程的代理人。在强化学习中，代理是通过这个试错过程随机计算的函数。机器学习中的这种思维过程功能就是 MDP 代理。这种形式的经验学习有时被称为 Q 学习。

通过三步法掌握 MDP 的理论和实现是先决条件。

这一章将详细介绍将你变成人工智能专家的三个步骤，概括地说:

1.  从描述一个要用真实案例解决的问题开始
2.  然后，建立一个考虑现实生活限制的数学模型
3.  然后，编写源代码或使用云平台解决方案

这是一种让你从一开始就以适应的态度对待任何项目的方法。这表明，通过解释我们如何建立输入，运行算法，以及使用我们代码的结果，人类将永远处于人工智能的中心。让我们考虑这个三步走的过程，并付诸行动。

# 使用三步法克服现实生活中的问题

本章的要点是避免编写永远不会用到的代码。首先，从作为主题专家理解主题开始。然后，用文字和数学来写分析，以确保你的推理反映了主题，最重要的是，程序在现实生活中有意义。最后，在步骤 3 中，只有在对整个项目有把握的情况下，才写代码。

太多的开发人员在开始编写代码时，没有停下来考虑一下代码的结果将如何在现实生活中表现出来。你可能花费数周时间为一个问题开发完美的代码，却发现一个外部因素使你的解决方案变得毫无用处。例如，如果你编写了一个太阳能机器人来清理院子里的积雪，却发现在冬天没有足够的阳光来驱动机器人，那会怎么样呢？

在这一章中，我们将解决 MDP (Q 函数),并通过贝尔曼方程将其应用于强化学习。然而，我们的做法与大多数人略有不同。我们将考虑实际应用，而不仅仅是代码执行。你可以在网上找到大量的源代码和例子。问题是，就像我们的雪地机器人一样，这样的源代码很少考虑现实生活中出现的复杂情况。比方说，你找到一个程序，它能为无人机送货找到最佳路径。不过，有一个问题。它有许多需要克服的限制，因为编写代码时没有考虑到现实生活中的实用性。作为一个适应性思考者，你会问一些问题:

*   如果一个主要城市上空同时有 5000 架无人机会怎么样？如果他们试图直线移动并撞到一起会发生什么？
*   无人机干扰合法吗？城市上空的噪音呢？旅游呢？
*   天气怎么样？天气预报很难做，那么这是怎么安排的呢？
*   如何解决充电站和停车站的使用协调问题？

几分钟后，你将成为一方面比你懂得更多的理论家和另一方面想得到解决方案却得不到的愤怒的经理们关注的焦点。你现实生活中的方法会解决这些问题。要做到这一点，你必须考虑以下三个步骤，从真正参与现实生活的主题开始。

为了成功实现我们的实际方法(包括上一节中概述的三个步骤),有几个先决条件:

*   **做一个主题专家(SME)** :首先，你要做一个 SME。如果一个理论家极客想出一百个TensorFlow函数来解决无人机轨迹问题，你现在知道这将是一段艰难的旅程，现实生活中的参数会限制算法。一个的中小企业了解这个主题，因此能够快速识别某个领域的关键因素。人工智能经常需要找到一个复杂问题的解决方案，即使是某个领域的专家也无法用数学表达。机器学习有时意味着找到人类不知道如何解释的问题的解决方案。涉及复杂网络的深度学习解决了更困难的问题。
*   **拥有足够的数学知识来理解 AI 概念**:一旦你有了适当的自然语言分析，你需要快速建立你的抽象表示。最好的方法是四处看看，找一个日常生活中的例子，并把它做成一个数学模型。数学不是 AI 中的选项，而是先决条件。努力是值得的。然后，你就可以开始写一段扎实的源代码，或者开始实现云平台 ML 解决方案。
*   **了解源代码以及它的潜力和局限性** : MDP 是一个很好的方法，可以让你从三个方面着手，让你适应:用文字详细描述你周围的事物，将其转化为数学表达，然后在你的源代码中实现结果。

记住这些先决条件，让我们看看如何通过遵循我们实用的三步过程成为解决问题的人工智能专家。不出所料，我们将从第一步开始。

## 第一步——描述要解决的问题:用自然语言描述 MDP

任何人工智能问题的第一步是尽你所能去理解你被要求代表的主题。如果是医学学科，不要只看数据；去医院或者研究中心。如果这是一个私人安全应用程序，那么去那些他们需要使用它的地方。如果是针对社交媒体，一定要和很多用户直接对话。要记住的关键概念是，你必须对主题有一种“感觉”，就好像你是真正的“用户”

例如，把它转换成你在日常生活(工作或个人)中知道的东西，你是 SME 中的一员。如果你有驾照，那么你就是一个开车的 SME。你通过认证了。这是一个相当常见的认证，所以让我们在接下来的例子中使用它作为我们的主题。如果你没有驾驶执照或者从来不开车，你可以很容易地想象你在步行而不是开车；你是一个从一个地方到另一个地方的 SME，不管可能涉及什么交通工具。然而，请记住，现实生活中的项目会涉及额外的技术方面，例如每个国家的交通法规，因此我们想象中的 SME 确实有其局限性。

以为例，假设你是一名电子商务业务司机，在你不熟悉的地方递送包裹。你是自动驾驶车辆的操作员。对于来说，你现在是手动驾驶。你有一个 GPS，上面有一张漂亮的彩色地图。你周围的地点用字母 **A** 到 **F** 来表示，如下图的简化地图所示。你目前在 **F** 。你的目标是到达位置 **C** 。你很开心，听着收音机。一切都很顺利，看起来你会准时到达。下图显示了您可以覆盖的位置和路线:

![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_01_01.png](img/B15438_01_02.png)

图 1.2:交付路线图

导航系统的状态指示到达 **C** 的完整路径。它在告诉你，你将要从 **F** 到 **B** 到 **D** ，然后到 **C** 。看起来不错！

为了进一步分析，我们假设:

*   The present state is the letter *s*. *s* is a variable, not an actual state. It can be one of the locations in *L*, the set of locations:

    *L* = { **A** ， **B** ， **C** ， **D** ， **E** ， **F** }

    我们说*当前状态*是因为学习过程中没有顺序。无记忆过程从一个当前状态到另一个当前状态。在本章的例子中，过程从位置 **F** 开始。

*   你的下一个动作是字母 *a* (动作)。此动作 *a* 不是位置 **A** 。此操作的目标是将我们带到图中的下一个可能位置。在这种情况下，只有 **B** 是可能的。 *a* 的目标是把我们从 *s* (现在的状态)带到*s’*(新的状态)。
*   动作 *a* (不是位置 **A** )是去位置 **B** 。你看看你的导航系统；它告诉你没有交通堵塞，从你现在的状态 **F** 到下一个状态 **B** ，只需要几分钟。假设下一个状态 **B** 是字母 **B** 。这个下一个状态 **B** 是*s’*。

在这一点上，你还是相当幸福的，我们可以用下面的事件顺序来概括你的情况:

*s* ， *a* ，*s’*

字母 *s* 就是你现在的状态，你现在的处境。字母 *a* 是你正在决定的行动，也就是去下一个地点；在那里，你会处于另一种状态，*s’*。我们可以说由于动作 *a* ，你会从 *s* 到*s’*。

现在，想象司机不再是你了。你因为某种原因累了。这就是自动驾驶汽车派上用场的时候。你把你的车设置成自动驾驶。现在，你不再开车了；这个系统是。让我们称这个系统为**代理**。在点 **F** 处，你将汽车设置为自动驾驶，让自动驾驶代理接管。

### 观察工作中的 MDP 特工

自动驾驶 AI 现在是掌管车辆。它是 MDP 的代理。现在，它看到了您要求它做的事情，并检查了它的**映射环境**，其中代表了前面图中从 **A** 到 **F** 的所有位置。

与此同时，你有理由担心。代理人会成功吗？你想知道它的策略是否符合你的。你有你的政策，你的思维方式，那就是走最短的路。代理会同意吗？它的机器大脑在想什么？你观察并开始意识到你以前从未注意到的事情。

由于这是你第一次使用这辆车和导航系统，代理是**无记忆**，这是 MDP 的特色。特工对之前发生的事一无所知。它似乎对从这个状态 *s* 到位置 **F** 的计算很满意。它将使用机器的力量运行尽可能多的计算，以达到它的目标。

另一件你正在观察的事情是从 **F** 到 **C** 的总距离，以检查事情是否正常。这意味着代理正在计算从 **F** 到 **C** 的所有状态。

在这种情况下，状态 **F** 就是状态 1，我们可以简化为写*s*1；b 是状态 2，我们可以写成*s*2 来简化；d 是*s*3；而 C 是*s*4。代理正在计算所有这些可能的状态以做出决策。

代理知道，到了 **D** ， **C** 会更好，因为去 C 的奖励会比其他任何地方都高。因为它不能吃一块蛋糕来奖励自己，代理使用数字。我们的代理人是一个真正的数字处理器。当它是错误的，它得到一个可怜的奖励或在这个模型中一无所获。当它正确时，它会得到一个由字母 *R* 表示的奖励，我们将在步骤 2 中遇到。这种行为-价值(回报)转换，通常被称为 Q 函数，是许多强化学习算法的核心。

当我们的代理从一个状态进入另一个状态时，它执行一个*转换*并获得一个奖励。例如，过渡可以是从 **F** 到 **B** ，从状态 1 到状态 2，或者从 *s* [1] 到 *s* [2] 。

你感觉很好，会很准时。你开始理解你的自动驾驶汽车中的机器学习代理是如何思考的。突然，你抬头看到交通堵塞正在形成。位置 **D** 还很远，现在你也不知道从 **D** 到 **C** 还是 **D** 到 **E** 好，以便走另外一条路到 **C** ，涉及的交通更少。你就要看你经纪人怎么想了！

代理考虑到堵车，固执己见，增加奖励以最短的方式到达 **C** 。它的政策是坚持最初的计划。你不同意。你有另一个政策。

你停车。在继续之前，你们双方都必须同意。你有你的观点和政策；代理不同意。在继续之前，你的观点需要**收敛**。**收敛**是确保计算正确的关键，也是评估计算质量的一种方式。

在这一点上，数学表示是表达整个过程的最佳方式，我们将在下面的步骤中描述。

## 步骤 2-建立数学模型:贝尔曼方程和 MDP 的数学表示

数学涉及到你对问题的看法的整个变化。你正在从文字走向功能，这是源代码的支柱。

用数学符号表达问题并不意味着迷失在学术数学中，以至于从来不写一行代码。只需利用数学高效完成工作。跳过数学表示将在人工智能项目的早期阶段快速跟踪一些功能。然而，当所有人工智能项目中出现的真正问题浮出水面时，仅靠源代码来解决它们将被证明几乎是不可能的。这里的目标是获得足够的数学知识，以便在现实生活的公司中实现解决方案。

有必要通过寻找我们周围熟悉的东西来思考一个问题，例如本章前面提到的路线模型。如前所述用一些抽象的字母和符号写下来，用 *a* 表示一个动作， *s* 表示一种状态，这是一件好事。一旦你理解了这个问题，并清楚地表达出来，你就可以更进一步。

现在，数学将有助于通过更短的描述来澄清这种情况。记住了主要思想，是时候把它们转换成方程式了。

### 从 MDP 到贝尔曼方程

在步骤 1 中，代理从 **F** 或状态 1 或 *s* 转到 **B** ，即状态 2 或*s’*。

一项战略推动了这一决定——以 T21 为代表的政策。一个数学表达式包含 MDP 状态转移函数:

*P*a(*s*，*s’*)

*P* 是策略，代理人通过动作 *a* 从 **F** 到 **B** 做出的策略。当从 **F** 到 **B** 时，这个状态转换被命名为**状态转换函数**:

*   *一个*是动作
*   *s* 为状态 1 ( **F** )，而*s’*为状态 2 ( **B** )

奖励(正确或错误)矩阵遵循相同的原则:

*R*a(*s*，*s’*)

这意味着 *R* 是从状态 *s* 到状态*s’*的行为的报酬。从一个状态到另一个状态将是一个随机的过程。潜在地，所有状态可以进入任何其他状态。

示例中矩阵中的每一行代表从 **A** 到 **F** 的一个字母，每一列代表从 **A** 到 **F** 的一个字母。代表了所有可能的状态。`1`值代表图形的节点(顶点)。那些是可能的地点。例如，第 1 行代表字母 **A** 的可能移动，第 2 行代表字母 **B** 的可能移动，第 6 行代表字母 **F** 的可能移动。在第一行， **A** 不能直接转到 **C** ，因此输入一个`0`值。但是，它可以去 **E** ，所以增加了一个`1`值。

有些模型以`-1`开始，表示不可能的选择，比如 **B** 直接到 **C** ，用`0`值定义位置。这个模型从`0`和`1`值开始。有时需要数周时间来设计创建奖励矩阵的函数(参见*第 2 章*、*构建奖励矩阵——设计数据集*)。

我们将要研究的例子输入了一个奖励矩阵，这样程序就可以选择最佳的行动方案。然后，代理将从一个州到另一个州，学习每个可能的起始位置点的最佳轨迹。MDP 的目标是前往 **C** (奖励矩阵中的第 3 行第 3 列)，在下面的 Python 代码中，它的起始值为 100:

```py
# Markov Decision Process (MDP) - The Bellman equations adapted to

# Reinforcement Learning

import numpy as ql

# R is The Reward Matrix for each state

R = ql.matrix([ [0,0,0,0,1,0],

                [0,0,0,1,0,1],

                [0,0,100,1,0,0],

                [0,1,1,0,1,0],

                [1,0,0,1,0,0],

                [0,1,0,0,0,0] ]) 
```

熟悉 Python 的人可能会奇怪为什么我用了`ql`而不是`np`。有些人可能会说“传统”、“主流”、“标准”我的回答是一个问题。有人能在这个快速发展的世界中定义什么是“标准”人工智能吗？对于 MDP，我的观点是使用`ql`作为“Q-learning”的缩写，而不是 NumPy 的“标准”缩写`np`。自然，除了这个 MDP 程序的特殊缩写，我将使用`np`。请记住，打破传统是为了让我们自由探索新的领域。只要确保你的程序运行良好！

这个决策过程有几个关键属性，其中包括:

*   **马尔可夫性质**:过程不考虑过去。这是这个决策过程的无记忆特性，就像你在一辆装有导航系统的汽车里一样。你朝着你的目标前进。
*   **无监督学习**:根据这种无记忆的马尔可夫性质，可以有把握地说，MDP 是非监督学习。监督学习意味着我们将拥有奖励矩阵 *R* 的所有标签，并从中学习。我们会知道 **A** 是什么意思，并使用该属性做出决定。在未来，我们会看着过去。MDP 不考虑这些标签。因此，MDP 使用无监督学习来训练。在不知道过去的状态或它们意味着什么的情况下，必须在每个状态下做出决定。这意味着，例如，汽车在每个位置都是独立的，这由它的每个状态来表示。
*   **随机过程**:在步骤 1 中，当到达状态 **D** 时，控制映射系统的代理和司机没有就去哪里达成一致。一个随机的选择可以通过试错的方式做出，就像扔硬币一样。这将是一个不成功便成仁的过程。代理人将投掷硬币很多次，并测量结果。这正是 MDP 的工作方式，也是代理学习的方式。
*   **强化学习**:利用来自代理环境的反馈，重复试错过程。
*   **马尔可夫链**:以随机的方式从一个状态到另一个状态的过程叫做马尔可夫链。

总而言之，我们有三个工具:

*   *P*[A](*s*，*s’*):从一种状态转移到另一种状态的**策略**， *P* 或
*   *T*[A](*s*，*s’*):A*T*，或随机(random) **转换**，执行该动作的函数
*   *R*[a](*s*，*s’*):a*R*或**奖励**，对于那个动作，可以是负的、空的或正的

*T* 是转移函数，它使代理决定用一个策略从一个点转到另一个点。在这种情况下，它将是随机的。这就是机器能力的用途，也是强化学习通常的实现方式。

#### 随机性

随机性是 MDP 的一个关键属性，将其定义为随机过程。

以下代码描述了**代理**将要做出的选择:

```py
next_action = int(ql.random.choice(PossibleAction,1))

return next_action 
```

代码在每集选择一个新的随机动作(状态)。

#### 贝尔曼方程

贝尔曼方程是编程强化学习之路。

贝尔曼方程完善了 MDP。为了计算一个状态的值，让我们使用 *Q* ，对于 *Q* 动作-奖励(或值)函数。对于一个单独的状态，贝尔曼方程的伪源代码可以表示如下:

![](img/B15438_01_001.png)

然后，源代码将该等式转换为机器表示形式，如以下代码所示:

```py
# The Bellman equation

    Q[current_state, action] = R[current_state, action] +

        gamma * MaxValue 
```

贝尔曼方程的源代码变量如下:

*   *Q* ( *s* ):这是为这个状态计算的值——总奖励。在步骤 1 中，当代理从 **F** 转到 **B** 时，奖励是一个数字，如 50 或 100，以向代理表明这是正确的。
*   R ( *s* ):这是到该点为止的值的总和。这是当时的总回报。
*   这是在提醒我们试错是有代价的。我们在浪费时间、金钱和精力。此外，我们甚至不知道下一步是对还是错，因为我们处于试错模式。**伽玛**就是经常设置为 0.8。那是什么意思？假设你正在考试。你研究了又研究，却不知道结果如何。你可能有 80/100(0.8)的机会清除它。这很痛苦，但这就是生活。伽玛惩罚或学习率使得贝尔曼方程变得现实而有效。
*   max( *s'* ): *s'* 是用*P*[a](*s*， *s'* )可以达到的可能状态之一；max 是该州线上的最高值(奖励矩阵中的位置线)。

至此，你已经完成了三分之二的工作:理解现实生活(过程)并在基础数学中表示出来。您已经建立了描述学习过程的数学模型，并且可以用代码实现该解决方案。现在，您已经准备好编码了！

## 步骤 3——编写源代码:用 Python 实现解决方案

在步骤 1 中，用自然语言描述一个问题,以便能够与专家对话并理解预期的内容。在第二步中，在自然语言和源代码之间建立了一座重要的数学桥梁。第 3 步是软件实现阶段。

当问题出现时——请放心，总是会出现问题——有可能与客户或公司团队一起回到数学桥梁上，如果有必要，甚至进一步回到自然语言过程。

这种方法保证了任何项目的成功。本章中的代码在 Python 3.x 中。它是一个使用 Q 函数的强化学习程序，具有以下奖励矩阵:

```py
import numpy as ql

R = ql.matrix([ [0,0,0,0,1,0],

                [0,0,0,1,0,1],

                [0,0,100,1,0,0],

                [0,1,1,0,1,0],

                [1,0,0,1,0,0],

                [0,1,0,0,0,0] ])

Q = ql.matrix(ql.zeros([6,6]))

gamma = 0.8 
```

`R`是数学分析中描述的奖励矩阵。

`Q`继承了与`R`相同的结构，但是所有的值都被设置为`0`，因为这是一个学习矩阵。它将逐步包含决策过程的结果。`gamma`变量是一个双重提醒，系统正在学习，它的决策每次只有 80%的机会是正确的。如以下代码所示，系统会在此过程中探索可能的操作:

```py
agent_s_state = 1

# The possible "a" actions when the agent is in a given state

def possible_actions(state):

    current_state_row = R[state,]

    possible_act = ql.where(current_state_row >0)[1]

    return possible_act

# Get available actions in the current state

PossibleAction = possible_actions(agent_s_state) 
```

例如，代理从状态 1 开始。你可以从任何地方开始，因为这是一个随机的过程。请注意，该过程只考虑> 0 的值。它们代表可能的行动(决策)。

当前状态通过一个分析过程来寻找可能的动作(下一个可能的状态)。你会注意到，没有传统意义上的算法有很多规则。这是一个纯粹的随机计算，如下面的`random.choice`函数所示:

```py
def ActionChoice(available_actions_range):

    if(sum(PossibleAction)>0):

        next_action = int(ql.random.choice(PossibleAction,1))

    if(sum(PossibleAction)<=0):

        next_action = int(ql.random.choice(5,1))

    return next_action

# Sample next action to be performed

action = ActionChoice(PossibleAction) 
```

现在是包含贝尔曼方程的系统的核心，翻译成下面的源代码:

```py
def reward(current_state, action, gamma):

    Max_State = ql.where(Q[action,] == ql.max(Q[action,]))[1]

    if Max_State.shape[0] > 1:

        Max_State = int(ql.random.choice(Max_State, size = 1))

    else:

        Max_State = int(Max_State)

    MaxValue = Q[action, Max_State]

    # Q function

    Q[current_state, action] = R[current_state, action] +

        gamma * MaxValue

# Rewarding Q matrix

reward(agent_s_state,action,gamma) 
```

您可以看到代理寻找随机选择的下一个可能状态的最大值。

理解这一点的最好方法是在您的 Python 环境中运行程序，并使用中间值。我建议你打开一个电子表格，记下这些值。这会让你清楚地看到整个过程。

最后一部分只是简单地运行学习过程 50，000 次，只是为了确保系统学习到所有要找到的东西。在每次迭代期间，代理将检测其当前状态，选择一个动作过程，并更新 Q 函数矩阵:

```py
for i in range(50000):

    current_state = ql.random.randint(0, int(Q.shape[0]))

    PossibleAction = possible_actions(current_state)

    action = ActionChoice(PossibleAction)

    reward(current_state,action,gamma)

# Displaying Q before the norm of Q phase

print("Q :")

print(Q)

# Norm of Q

print("Normed Q :")

print(Q/ql.max(Q)*100) 
```

这个过程一直持续到学习过程结束。然后，程序将打印`Q`中的结果和标准化结果。赋范结果是将所有值除以找到的值的总和的过程。`print(Q/ql.max(Q)*100)`用`Q`除以`q1.max(Q)*100`得到的规范`Q`。结果显示为标准百分比。

您可以使用`mdp01.py`运行流程。

# 强化学习的教训

无监督的强化机器学习，如 MDP 驱动的贝尔曼方程，正在逐个地点地颠覆传统的决策软件。无记忆强化学习几乎不需要商业规则，因此不需要人类知识来运行。

成为适应性的下一代人工智能思考者包括三个先决条件:努力成为 SME，研究数学模型以像机器一样思考，以及理解你的源代码的潜力和限制。

机器能力和强化学习给我们上了两堂重要的课:

*   **第一课**:通过强化学习的机器学习在很多情况下是可以打败人类智能的。打架没用！技术和解决方案已经存在于战略领域。
*   **第二课**:机器没有情感，但你有。你周围的人也是如此。人类的情感和团队合作是一种重要的资产。成为团队的 SME。学习如何直观地理解他们想说的话，并为他们做出数学表述。你的工作永远不会消失，即使你正在建立不需要太多开发的解决方案，比如 AutoML。AutoML，或自动机器学习，自动化许多任务。AutoML 自动化了数据集管道、超参数等功能。发展受到部分或全部抑制。但你还是要确保整个系统设计得很好。

强化学习表明没有人能像机器一样解决问题。随机搜索的 50，000 次迭代不是人类的选择。使用梯度下降的数值收敛形式可以显著减少经验事件的数量(参见*第 3 章*、*机器智能-评估函数和数值收敛*)。

人类需要更直观，做几个决定，然后看看会发生什么，因为人类无法尝试成千上万种方法来做某件事。强化学习通过在战略领域超越人类的推理能力，标志着人类思维的新时代。

另一方面，强化学习需要数学模型发挥作用。人类擅长数学抽象，为那些强大的机器提供强大的智力燃料。

人类和机器之间的界限已经改变。人类建立数学模型和不断增长的云平台的能力将服务于在线机器学习服务。

找出如何使用我们刚刚研究的强化学习程序的输出，表明人类将如何永远保持在人工智能的中心。

## 如何使用输出

我们研究的强化程序不像传统软件那样包含特定领域的痕迹。该程序包含基于奖励矩阵的随机选择的贝尔曼方程。目标是找到一条到 **C** (第 3 行第 3 列)的路线，这条路线有诱人的奖励(`100`):

```py
# Markov Decision Process (MDP) – The Bellman equations adapted to

# Reinforcement Learning with the Q action-value(reward) matrix

import numpy as ql

# R is The Reward Matrix for each state

R = ql.matrix([ [0,0,0,0,1,0],

                [0,0,0,1,0,1],

                [0,0,100,1,0,0],

                [0,1,1,0,1,0],

                [1,0,0,1,0,0],

                [0,1,0,0,0,0] ]) 
```

该奖励矩阵通过贝尔曼方程并在 Python 中产生一个结果:

```py
Q :

[[ 0\. 0\. 0\. 0\. 258.44 0\. ]

 [ 0\. 0\. 0\. 321.8 0\. 207.752]

 [ 0\. 0\. 500\. 321.8 0\. 0\. ]

 [ 0\. 258.44 401\. 0\. 258.44 0\. ]

 [ 207.752 0\. 0\. 321.8 0\. 0\. ]

 [ 0\. 258.44 0\. 0\. 0\. 0\. ]]

Normed Q :

[[ 0\. 0\. 0\. 0\. 51.688 0\. ]

 [ 0\. 0\. 0\. 64.36 0\. 41.5504]

 [ 0\. 0\. 100\. 64.36 0\. 0\. ]

 [ 0\. 51.688 80.2 0\. 51.688 0\. ]

 [ 41.5504 0\. 0\. 64.36 0\. 0\. ]

 [ 0\. 51.688 0\. 0\. 0\. 0\. ]] 
```

结果包含由强化学习过程产生的每个状态的值，以及一个赋范的`Q`(最大值除以其他值)。

作为 Python 爱好者，我们喜出望外！我们做了一些相当困难的工作，即强化学习。作为数学爱好者，我们兴高采烈。我们知道 MDP 和贝尔曼方程意味着什么。

但是，作为自然语言的思考者，我们的进步很小。任何客户或用户都无法阅读这些数据并理解其含义。此外，我们无法解释我们是如何在机器中实现它们工作的智能版本的。我们没有。

我们几乎不敢说强化学习可以击败公司里的任何人，随机选择 50，000 次，直到出现正确的答案。

此外，我们让程序运行起来，但我们自己却不知道如何处理结果。由于解决方案的矩阵格式，项目顾问无法提供帮助。

成为一个适应性思考者意味着知道如何在项目的所有步骤中做得好。为了解决这个新问题，让我们带着结果回到步骤 1。回到第一步意味着如果你对结果本身或对结果的理解有问题，有必要回到 SME 层面，现实生活中的情况，看看哪里出了问题。

通过在 Python、图形工具或电子表格中格式化结果，结果可以显示如下:

|  | **答** | **B** | **C** | **D** | **E** | **F** |
| **答** | - | - | - | - | 258.44 | - |
| **B** | - | - | - | 321.8 | - | 207.752 |
| **C** | - | - | 500 | 321.8 | - | - |
| **D** | - | 258.44 | 401. | - | 258.44 | - |
| **E** | 207.752 | - | - | 321.8 | - | - |
| **F** | - | 258.44 | - | - | - | - |

现在，我们可以开始阅读解决方案了:

*   选择一个开始状态。以 **F** 为例。
*   **F** 线代表状态。由于在 **B** 列中的最大值是 258.44，我们转到状态 **B** ，第二行。
*   第二行中状态 **B** 的最大值将我们引向第四列中的 **D** 状态。
*   **D** 状态(第四行)的最高最大值将我们引向 **C** 状态。

注意，如果您从 **C** 状态开始，并决定不停留在 **C** 状态，则 **D** 状态变为最大值，这将引导您回到 **C** 。然而，MDP 自然不会这样做。你必须强迫系统去做。

你现在已经获得了一个序列:**F**->**B**->**D**->**C**。通过选择其他出发点，只需对表进行排序，就可以获得其他序列。

一种有用的方式是保持以百分比表示的标准化版本，如下表所示:

|  | **答** | **B** | **C** | **D** | **E** | **F** |
| **答** | - | - | - | - | 51.68% | - |
| **B** | - | - | - | 64.36% | - | 41.55% |
| **C** | - | - | 100% | 64.36% | - | - |
| **D** | - | 51.68% | 80.2% | - | 51.68% | - |
| **E** | 41.55% | - | - | 64.36% | - | - |
| **F** | - | 51.68% | - | - | - | - |

现在到了非常棘手的部分。我们以一次旅行开始了这一章。但是我在结果分析中没有提到。

强化学习的一个重要属性来自于这样一个事实，即我们正在与一个可以应用于任何事情的数学模型一起工作。不需要人为的规则。我们可以将这个程序用于许多其他主题，而无需编写成千上万行代码。

### 可能的使用案例

对于有许多情况，我们可以在不改变任何细节的情况下调整我们的强化学习模型。

#### 案例 1:为司机(不管是不是人)优化交付

本章描述了这种模式。

#### 案例 2:优化仓库流程

同样的奖励矩阵也适用于仓库中从点 **F** 到 **C** 的情况，如下图所示:

![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_01_02.png](img/B15438_01_03.png)

图 1.3:说明仓库流程问题的图表

在这个仓库里，**F**->**B**->**D**->**C**的顺序很有视觉意义。如果有人从点 **F** 到 **C** ，那么这条物理路径不穿墙也是有意义的。

它可以用于视频游戏、工厂或任何形式的布局。

#### 案例 3:自动计划与排程(APS)

通过将系统转换为调度向量，整个场景发生变化。我们已经离开了对字母、面孔和旅行进行物理处理的更舒适的世界。虽然很神奇，但这些应用只是社交媒体的冰山一角。人工智能的真正挑战始于人类思维的抽象宇宙。

每一个单独的公司、个人或系统都需要自动规划和调度(参见*第十二章*、 *AI 和物联网(IoT)* )。本章示例中的六个 **A** 到 **F** 步骤很可能是以给定的未知顺序执行的六个任务，由以下向量 *x* 表示:

![](img/B15438_01_003.png)

奖励矩阵然后反映要执行的向量 *x* 的任务的约束的权重。例如，在一个工厂里，你不能在制造产品之前组装它们。

在这种情况下，获得的序列代表制造过程的时间表。

#### 案例 4 及更多:你的想象力

通过使用物理布局或抽象决策向量、矩阵和张量，您可以在数学强化学习模型中构建一个解决方案世界。自然，接下来的章节将会用许多其他的概念来增强你的工具箱。

在继续之前，你可能想想象一些你可以用 A 到 F 字母来表达某种路径的情况。

为了帮助你进行这些思维实验模拟，打开`mdp02.py`并转到第 97 行，它从下面启用模拟工具的代码开始。`nextc`和`nextci`是简单的变量，用来记住路径的起点和终点。它们被设置为`-1`以避开 0，0 是一个位置。

主要目标是关注“概念代码”的表达地点已经成为你想要的任何概念。a 可能是你的卧室，C 可能是你的厨房。这条路会从你醒来的地方到你吃早餐的地方。a 可能是你的一个想法，F 可能是一个思考过程的结束。路径将从 A(我怎么把这幅画挂在墙上？)到 E(我需要钻一个洞)，经过几个阶段，到 F(我把画挂在墙上)。只要您定义了奖励矩阵、“概念代码”和一个起点，您就可以想象成千上万条这样的路径:

```py
"""# Improving the program by introducing a decision-making process"""

nextc=-1

nextci=-1

conceptcode=["A","B","C","D","E","F"] 
```

此代码获取计算结果，标记结果矩阵，并接受输入，如以下代码片段所示:

```py
origin=int(input(

    "index number origin(A=0,B=1,C=2,D=3,E=4,F=5): ")) 
```

输入只接受标签数字代码:`A=0`、`B=1` … `F=5`。然后，该函数对结果进行经典计算，以找到最佳路径。让我们举个例子。

当系统提示您输入起点时，输入`5`，例如，如下所示:

```py
index number origin(A=0,B=1,C=2,D=3,E=4,F=5): 5 
```

然后，程序将根据 MDP 过程的输出生成最佳路径，如以下输出所示:

```py
Concept Path

-> F

-> B

-> D

-> C 
```

尝试多种场景和可能性。想象一下你可以将它应用于:

*   An e-commerce website flow (visit, cart, checkout, purchase) imagining that a user visits the site and then resumes a session at a later time. You can use the same reward matrix and "concept code" explored in this chapter. For example, a visitor visits a web page at 10 a.m., starting at point A of your website. Satisfied with a product, the visitor puts the product in a cart, which is point E of your website. Then, the visitor leaves the site before going to the purchase page, which is C. D is the critical point. Why didn't the visitor purchase the product? What's going on?

    你可以决定在 24 小时后自动发送一封电子邮件说:“在接下来的 48 小时内，所有的购买都有 10%的折扣。”这样你就把目标对准了所有卡在 D 的访客，把他们推向 c。

*   句子中可能出现的单词序列(主语、动词、宾语)。预测字母和单词是 100 多年前安德烈·马尔科夫的第一个应用之一！你可以想象 B 是字母表中的字母“a”。如果 D 是“t”，那么它比 F 是“o”的可能性大得多，而在英语中这种可能性较小。如果构建了一个 MDP 回报矩阵，例如 B 导致 D 或 F，那么 B 可以到 D 或 F。因此有两种可能性，D 或 F。例如，Andrey Markov 会假设 B 是代表字母“a”的变量，D 是代表字母“t”的变量，F 是代表字母“o”的变量。在仔细研究了语言的结构之后，他会发现在英语中，字母“a”后面更可能是“t ”,而不是“o”。如果你观察英语，你会发现“a-t”序列比“a-o”序列更有可能。在马尔可夫决策过程中，“a-t”序列有较高的概率，而“a-o”序列有较低的概率。如果回到变量，B-D 序列将比 B-F 序列更有可能出现。
*   你能找到的任何符合工作模式的东西都很棒！

## 机器学习与传统应用

基于随机过程的强化学习将超越传统方法。在过去，我们会坐下来倾听未来的用户，了解他们的思维方式。

然后，我们会回到键盘前，试着模仿人类的思维方式。那些日子已经过去了。我们需要适当的数据集和 ML/DL 方程来向前提升。应用数学将强化学习带到了一个新的高度。在我看来，传统软件很快就会被放在计算机科学博物馆里。我们面临的海量数据的复杂性将在某个时候需要人工智能。

一个人工自适应思考者通过转化为机器表示的应用数学来看待世界。

以不同的方式使用本章提供的 Python 源代码示例。运行它并尝试更改一些参数，看看会发生什么。也可以考虑迭代的次数。将数字从 50，000 降低到您认为最合适的值。稍微改变一下奖励矩阵，看看会发生什么。设计你的奖励矩阵轨迹。这可以是一个行程或决策过程。

# 摘要

目前，人工智能主要是应用数学的一个分支，而不是神经科学。你必须掌握线性代数和概率的基础知识。对于习惯直觉创造力的开发人员来说，这是一项艰巨的任务。有了这些知识，你会发现人类无法与拥有 CPU 和数学功能的机器竞争。你也会明白，机器，和你身边的炒作相反，是没有情感的；虽然我们可以在聊天机器人中把它们表现到一个可怕的地步(见*第十六章，改善聊天机器人的情商缺陷*)。

多维方法是 AI/ML/DL 项目的先决条件。首先，谈论和撰写项目，然后进行数学表示，最后进行软件制作(建立现有平台或编写代码)。在现实生活中，人工智能解决方案并不像一些炒作让我们相信的那样，只是在公司中自发增长。你需要和团队交流，和他们一起工作。这部分是一个项目真正令人满意的方面——首先想象它，然后和一群现实生活中的人一起实现它。

MDP，一个由贝尔曼方程增强的随机行为奖励(价值)系统，将为许多人工智能问题提供有效的解决方案。这些数学工具非常适合企业环境。

使用 Q 动作值函数的强化学习是无记忆的(没有过去)和无监督的(数据没有被标记或分类)。MDP 为解决现实生活中的问题提供了无尽的途径，而无需花费数小时去发明让系统运行的规则。

现在你已经处于谷歌 DeepMind 方法的核心，是时候进入*第二章*、*构建奖励矩阵——设计你的数据集*，并发现如何通过解释和源代码首先创建奖励矩阵。

# 问题

问题的答案在*附录 B* 中，有进一步的解释:

1.  强化学习是无记忆的吗？(是|否)
2.  强化学习使用随机(random)函数吗？(是|否)
3.  MDP 是建立在规则基础上的吗？(是|否)
4.  Q 函数是基于 MDP 的吗？(是|否)
5.  数学对 AI 来说是必不可少的吗？(是|否)
6.  本章中的贝尔曼-MDP 过程能适用于许多问题吗？(是|否)
7.  机器学习程序是不是不可能自己创造另一个程序？(是|否)
8.  顾问是否需要在强化学习计划中输入业务规则？(是|否)
9.  强化学习是有监督的还是无监督的？(有人监督|无人监督)
10.  Q-learning 没有奖励矩阵能运行吗？(是|否)

# 进一步阅读

*   安德烈·马尔科夫:[https://www . Britannica . com/传记/安德烈-安德烈耶维奇-马尔科夫](https://www.britannica.com/biography/Andrey-Andreyevich-Markov)
*   马尔可夫过程:[https://www.britannica.com/science/Markov-process](https://www.britannica.com/science/Markov-process)