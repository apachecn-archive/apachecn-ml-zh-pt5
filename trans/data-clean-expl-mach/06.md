

# *第 4 章*:编码、变换和缩放特征

这本书的前三章着重于数据清理、探索以及如何识别缺失值和异常值。接下来的几章将深入研究特征工程，本章从编码、转换和缩放数据的技术开始，以提高机器学习模型的性能。

通常，机器学习算法需要某种形式的变量编码。此外，我们的模型通常在缩放时表现更好，因此可变性更高的特性不会淹没优化。我们将向您展示当您的要素具有显著不同的范围时，如何使用不同的缩放技术。

具体来说，在本章中，我们将探讨以下主要主题:

*   创建训练数据集并避免数据泄漏
*   识别要删除的不相关或多余的观察值
*   编码分类特征
*   对中基数或高基数的要素进行编码
*   转换功能
*   宁滨特色
*   缩放功能

# 技术要求

在这一章中，我们将广泛地使用`feature-engine`和`category_encoders`包以及`sklearn`库。您可以使用`pip`来安装这些带有`pip install feature-engine`、`pip install`、`category_encoders`和`pip install scikit-learn`的软件包。本章代码使用`sklearn`的 0.24.2 版本、`feature-engine`的 1.1.2 版本和`category_encoders`的 2.2.2 版本。注意`pip install feature-engine`或`pip install feature_engine`都可以工作。

本章的所有代码都可以在 GitHub 的 https://GitHub . com/packt publishing/Data-Cleaning-and-Exploration-with-Machine-Learning/tree/main/4 上找到。% 20 pruningencodingandrescalingfeatures。

# 创建训练数据集，避免数据泄露

对我们模型性能的最大威胁之一是数据泄漏。**数据泄露**每当我们的模型被不在训练数据集中的数据告知时就会发生。有时，我们不经意地用无法从训练数据中单独收集的信息来帮助我们的模型训练，并以对我们的模型准确性的过于乐观的评估而告终。

数据科学家并不真的希望这种情况发生，因此出现了术语*泄漏*。这不是*不要做*那种讨论。我们都知道不要这样做。这更像是一个*我应该采取哪些步骤来避免这个问题？*讨论。事实上，很容易发生数据泄漏，除非我们制定出防止泄漏的程序。

例如，如果我们缺少某个要素的值，我们可以估算这些值在整个数据集内的平均值。然而，为了验证我们的模型，我们随后将数据分成训练和测试数据集。然后，我们可能会意外地将数据泄漏引入我们的训练数据集，因为可能会使用来自整个数据集的信息(即全局平均值)。

为了避免这种情况，数据科学家采用的一种做法是在尽可能接近分析开始的地方建立单独的训练和测试数据集。对于交叉验证等验证技术来说，这可能会变得更复杂一些，但是在接下来的章节中，我们将讨论如何在各种情况下避免数据泄漏。

我们可以使用 scikit-learn 为国家青年数据纵向调查创建训练和测试数据框架。

注意

青年的**全国纵向调查** ( **NLS** )由美国劳工统计局进行。这项调查始于 1997 年的一群人，他们出生于 1980 年至 1985 年，每年都进行跟踪调查，直到 2017 年。在这一部分，我从调查中的数百个数据项目中提取了 89 个变量，包括分数、就业、收入和对政府的态度。可以从存储库中下载 SPSS、Stata 和 SAS 的单独文件。可以从 https://www.nlsinfo.org/investigator/pages/search[下载 NLS 的数据供公众使用。](https://www.nlsinfo.org/investigator/pages/search)

让我们开始创建数据帧:

1.  首先，我们从`sklearn`导入`train_test_split`模块，并加载 NLS 数据:

    ```
    import pandas as pd from sklearn.model_selection import train_test_split nls97 = pd.read_csv("data/nls97b.csv") nls97.set_index("personid", inplace=True)
    ```

2.  然后，我们可以为特征(`X_train`和`X_test`)和目标(`y_train`和`y_test`)创建训练和测试数据框架。在本例中，`wageincome`是目标变量。我们将`test_size`参数设置为`0.3`，以留出 30%的观察值用于测试。请注意，我们将只使用来自 NLS

    ```
    feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97[feature_cols],\   nls97[['wageincome']], test_size=0.3, \   random_state=0)
    ```

    的**学业评估测试** ( **SAT** )和**平均绩点** ( **GPA** )数据
3.  让我们看看用`train_test_split`创建的训练数据帧。我们得到了预期的观察值，6288，这是 NLS 数据帧中 8984:

    ```
    nls97.shape[0] 8984 X_train.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 6288 entries, 574974 to 370933 Data columns (total 6 columns):  #   Column        Non-Null Count     Dtype ---  ------        --------------   -------  0   satverbal      1001 non-null   float64  1   satmath      1001 non-null   float64  2   gpascience     3998 non-null   float64  3   gpaenglish     4078 non-null   float64  4   gpamath        4056 non-null   float64  5   gpaoverall     4223 non-null   float64 dtypes: float64(6) memory usage: 343.9 KB y_train.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 6288 entries, 574974 to 370933 Data columns (total 1 columns):  #   Column        Non-Null Count    Dtype ---  ------        --------------  -------  0   wageincome    3599 non-null   float64 dtypes: float64(1) memory usage: 98.2 KB
    ```

    观察值总数的 70%
4.  此外，让我们看看测试数据帧。我们得到了总数的 30%的个观察值，正如预期的那样:

    ```
    X_test.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 2696 entries, 363170 to 629736 Data columns (total 6 columns):  #   Column        Non-Null Count    Dtype   ---  ------        --------------  -------    0   satverbal      405 non-null   float64  1   satmath        406 non-null   float64  2   gpascience    1686 non-null   float64  3   gpaenglish    1720 non-null   float64  4   gpamath       1710 non-null   float64  5   gpaoverall    1781 non-null   float64 dtypes: float64(6) memory usage: 147.4 KB y_test.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 2696 entries, 363170 to 629736 Data columns (total 1 columns):  #   Column          Non-Null Count    Dtype   ---  ------          --------------  -------    0   wageincome      1492 non-null   float64 dtypes: float64(1) memory usage: 42.1 KB
    ```

在本章的剩余部分，我们将使用 scikit-learn 的`test_train_split`来创建单独的训练和测试数据框架。我们将在第 6 章 、*准备模型评估*中介绍更复杂的构建测试数据集进行验证的策略。

接下来，我们通过移除明显无用的特性来开始我们的特性工程工作。这是因为它们与另一个特征具有相同的数据，或者在响应中没有变化。

# 删除多余或无用的功能

在数据清理和操作的过程中，我们经常会得到不再有意义的数据。也许我们基于单个特征值对数据进行了子集化，并且我们保留了该特征，即使它现在对于所有的观察都具有相同的值。或者，对于我们正在使用的数据子集，两个特征具有相同的值。理想情况下，我们在数据清理过程中捕获这些冗余。然而，如果我们在这个过程中没有发现它们，我们可以使用开源的`feature-engine`包来帮助我们。

此外，可能有一些高度相关的特性，以至于我们不太可能构建一个可以有效使用所有这些特性的模型。`feature-engine`有一个方法`DropCorrelatedFeatures`，当一个特性与另一个特性高度相关时，可以很容易地删除它。

在本节中，我们将使用陆地温度数据以及 NLS 数据。请注意，这里我们将只加载波兰的温度数据。

数据注释

陆地温度数据集包含 2019 年全球 12，000 多个站点的平均温度读数(摄氏度)，尽管大多数站点位于美国。原始数据来自全球历史气候学网络综合数据库。美国国家海洋和大气管理局已通过 https://www . ncdc . NOAA . gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4 向公众开放使用。

让我们开始删除多余和无用的功能:

1.  从`feature_engine`和`sklearn`导入需要的模块，加载波兰的 NLS 数据和温度数据。波兰的数据来自世界各地 12，000 个气象站的更大数据集。我们使用`dropna`删除带有任何缺失数据的观察值:

    ```
    import pandas as pd import feature_engine.selection as fesel from sklearn.model_selection import train_test_split nls97 = pd.read_csv("data/nls97b.csv") nls97.set_index("personid", inplace=True) ltpoland = pd.read_csv("data/ltpoland.csv") ltpoland.set_index("station", inplace=True) ltpoland.dropna(inplace=True)
    ```

2.  接下来，我们创建训练和测试数据框架，正如我们在上一节中所做的:

    ```
    feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97[feature_cols],\   nls97[['wageincome']], test_size=0.3, \   random_state=0)
    ```

3.  我们可以用熊猫`corr`的方法来看看这些特征是如何相互关联的:

    ```
    X_train.corr()           satverbal  satmath  gpascience  gpaenglish \ satverbal     1.000    0.729       0.439      0.444   satmath       0.729    1.000       0.480      0.430 gpascience    0.439    0.480       1.000      0.672 gpaenglish    0.444    0.430       0.672      1.000 gpamath       0.375    0.518       0.606      0.600 gpaoverall    0.421    0.485       0.793      0.844             gpamath   gpaoverall   satverbal     0.375        0.421   satmath       0.518        0.485   gpascience    0.606        0.793   gpaenglish    0.600        0.844   gpamath       1.000        0.750   gpaoverall    0.750        1.000 
    ```

这里，`gpaoverall`与`gpascience`、`gpaenglish`、`gpamath`高度相关。默认情况下，`corr`方法返回皮尔逊系数。当我们可以假设特征之间的线性关系时，这是很好的。然而，当这个假设没有意义时，我们应该考虑要求 Spearman 系数。我们可以通过将`spearman`传递给`corr`的方法参数来做到这一点。

1.  让我们删除与另一个要素的相关性高于 0.75 的要素。我们将 0.75 传递给`DropCorrelatedFeatures`的`threshold`参数，表明我们想要使用皮尔逊系数，并且我们想要通过将变量设置为`None`来评估所有特性。我们对训练数据使用`fit`方法，然后转换训练和测试数据。`info`方法显示，得到的训练数据帧(`X_train_tr`)具有除`gpaoverall`之外的所有特征，T6 分别与`gpascience`和`gpaenglish`具有 0.793 和 0.844 的相关性(`DropCorrelatedFeatures`将从左到右进行评估，因此如果`gpamath`和`gpaoverall`高度相关，它将丢弃`gpaoverall`。如果`gpaoverall`在`gpamath`的左边，它就会掉下`gpamath` ):

    ```
    tr = fesel.DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.75) tr.fit(X_train) X_train_tr = tr.transform(X_train) X_test_tr = tr.transform(X_test) X_train_tr.info() <class 'pandas.core.frame.DataFrame'> Int64Index: 6288 entries, 574974 to 370933 Data columns (total 5 columns):  #   Column       Non-Null Count     Dtype   ---  ------        --------------  -------    0   satverbal     1001 non-null   float64  1   satmath       1001 non-null   float64  2   gpascience    3998 non-null   float64  3   gpaenglish    4078 non-null   float64  4   gpamath       4056 non-null   float64 dtypes: float64(5) memory usage: 294.8 KB
    ```

通常，我们会在决定放弃一个特性之前更仔细地评估它。然而，有时候特性选择是流水线的一部分，我们需要自动化这个过程。这可以用`DropCorrelatedFeatures`来完成，因为所有的`feature_engine`方法都可以放入 scikit-learn 管道中。

1.  现在，让我们根据波兰的土地温度数据创建训练和测试数据框架。对于所有观察值来说，`year`的值是相同的，对于`country`来说也是如此。此外，`latabs`的值与每次观察的`latitude`值相同:

    ```
    feature_cols = ['year','month','latabs',    'latitude','elevation', 'longitude','country'] X_train, X_test, y_train, y_test =  \   train_test_split(ltpoland[feature_cols],\   ltpoland[['temperature']], test_size=0.3, \   random_state=0) X_train.sample(5, random_state=99)          year  month  latabs  latitude  elevation  longitude country station   SIEDLCE   2019   11    52    52    152    22    Poland OKECIE    2019    6    52    52    110    21    Poland BALICE    2019    1    50    50    241    20    Poland BALICE    2019    7    50    50    241    20    Poland BIALYSTOK 2019   11    53    53    151    23    Poland X_train.year.value_counts() 2019    84 Name: year, dtype: int64 X_train.country.value_counts() Poland    84 Name: country, dtype: int64 (X_train.latitude!=X_train.latabs).sum() 0
    ```

2.  让我们删除整个训练数据集中具有相同值的特征。请注意，`year`和`country`在转换后被移除:

    ```
    tr = fesel.DropConstantFeatures() tr.fit(X_train) X_train_tr = tr.transform(X_train) X_test_tr = tr.transform(X_test) X_train_tr.head()            month  latabs  latitude  elevation  longitude station                                                  OKECIE       1      52        52        110         21 LAWICA       8      52        52         94         17 LEBA        11      55        55          2         18 SIEDLCE     10      52        52        152         22 BIALYSTOK   11      53        53        151         23
    ```

3.  让我们删除与其他特性具有相同值的特性。在这种情况下，变换会丢弃`latitude`，它与`latabs` :

    ```
    tr = fesel.DropDuplicateFeatures() tr.fit(X_train_tr) X_train_tr = tr.transform(X_train_tr) X_train_tr.head()            month   latabs   elevation   longitude station                                        OKECIE         1       52         110          21 LAWICA         8       52          94          17 LEBA          11       55           2          18 SIEDLCE       10       52         152          22 BIALYSTOK     11       53         151          23
    ```

    具有相同的值

这修复了 NLS 数据和波兰陆地温度数据中的一些明显问题。我们从具有其他 GPA 特征的数据框架中删除了`gpaoverall`,因为它与这些特征高度相关。此外，我们删除了冗余数据，删除了整个数据帧中具有相同值的要素以及与另一个要素的值重复的要素。

本章的其余部分探讨了一些更为复杂的特性工程挑战:编码、转换、宁滨和缩放。

# 编码分类特征

有几个原因为什么我们可能需要在大多数机器学习算法中使用它们之前对特征进行编码。首先，这些算法通常需要数字数据。第二，当分类特征*用数字*表示时，例如，1 代表女性，2 代表男性，我们需要对这些值进行编码，以便它们被识别为分类特征。第三，该特性实际上可能是有序的，用离散数量的值表示一些有意义的排名。我们的模型需要捕捉这个排名。最后，分类特征可能有大量的值(称为高基数)，我们可能希望我们的编码折叠类别。

我们可以用一键编码来处理具有有限数量的值(比如 15 个或更少)的特性的编码。在这一节中，我们将首先讨论一次性编码，然后讨论顺序编码。在下一节中，我们将研究处理高基数分类特征的策略。

## 一键编码

一键编码一个特征为该特征的每个值创建一个二进制向量。因此，如果一个名为*字母*的特征有三个唯一值， *A* 、 *B* 和 *C* ，一键编码会创建三个二进制向量来表示这些值。第一个二元向量，我们可以称之为 *letter_A，*在*字母*的值为 *A* 时为 1，在 *B* 或 *C* 时为 0。*字母 B* 和*字母 C* 的编码类似。转换后的特征，*字母 _A* 、*字母 _B* 和*字母 _C* ，通常被称为虚拟变量。*图 4.1* 显示了一键编码:

![Figure 4.1 – The one-hot encoding of a categorical feature

](img/Figure_1.1_B17978.jpg)

图 4.1–分类特征的一键编码

来自 NLS 数据的许多特征适合于一次性编码。在下面的代码块中，我们对其中一些特性进行了编码:

1.  让我们从从`feature_engine`导入`OneHotEncoder`模块并加载数据开始。此外，我们从 scikit-learn 中导入了`OrdinalEncoder`模块，因为我们稍后会用到它:

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.preprocessing import OrdinalEncoder from sklearn.model_selection import train_test_split nls97 = pd.read_csv("data/nls97b.csv") nls97.set_index("personid", inplace=True)
    ```

2.  接下来，我们为 NLS 数据创建训练和测试数据框架:

    ```
    feature_cols =['gender','maritalstatus','colenroct99'] nls97demo = nls97[['wageincome'] + feature_cols].dropna() X_demo_train, X_demo_test, y_demo_train, y_demo_test=\   train_test_split(nls97demo[feature_cols],\   nls97demo[['wageincome']], test_size=0.3, \   random_state=0)
    ```

3.  编码的一个选择是 pandas `get_dummies`方法。我们可以用它来表明我们想要转换`gender`和`maritalstatus`特征。`get_dummies`为`gender`和`maritalstatus`的每个值提供一个虚拟变量。例如，`gender`有`Female`和`Male`的值。`get_dummies`创建一个特征`gender_Female`，当`gender`为`Female`时为 1，当`gender`为`Male`时为 0。当`gender`为`Male`时，`gender_Male`为 1，`gender_Female`为 0。这是一种行之有效的编码方法，多年来一直很好地服务于统计学家

我们没有保存由`get_dummies`创建的数据帧，因为在本节的后面，我们将使用不同的技术进行编码。

通常，我们为特性的 *k* 唯一值创建 *k-1* 虚拟变量。因此，如果`gender`在我们的数据中有两个值，我们只需要创建一个虚拟变量。如果我们知道`gender_Female`的值，我们也知道`gender_Male`的值；因此，后一个变量是多余的。同样，如果我们知道其他`maritalstatus`假人的值，我们就知道`maritalstatus_Divorced`的值。以这种方式创建冗余被不恰当地称为**虚拟变量陷阱**。为了避免这个问题，我们从每组中选出一个假人。

注意

对于一些机器学习算法，如线性回归，实际上需要删除一个虚拟变量。在估计线性模型的参数时，矩阵是求逆的。如果我们的模型有一个截距，并且所有的虚拟变量都包括在内，那么矩阵就不能求逆。

1.  我们可以将的`get_dummies` `drop_first`参数设置为`True`以从每组中丢弃第一个假人:

    ```
    pd.get_dummies(X_demo_train, \   columns=['gender','maritalstatus'],   drop_first=True).head(2).T personid                      736081          832734 colenroct99                  1\. Not enrolled  1\. Not enrolled gender_Male                  0                1 maritalstatus_Married        1                0 maritalstatus_Never-married  0                1 maritalstatus_Separated      0                0 maritalstatus_Widowed        0                0
    ```

`get_dummies`的替代方案是`sklearn`或`feature_engine`中的一键编码器。这些一键编码器的优势在于，它们可以很容易地进入机器学习管道，并且可以将从训练数据集收集的信息保存到测试数据集。

1.  让我们使用来自`feature_engine`的`OneHotEncoder`模块来进行编码。我们将`drop_last`设置为`True`，从每组中选出一个假人。我们对训练数据进行编码，然后转换训练和测试数据:

    ```
    ohe = OneHotEncoder(drop_last=True,   variables=['gender','maritalstatus']) ohe.fit(X_demo_train) X_demo_train_ohe = ohe.transform(X_demo_train) X_demo_test_ohe = ohe.transform(X_demo_test) X_demo_train_ohe.filter(regex='gen|mar', axis="columns").head(2).T personid                     736081          832734 gender_Female                1               0 maritalstatus_Married        1               0 maritalstatus_Never-married  0               1 maritalstatus_Divorced       0               0 maritalstatus_Separated      0               0
    ```

这证明了一键编码是为机器学习算法准备名义数据的一种相当直接的方式。但是，如果我们的分类特征是有序的，而不是名义的呢？在这种情况下，我们需要使用序数编码。

## 序数编码

分类特征既可以是名义的，也可以是序数的，如 [*第 1 章*](B17978_01_ePub.xhtml#_idTextAnchor014) 、*检查特征和目标的分布*中所述。性别和婚姻状况都是名义上的。他们的价值观并不意味着秩序。例如，“从未结过婚”并不比“离过婚”的价值高。

但是，当分类特征是有序的时，我们希望编码能够捕获值的排序。例如，如果我们有一个具有低、中和高值的特征，那么一键编码将会丢失这种排序。相反，低、中和高值分别为 1、2 和 3 的变换后的要素会更好。我们可以通过顺序编码来实现这一点。

NLS 数据集中的大学入学要素可被视为序号要素。数值范围从 *1。未注册*至 *3。4 年制大专*。我们应该使用序数编码来为建模做准备。我们接下来将这样做:

1.  我们可以使用`sklearn`的`OrdinalEncoder`模块对 1999 年的大学招生特征进行编码。首先，让我们看看编码前`colenroct99`和的值。值是字符串，但是有一个隐含的顺序:

    ```
    X_demo_train.colenroct99.unique() array(['1\. Not enrolled', '2\. 2-year college ',         '3\. 4-year college'], dtype=object) X_demo_train.head()             gender   maritalstatus   colenroct99 personid                                            736081      Female   Married         1\. Not enrolled 832734      Male     Never-married   1\. Not enrolled 453537      Male     Married         1\. Not enrolled 322059      Female   Divorced        1\. Not enrolled 324323      Female   Married         2\. 2-year college
    ```

2.  我们可以通过将前面的数组传递给`categories`参数来告诉`OrdinalEncoder`模块按照相同的顺序排列值。那么，我们可以用`fit_transform`来改造高校招生领域，`colenroct99`。(`sklearn` `OrdinalEncoder`模块的`fit_transform`方法返回一个 NumPy 数组，所以我们需要使用 pandas DataFrame 方法创建一个 DataFrame。)最后，我们将编码特征与来自训练数据的其他特征结合:

    ```
    oe = OrdinalEncoder(categories=\   [X_demo_train.colenroct99.unique()]) colenr_enc = \   pd.DataFrame(oe.fit_transform(X_demo_train[['colenroct99']]),     columns=['colenroct99'], index=X_demo_train.index) X_demo_train_enc = \   X_demo_train[['gender','maritalstatus']].\   join(colenr_enc)
    ```

3.  让我们来看看对结果数据帧的一些观察。此外，我们应该比较原始学院注册特征的计数和转换后的特征:

    ```
    X_demo_train_enc.head()              gender       maritalstatus    colenroct99 personid                                     736081       Female       Married          0 832734       Male         Never-married    0 453537       Male         Married          0 322059       Female       Divorced         0 324323       Female       Married          1 X_demo_train.colenroct99.value_counts().sort_index() 1\. Not enrolled        3050 2\. 2-year college       142 3\. 4-year college       350 Name: colenroct99, dtype: int64 X_demo_train_enc.colenroct99.value_counts().sort_index() 0       3050 1       142 2       350 Name: colenroct99, dtype: int64
    ```

序数编码用从 0 到 2 的数字替换`colenroct99`的初始值。它现在是一种可由许多机器学习模型消费的形式，我们保留了有意义的排名信息。

注意

顺序编码适用于非线性模型，如决策树。在线性回归模型中可能没有意义，因为这将假设值之间的距离在整个分布中具有相同的意义。在本例中，假设从 0 增加到 1(即从无注册到 2 年注册)与从 1 增加到 2(即从 2 年注册到 4 年注册)是一回事。

一次性编码和顺序编码是设计分类特征的相对简单的方法。当有更多的唯一值时，处理分类特征可能会更复杂。在下一节中，我们将介绍一些处理这些特性的技术。

# 编码中或高基数的分类特征

当我们处理具有许多唯一值(比如 10 个或更多)的分类特征时，为每个值创建一个虚拟变量是不切实际的。当基数较高时，也就是说，有大量的唯一值时，对于某些值的观察可能太少，无法为我们的模型提供太多信息。在极端情况下，对于 ID 变量，每个值只有一个观察值。

有几种方法可以处理中基数或高基数。一种方法是为顶部的 *k* 类别创建虚拟值，并将剩余的值归入*其他*类别。另一种方法是使用特征散列，也称为散列技巧。在本节中，我们将探讨这两种策略。在本例中，我们将使用新冠肺炎数据集:

1.  让我们从新冠肺炎数据创建训练和测试数据框架，并导入`feature_engine`和`category_encoders`库:

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from category_encoders.hashing import HashingEncoder from sklearn.model_selection import train_test_split covidtotals = pd.read_csv("data/covidtotals.csv") feature_cols = ['location','population',     'aged_65_older','diabetes_prevalence','region'] covidtotals = covidtotals[['total_cases'] + feature_cols].dropna() X_train, X_test, y_train, y_test =  \   train_test_split(covidtotals[feature_cols],\   covidtotals[['total_cases']], test_size=0.3,    random_state=0)
    ```

特征区域有 16 个唯一值，其中前 6 个的计数为 10 或更多:

```
X_train.region.value_counts()
Eastern Europe  16
East Asia  12
Western Europe  12
West Africa  11
West Asia  10
East Africa  10
South America  7
South Asia  7
Central Africa  7
Southern Africa  7
Oceania / Aus  6
Caribbean  6
Central Asia  5
North Africa  4
North America  3
Central America  3
Name: region, dtype: int64
```

1.  我们可以再次使用来自`feature_engine`的`OneHotEncoder`模块来编码`region`特征。这一次，我们使用`top_categories`参数来表明我们只想为前六个类别值创建虚拟对象。任何不在前六位的值对于所有的虚拟值都是 0:

    ```
    ohe = OneHotEncoder(top_categories=6, variables=['region']) covidtotals_ohe = ohe.fit_transform(covidtotals) covidtotals_ohe.filter(regex='location|region',   axis="columns").sample(5, random_state=99).T           97      173      92         187        104 Location  Israel  Senegal  Indonesia  Sri Lanka  Kenya region_Eastern Europe  0      0      0      0      0 region_Western Europe  0      0      0      0      0 region_West Africa     0      1      0      0      0 region_East Asia       0      0      1      0      0 region_West Asia       1      0      0      0      0 region_East Africa     0      0      0      0      1
    ```

当一个分类特征有许多唯一值时，另一种方法是使用**特征散列**。

## 特征散列

特征散列法将大量独特的特征值映射到少量虚拟变量。我们可以指定要创建的虚拟变量的数量。然而，碰撞是可能的；也就是说，一些特征值可能映射到相同的虚拟变量组合。随着我们减少所请求的虚拟变量的数量，冲突的数量会增加。

我们可以使用来自`category_encoders`的`HashingEncoder`来做特征散列。我们使用`n_components`来表示我们想要六个虚拟变量(在进行转换之前，我们复制了`region`的特性，这样我们就可以将原始值与新的虚拟变量进行比较):

```
X_train['region2'] = X_train.region
```

```
he = HashingEncoder(cols=['region'], n_components=6)
```

```
X_train_enc = he.fit_transform(X_train)
```

```
X_train_enc.\
```

```
 groupby(['col_0','col_1','col_2','col_3','col_4',
```

```
   'col_5','region2']).\
```

```
    size().reset_index().rename(columns={0:'count'})
```

```
 col_0 col_1 col_2 col_3 col_4 col_5 region2         count
```

```
0   0     0     0     0     0     1   Caribbean       6
```

```
1   0     0     0     0     0     1   Central Africa  7
```

```
2   0     0     0     0     0     1   East Africa     10
```

```
3   0     0     0     0     0     1   North Africa    4
```

```
4   0     0     0     0     1     0   Central America 3
```

```
5   0     0     0     0     1     0   Eastern Europe  16
```

```
6   0     0     0     0     1     0   North America   3
```

```
7   0     0     0     0     1     0   Oceania / Aus   6
```

```
8   0     0     0     0     1     0   Southern Africa 7
```

```
9   0     0     0     0     1     0   West Asia       10
```

```
10  0     0     0     0     1     0   Western Europe  12
```

```
11  0     0     0     1     0     0   Central Asia    5
```

```
12  0     0     0     1     0     0   East Asia       12
```

```
13  0     0     0     1     0     0   South Asia      7
```

```
14  0     0     1     0     0     0   West Africa     11
```

```
15  1     0     0     0     0     0   South America   7
```

不幸的是，这给了我们大量的碰撞。例如，加勒比海、中非、东非和北非都获得相同的虚拟变量值。至少在这种情况下，使用 one-hot 编码并指定类别的数量，就像我们在上一节所做的那样，是一个更好的解决方案。

在前两节中，我们介绍了常见的编码策略:一键编码、顺序编码和特性散列。在我们可以在模型中使用它们之前，几乎所有的分类特征都需要某种编码。然而，有时，我们需要以其他方式改变我们的特征，包括变换、宁滨和缩放。在接下来的三个部分中，我们将考虑为什么我们可能需要以这些方式改变我们的特性，并探索这样做的工具。

# 使用数学变换

有时，我们希望使用不具有高斯分布的特征，而机器学习算法假设我们的特征以这种方式分布。当这种情况发生时，我们要么需要改变使用哪种算法的想法(例如，我们可以选择 KNN 而不是线性回归)，要么转换我们的特征，使它们近似于高斯分布。在本节中，我们将回顾实现后者的几种策略:

1.  我们首先从`feature_engine`导入转换模块，从`sklearn`导入`train_test_split`，从`scipy`导入`stats`。此外，我们使用新冠肺炎数据创建训练和测试数据框架:

    ```
    import pandas as pd from feature_engine import transformation as vt from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt from scipy import stats covidtotals = pd.read_csv("data/covidtotals.csv") feature_cols = ['location','population',     'aged_65_older','diabetes_prevalence','region'] covidtotals = covidtotals[['total_cases'] + feature_cols].dropna() X_train, X_test, y_train, y_test =  \   train_test_split(covidtotals[feature_cols],\   covidtotals[['total_cases']], test_size=0.3, \   random_state=0)
    ```

2.  我们来看看各国病例总数是如何分布的。我们还应该计算偏斜:

    ```
    y_train.total_cases.skew() 6.313169268923333 plt.hist(y_train.total_cases) plt.title("Total COVID Cases  (in millions)") plt.xlabel('Cases') plt.ylabel("Number of Countries") plt.show()
    ```

这会产生以下直方图:

![Figure 4.2 – A histogram of the total number of COVID cases

](img/Figure_1.2_B17978.jpg)

图 4.2–COVID 病例总数的直方图

这说明了案例总数的非常高的偏差。事实上，它看起来是对数正态的，考虑到大量非常低的值和几个非常高的值，这并不奇怪。

注意

有关偏斜和峰度测量的更多信息，请参考第 1 章 、*检查特征和目标的分布*。

1.  让我们尝试一个对数转换。我们需要做的就是让`feature_engine`进行转换，调用`LogTranformer`并传递我们想要转换的特性:

    ```
    tf = vt.LogTransformer(variables = ['total_cases']) y_train_tf = tf.fit_transform(y_train) y_train_tf.total_cases.skew() -1.3872728024141519 plt.hist(y_train_tf.total_cases) plt.title("Total COVID Cases (log transformation)") plt.xlabel('Cases') plt.ylabel("Number of Countries") plt.show()
    ```

这会产生以下直方图:

![Figure 4.3 – A histogram of the total number of COVID cases with log transformation

](img/Figure_1.3_B17978.jpg)

图 4.3–采用对数变换的 COVID 案例总数直方图

实际上，log 变换增加了分布低端的可变性，减少了高端的可变性。这产生了更对称的分布。这是因为对数函数的斜率在较小值时比在较大值时更陡。

1.  这绝对是一个很大的进步，但是现在有一些负面的偏差。也许 Box-Cox 变换会产生更好的结果。让我们试试:

    ```
    tf = vt.BoxCoxTransformer(variables = ['total_cases']) y_train_tf = tf.fit_transform(y_train) y_train_tf.total_cases.skew() 0.07333475786753735 plt.hist(y_train_tf.total_cases) plt.title("Total COVID Cases (Box-Cox transformation)") plt.xlabel('Cases') plt.ylabel("Number of Countries") plt.show()
    ```

这就产生了下面的情节:

![Figure 4.4 – A histogram of the total number of COVID cases with a Box-Cox transformation

](img/Figure_1.4_B17978.jpg)

图 4.4–采用 Box-Cox 变换的 COVID 病例总数直方图

Box-Cox 变换确定-5 和 5 之间的λ值，该值生成最接近正态的分布。它使用以下等式进行转换:

![](img/B17978_04_001.png)

或者

![](img/B17978_04_002.png)

在这里，![](img/B17978_04_003.png)是我们的转换后的特征。只是为了好玩，让我们看看用于转换`total_cases`的λ的值:

```
stats.boxcox(y_train.total_cases)[1]
```

```
0.10435377585681517
```

Box-Cox 变换的λ是`0.104`。作为比较，具有高斯分布的要素的λ将为 1.000，这意味着不需要任何变换。

既然我们的 transformed total cases 特性看起来不错，我们可以以它为目标构建一个模型。此外，我们可以设置管道，以便在进行预测时将值恢复到原始比例。`feature_engine`有许多其他的转换，实现类似于 log 和 Box-Cox 转换。

# 特写宁滨

有时，我们会想把一个连续特征转换成一个分类特征。从分布的最小值到最大值创建等间距间隔的过程被称为宁滨，或者不太友好的术语，离散化。宁滨可以用一个特征解决几个重要问题:偏斜、过度峰度和异常值的存在。

## 等宽等频宁滨

对于 COVID 案例数据，宁滨可能是一个不错的选择。让我们尝试一下(这可能对数据集中的其他变量也有用，包括总死亡人数和人口，但我们现在只处理总病例。`total_cases`是以下代码中的目标变量，因此它是`y_train`数据帧上的一列(唯一的一列):

1.  首先，我们需要从`feature_engine`导入`EqualFrequencyDiscretiser`和`EqualWidthDiscretiser`。此外，我们需要从 COVID 数据中创建训练和测试数据框架:

    ```
    import pandas as pd from feature_engine.discretisation import EqualFrequencyDiscretiser as efd from feature_engine.discretisation import EqualWidthDiscretiser as ewd from sklearn.preprocessing import KBinsDiscretizer from sklearn.model_selection import train_test_split covidtotals = pd.read_csv("data/covidtotals.csv") feature_cols = ['location','population',     'aged_65_older','diabetes_prevalence','region'] covidtotals = covidtotals[['total_cases'] + feature_cols].dropna() X_train, X_test, y_train, y_test =  \   train_test_split(covidtotals[feature_cols],\   covidtotals[['total_cases']], test_size=0.3, random_state=0)
    ```

2.  我们可以使用熊猫`qcut`方法及其`q`参数来创建 10 个频率相对相等的仓:

    ```
    y_train['total_cases_group'] = pd.qcut(y_train.total_cases, q=10, labels=[0,1,2,3,4,5,6,7,8,9]) y_train.total_cases_group.value_counts().sort_index() 0   13 1   13 2   12 3   13 4   12 5   13 6   12 7   13 8   12 9   13 Name: total_cases_group, dtype: int64
    ```

3.  我们可以用`EqualFrequencyDiscretiser`完成同样的事情。首先，我们定义一个函数来运行转换。该函数采用一个`feature_engine`变换以及训练和测试数据帧。它返回转换后的数据帧(不需要定义函数，但在这里有意义，因为我们将在后面重复这些步骤):

    ```
    def runtransform(bt, dftrain, dftest):   bt.fit(dftrain)   train_bins = bt.transform(dftrain)   test_bins = bt.transform(dftest)   return train_bins, test_bins
    ```

4.  接下来，我们创建一个`EqualFrequencyDiscretiser`转换器，并调用我们刚刚创建的`runtransform`函数【T21:

    ```
    y_train.drop(['total_cases_group'], axis=1, inplace=True) bintransformer = efd(q=10, variables=['total_cases']) y_train_bins, y_test_bins = runtransform(bintransformer, y_train, y_test) y_train_bins.total_cases.value_counts().sort_index() 0  13 1  13 2  12 3  13 4  12 5  13 6  12 7  13 8  12 9  13 Name: total_cases, dtype: int64
    ```

这给了我们与`qcut`相同的结果，但它的优势是更容易引入机器学习管道，因为我们使用`feature_engine`来产生它。等频宁滨解决了偏斜和异常值问题。

注意

我们将在本书中详细探索机器学习管道，从 [*第六章*](B17978_06_ePub.xhtml#_idTextAnchor078)*准备模型评估*开始。这里的关键点是，功能引擎转换器可以是包括其他`sklearn`兼容转换器的管道的一部分，甚至是我们自己构建的转换器。

1.  `EqualWidthDiscretiser`作品同样:

    ```
    bintransformer = ewd(bins=10, variables=['total_cases']) y_train_bins, y_test_bins = runtransform(bintransformer, y_train, y_test) y_train_bins.total_cases.value_counts().sort_index() 0  119 1  4 5  1 9  2 Name: total_cases, dtype: int64
    ```

这是一次远不那么成功的转型。在宁滨之前，几乎所有的值都位于数据分布的底部，因此等宽宁滨出现同样的问题也就不足为奇了。结果只有 4 个箱子，尽管我们要求 10 个。

1.  让我们检查一下每个箱的范围。在这里，我们可以看到等宽面元甚至不能构建等宽面元，因为分布顶部的观察值数量很少:

    ```
    pd.options.display.float_format = '{:,.0f}'.format y_train_bins = y_train_bins.\   rename(columns={'total_cases':'total_cases_group'}).\   join(y_train) y_train_bins.groupby("total_cases_group")["total_cases"].agg(['min','max'])   min  max total_cases_group        0  1           3,304,135 1  3,740,567   5,856,682 5  18,909,037  18,909,037 9  30,709,557  33,770,444
    ```

虽然在这种情况下，等宽宁滨是一个糟糕的选择，但有很多时候是有意义的。当数据分布更加均匀或等宽有实际意义时，它会很有用。

## K-意为宁滨

另一个选择是使用*k*-均值聚类来确定仓。 *k* 均值算法随机选择 *k* 个数据点作为聚类中心，然后将其他数据点分配给最近的聚类。计算每个聚类的平均值，并将数据点重新分配给最近的新聚类。重复这个过程，直到找到最佳中心。

当 *k* -means 用于宁滨时，同一簇中的所有数据点将具有相同的序数值:

1.  我们可以使用 scikit-learn 的`KBinsDiscretizer`来创建包含 COVID 案例数据的箱:

    ```
    kbins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans') y_train_bins = \   pd.DataFrame(kbins.fit_transform(y_train),   columns=['total_cases']) y_train_bins.total_cases.value_counts().sort_index() 0  49 1  24 2  23 3  11 4  6 5  6 6  4 7  1 8  1 9  1 Name: total_cases, dtype: int64
    ```

2.  让我们比较一下原始总病例变量和装箱变量的偏斜度和峰度。回想一下，对于具有高斯分布的变量，我们会期望偏斜度为 0，峰度接近 3。入库变量的分布更接近高斯分布:

    ```
    y_train.total_cases.agg(['skew','kurtosis']) skew          6.313 kurtosis     41.553 Name: total_cases, dtype: float64 y_train_bins.total_cases.agg(['skew','kurtosis']) skew            1.439 kurtosis        1.923 Name: total_cases, dtype: float64
    ```

宁滨可以帮助我们解决数据中的偏斜、峰度和异常值。然而，它确实掩盖了特征中的许多变化，并降低了它的解释潜力。通常，某种形式的缩放，如最小-最大或 z 分数，是更好的选择。接下来让我们来看看特征缩放。

# 特征缩放

通常，我们想要在我们的模型中使用的特性在很大程度上是不同的。简而言之，最小值和最大值之间的距离或范围在可能的特征之间变化很大。例如，在新冠肺炎的数据中，总病例特征从 100 万到几乎 3400 万，而 65 岁或以上从 9 到 27(该数字代表人口的百分比)。

具有非常不同尺度的特征会影响许多机器学习算法。例如，KNN 模型通常使用欧氏距离，范围越大的要素对模型的影响越大。缩放可以解决这个问题。

在本节中，我们将介绍两种流行的缩放方法:**最小-最大缩放**和**标准**(或 **z 值**)缩放。最小-最大缩放将每个值替换为其在范围内的位置。更准确地说，会发生以下情况:

![](img/B17978_04_004.png) = ![](img/B17978_04_005.png)

这里，![](img/B17978_04_006.png)是最小-最大值，![](img/B17978_04_007.png)是![](img/B17978_04_009.png)特征的![](img/B17978_04_008.png)观察值，![](img/B17978_04_010.png)和![](img/B17978_04_011.png)是![](img/B17978_04_012.png)特征的最小值和最大值。

标准缩放将特征值归一化到平均值 0 左右。学过本科统计学的人会把它认作 z 分。具体如下:

![](img/B17978_04_013.png)

这里，![](img/B17978_04_014.png)是![](img/B17978_04_009.png)特征的![](img/B17978_04_015.png)观察值，![](img/B17978_04_017.png)是特征![](img/B17978_04_018.png)的平均值，![](img/B17978_04_019.png)是该特征的标准偏差。

我们可以使用 scikit-learn 的预处理模块来获取最小-最大和标准缩放器:

1.  我们从导入预处理模块并从新冠肺炎数据创建训练和测试数据帧开始:

    ```
    import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler covidtotals = pd.read_csv("data/covidtotals.csv") feature_cols = ['population','total_deaths',     'aged_65_older','diabetes_prevalence'] covidtotals = covidtotals[['total_cases'] + feature_cols].dropna() X_train, X_test, y_train, y_test =  \   train_test_split(covidtotals[feature_cols],\   covidtotals[['total_cases']], test_size=0.3, random_state=0)
    ```

2.  现在，我们可以运行最小-最大缩放器。`sklearn`的`fit_transform`方法将返回一个`numpy`数组。我们使用来自训练数据帧的列和索引将其转换成熊猫数据帧。注意现在所有特性的值都在 0 和 1 之间:

    ```
    scaler = MinMaxScaler() X_train_mms = pd.DataFrame(scaler.fit_transform(X_train),   columns=X_train.columns, index=X_train.index) X_train_mms.describe()      population total_deaths aged_65_older diabetes_prevalence count  123.00      123.00        123.00        123.00 mean   0.04        0.04          0.30          0.41 std    0.13        0.14          0.24          0.23 min    0.00        0.00          0.00          0.00 25%    0.00        0.00          0.10          0.26 50%    0.01        0.00          0.22          0.37 75%    0.02        0.02          0.51          0.54 max    1.00        1.00          1.00          1.00
    ```

3.  我们以同样的方式运行标准定标器:

    ```
    scaler = StandardScaler() X_train_ss = pd.DataFrame(scaler.fit_transform(X_train),   columns=X_train.columns, index=X_train.index) X_train_ss.describe()        population  total_deaths  aged_65_older  diabetes_prevalence count  123.00      123.00        123.00       123.00 mean  -0.00       -0.00         -0.00        -0.00 std    1.00        1.00          1.00         1.00 min   -0.29       -0.32         -1.24        -1.84 25%   -0.27       -0.31         -0.84        -0.69 50%   -0.24       -0.29         -0.34        -0.18 75%   -0.11       -0.18          0.87         0.59 max    7.58        6.75          2.93         2.63
    ```

如果我们的数据中有异常值，健壮的伸缩可能是一个好的选择。稳健缩放从变量的每个值中减去中值，并将该值除以四分位间距。所以，每个值如下:

![](img/B17978_04_020.png)

这里，![](img/B17978_04_021.png)是![](img/B17978_04_022.png)特征的值，![](img/B17978_04_023.png)、![](img/B17978_04_024.png)和![](img/B17978_04_025.png)是![](img/B17978_04_026.png)特征的中间值、第三分位数和第一分位数。鲁棒缩放对极值不太敏感，因为它不使用平均值或方差。

1.  我们可以使用 scikit-learn 的`RobustScaler`模块来进行健壮的缩放:

    ```
    scaler = RobustScaler() X_train_rs = pd.DataFrame(   scaler.fit_transform(X_train),   columns=X_train.columns, index=X_train.index) X_train_rs.describe()      population total_deaths aged_65_older diabetes_prevalence count  123.00      123.00      123.00      123.00 mean   1.47        2.22        0.20        0.14 std    6.24        7.65        0.59        0.79 min   -0.35       -0.19       -0.53       -1.30 25%   -0.24       -0.15       -0.30       -0.40 50%    0.00        0.00        0.00        0.00 75%    0.76        0.85        0.70        0.60 max    48.59       53.64       1.91        2.20
    ```

我们在大多数机器学习算法中使用特征缩放。虽然不经常需要，但它会产生明显更好的结果。最小-最大缩放和标准缩放是流行的缩放技术，但有时鲁棒缩放可能是更好的选择。

# 总结

在这一章中，我们讨论了广泛的特征工程技术。我们使用工具来删除冗余或高度相关的功能。我们探讨了最常见的编码方式——一键编码、顺序编码和哈希编码。接下来，我们使用变换来改善我们的特征的分布。最后，我们使用常见的宁滨和缩放方法来处理偏斜、峰度和异常值，并调整范围差异很大的特征。

我们在本章中讨论的一些技术是大多数机器学习模型所需要的。为了正确理解算法，我们几乎总是需要对算法的特征进行编码。例如，大多数算法无法理解*女性*或*男性*的值，或者知道不要将邮政编码视为序数。虽然通常没有必要，但当我们拥有范围差异很大的要素时，缩放通常是一个非常好的主意。当我们使用假设特征为高斯分布的算法时，可能需要某种形式的变换来使我们的特征与该假设一致。

我们现在对我们的特征是如何分布的有了很好的认识，估算了缺失值，并在必要的地方做了一些特征工程。我们现在准备开始可能是模型构建过程中最有趣和最有意义的部分——特征选择。

在下一章中，我们将基于我们到目前为止所做的特征清理、探索和工程工作，检查关键的特征选择任务。