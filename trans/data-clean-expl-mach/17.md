

# *第十三章*:支持向量机分类

支持向量分类模型和 k 近邻模型有一些相似之处。它们既直观又灵活。然而，由于算法的性质，支持向量分类比 k-最近邻具有更好的伸缩性。与逻辑回归不同，它可以相当容易地处理非线性模型。当我们使用支持向量机进行回归时，使用支持向量机进行分类的策略和问题与我们在第 8 章 、*支持向量回归*中讨论的类似。

**支持向量分类** ( **SVC** )的一个关键优势是它赋予我们在不增加特征空间的情况下降低模型复杂度的能力。但它也提供了我们可以调整的多个杠杆来限制过度拟合的可能性。我们可以选择一个线性模型，也可以从几个非线性核中选择。我们可以使用正则化参数，就像我们对逻辑回归所做的那样。通过扩展，我们也可以使用同样的技术来构建多类模型。

我们将在本章中探讨以下主题:

*   SVC 的关键概念
*   线性 SVC 模型
*   非线性 SVM 分类模型
*   多类分类的支持向量机

# 技术要求

在本章中，我们将坚持使用 pandas、NumPy 和 scikit-learn 库。本章中的所有代码都是用 scikit-learn 版本 0.24.2 和 1.0.2 测试的。显示决策边界的代码需要 scikit-learn 版本 1.1.1 或更高版本。

# SVC 的关键概念

我们可以使用**支持向量机** ( **支持向量机**)找到一条线或曲线来按类分离实例。当类可以用一条线来区分时，就说是**线性可分的**。

然而，可能有多种可能的线性分类器，正如我们在*图 13.1* 中看到的。每条线使用两个特征 x1 和 x2 成功地区分了由点和正方形表示的两个类别。关键的区别在于线条如何对新实例进行分类，用透明的矩形表示。使用最接近正方形的线会导致透明矩形被归类为点。使用其他两条线中的任何一条都会将其归类为正方形。

![Figure 13.1 – Three possible linear classifiers

](img/B17978_13_001.jpg)

图 13.1-三种可能的线性分类器

当线性判别式非常接近训练实例时，如图*图 13.2* 中两条线的情况，新实例分类错误的风险更大。我们想要一条线，给我们类之间的最大空白；离每个类的边界数据点最远的一个。那是*图 13.1* 中的中间线，但在*图 13.2* 中可以看得更清楚:

![Figure 13.2 – SVM classification and maximum margin

](img/B17978_13_002.jpg)

图 13.2-SVM 分类和最大利润

粗线分割最大余量，被称为决策边界。每个类别的边界数据点称为支持向量。

我们使用 SVM 来寻找类间最大间隔的线性判别式。它通过找到一个表示可以最大化的边缘的方程来做到这一点，其中边缘是数据点和分离超平面之间的距离。有了两个特征，如图*图 13.2* 所示，超平面只是一条线。然而，这可以推广到具有更多维度的特征空间。

有了*图 13.2* 中的数据点，我们可以使用所谓的**硬边界分类**而不会出现问题；也就是说，我们可以严格要求每个类的所有观察都在决策边界的正确一侧。但是如果我们的数据点看起来像*图 13.3* 中的数据点呢？这里，有一个正方形非常靠近这些点。左边一行是硬边界分类器，给了我们非常小的边界。

![Figure 13.3 – SVMs with hard and soft margins

](img/B17978_13_003.jpg)

图 13.3–具有硬边界和软边界的支持向量机

如果我们使用**软边际分类**，我们得到右边的线。软边界分类放松了所有实例必须被正确分离的约束。正如*图 13.3* 中的数据一样，允许训练数据中少量的错误分类可以给我们更大的余量。我们忽略任性的方块，得到一个由软边缘线表示的决策边界。

约束的放松量由 *C* 超参数决定。 *C* 的值越大，对保证金违规的处罚就越大。毫不奇怪，具有较大 *C* 值的模型更容易过度拟合。*图 13.4* 显示了裕量如何随着 *C* 的值而变化。在 *C = 1* 时，错误分类的惩罚很低，这给了我们比 *C* 为 100 时大得多的余量。然而，即使在 C 为 100 的情况下，仍然会发生一些违反利润的情况。

![Figure 13.4 – Soft margins at different C values

](img/B17978_13_004.jpg)

图 13.4-不同 C 值下的软边界

作为一个实际问题，我们几乎总是建立我们的 SVC 模型。scikit-learn 中 *C* 的默认值为 1。

## 非线性 SVM 和内核诡计

我们还没有完全解决 SVC 的线性可分性问题。为了简单起见，回到涉及两个特征的分类问题是有帮助的。假设一个分类目标的两个特征图看起来像图 13.5 中的插图。目标有两个可能的值，用点和方块表示。x1 和 x2 是数值，具有负值。

![Figure 13.5 – Class labels not linearly separable with two features

](img/B17978_13_005.jpg)

图 13.5–两个特征不可线性分离的分类标签

在这种情况下，我们能做些什么来确定类别之间的差额？通常情况下，可以在更高的维度上识别边缘。在本例中，我们可以使用多项式变换，如图*图 13.6* 所示:

![Figure 13.6 – Using polynomial transformation to establish the margin

](img/B17978_13_006.jpg)

图 13.6–使用多项式变换建立余量

现在有了第三维度，也就是 x1 和 x2 的平方和。圆点都比正方形高。这类似于我们对线性回归使用多项式变换。

这种方法的一个缺点是，我们可能很快就会因为太多的特性而让我们的模型表现不佳。这就是**内核技巧**派上用场的地方。SVC 可以使用内核函数隐式地扩展特征空间，而无需实际创建更多的特征。这是通过创建可用于拟合非线性余量的值的向量来实现的。

虽然这允许我们拟合一个多项式变换，就像图 13.6 中的*所示，但是 SVC 最常用的核函数是**径向基函数** ( **RBF** )。RBF 很受欢迎，因为它比其他常见的内核函数更快，还因为它可以与 gamma 超参数一起使用，以获得额外的灵活性。RBF 核的公式如下:*

![](img/B17978_13_0011.jpg)

这里，![](img/B17978_13_002.png)和![](img/B17978_13_003.png)是数据点。γ，![](img/B17978_13_004.png)，决定了每个点的影响量。使用高 gamma 值时，点必须彼此非常接近才能组合在一起。在非常高的伽马值下，我们开始看到点岛。

当然，伽马值或 C 值的高值部分取决于我们的数据。一个好的方法是在进行大量建模之前，在 gamma 和 *C* 的不同值下创建决策边界的可视化。这将让我们知道在不同的超参数值下，我们是欠拟合还是过拟合。在本章中，我们将绘制不同伽马值和 *C* 的决策边界。

## 利用 SVC 进行多类分类

到目前为止，我们所有关于 SVC 的讨论都集中在二进制分类上。幸运的是，当我们的目标有两个以上的可能值时，应用于 SVM 进行二元分类的所有关键概念也适用于分类。我们通过将多类问题建模为**一对一**或**一对一**问题，将多类问题转化为二分类问题。

![Figure 13.7 – Multiclass SVC options

](img/B17978_13_007.jpg)

图 13.7–多类 SVC 选项

一对一的分类是容易的以一个三类的例子来说明，如*图 13.7* 左侧所示。估计每个类别和每个其他类别之间的判定边界。例如，虚线是点类和正方形类的判定边界。实线是点和椭圆之间的判定边界。

使用一对其余分类，在每个类和不属于该类的那些实例之间构建一个决策边界。图 13.7 右侧对此进行了说明。实线是点和非点的实例(正方形或椭圆形)之间的判定边界。虚线和双线分别是正方形相对于其余部分和椭圆形相对于实例其余部分的判定边界。

我们可以使用一对一或一对一分类来构建线性和非线性 SVC 模型。我们还可以指定 *C* 的值来构建软边距。然而，使用这些技术中的每一种来构建更多的决策边界，要求比用于二进制分类的 SVC 更多的计算资源。如果我们有大量的观察结果、许多特性和不止一对参数要调优，我们可能需要非常好的系统资源来获得及时的结果。

三类示例隐藏了一对一和一对多分类器的一个不同之处。对于三个类，它们使用相同数量的分类器(三个)，但是一对一情况下分类器的数量增加相对较快。分类器的数量将始终等于一对一情况下类值的数量，而一对一情况下，分类器的数量等于:

![](img/B17978_13_0051.jpg)

这里， *S* 是分类器的数量， *N* 是目标的基数(类值的数量)。因此，基数为 4 时，一对一需要 4 个分类器，一对一需要 6 个。

我们将在本章的最后一节探讨多类 SVC 模型，但是让我们从一个相对简单的线性模型开始，看看 SVC 是如何工作的。在对 SVC 模型进行预处理时，需要记住两件事。首先，SVC 对特征的尺度很敏感，所以在拟合我们的模型之前，我们需要解决这个问题。第二，如果我们对 *C* 使用硬边界或高值，异常值可能会对我们的模型产生很大的影响。

# 线性 SVC 模型

我们可以通过使用线性 SVC 模型得到很好的结果。当我们有两个以上的特征时，没有简单的方法来可视化我们的数据是否是线性可分的。我们通常基于超参数调谐来决定线性或非线性。对于本节，我们将假设我们可以通过线性模型和软利润获得良好的性能。

我们将在这一部分中使用国家篮球协会的比赛数据。该数据集包含从 2017/2018 赛季到 2020/2021 赛季的每场 NBA 比赛的统计数据。这包括主队、主队是否获胜、客队、客队和主队的投篮命中率、失误、篮板和两队的助攻，以及许多其他指标。

注意

公众可以在 https://www.kaggle.com/datasets/wyattowalsh/basketball[下载 NBA 比赛数据。该数据集包含从 1946/1947 NBA 赛季开始的比赛数据。它使用`nba_api`从](https://www.kaggle.com/datasets/wyattowalsh/basketball)[nba.com](http://nba.com)获取数据。该 API 可在 https://github.com/swar/nba_api 获得。

让我们建立一个线性 SVC 模型:

1.  我们从加载熟悉的库开始。仅有的新模块是`LinearSVC`和`DecisionBoundaryDisplay`。我们将使用`DecisionBoundaryDisplay`来显示线性模型的边界:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.svm import LinearSVC from scipy.stats import uniform from sklearn.impute import SimpleImputer from sklearn.pipeline import make_pipeline from sklearn.compose import ColumnTransformer from sklearn.feature_selection import RFECV from sklearn.inspection import DecisionBoundaryDisplay from sklearn.model_selection import cross_validate, \   RandomizedSearchCV, RepeatedStratifiedKFold import sklearn.metrics as skmet import seaborn as sns import os import sys sys.path.append(os.getcwd() + "/helperfunctions") from preprocfunc import OutlierTrans
    ```

2.  我们已经准备好加载 NBA 比赛数据。我们只是有一点清洁工作要做。对于我们的目标`WL_HOME`，无论主队是否获胜，都有少量的观察值缺失。我们删除这些观察。我们将`WL_HOME`特征转换为`0`和`1`特征。

这里的阶级不平衡并没有太大的问题。这将为我们节省一些时间:

```
nbagames = pd.read_csv("data/nbagames2017plus.csv", parse_dates=['GAME_DATE'])
nbagames = \
  nbagames.loc[nbagames.WL_HOME.isin(['W','L'])]
nbagames.shape
(4568, 149)
nbagames['WL_HOME'] = \
  np.where(nbagames.WL_HOME=='L',0,1).astype('int')
nbagames.WL_HOME.value_counts(dropna=False)
1    2586
0    1982
Name: WL_HOME, dtype: int64
```

1.  让我们按照数据类型组织我们的特性:

    ```
    num_cols = ['FG_PCT_HOME','FTA_HOME','FG3_PCT_HOME',   'FTM_HOME','FT_PCT_HOME','OREB_HOME','DREB_HOME',   'REB_HOME','AST_HOME','STL_HOME','BLK_HOME',   'TOV_HOME','FG_PCT_AWAY','FTA_AWAY','FG3_PCT_AWAY',   'FT_PCT_AWAY','OREB_AWAY','DREB_AWAY','REB_AWAY',   'AST_AWAY','STL_AWAY','BLK_AWAY','TOV_AWAY'] cat_cols = ['SEASON']
    ```

2.  我们来看一些描述性统计。(为了节省空间，我从打印输出中省略了一些功能。)我们将需要缩放这些特征，因为它们具有非常不同的范围。没有缺失值，但是当我们给极值分配缺失值时，我们会产生一些缺失值:

    ```
    nbagames[['WL_HOME'] + num_cols].agg(['count','min','median','max']).T                   count     min     median  max WL_HOME           4,568     0.00    1.00    1.00 FG_PCT_HOME       4,568 0.27    0.47    0.65 FTA_HOME          4,568     1.00    22.00   64.00 FG3_PCT_HOME      4,568     0.06    0.36 0.84 FTM_HOME          4,568     1.00    17.00   44.00 FT_PCT_HOME       4,568     0.14    0.78    1.00 OREB_HOME         4,568 1.00    10.00   25.00 DREB_HOME         4,568     18.00   35.00   55.00 REB_HOME          4,568     22.00   45.00 70.00 AST_HOME          4,568     10.00   24.00   50.00 ......... FT_PCT_AWAY       4,568     0.26    0.78    1.00 OREB_AWAY 4,568     0.00    10.00   26.00 DREB_AWAY         4,568     18.00   34.00   56.00 REB_AWAY          4,568     22.00 44.00   71.00 AST_AWAY          4,568     9.00    24.00   46.00 STL_AWAY          4,568     0.00    8.00    19.00 BLK_AWAY 4,568     0.00    5.00    15.00 TOV_AWAY          4,568     3.00    14.00   30.00
    ```

3.  我们还应该回顾特性的相关性:

    ```
    corrmatrix = nbagames[['WL_HOME'] + \   num_cols].corr(method="pearson") sns.heatmap(corrmatrix,    xticklabels=corrmatrix.columns,   yticklabels=corrmatrix.columns, cmap="coolwarm") plt.title('Heat Map of Correlation Matrix') plt.tight_layout() plt.show()
    ```

这会产生以下情节:

![Figure 13.8 – Heat map of NBA game statistics correlations

](img/B17978_13_008.jpg)

图 13.8-NBA 比赛统计数据相关性热图

几个特征与目标相关，包括主队的投篮命中率(`FG_PCT_HOME`)和主队的防守篮板(`DREB_HOME`)。

特征之间也有相关性。比如主队的投篮命中率(`FG_PCT_HOME`)和主队的 3 分投篮命中率(`FG3_PCT_HOME`)是正相关的，不奇怪。此外，主队的篮板(`REB_HOME`)和主队的防守篮板(`DREB_HOME`)可能过于密切相关，任何模型都无法理清它们的影响。

1.  接下来，我们创建训练和测试数据框架:

    ```
    X_train, X_test, y_train, y_test =  \   train_test_split(nbagames[num_cols + cat_cols],\   nbagames[['WL_HOME']], test_size=0.2, random_state=0)
    ```

2.  我们需要设置我们的列转换。对于数字列，我们检查异常值并缩放数据。我们对一个分类特征`SEASON`进行一次性编码。我们将在稍后的网格搜索管道中使用这些转换:

    ```
    ohe = OneHotEncoder(drop='first', sparse=False) cattrans = make_pipeline(ohe) standtrans = make_pipeline(OutlierTrans(2),   SimpleImputer(strategy="median"), StandardScaler()) coltrans = ColumnTransformer(   transformers=[     ("cat", cattrans, cat_cols),     ("stand", standtrans, num_cols)   ] )
    ```

3.  在构建我们的模型之前，让我们看看线性 SVC 模型的决策边界。我们将界限建立在与目标相关的两个特征上:主队的投篮命中率(`FG_PCT_HOME`)和主队的防守篮板(`DREB_HOME`)。

我们创建一个函数`dispbound`，它将使用`DecisionBoundaryDisplay`模块来显示边界。scikit-learn 版本 1.1.1 或更高版本提供了此模块。`DecisionBoundaryDisplay`需要一个适合的模型、两个特征和目标值:

```
pipe0 = make_pipeline(OutlierTrans(2),
  SimpleImputer(strategy="median"), StandardScaler())
X_train_enc = pipe0.\
  fit_transform(X_train[['FG_PCT_HOME','DREB_HOME']])
def dispbound(model, X, xvarnames, y, title):
  dispfit = model.fit(X,y)
  disp = DecisionBoundaryDisplay.from_estimator(
    dispfit, X, response_method="predict",
    xlabel=xvarnames[0], ylabel=xvarnames[1],
    alpha=0.5,
  )
  scatter = disp.ax_.scatter(X[:,0], X[:,1],
    c=y, edgecolor="k")

  disp.ax_.set_title(title)
  legend1 = disp.ax_.legend(*scatter.legend_elements(),
    loc="lower left", title="Home Win")
  disp.ax_.add_artist(legend1)
dispbound(LinearSVC(max_iter=1000000,loss='hinge'),
  X_train_enc, ['FG_PCT_HOME','DREB_HOME'],
  y_train.values.ravel(),
  'Linear SVC Decision Boundary')
```

这产生了下面的图:

![Figure 13.9 – Decision boundary for a two-feature linear SVC model

](img/B17978_13_009.jpg)

图 13.9-双特征线性 SVC 模型的决策边界

只有这两个特征，我们得到了一个相当不错的线性边界。这是个好消息，但是让我们做一个更仔细构建的模型。

1.  为了构建我们的模型，我们首先实例化一个线性 SVC 对象，并设置递归特征消除。然后，我们将列转换、特征选择和线性 SVC 添加到管道中，并对其进行拟合:

    ```
    svc = LinearSVC(max_iter=1000000, loss='hinge',    random_state=0) rfecv = RFECV(estimator=svc, cv=5) pipe1 = make_pipeline(coltrans, rfecv, svc) pipe1.fit(X_train, y_train.values.ravel())
    ```

2.  让我们看看从我们的递归特征消除中选择了哪些特征。在一次性编码之后，我们需要首先获得列名。然后我们可以使用`rfecv`对象的`get_support`方法来获取所选择的特性。(如果您使用的是 scikit-learn 版本 1 或更高版本，您将收到关于`get_feature_names`的不推荐警告。您可以使用`get_feature_names_out`来代替，尽管这在 scikit-learn 的早期版本中不起作用。):

    ```
    new_cat_cols = \   pipe1.named_steps['columntransformer'].\   named_transformers_['cat'].\   named_steps['onehotencoder'].\   get_feature_names(cat_cols) new_cols = np.concatenate((new_cat_cols, np.array(num_cols))) sel_cols = new_cols[pipe1['rfecv'].get_support()] np.set_printoptions(linewidth=55) sel_cols array(['SEASON_2018', 'SEASON_2019', 'SEASON_2020',        'FG_PCT_HOME', 'FTA_HOME', 'FG3_PCT_HOME',        'FTM_HOME', 'FT_PCT_HOME', 'OREB_HOME',        'DREB_HOME', 'REB_HOME', 'AST_HOME',        'TOV_HOME', 'FG_PCT_AWAY', 'FTA_AWAY',        'FG3_PCT_AWAY', 'FT_PCT_AWAY', 'OREB_AWAY',        'DREB_AWAY', 'REB_AWAY', 'AST_AWAY',        'BLK_AWAY', 'TOV_AWAY'], dtype=object)
    ```

3.  我们应该看看系数。每个所选列的系数可以通过`linearsvc`对象的`coef_`属性来访问。也许并不奇怪，主队(`FG_PCT_HOME`)和客场(`FG_PCT_AWAY`)的投篮命中率是主队获胜的最重要的正负预测因素。下一个最重要的特征是主客场球队的失误次数:

    ```
    pd.Series(pipe1['linearsvc'].\   coef_[0], index=sel_cols).\   sort_values(ascending=False) FG_PCT_HOME     2.21 TOV_AWAY        1.20 REB_HOME        1.19 FTM_HOME        0.95 FG3_PCT_HOME    0.94 FT_PCT_HOME     0.31 AST_HOME        0.25 OREB_HOME       0.18 DREB_AWAY       0.11 SEASON_2018     0.10 FTA_HOME       -0.05 BLK_AWAY       -0.07 SEASON_2019    -0.11 SEASON_2020    -0.19 AST_AWAY       -0.44 OREB_AWAY      -0.47 DREB_HOME      -0.49 FT_PCT_AWAY    -0.53 REB_AWAY       -0.63 FG3_PCT_AWAY   -0.80 FTA_AWAY       -0.81 TOV_HOME       -1.19 FG_PCT_AWAY    -1.91 dtype: float64
    ```

4.  让我们来看看这些预测。我们的模型很好地预测了主队获胜:

    ```
    pred = pipe1.predict(X_test) print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),   skmet.precision_score(y_test.values.ravel(), pred))) accuracy: 0.93, sensitivity: 0.95, specificity: 0.92, precision: 0.93
    ```

5.  我们应该通过做一些交叉验证来确认这些指标不是侥幸的。我们使用重复的分层 k 次折叠进行验证，表明我们需要 7 次折叠和 10 次迭代。我们得到了与上一步几乎相同的结果:

    ```
    kf = RepeatedStratifiedKFold(n_splits=7,n_repeats=10,\   random_state=0) scores = cross_validate(pipe1, X_train, \   y_train.values.ravel(), \   scoring=['accuracy','precision','recall','f1'], \   cv=kf, n_jobs=-1) print("accuracy: %.2f, precision: %.2f, sensitivity: %.2f, f1: %.2f"  %   (np.mean(scores['test_accuracy']),\   np.mean(scores['test_precision']),\   np.mean(scores['test_recall']),\   np.mean(scores['test_f1']))) accuracy: 0.93, precision: 0.93, sensitivity: 0.95, f1: 0.94
    ```

6.  到目前为止，我们一直使用`1`的默认值`C`。我们可以尝试使用随机网格搜索来确定`C`的更好值:

    ```
    svc_params = {  'linearsvc__C': uniform(loc=0, scale=100) } rs = RandomizedSearchCV(pipe1, svc_params, cv=10,    scoring='accuracy', n_iter=20, random_state=0) rs.fit(X_train, y_train.values.ravel()) rs.best_params_ {'linearsvc__C': 54.88135039273247} rs.best_score_ 0.9315809566584325
    ```

最佳`C`值为 2.02，最佳精度得分为 0.9316。

1.  让我们仔细看看我们进行网格搜索的 20 次中每一次的得分。每个分数是 10 次折叠的平均准确度分数。不管`C`值是多少:

    ```
    results = \   pd.DataFrame(rs.cv_results_['mean_test_score'], \     columns=['meanscore']).\   join(pd.DataFrame(rs.cv_results_['params'])).\   sort_values(['meanscore'], ascending=False) results               meanscore        linearsvc__C 0             0.93             54.88 8             0.93             96.37 18            0.93             77.82 17            0.93             83.26 13            0.93             92.56 12            0.93             56.80 11            0.93             52.89 1             0.93             71.52 10            0.93             79.17 7             0.93             89.18 6             0.93             43.76 5             0.93             64.59 3             0.93             54.49 2             0.93             60.28 19            0.93             87.00 9             0.93             38.34 4             0.93             42.37 14            0.93             7.10 15            0.93             8.71 16            0.93             2.02
    ```

    ，我们实际上得到的分数都差不多
2.  现在让我们来看看一些预测。我们的模型在各方面都做得很好，但并不比最初的模型好多少:

    ```
    pred = rs.predict(X_test) print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),   skmet.precision_score(y_test.values.ravel(), pred))) accuracy: 0.93, sensitivity: 0.95, specificity: 0.92, precision: 0.93
    ```

3.  让我们也看一个混淆矩阵:

    ```
    cm = skmet.confusion_matrix(y_test, pred) cmplot = \   skmet.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Loss', 'Won']) cmplot.plot() cmplot.ax_.set(title='Home Team Win Confusion Matrix',    xlabel='Predicted Value', ylabel='Actual Value')
    ```

这会产生以下情节:

![Figure 13.10 – Confusion matrix for wins by the home team

](img/B17978_13_010.jpg)

图 13.10–主队获胜的混淆矩阵

我们的模型很大程度上正确预测了主队的输赢。调整`C`的值并没有太大的区别，因为不管`C`的值如何，我们都获得了几乎相同的精度。

注意

您可能已经注意到，我们在 NBA 比赛数据中使用准确性指标的频率要高于我们在前面章节中使用的心脏病和机器故障数据。我们更加关注这些数据的敏感性。这有两个原因。首先，由于我们在 [*第 6 章*](B17978_06_ePub.xhtml#_idTextAnchor078) 、*准备模型评估*中详细讨论的原因，当类更接近平衡时，准确性是一个更有说服力的度量。第二，在预测心脏病和机器电源故障时，我们偏向于敏感性，因为在这些领域假阴性的成本高于假阳性。对于预测 NBA 比赛，没有这种偏见。

线性 SVC 模型的一个优点是它们很容易解释。我们能够查看系数，这有助于我们理解模型，并向他人传达我们预测的基础。尽管如此，确认非线性模型并不能得到更好的结果还是很有帮助的。我们将在下一节中介绍这一点。

# 非线性 SVM 分类模型

尽管非线性 SVC 在概念上比线性 SVC 更复杂，但正如我们在本章第一节中看到的，用 scikit-learn 运行非线性模型相对简单。与线性模型的主要区别在于，我们需要进行更多的超参数调整。我们必须为`C`、`gamma`和我们想要使用的内核指定值。

虽然有理论上的理由假设，对于给定的建模挑战，一些超参数值可能比其他值更好，但我们通常凭经验解决这些值，也就是说，通过超参数调整。我们在这一节中尝试使用我们在上一节中使用的相同 NBA 比赛数据:

1.  我们加载在上一节中使用的相同的库。我们还导入了`LogisticRegression`模块。稍后我们将使用特性选择包装器方法:

    ```
    import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from scipy.stats import uniform from sklearn.feature_selection import RFECV from sklearn.impute import SimpleImputer from scipy.stats import randint from sklearn.model_selection import RandomizedSearchCV import sklearn.metrics as skmet import os import sys sys.path.append(os.getcwd() + "/helperfunctions") from preprocfunc import OutlierTrans
    ```

2.  我们导入`nbagames`模块，该模块包含加载和预处理 NBA 比赛数据的代码。这只是我们在上一节中运行的为建模准备数据的代码的副本。这里没有必要重复这些步骤。

我们还导入了在上一节中使用的`dispbound`函数来显示决策边界。我们将该代码复制到当前目录的`helperfunctions`子文件夹中名为`displayfunc.py`的文件中:

```
import nbagames as ng
from displayfunc import dispbound
```

1.  我们使用`nbagames`模块获取训练和测试数据:

    ```
    X_train = ng.X_train X_test = ng.X_test y_train = ng.y_train y_test = ng.y_test
    ```

2.  在构建一个模型之前，让我们来看看几个不同的具有两个特征的内核的决策边界:主队的投篮命中率(`FG_PCT_HOME`)和主队的防守篮板(`DREB_HOME`)。我们从内核`rbf`开始，对`gamma`和`C`使用不同的值:

    ```
    pipe0 = make_pipeline(OutlierTrans(2),   SimpleImputer(strategy="median"),   StandardScaler()) X_train_enc = \   pipe0.fit_transform(X_train[['FG_PCT_HOME',    'DREB_HOME']]) dispbound(SVC(kernel='rbf', gamma=30, C=1),   X_train_enc,['FG_PCT_HOME','DREB_HOME'],   y_train.values.ravel(),   "SVC with rbf kernel-gamma=30, C=1")
    ```

以这几种不同的方式运行会产生以下图形:

![Figure 13.11 – Decision boundaries with the rbf kernel and different gamma and C values

](img/B17978_13_011.jpg)

图 13.11-具有 rbf 核和不同 gamma 和 C 值的决策边界

当`gamma`和`C`的值接近默认值时，我们看到决策边界有所弯曲，以适应损失类中的一些任意点。这些是主队尽管有很高的防守篮板总数还是输了的例子。有了`rbf`内核，其中两个实例现在被正确分类了。也有几个主队投篮命中率高但主队防守篮板低的例子，现在被正确分类了。然而，与上一节的线性模型相比，我们的预测总体上没有太大变化。

但是，如果我们增加`C`或`gamma`的值，情况会发生显著变化。回想一下，较高的`C`值会增加错误分类的惩罚。这导致边界更多地缠绕在实例周围。

将`gamma`增加到`30`会导致大量过度拟合。`gamma`的高值意味着数据点必须彼此非常接近才能分组在一起。这导致决策边界与少量实例紧密相关，有时只有一个实例。

1.  我们还可以展示多项式核的边界。我们将保持默认的`C`值来关注改变度数的效果:

    ```
    dispbound(SVC(kernel='poly', degree=7),   X_train_enc, ['FG_PCT_HOME','DREB_HOME'],   y_train.values.ravel(),   "SVC with polynomial kernel - degree=7")
    ```

以几种不同的方式运行它会产生以下图:

![Figure 13.12 – Decision boundaries with polynomial kernel and different degrees

](img/B17978_13_012.jpg)

图 13.12-具有多项式核和不同次数的决策边界

我们可以看到决策边界在更高程度上有所弯曲，以处理一些不寻常的情况。这里没有太多的过度拟合，但是我们的预测也没有太大的改进。

这至少暗示了当我们构建模型时会发生什么。我们应该尝试一些非线性模型，但是它们很有可能不会比我们在上一节中使用的线性模型有更大的改进。

1.  现在，我们已经准备好设置用于非线性 SVC 的管道。我们的管道将进行列转换和递归特性消除。我们使用逻辑回归进行特征选择:

    ```
    rfecv = RFECV(estimator=LogisticRegression()) svc = SVC() pipe1 = make_pipeline(ng.coltrans, rfecv, svc)
    ```

2.  We create a dictionary to use for our hyperparameter tuning. This dictionary is structured somewhat differently from other dictionaries we have used for this purpose. That is because certain hyperparameters only work with certain other hyperparameters. For example, `gamma` does not work with a linear kernel:

    ```
    svc_params = [
      {
        'svc__kernel': ['rbf'],
        'svc__C': uniform(loc=0, scale=20),
        'svc__gamma': uniform(loc=0, scale=100)
      },
      {
        'svc__kernel': ['poly'],
        'svc__degree': randint(1, 5),
        'svc__C': uniform(loc=0, scale=20),
        'svc__gamma': uniform(loc=0, scale=100)
      },
      {
        'svc__kernel': ['linear','sigmoid'],
        'svc__C': uniform(loc=0, scale=20)
      }
    ]
    ```

    注意

    您可能已经注意到我们将使用的一个内核是线性的，并且想知道它与我们在上一节中使用的线性 SVC 模块有什么不同。通常会收敛得更快，尤其是对于大型数据集。它不使用内核技巧。我们也可能得到不同的结果，因为优化在几个方面是不同的。

3.  现在我们准备好安装 SVC 模型。最好的模型实际上是具有线性内核的模型:

    ```
    rs = RandomizedSearchCV(pipe1, svc_params, cv=5,    scoring='accuracy', n_iter=10, n_jobs=-1,   verbose=5, random_state=0) rs.fit(X_train, y_train.values.ravel()) rs.best_params_ {'svc__C': 1.1342595463488636, 'svc__kernel': 'linear'} rs.best_score_ 0.9299405955437289
    ```

4.  让我们仔细看看所选的超参数和相关的准确度分数。我们可以从网格对象的`cv_results_`字典的`params`列表中获得 20 个随机选择的超参数组合。我们可以从同一本字典中得到平均考试分数。

我们按准确度分数降序排序。线性核优于多项式核和`rbf`核，尽管在`3`、`4`和`5`度上并不比多项式好多少。`rbf`内核表现特别差:

```
results = \
  pd.DataFrame(rs.cv_results_['mean_test_score'], \
    columns=['meanscore']).\
  join(pd.json_normalize(rs.cv_results_['params'])).\
  sort_values(['meanscore'], ascending=False)
results
 C          gamma     kernel     degree
meanscore 
0.93        1.13       NaN       linear     NaN
0.89        1.42       64.82     poly       3.00
0.89        9.55       NaN       sigmoid    NaN
0.89        11.36      NaN       sigmoid    NaN
0.89        2.87       75.86     poly       5.00
0.64        12.47      43.76     poly       4.00
0.64        15.61      72.06     poly       4.00
0.57        11.86      84.43     rbf        NaN
0.57        16.65      77.82     rbf        NaN
0.57        19.57      79.92     rbf        NaN
```

注意

我们使用 pandas `json_nomalize`方法来处理从`params`列表中提取的有些混乱的超参数组合。这很混乱，因为根据所使用的内核，可以使用不同的超参数。这意味着`params`列表中的 20 个字典将有不同的关键字。例如，多项式核将具有度数值。线性和`rbf`内核不会。

1.  我们可以通过属性`best_estimator_`访问支持向量。有 625 个支持向量*支撑着*决策边界:

    ```
    rs.best_estimator_['svc'].\   support_vectors_.shape (625, 18)
    ```

2.  最后，我们可以看看预测。毫不奇怪，我们并没有得到比我们在上一节中运行的线性 SVC 模型更好的结果。我说不奇怪是因为发现最好的模型是具有线性核的模型:

    ```
    pred = rs.predict(X_test) print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred,      pos_label=0),   skmet.precision_score(y_test.values.ravel(), pred))) accuracy: 0.93, sensitivity: 0.94, specificity: 0.91, precision: 0.93
    ```

虽然我们没有改进上一节的模型，但是尝试一些非线性模型仍然是值得的。事实上，这通常是我们发现是否有可以成功线性分离的数据的方法。这通常很难可视化，因此我们依靠超参数调整来告诉我们哪个内核对我们的数据分类最好。

本节和上一节演示了使用支持向量机进行二元分类的关键技术。到目前为止，我们所做的大部分工作也适用于多类分类。在下一节中，我们将看看当我们的目标有两个以上的值时的 SVC 建模策略。

# 支持向量机用于多类分类

当我们使用 SVC 进行二元分类时，所有的关注点同样适用于多类分类。我们需要确定类是否是线性可分的，如果不是，哪个核将产生最好的结果。正如本章第一节所讨论的，我们还需要决定这种分类是一对一还是一对多。一对一查找将每个类与其他每个类分开的决策边界。one-vs-rest 找到将每个类与所有其他实例区分开来的决策边界。在本节中，我们尝试了这两种方法。

我们将使用在前几章中处理过的机器故障数据。

注意

该机器故障数据集可在[https://www . ka ggle . com/datasets/Shiva MB/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)上公开使用。有 10，000 个观察值、12 个特征和两个可能的目标。一个是二进制:机器故障或者没有。另一种是失败的类型。该数据集中的实例是合成的，由旨在模拟机器故障率和原因的流程生成。

让我们构建一个多类 SVC 模型:

1.  我们首先加载我们在本章中使用过的库:

    ```
    import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, MinMaxScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC from scipy.stats import uniform from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.model_selection import RandomizedSearchCV import sklearn.metrics as skmet import os import sys sys.path.append(os.getcwd() + "/helperfunctions") from preprocfunc import OutlierTrans
    ```

2.  我们将加载机器故障类型数据集，并查看其结构。字符和数字数据混合在一起。没有丢失的值:

    ```
    machinefailuretype = pd.read_csv("data/machinefailuretype.csv") machinefailuretype.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 10000 entries, 0 to 9999 Data columns (total 10 columns):  #   Column                 Non-Null Count     Dtype   ---  ------                 --------------     -----    0   udi                    10000 non-null     int64    1   product                10000 non-null     object   2   machinetype            10000 non-null     object   3   airtemp                10000 non-null     float64  4   processtemperature     10000 non-null     float64  5   rotationalspeed        10000 non-null     int64    6   torque                 10000 non-null     float64  7   toolwear               10000 non-null     int64    8   fail                   10000 non-null     int64    9   failtype               10000 non-null     object  dtypes: float64(3), int64(4), object(3) memory usage: 781.4+ KB
    ```

3.  让我们来看一些观察:

    ```
    machinefailuretype.head()    udi product machinetype airtemp processtemperature\ 0  1   M14860       M         298        309  1  2   L47181       L         298        309  2  3   L47182       L         298        308  3  4   L47183       L         298        309  4  5   L47184       L         298        309     rotationalspeed  torque  toolwear  fail  failtype   0        1551         43        0       0   No Failure   1        1408         46        3       0   No Failure   2        1498         49        5       0   No Failure   3        1433         40        7       0   No Failure   4        1408         40        9       0   No Failure
    ```

4.  再来看目标的分布。我们有一个明显的阶级不平衡，所以我们需要以某种方式来解决这个问题:

    ```
    machinefailuretype.failtype.\   value_counts(dropna=False).sort_index() Heat Dissipation Failure     112 No Failure                   9652 Overstrain Failure           78 Power Failure                95 Random Failures              18 Tool Wear Failure            45 Name: failtype, dtype: int64
    ```

5.  我们可以通过为失败类型创建一个数字代码来省去一些麻烦，我们将使用它而不是字符值。我们不需要将它放入 a 管道，因为我们不会在转换中引入任何数据泄漏:

    ```
    def setcode(typetext):   if (typetext=="No Failure"):     typecode = 1   elif (typetext=="Heat Dissipation Failure"):     typecode = 2   elif (typetext=="Power Failure"):     typecode = 3   elif (typetext=="Overstrain Failure"):     typecode = 4   else:     typecode = 5   return typecode machinefailuretype["failtypecode"] = \   machinefailuretype.apply(lambda x: setcode(x.failtype), axis=1)
    ```

6.  我们还应该看一些描述性的统计数据。我们将需要扩展功能:

    ```
    num_cols = ['airtemp','processtemperature',   'rotationalspeed','torque','toolwear'] cat_cols = ['machinetype'] machinefailuretype[num_cols].agg(['min','median','max']).T                        min        median     max airtemp                295.30     300.10     304.50 processtemperature     305.70     310.10     313.80 rotationalspeed        1,168.00   1,503.00   2,886.00 torque                 3.80       40.10      76.60 toolwear               0.00       108.00     253.00
    ```

7.  现在让我们创建训练和测试数据框架。我们还应该使用`stratify`参数来确保我们的训练和测试数据中目标值的平均分布:

    ```
    X_train, X_test, y_train, y_test =  \   train_test_split(machinefailuretype[num_cols + cat_cols],\   machinefailuretype[['failtypecode']],\   stratify=machinefailuretype[['failtypecode']], \   test_size=0.2, random_state=0)
    ```

8.  我们设置了需要运行的列转换。对于数字列，我们将离群值设置为中值，然后调整数值。我们对一个分类特征进行一次性编码`machinetype`。它有高、中、低质量的`H`、`M`和`L`值:

    ```
    ohe = OneHotEncoder(drop='first', sparse=False) cattrans = make_pipeline(ohe) standtrans = make_pipeline(OutlierTrans(3),   SimpleImputer(strategy="median"),   MinMaxScaler()) coltrans = ColumnTransformer(   transformers=[     ("cat", cattrans, cat_cols),     ("stand", standtrans, num_cols),   ] )
    ```

9.  接下来，我们用列转换和 SVC 实例建立一个管道。我们将`class_weight`参数设置为`balanced`来处理类不平衡。这应用了与目标类别的频率成反比的权重:

    ```
    svc = SVC(class_weight='balanced', probability=True) pipe1 = make_pipeline(coltrans, svc)
    ```

在这种情况下，我们只有少数几个功能，所以我们不会担心功能选择。(我们可能仍然会担心高度相关的要素，但这不是这个数据集的问题。)

1.  我们用超参数组合创建了一个字典，用于网格搜索。这在很大程度上与我们在上一节中使用的字典相同，只是我们添加了一个决策函数形状键。这将导致网格搜索尝试一对一(`ovo`)和一对其余(`ovr`)分类:

    ```
    svc_params = [   {     'svc__kernel': ['rbf'],     'svc__C': uniform(loc=0, scale=20),     'svc__gamma': uniform(loc=0, scale=100),     'svc__decision_function_shape': ['ovr','ovo']   },   {     'svc__kernel': ['poly'],     'svc__degree': np.arange(0,6),     'svc__C': uniform(loc=0, scale=20),     'svc__gamma': uniform(loc=0, scale=100),     'svc__decision_function_shape': ['ovr','ovo']   },   {     'svc__kernel': ['linear','sigmoid'],     'svc__C': uniform(loc=0, scale=20),     'svc__decision_function_shape': ['ovr','ovo']   } ]
    ```

2.  现在我们已经准备好运行随机网格搜索。我们将根据 ROC 曲线下的面积进行评分。最好的超参数包括一对一决策函数和`rbf`内核:

    ```
    rs = RandomizedSearchCV(pipe1, svc_params, cv=7, scoring="roc_auc_ovr", n_iter=10) rs.fit(X_train, y_train.values.ravel()) rs.best_params_ {'svc__C': 5.609789456747942,  'svc__decision_function_shape': 'ovo',  'svc__gamma': 27.73459801111866,  'svc__kernel': 'rbf'} rs.best_score_ 0.9187636814475847
    ```

3.  让我们看看每次迭代的分数。除了我们在上一步中看到的最佳模型之外，还有其他几个超参数组合的得分也几乎一样高。具有线性核的 one-vs-rest 几乎与表现最好的模型一样好:

    ```
    results = \   pd.DataFrame(rs.cv_results_['mean_test_score'], \     columns=['meanscore']).\   join(pd.json_normalize(rs.cv_results_['params'])).\   sort_values(['meanscore'], ascending=False) results meanscore  svc__C svc__decision_function_shape  svc__gamma svc__kernel 7     0.92     5.61     ovo     27.73     rbf 5     0.91     9.43     ovr     NaN       linear 3     0.91     5.40     ovr     NaN       linear 0     0.90     19.84    ovr     28.70     rbf 8     0.87     5.34     ovo     93.87     rbf 6     0.86     8.05     ovr     80.57     rbf 9     0.86     4.41     ovo     66.66     rbf 1     0.86     3.21     ovr     85.35     rbf 4     0.85     0.01     ovo     38.24     rbf 2     0.66     7.61     ovr     NaN       sigmoid
    ```

4.  我们应该看看混淆矩阵:

    ```
    pred = rs.predict(X_test) cm = skmet.confusion_matrix(y_test, pred) cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm,    display_labels=['None', 'Heat','Power','Overstrain','Other']) cmplot.plot() cmplot.ax_.set(title='Machine Failure Type Confusion Matrix',    xlabel='Predicted Value', ylabel='Actual Value')
    ```

这就产生了下面的情节:

![Figure 13.13 – Confusion matrix for machine failure type prediction

](img/B17978_13_013.jpg)

图 13.13-机器故障类型预测的混淆矩阵

1.  我们也来做一个分类报告。虽然我们的模型确实很好地预测了过热和过度应变故障，但我们在大多数类别的灵敏度方面并没有得到高分:

    ```
    print(skmet.classification_report(y_test, pred,   target_names=['None', 'Heat','Power', 'Overstrain', 'Other']))               precision    recall  f1-score   support         None       0.99      0.97      0.98      1930         Heat       0.50      0.91      0.65        22        Power       0.60      0.47      0.53        19   Overstrain       0.65      0.81      0.72        16        Other       0.06      0.15      0.09        13     accuracy                           0.96      2000    macro avg       0.56      0.66      0.59      2000 weighted avg       0.97      0.96      0.96      2000
    ```

当对目标进行建模时，例如机器故障类型具有很高的类别不平衡，我们通常更关心度量而不是准确性。这部分是由我们的领域知识决定的。避免假阴性可能比避免假阳性更重要。过早对机器进行彻底的检查肯定比过晚进行更好。

96%到 97%的加权精度、召回率(灵敏度)和 f1 分数不能很好地反映我们模型的性能。它们主要反映了巨大的阶级不平衡和很容易预测没有机器故障的事实。低得多的宏观平均值(只是不同类别的简单平均值)表明我们的模型难以预测某些类型的机器故障。

这个例子说明了将 SVC 扩展到具有两个以上值的模型是相对容易的。我们可以指定是使用一对一分类还是一对一分类。当类别的数量超过三个时，一对一的方法会更快，因为训练的分类器会更少。

# 总结

在本章中，我们探讨了实现 SVC 的不同策略。我们使用了线性 SVC(它不使用内核)，当我们的类是线性可分的时，它可以执行得很好。然后，我们研究了如何使用内核技巧将 SVC 扩展到类不是线性可分的情况。最后，我们使用一对一和一对多分类来处理具有两个以上值的目标。

SVC 对于二进制和多类分类是非常有用的技术。它可以处理特征和目标之间简单和复杂的关系。很少有监督学习问题至少不应该考虑支持向量机。然而，对于非常大的数据集，它不是非常有效。

在下一章中，我们将探索另一种流行且灵活的分类算法，朴素贝叶斯。