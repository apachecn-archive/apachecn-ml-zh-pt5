<html><head/><body>









<title>Chapter 16: K-Means and DBSCAN Clustering</title>







<div><div><h1 id="_idParaDest-174"><em class="italic"> <a id="_idTextAnchor177"/>第十六章</em> : K-Means 和 DBSCAN 聚类</h1>

<p>数据聚类允许我们将未标记的数据组织成观察组，这些观察组与组内其他成员的共同点多于与组外观察组的共同点。无论是作为机器学习管道的最终模型，还是作为另一个模型的输入，聚类的应用数量惊人地多。这包括市场研究、图像处理和文档分类。我们有时也使用聚类来改进探索性数据分析或创建更有意义的可视化。</p>

<p>K-means <a id="_idIndexMarker1137"/>和<strong class="bold">基于密度的应用空间聚类与噪声</strong> ( <strong class="bold"> DBSCAN </strong>)聚类，像<strong class="bold">主成分分析</strong> ( <strong class="bold"> PCA </strong>)，都是<a id="_idIndexMarker1138"/>无监督学习算法。没有标签可用作预测的基础。该算法的目的是基于实例的特征来识别挂在一起的实例。彼此非常接近而远离其他实例的实例可以被认为是在一个群集中。有多种方法可以测量<a id="_idIndexMarker1139"/>的接近度。<strong class="bold">基于分区的聚类</strong>，如 k-means<a id="_idIndexMarker1140"/>和基于密度的聚类，如 DBSCAN，是两种比较流行的方法。我们将在本章中探讨这些方法。</p>

<p>具体来说，我们将讨论以下主题:</p>

<ul>

<li>k-means 和 DBSCAN 聚类的关键概念</li>

<li>实现 k 均值聚类</li>

<li>实现 DBSCAN 集群</li>

</ul>

<h1 id="_idParaDest-175"><a id="_idTextAnchor179"/>技术要求</h1>

<p>在本章中，我们将主要关注 pandas、NumPy 和 scikit-learn 库。</p>

<h1 id="_idParaDest-176"><a id="_idTextAnchor180"/>k-means 和 DBSCAN 聚类的关键概念</h1>

<p>使用 k-means <a id="_idIndexMarker1141"/>聚类，我们<a id="_idIndexMarker1142"/>识别<em class="italic"> k 个</em>聚类，每个聚类都有一个<a id="_idIndexMarker1143"/>中心，或<strong class="bold">质心</strong>。质心是使其与聚类中其他数据点之间的总平方距离最小的点。</p>

<p>一个虚构数据的例子应该有所帮助。<em class="italic">图 16.1 </em>中的数据点似乎在三个集群中。(通常不容易看到集群的数量，<em class="italic"> k </em>。)</p>

<div><div><img alt="Figure 16.1 – Data points with three discernible clusters&#10;&#10;" height="550" src="img/B17978_16_001.jpg" width="816"/>

</div>

</div>

<p class="figure-caption">图 16.1–具有三个可辨别集群的数据点</p>

<p>我们执行以下步骤来构建集群:</p>

<ol>

<li>指定一个随机点作为每个簇的中心。</li>

<li>计算每个点到每个聚类中心的距离。</li>

<li>根据数据点与中心点的接近程度将数据点分配给一个聚类。前三个步骤如图<em class="italic">图 16.2 </em>所示。带有<strong class="bold"> X </strong>的点是随机选择的聚类中心(其中<em class="italic"> k </em>设置为 3)。比其他聚类中心点更靠近聚类中心点的数据点被分配给该聚类。</li>

</ol>

<div><div><img alt="Figure 16.2 – Random points assigned as the center of the cluster&#10;&#10;" height="547" src="img/B17978_16_002.jpg" width="814"/>

</div>

</div>

<p class="figure-caption">图 16.2–随机点被指定为聚类的中心</p>

<ol>

<li value="4">为新群计算新的<a id="_idIndexMarker1145"/>中心点。这在<em class="italic">图 16.3 </em>中进行了说明。</li>

</ol>

<div><div><img alt="Figure 16.3 – New cluster centers calculated&#10;&#10;" height="548" src="img/B17978_16_003.jpg" width="817"/>

</div>

</div>

<p class="figure-caption">图 16.3–计算出的新聚类中心</p>

<ol>

<li value="5">重复步骤 2 到 4，直到中心没有太大变化。</li>

</ol>

<p>K-means 聚类是一种非常流行的聚类算法，原因有几个。这是非常直观的<a id="_idIndexMarker1146"/>和<a id="_idIndexMarker1147"/>通常相当快。然而，它也有一些缺点。它将每个数据点作为一个聚类的一部分进行处理，因此这些聚类可能会因极值而改变。它还假设星团是球形的。</p>

<p>无监督模型的评估不如有监督模型清晰，因为我们没有一个目标来比较我们的预测。聚类模型的一个相当常见的度量是<a id="_idIndexMarker1148"/><strong class="bold">轮廓分数</strong>。轮廓分数是所有<a id="_idIndexMarker1149"/>实例的平均轮廓系数。<strong class="bold">轮廓系数</strong>如下:</p>

<div><div><img alt="" height="124" src="img/B17978_16_0011.jpg" width="371"/>

</div>

</div>

<p>这里，<img alt="" height="49" src="img/B17978_16_002.png" width="38"/>是到第 I 个实例的下一个最近的聚类的所有实例的平均距离，<img alt="" height="37" src="img/B17978_16_003.png" width="40"/>是到所分配的聚类的实例的平均距离。该系数的范围从-1 到 1，分数接近 1 意味着该实例很好地位于指定的分类中。</p>

<p>评估我们集群的另一个指标是<a id="_idIndexMarker1150"/><strong class="bold">惯性得分</strong>。这是每个实例与其质心之间距离的平方之和。随着我们增加聚类数量，这种距离将会减小，但增加聚类数量最终会导致边际收益递减。惯性分数随<em class="italic"> k </em>的变化通常使用<strong class="bold">肘图</strong>可视化<a id="_idIndexMarker1151"/>。这个图被称为肘形图，因为当我们增加<em class="italic"> k </em>时，斜率越来越接近 0，如此接近以至于它像一个肘形图。这如图 16.4 中的<em class="italic">所示。在这种情况下，我们<a id="_idIndexMarker1152"/>将选择靠近<a id="_idIndexMarker1153"/>弯头的<em class="italic"> k </em>值。</em></p>

<div><div><img alt="Figure 16.4 – An elbow plot with inertia and k&#10;&#10;" height="634" src="img/B17978_16_004.jpg" width="801"/>

</div>

</div>

<p class="figure-caption">图 16.4-带有惯性和 k 的弯管图</p>

<p>在评估一个聚类<a id="_idIndexMarker1154"/>模型时，另一个常用的指标是<strong class="bold">兰德指数</strong>。Rand 索引告诉我们两个集群将同一个集群分配给实例的频率。Rand 指数的值将介于 0 和 1 之间。我们通常使用调整后的 Rand 指数，它在相似性计算中校正了偶然性。调整后的 Rand 指数值有时可能为负值。</p>

<p>DBSCAN 采用了一种不同的聚类方法。对于每个实例，它计算该实例指定距离内的实例数。一个实例在ɛ的所有实例都在该实例的ɛ-neighborhood.中当ɛ-neighborhood 中的实例数量等于或超过我们指定的最小样本值时，该实例被视为核心实例，而ɛ-neighborhood 被视为集群。任何比另一个实例的ɛ大的实例都被认为是噪声。这就是图 16.5 中<em class="italic">所示的<a id="_idIndexMarker1155"/>。</em></p>

<div><div><img alt="Figure 16.5 – DBSCAN clustering with minimum samples = five&#10;&#10;" height="604" src="img/B17978_16_005.jpg" width="778"/>

</div>

</div>

<p class="figure-caption">图 16.5–最小样本数= 5 的 DBSCAN 聚类</p>

<p>这种基于密度的方法有几个优点。簇不必是球形的。它们可以是任何形状。我们不需要猜测集群的数量，尽管我们需要提供ɛ.的值离群值只是被解释为噪声，因此不会影响聚类。(最后一点暗示了 DBSCAN 的另一个有用的应用:识别异常。)</p>

<p>在本章的后面，我们将使用 DBSCAN <a id="_idIndexMarker1157"/>进行<a id="_idIndexMarker1158"/>聚类。首先，我们将研究如何使用 k-means 进行聚类，包括如何为 k 选择一个好的值。</p>

<h1 id="_idParaDest-177"><a id="_idTextAnchor181"/>实现 k 均值聚类</h1>

<p>我们可以将 k-means <a id="_idIndexMarker1159"/>与我们在前面章节开发的监督学习模型中使用的一些相同数据一起使用。不同的是，我们不再有一个可以预测的目标。相反，我们感兴趣的是某些实例是如何结合在一起的。想想人们是如何在刻板的高中午休时间安排自己分组的，你就会有一个大概的想法。</p>

<p>我们还需要做很多与监督学习模型相同的预处理工作。在本节中，我们将从这一点开始。我们将研究男女收入差距、劳动力参与率、受教育程度、青少年生育频率以及女性参与最高级别政治的数据。</p>

<p class="callout-heading">注意</p>

<p class="callout">收入差距数据集由<em class="italic">联合国开发计划署</em>在<a href="https://www.kaggle.com/datasets/undp/human-development">https://www.kaggle.com/datasets/undp/human-development</a>公开使用。2015 年，每个国家都有一份按性别分列的就业、收入和教育数据记录。</p>

<p>让我们构建一个 k 均值聚类模型:</p>

<ol>

<li value="1">我们加载熟悉的库。我们还加载了<code>KMeans</code>和<code>silhouette_score</code>模块。回想一下，剪影分数通常用于评估我们的模型在聚类方面做得有多好。我们还加载了<code>rand_score</code>，这将允许我们计算不同聚类之间相似性的 Rand 指数:<pre>import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import make_pipeline from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score from sklearn.metrics.cluster import rand_score from sklearn.impute import KNNImputer import seaborn as sns import matplotlib.pyplot as plt</pre></li>

<li>接下来，我们加载<a id="_idIndexMarker1160"/>收入差距数据:<pre>un_income_gap = pd.read_csv("data/un_income_gap.csv") un_income_gap.set_index('country', inplace=True) un_income_gap['incomeratio'] = \   un_income_gap.femaleincomepercapita / \     un_income_gap.maleincomepercapita un_income_gap['educratio'] = \   un_income_gap.femaleyearseducation / \      un_income_gap.maleyearseducation un_income_gap['laborforcepartratio'] = \   un_income_gap.femalelaborforceparticipation / \      un_income_gap.malelaborforceparticipation un_income_gap['humandevratio'] = \   un_income_gap.femalehumandevelopment / \      un_income_gap.malehumandevelopment</pre></li>

<li>我们来看一些描述性统计:<pre>num_cols = ['educratio','laborforcepartratio','humandevratio',   'genderinequality','maternalmortality','incomeratio',   'adolescentbirthrate', 'femaleperparliament',   'incomepercapita'] gap = un_income_gap[num_cols] gap.agg(['count','min','median','max']).T <strong class="bold">                     count   min    median  max</strong> <strong class="bold">educratio            170.00  0.24   0.93    1.35</strong> <strong class="bold">laborforcepartratio  177.00  0.19   0.75    1.04</strong> <strong class="bold">humandevratio        161.00  0.60   0.95    1.03</strong> <strong class="bold">genderinequality     155.00  0.02   0.39    0.74</strong> <strong class="bold">maternalmortality    178.00  1.00   64.00   1,100.00</strong> <strong class="bold">incomeratio          177.00  0.16   0.60    0.93</strong> <strong class="bold">adolescentbirthrate  183.00  0.60   40.90   204.80</strong> <strong class="bold">femaleperparliament  185.00  0.00   19.60   57.50</strong> <strong class="bold">incomepercapita      188.00  581.00 10,667.00  23,124.00</strong></pre></li>

<li>我们还应该<a id="_idIndexMarker1161"/>看看一些相关性。教育比率(女性教育水平与男性教育水平的比率)与人类发展比率高度相关，性别不平等和青少年出生率以及收入比率和劳动力参与率也是如此:<pre>corrmatrix = gap.corr(method="pearson") sns.heatmap(corrmatrix,    xticklabels=corrmatrix.columns,   yticklabels=corrmatrix.columns, cmap="coolwarm") plt.title('Heat Map of Correlation Matrix') plt.tight_layout() plt.show()</pre></li>

</ol>

<p>这会产生以下情节:</p>

<div><div><img alt="Figure 16.6 – A heat map of the correlation matrix&#10;&#10;" height="463" src="img/B17978_16_006.jpg" width="610"/>

</div>

</div>

<p class="figure-caption">图 16.6–关联矩阵的热图</p>

<ol>

<li value="5">在运行我们的模型之前，我们需要<a id="_idIndexMarker1162"/>缩放数据。我们还<a id="_idIndexMarker1163"/>使用<strong class="bold"> KNN 插补</strong>来处理缺失值:<pre>pipe1 = make_pipeline(MinMaxScaler(), KNNImputer(n_neighbors=5)) gap_enc = pd.DataFrame(pipe1.fit_transform(gap),   columns=num_cols, index=gap.index)</pre></li>

<li>现在，我们准备运行 k-means 聚类。我们为集群的数量指定一个值。</li>

</ol>

<p>在拟合模型之后，我们可以生成轮廓分数。我们的剪影评分不是很好。这表明我们的星团相距不远。稍后，我们将查看是否可以通过更多或更少的聚类获得更好的分数:</p>

<pre>kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(gap_enc)
<strong class="bold">KMeans(n_clusters=3, random_state=0)</strong>
silhouette_score(gap_enc, kmeans.labels_)
<strong class="bold">0.3311928353317411</strong></pre>

<ol>

<li value="7">让我们仔细看看这些集群。我们可以使用<code>labels_</code>属性来获得集群:<pre>gap_enc['cluster'] = kmeans.labels_ gap_enc.cluster.value_counts().sort_index() <strong class="bold">0     40</strong> <strong class="bold">1    100</strong> <strong class="bold">2     48</strong> <strong class="bold">Name: cluster, dtype: int64</strong></pre></li>

<li>我们可以使用<code>fit_predict</code>方法来获得集群，就像这样:<pre>pred = pd.Series(kmeans.fit_predict(gap_enc)) pred.value_counts().sort_index() <strong class="bold">0     40</strong> <strong class="bold">1    100</strong> <strong class="bold">2     48</strong> <strong class="bold">dtype: int64</strong></pre></li>

<li>检查分类在特征值方面的差异是很有帮助的。第 0 组国家的孕产妇死亡率和青少年出生率比其他组国家高得多。第一组国家的孕产妇死亡率很低，人均收入很高。第二组国家的劳动力参与率(女性劳动力参与率与男性劳动力参与率的比率)和收入比率非常低。回想一下，我们<a id="_idIndexMarker1165"/>已经缩放了数据:<pre>gap_cluster = gap_enc.join(cluster) gap_cluster[['cluster'] + num_cols].groupby(['cluster']).mean().T <strong class="bold">cluster              0            1           2</strong> <strong class="bold">educratio            0.36         0.66        </strong><strong class="bold">0.54</strong> <strong class="bold">laborforcepartratio  0.80         0.67        0.32</strong> <strong class="bold">humandevratio        0.62         0.87        0.68</strong> <strong class="bold">genderinequality     0.79         0.32        0.62</strong> <strong class="bold">maternalmortality    </strong><strong class="bold">0.44         0.04        0.11</strong> <strong class="bold">incomeratio          0.71         0.60        0.29</strong> <strong class="bold">adolescentbirthrate  0.51         0.15        0.20</strong> <strong class="bold">femaleperparliament  0.33         </strong><strong class="bold">0.43        0.24</strong> <strong class="bold">incomepercapita      0.02         0.19        0.12</strong></pre></li>

<li>我们可以使用<code>cluster_centers_</code>属性来获得每个集群的中心。有九个值代表三个聚类的中心，因为我们使用九个特征进行聚类:<pre>centers = kmeans.cluster_centers_ centers.shape <strong class="bold">(3, 9)</strong> np.set_printoptions(precision=2) centers <strong class="bold">array([[0.36, 0.8 , 0.62, 0.79, 0.44, 0.71, 0.51, 0.33, 0.02],</strong> <strong class="bold">       [0.66, 0.67, 0.87, 0.32, 0.04, 0.6 , 0.15, 0.43, 0.19],</strong> <strong class="bold">       [0.54, 0.32, 0.68, 0.62, 0.11, 0.29, 0.2 , 0.24, 0.12]])</strong></pre></li>

<li>我们根据它们的一些特征以及中心来绘制<a id="_idIndexMarker1166"/>簇。我们将该集群的编号放在该集群的质心:<pre>fig = plt.figure() plt.suptitle("Cluster for each Country") ax = plt.axes(projection='3d') ax.set_xlabel("Maternal Mortality") ax.set_ylabel("Adolescent Birth Rate") ax.set_zlabel("Income Ratio") ax.scatter3D(gap_cluster.maternalmortality,   gap_cluster.adolescentbirthrate,   gap_cluster.incomeratio, c=gap_cluster.cluster, cmap="brg") for j in range(3):   ax.text(centers2[j, num_cols.index('maternalmortality')],   centers2[j, num_cols.index('adolescentbirthrate')],   centers2[j, num_cols.index('incomeratio')],   c='black', s=j, fontsize=20, fontweight=800) plt.tight_layout() plt.show()</pre></li>

</ol>

<p>这就产生了<a id="_idIndexMarker1167"/>下面的情节:</p>

<div><div><img alt="Figure 16.7 – A 3D scatter plot of three clusters &#10;&#10;" height="492" src="img/B17978_16_007.jpg" width="510"/>

</div>

</div>

<p class="figure-caption">图 16.7–三个集群的 3D 散点图</p>

<p>我们可以看到，第 0 组国家的孕产妇死亡率和青少年出生率较高。第 0 组国家的收入比率较低。</p>

<ol>

<li value="12">到目前为止，我们已经假设用于我们模型的最佳聚类数是 3。让我们构建一个五个集群的模型，看看这些结果是什么样的。</li>

</ol>

<p>轮廓得分比三组模型有所下降。这可能表明<a id="_idIndexMarker1168"/>至少有一些星团非常接近:</p>

<pre>gap_enc = gap_enc[num_cols]
kmeans2 = KMeans(n_clusters=5, random_state=0)
kmeans2.fit(gap_enc)
silhouette_score(gap_enc, kmeans2.labels_)
<strong class="bold">0.2871811434351394</strong>
gap_enc['cluster2'] = kmeans2.labels_
gap_enc.cluster2.value_counts().sort_index()
<strong class="bold">0    21</strong>
<strong class="bold">1    40</strong>
<strong class="bold">2    48</strong>
<strong class="bold">3    16</strong>
<strong class="bold">4    63</strong>
<strong class="bold">Name: cluster2, dtype: int64</strong></pre>

<ol>

<li value="13">让我们绘制新的集群，以更好地了解它们的位置:<pre>fig = plt.figure() plt.suptitle("Cluster for each Country") ax = plt.axes(projection='3d') ax.set_xlabel("Maternal Mortality") ax.set_ylabel("Adolescent Birth Rate") ax.set_zlabel("Income Ratio") ax.scatter3D(gap_cluster.maternalmortality,   gap_cluster.adolescentbirthrate,   gap_cluster.incomeratio, c=gap_cluster.cluster2,    cmap="brg") for j in range(5):   ax.text(centers2[j, num_cols.index('maternalmortality')],   centers2[j, num_cols.index('adolescentbirthrate')],   centers2[j, num_cols.index('incomeratio')],   c='black', s=j, fontsize=20, fontweight=800) plt.tight_layout() plt.show()</pre></li>

</ol>

<p>此<a id="_idIndexMarker1169"/>产生以下图形:</p>

<div><div><img alt="Figure 16.8 – A 3D scatter plot of five clusters&#10;&#10;" height="444" src="img/B17978_16_008.jpg" width="479"/>

</div>

</div>

<p class="figure-caption">图 16.8–五个集群的 3D 散点图</p>

<ol>

<li value="14">我们可以使用一个叫做 Rand 指数的统计来衡量聚类之间的相似性:<pre>rand_score(kmeans.labels_, kmeans2.labels_) 0.7439412902491751</pre></li>

<li>我们尝试了<a id="_idIndexMarker1170"/>三集群和五集群模式，但是这两种模式都是好的选择吗？让我们看看一系列<em class="italic"> k </em>值的分数:<pre>gap_enc = gap_enc[num_cols] iner_scores = [] sil_scores = [] for j in range(2,20):   kmeans=KMeans(n_clusters=j, random_state=0)   kmeans.fit(gap_enc)   iner_scores.append(kmeans.inertia_)   sil_scores.append(silhouette_score(gap_enc,     kmeans.labels_))</pre></li>

<li>让我们用肘图来绘制惯性分数:<pre>plt.title('Elbow Plot') plt.xlabel('k') plt.ylabel('Inertia') plt.plot(range(2,20),iner_scores)</pre></li>

</ol>

<p>这就产生了<a id="_idIndexMarker1171"/>下面的情节:</p>

<div><div><img alt="Figure 16.9 – An elbow plot of inertia scores&#10;&#10;" height="445" src="img/B17978_16_009.jpg" width="559"/>

</div>

</div>

<p class="figure-caption">图 16.9-惯性得分的肘图</p>

<ol>

<li value="17">我们还创建了一个剪影分数的绘图:<pre>plt.title('Silhouette Score') plt.xlabel('k') plt.ylabel('Silhouette Score') plt.plot(range(2,20),sil_scores)</pre></li>

</ol>

<p>这就产生了<a id="_idIndexMarker1172"/>下面的情节:</p>

<div><div><img alt="Figure 16.10 – An plot of silhouette scores&#10;&#10;" height="444" src="img/B17978_16_010.jpg" width="569"/>

</div>

</div>

<p class="figure-caption">图 16.10–轮廓得分图</p>

<p>肘形图表明大约 6 或 7 的 k 值是最好的。在超过这个值时，我们开始得到惯性收益递减。侧影得分图显示了一个较小的<em class="italic"> k </em>，因为在此之后侧影得分急剧下降。</p>

<p>K-means 聚类帮助我们理解了各国在收入、教育和就业方面的男女差距数据。我们现在可以看到某些特征是如何结合在一起的，以一种我们之前所做的简单关联没有揭示的方式。然而，这在很大程度上是假设我们的星团是球形的，我们必须做一些工作来确认我们的 k 值是最好的。我们不会有任何与 DBSCAN 集群相同的<a id="_idIndexMarker1173"/>问题，所以我们将在下一节中尝试。</p>

<h1 id="_idParaDest-178"><a id="_idTextAnchor182"/>实现 DBSCAN 聚类</h1>

<p>DBSCAN 是一种非常灵活的集群方法。我们只需要为ɛ指定一个值，也称为<a id="_idIndexMarker1175"/>EPS。正如我们所讨论的，ɛ值决定了实例周围ɛ-neighborhood 的大小。最小样本超参数表示一个实例周围需要多少个实例才能被认为是核心实例。</p>

<p class="callout-heading">注意</p>

<p class="callout">我们使用 DBSCAN 对上一节中使用的相同收入差距数据进行聚类。</p>

<p>让我们构建一个 DBSCAN 集群模型:</p>

<ol>

<li value="1">我们首先加载熟悉的库，加上<code>DBSCAN</code>模块:<pre>import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import make_pipeline from sklearn.cluster import DBSCAN from sklearn.impute import KNNImputer from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt import os import sys sys.path.append(os.getcwd() + "/helperfunctions")</pre></li>

<li>我们导入代码来加载和预处理我们在上一节中使用的工资收入数据。既然那个代码没变，这里就没必要重复了:<pre>import incomegap as ig gap = ig.gap num_cols = ig.num_cols</pre></li>

<li>我们现在<a id="_idIndexMarker1176"/>准备好预处理数据并拟合 DBSCAN 模型。我们在这里主要通过反复试验选择了 0.35 的 eps 值。我们还可以循环一系列 eps 值，并比较轮廓得分:<pre>pipe1 = make_pipeline(MinMaxScaler(),   KNNImputer(n_neighbors=5)) gap_enc = pd.DataFrame(pipe1.fit_transform(gap),   columns=num_cols, index=gap.index) dbscan = DBSCAN(eps=0.35, min_samples=5) dbscan.fit(gap_enc) silhouette_score(gap_enc, dbscan.labels_) <strong class="bold">0.31106297603736455</strong></pre></li>

<li>我们可以使用<code>labels_</code>属性来查看集群。我们有 17 个噪声实例，它们的聚类为-1。其余的观察结果属于两个集群之一:<pre>gap_enc['cluster'] = dbscan.labels_ gap_enc.cluster.value_counts().sort_index() <strong class="bold">-1     17</strong> <strong class="bold"> 0    139</strong> <strong class="bold"> 1     32</strong> <strong class="bold">Name: cluster, dtype: int64</strong> gap_enc = \  gap_enc.loc[gap_enc.cluster!=-1]</pre></li>

<li>让我们仔细看看每个集群都有哪些相关的特性。第 1 类国家与第 0 类国家在<code>maternalmortality</code>、<code>adolescentbirthrate</code>和<code>genderinequality</code>方面有很大不同。这些也是 k-means 集群的重要特性，但是 DBSCAN 的集群少了一个，而且绝大多数实例都归入一个集群:<pre>gap_enc[['cluster'] + num_cols].\   groupby(['cluster']).mean().T <strong class="bold">cluster                     0            1</strong> <strong class="bold">educratio                   0.63</strong><strong class="bold">         0.35</strong> <strong class="bold">laborforcepartratio         0.57         0.82</strong> <strong class="bold">humandevratio               0.82         0.62</strong> <strong class="bold">genderinequality            0.40         0.79</strong> <strong class="bold">maternalmortality           0.05         0.45</strong> <strong class="bold">incomeratio                 0.51</strong><strong class="bold">         0.71</strong> <strong class="bold">adolescentbirthrate         0.16         0.50</strong> <strong class="bold">femaleperparliament         0.36         0.30</strong> <strong class="bold">incomepercapita             0.16         0.02</strong></pre></li>

<li>让我们想象一下集群:<pre>fig = plt.figure() plt.suptitle("Cluster for each Country") ax = plt.axes(projection='3d') ax.set_xlabel("Maternal Mortality") ax.set_ylabel("Adolescent Birth Rate") ax.set_zlabel("Gender Inequality") ax.scatter3D(gap_cluster.maternalmortality,   gap_cluster.adolescentbirthrate,   gap_cluster.genderinequality, c=gap_cluster.cluster,    cmap="brg") plt.tight_layout() plt.show()</pre></li>

</ol>

<p>这就产生了<a id="_idIndexMarker1178"/>下面的情节:</p>

<div><div><img alt="Figure 16.11 – A 3D scatter plot of the cluster for each country&#10;&#10;" height="438" src="img/B17978_16_011.jpg" width="463"/>

</div>

</div>

<p class="figure-caption">图 16.11–每个国家的聚类的 3D 散点图</p>

<p>DBSCAN 是一个优秀的集群工具，特别是当我们的数据特征意味着 k-means 集群不是一个好的选择时；例如，当簇不是球形时。它还具有不受离群值影响的优点。</p>

<h1 id="_idParaDest-179"><a id="_idTextAnchor183"/>总结</h1>

<p>我们有时需要将实例组织成具有相似特征的组。即使没有目标可以预测，这也是有用的。我们可以使用为可视化而创建的集群，就像我们在本章中所做的那样。由于聚类很容易解释，我们可以用它们来假设为什么一些特征会一起移动。我们还可以在后续分析中使用聚类结果。</p>

<p>本章探讨了两种流行的聚类技术，k-means 和 DBSCAN。这两种技术都是直观、有效的，并且可靠地处理聚类。</p>

</div>

</div>



</body></html>