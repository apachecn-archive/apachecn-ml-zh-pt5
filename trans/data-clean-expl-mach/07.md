

# *第五章*:功能选择

根据你如何开始你的数据分析工作和你自己的知识兴趣，你可能对**特征选择**的主题有不同的观点。你可能会想，*是的，是的，这是一个重要的话题，但我真的想去模型建筑*。或者，在另一个极端，您可能会将特征选择视为模型构建的核心，并且相信一旦您选择了特征，您就有 90%的机会拥有您的模型。现在，让我们一致认为，在我们进行任何严肃的模型规范之前，我们应该花大量的时间来理解特征之间的关系——以及它们与目标之间的关系(如果我们正在构建监督模型的话)。

以少即是多的态度对待我们的特征选择工作是有帮助的。如果我们可以达到几乎相同的准确度，或者用较少的特征解释尽可能多的差异，我们应该选择更简单的模型。有时候，我们实际上可以用更少的特征获得更好的准确性。这可能很难让我们的大脑理解，甚至让我们这些在构建模型讲述丰富复杂的故事时初露头角的人有点失望。

但在拟合机器学习模型时，我们更关心的是预测的准确性，而不是参数估计。不必要的功能会导致硬件资源的过度配置和负担。

我们有时会花几个月的时间来指定模型的特性，即使数据中的列数有限。双变量相关性，例如在 [*第 2 章*](B17978_02_ePub.xhtml#_idTextAnchor025) 、*中创建的那些，检查特征和目标之间的双变量和多变量关系*，给我们一些预期的感觉，但是一旦引入其他潜在的解释性特征，特征的重要性可能会显著变化。该特征可能不再重要，或者相反，可能只有在包括其他特征时才重要。两个特征可能高度相关，以至于包括两个特征比只包括一个特征提供的额外信息少得多。

本章仔细研究了适用于各种预测建模任务的特征选择技术。具体来说，我们将探讨以下主题:

*   为分类模型选择特征
*   为回归模型选择要素
*   使用向前和向后特征选择
*   使用详尽的特征选择
*   递归消除回归模型中的特征
*   在分类模型中递归消除特征
*   使用 Boruta 进行特征选择
*   使用正则化和其他嵌入方法
*   使用主成分分析

# 技术要求

除了`scikit-learn`库之外，我们将在本章中使用`feature_engine`、`mlxtend`和`boruta`包。你可以使用`pip`来安装这些软件包。我为本章的工作选择了一个带有少量观察值的数据集，所以代码即使在次优的工作站上也应该工作得很好。

注意

在本章中，我们将专门研究美国劳工统计局进行的全国青年纵向调查的数据。这项调查始于 1997 年的一群人，他们出生于 1980 年至 1985 年之间，每年都进行年度随访，直到 2017 年。我们将使用教育程度、家庭人口统计、工作周数和工资收入数据。工资收入栏代表 2016 年挣的工资。可以在 https://www.nlsinfo.org/investigator/pages/search[下载 NLS 数据集供公众使用。](https://www.nlsinfo.org/investigator/pages/search)

# 为分类模型选择特征

最直接的特征选择方法基于每个特征与目标变量的关系。接下来的两个部分检查了根据与目标的线性或非线性关系确定最佳特征的技术。这些被称为过滤方法。它们有时也被称为单变量方法，因为它们独立于其他特征的影响来评估特征和目标之间的关系。

当目标是明确的，而不是连续的时，我们使用的策略会有所不同。我们将在这一节讨论前者，在下一节讨论后者。

## 针对具有分类目标的特征选择的互信息分类

当我们有一个分类的目标时，我们可以使用**互信息**分类或**方差分析** ( **ANOVA** )测试来选择特征。我们先尝试互信息分类，然后用 ANOVA 进行比较。

互信息是通过知道另一个变量的值来提供关于一个变量的多少信息的度量。在极端情况下，当特征完全独立时，互信息得分为 0。

我们可以使用`scikit-learn`的`SelectKBest`类来选择 *k* 特征，这些特征基于互信息分类或一些其他适当的度量具有最高的预测强度。我们可以使用超参数调谐来选择 *k* 的值。我们还可以检查所有特性的得分，无论它们是否被确定为最佳特性之一，正如我们将在本节中看到的。

让我们首先尝试互信息分类来识别与完成学士学位相关的特征。稍后，我们将与使用方差分析 F 值作为选择基础进行比较:

1.  我们首先从`feature_engine`导入`OneHotEncoder`来编码一些数据，从`scikit-learn`导入`train_test_split`来创建训练和测试数据。我们还需要`scikit-learn`的`SelectKBest`、`mutual_info_classif`和`f_classif`模块来选择我们的特性:

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import SelectKBest,\   mutual_info_classif, f_classif
    ```

2.  We load NLS data that has a binary variable for having completed a bachelor's degree and features possibly related to degree attainment: `gender` feature, and scale the other data:

    ```
    nls97compba = pd.read_csv("data/nls97compba.csv")
    feature_cols = ['gender','satverbal','satmath',
      'gpascience', 'gpaenglish','gpamath','gpaoverall',
      'motherhighgrade','fatherhighgrade','parentincome']
    X_train, X_test, y_train, y_test =  \
      train_test_split(nls97compba[feature_cols],\
      nls97compba[['completedba']], test_size=0.3, random_state=0)
    ohe = OneHotEncoder(drop_last=True, variables=['gender'])
    X_train_enc = ohe.fit_transform(X_train)
    scaler = StandardScaler()
    standcols = X_train_enc.iloc[:,:-1].columns
    X_train_enc = \
      pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),
      columns=standcols, index=X_train_enc.index).\
      join(X_train_enc[['gender_Female']])
    ```

    注意

    在本章中，我们将对 NLS 的数据进行完整的案例分析；也就是说，我们将移除任何要素中缺失值的所有观测值。这通常不是一个好方法，当数据不是随机丢失或者一个或多个要素有大量丢失值时，这尤其成问题。在这种情况下，最好使用我们在 [*第 3 章*](B17978_03_ePub.xhtml#_idTextAnchor034)*中使用的一些方法来识别和修复缺失值*。我们将在这一章做一个完整的案例分析，让例子尽可能的简单明了。

3.  现在我们已经准备好为我们的学士学位完成模型选择特征。一种方法是使用互信息分类。为此，我们将`SelectKBest`的`score_func`值设置为`mutual_info_classif`，并指出我们想要五个最佳特性。然后，我们调用`fit`并使用`get_support`方法获得五个最佳特性:

    ```
    ksel = SelectKBest(score_func=mutual_info_classif, k=5) ksel.fit(X_train_enc, y_train.values.ravel()) selcols = X_train_enc.columns[ksel.get_support()] selcols Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')
    ```

4.  If we also want to see the score for each feature, we can use the `scores_` attribute, though we need to do a little work to associate the scores with a particular feature name and sort the scores in descending order:

    ```
    pd.DataFrame({'score': ksel.scores_,
      'feature': X_train_enc.columns},
       columns=['feature','score']).\
       sort_values(['score'], ascending=False)
     feature              score
    5       gpaoverall           0.108
    1       satmath 0.074
    3       gpaenglish           0.072
    0       satverbal            0.069
    2       gpascience           0.047
    4       gpamath              0.038
    8       parentincome 0.024
    7       fatherhighgrade      0.022
    6       motherhighgrade      0.022
    9       gender_Female        0.015
    ```

    注意

    这是一个随机过程，所以每次运行都会得到不同的结果。

为了每次都得到相同的结果，可以将部分函数传递给`score_func`:

```
from functools import partial
SelectKBest(score_func=partial(mutual_info_classif, 
                               random_state=0), k=5) 
```

1.  我们可以使用我们使用`get_support`创建的数组来创建一个只包含重要特性的数据帧。(我们可以用`SelectKBest`的`transform`方法来代替。这将会以 NumPy 数组的形式返回所选要素的值。)

    ```
    X_train_analysis = X_train_enc[selcols]  X_train_analysis.dtypes satverbal       float64 satmath         float64 gpascience      float64 gpaenglish float64 gpaoverall      float64 dtype: object
    ```

这就是我们使用交互信息为我们的模型选择k*k*最佳特性所需要做的全部工作。

## 针对分类目标的特征选择的方差分析 F 值

或者，我们可以使用 ANOVA 代替互信息。ANOVA 评估每个目标类的特征均值的差异程度。当我们可以假设特征和目标之间的线性关系并且我们的特征是正态分布时，这对于单变量特征选择是一个很好的度量。如果这些假设不成立，互信息分类是一个更好的选择。

让我们尝试使用方差分析进行特征选择。我们可以根据方差分析将`SelectKBest`的`score_func`参数设置为`f_classif`进行选择:

```
ksel = SelectKBest(score_func=f_classif, k=5)
```

```
ksel.fit(X_train_enc, y_train.values.ravel())
```

```
selcols = X_train_enc.columns[ksel.get_support()]
```

```
selcols
```

```
Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')
```

```
pd.DataFrame({'score': ksel.scores_,
```

```
  'feature': X_train_enc.columns},
```

```
   columns=['feature','score']).\
```

```
   sort_values(['score'], ascending=False)
```

```
 feature                score
```

```
5      gpaoverall           119.471
```

```
3      gpaenglish           108.006
```

```
2      gpascience            96.824
```

```
1 satmath               84.901
```

```
0      satverbal             77.363
```

```
4      gpamath               60.930
```

```
7      fatherhighgrade       37.481
```

```
6      motherhighgrade       29.377
```

```
8 parentincome          22.266
```

```
9      gender_Female         15.098
```

这选择了与我们使用交互信息时所选择的相同的特征。显示分数给我们一些指示，表明为 *k* 选择的值是否有意义。例如，从第五名到第六名(77-61)比从第四名到第五名(85-77)的得分下降更大。然而，从第六名到第七名有更大的下降(61-37)，这表明我们至少应该考虑将 k 的值设为 6。

方差分析测试和我们之前做的互信息分类没有考虑只在多变量分析中重要的特征。例如，`fatherhighgrade`在 GPA 或 SAT 分数相似的个体中可能很重要。我们在本章后面使用多元特征选择方法。我们将在下一节中进行更多的单变量特征选择，在这一节中，我们将探索适用于连续目标的选择技术。

# 为回归模型选择特征

`scikit-learn`的选择模块提供了几个选项，用于在构建回归模型时选择特征。(这里的回归模型，我不是指线性回归模型。我只是指有连续目标的*车型*。)两个好的选择是基于 f 检验的选择和基于用于回归的互信息的选择。让我们从 f 检验开始。

## 用连续目标进行特征选择的 f 检验

F 统计量是对目标和单个回归变量之间的线性相关性的强度的度量。`Scikit-learn`有一个`f_regression`评分函数，返回 F 统计。我们可以使用它和`SelectKBest`来选择基于统计的特性。

让我们使用 F 统计量来选择工资模型的特征。在下一节中，我们使用互信息进行回归，以便为同一目标选择特征:

1.  我们从从`feature_engine`和`train_test_split`以及`scikit-learn`导入一键编码器开始。我们还导入`f_regression`来获得 F 统计:

    ```
    import pandas as pd import numpy as np from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import SelectKBest, f_regression
    ```

2.  接下来，我们加载 NLS 数据，包括教育程度、父母收入和工资收入数据:

    ```
    nls97wages = pd.read_csv("data/nls97wages.csv") feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall','gender',   'motherhighgrade','fatherhighgrade','parentincome',   'completedba']
    ```

3.  Then, we create training and testing DataFrames, encode the `gender` feature, and scale the training data. We need to scale the target in this case since it is continuous:

    ```
    X_train, X_test, y_train, y_test =  \
      train_test_split(nls97wages[feature_cols],\
      nls97wages[['wageincome']], test_size=0.3, random_state=0)
    ohe = OneHotEncoder(drop_last=True, variables=['gender'])
    X_train_enc = ohe.fit_transform(X_train)
    scaler = StandardScaler()
    standcols = X_train_enc.iloc[:,:-1].columns
    X_train_enc = \
      pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),
      columns=standcols, index=X_train_enc.index).\
      join(X_train_enc[['gender_Male']])
    y_train = \
      pd.DataFrame(scaler.fit_transform(y_train),
      columns=['wageincome'], index=y_train.index)
    ```

    注意

    您可能已经注意到，我们没有对测试数据进行编码或缩放。我们最终需要这样做来验证我们的模型。我们将在本章后面介绍验证，并在下一章更详细地讨论它。

4.  现在，我们准备好选择特性。我们将`SelectKBest`的`score_func`设置为`f_regression`，并指示我们想要五个最好的特性。`SelectKBest`的`get_support`方法为每个被选择的特征返回`True`:

    ```
    ksel = SelectKBest(score_func=f_regression, k=5) ksel.fit(X_train_enc, y_train.values.ravel()) selcols = X_train_enc.columns[ksel.get_support()] selcols Index(['satmath', 'gpascience', 'parentincome',  'completedba','gender_Male'],       dtype='object')
    ```

5.  我们可以使用`scores_`属性来查看每个特性

    ```
    pd.DataFrame({'score': ksel.scores_,   'feature': X_train_enc.columns},    columns=['feature','score']).\    sort_values(['score'], ascending=False)                 feature              score 1             satmath              45 9             completedba          38 10            gender_Male          26 8 parentincome         24 2             gpascience           21 0             satverbal            19 5             gpaoverall           17 4             gpamath              13 3 gpaenglish           10 6             motherhighgrade       9 7             fatherhighgrade       8
    ```

    的得分

F 统计量的缺点是它假设每个特征和目标之间存在线性关系。当这种假设没有意义时，我们可以使用互信息进行回归。

## 针对连续目标的特征选择的互信息

我们也可以使用`SelectKBest`选择特征，使用互信息进行回归:

1.  我们需要将`SelectKBest`的`score_func`参数设置为`mutual_info_regression`，但是有一点小复杂。为了在每次运行特征选择时得到相同的结果，我们需要设置一个`random_state`值。正如我们在上一节中讨论的，我们可以使用部分函数。我们将`partial(mutual_info_regression, random_state=0)`传递给 score 函数。
2.  然后，我们可以运行`fit`方法，并使用`get_support`来获取选定的特性。我们可以使用`scores_`属性给出每个特性的分数:

    ```
    from functools import partial ksel = SelectKBest(score_func=\   partial(mutual_info_regression, random_state=0),   k=5) ksel.fit(X_train_enc, y_train.values.ravel()) selcols = X_train_enc.columns[ksel.get_support()] selcols Index(['satmath', 'gpascience', 'fatherhighgrade', 'completedba','gender_Male'],dtype='object') pd.DataFrame({'score': ksel.scores_,   'feature': X_train_enc.columns},    columns=['feature','score']).\    sort_values(['score'], ascending=False)            feature               score 1          satmath               0.101 10         gender_Male           0.074 7 fatherhighgrade       0.047 2          gpascience            0.044 9          completedba           0.044 4          gpamath               0.016 8          parentincome          0.015 6 motherhighgrade       0.012 0          satverbal             0.000 3          gpaenglish            0.000 5          gpaoverall            0.000
    ```

我们用互信息回归得到了与 f 检验相当相似的结果。`parentincome`通过 f 检验选择，而`fatherhighgrade`通过互信息选择。否则，将选择相同的特征。

与 f 检验相比，回归的互信息的一个关键优势是，它不假设特征和目标之间的线性关系。如果这种假设被证明是没有根据的，那么相互信息是一种更好的方法。(同样，在评分中也存在一些随机性，每个特征的分数可以在有限的范围内波动。)

注意

我们选择`k=5`来获得五个最佳特性是相当随意的。我们可以通过一些超参数调整使它更加科学。我们将在下一章讨论调音。

到目前为止，我们使用的特征选择方法被称为*过滤方法*。他们检查每个特征和目标之间的单变量关系。它们是一个很好的起点。类似于我们在前面章节中讨论的在我们开始检查多元关系之前准备好相关性的有用性，至少探索一下过滤方法是有帮助的。然而，当其他特征也被包括时，我们的模型拟合通常需要考虑重要或不重要的特征。要做到这一点，我们需要使用包装器或嵌入式方法进行特性选择。我们将在接下来的几节中探讨包装器方法，从向前和向后特性选择开始。

# 使用向前和向后特征选择

前向和后向特征选择，顾名思义，是通过一个接一个地添加特征来选择特征——或者对于后向选择是减去它们——并在每次迭代后评估对模型性能的影响。因为这两种方法都是基于给定的算法来评估性能，所以它们被认为是**包装器**选择方法。

包装器特征选择方法比我们到目前为止探索的过滤方法有两个优点。首先，当包含其他功能时，他们评估功能的重要性。第二，因为特性是基于它们对特定算法性能的贡献来评估的，所以我们可以更好地理解哪些特性最终是重要的。例如，根据我们上一节的结果，`satmath`似乎是一个重要的特性。但是有可能`satmath`只在我们使用特定的模型时才重要，比如线性回归，而不是像决策树回归这样的替代模型。包装器选择方法可以帮助我们发现这一点。

包装方法的主要缺点是它们在计算上非常昂贵，因为它们在每次迭代后都要重新训练模型。在本节中，我们将研究向前和向后特征选择。

## 使用正向特征选择

**正向特征选择**从识别特征子集开始，这些特征分别与目标有显著关系，与过滤方法相似。但是它随后评估所选特征的所有可能的组合，以得到用所选算法执行得最好的组合。

我们可以使用正向特征选择来开发学士学位完成的模型。由于包装器方法要求用户选择一个算法，并且这是一个二进制目标，让我们使用`mlxtend`的`feature_selection`模块来进行选择特性所需的迭代:

1.  我们从导入必要的库开始:

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from mlxtend.feature_selection import SequentialFeatureSelector
    ```

2.  然后，我们再次加载 NLS 数据。我们还创建了一个训练数据帧，编码了`gender`特征，并标准化了剩余的特征:

    ```
    nls97compba = pd.read_csv("data/nls97compba.csv") feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall','gender',   'motherhighgrade','fatherhighgrade','parentincome'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97compba[feature_cols],\   nls97compba[['completedba']], test_size=0.3, random_state=0) ohe = OneHotEncoder(drop_last=True, variables=['gender']) X_train_enc = ohe.fit_transform(X_train) scaler = StandardScaler() standcols = X_train_enc.iloc[:,:-1].columns X_train_enc = \   pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),   columns=standcols, index=X_train_enc.index).\   join(X_train_enc[['gender_Female']])
    ```

3.  我们创建一个随机的森林分类器对象，然后将该对象传递给`mlxtend`的特征选择器。我们指示我们希望它选择五个特性，并且它应该向前选择。(我们也可以使用顺序特征选择器来向后选择。)运行`fit`后，我们可以使用`k_feature_idx_`属性来获取所选特性的列表:

    ```
    rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0) sfs = SequentialFeatureSelector(rfc, k_features=5,   forward=True, floating=False, verbose=2,   scoring='accuracy', cv=5) sfs.fit(X_train_enc, y_train.values.ravel()) selcols = X_train_enc.columns[list(sfs.k_feature_idx_)] selcols Index(['satverbal', 'satmath', 'gpaoverall', 'parentincome', 'gender_Female'], dtype='object')
    ```

您可能还记得本章的第一节，我们对已完成的学士学位目标的单变量特征选择给出了有些不同的结果:

```
Index(['satverbal', 'satmath', 'gpascience',
 'gpaenglish', 'gpaoverall'], dtype='object')
```

其中三个特征——`satmath`、`satverbal`和`gpaoverall`——是相同的。但是我们的正向特征选择已经确定`parentincome`和`gender_Female`比在单变量分析中选择的`gpascience`和`gpaenglish`更重要。事实上，`gender_Female`在早期的分析中得分最低。这些差异可能反映了包装器特征选择方法的优势。我们可以识别不重要的功能，除非包括其他功能，并且我们正在评估对特定算法性能的影响，在这种情况下，随机森林分类。

前向选择的一个缺点是*一旦选择了一个特性，它就不会被删除，即使它的重要性会随着附加特性的增加而下降*。(回想一下，前向特征选择基于该特征对模型的贡献迭代地添加特征。)

让我们看看我们的结果是否随着向后特征选择而变化。

## 使用向后特征选择

向后特征选择从所有特征开始，排除最不重要的特征。然后，它对其余的特征重复这一过程。我们可以使用`mlxtend`的`SequentialFeatureSelector`进行向后选择，就像我们使用它进行向前选择一样。

我们从`scikit-learn`库中实例化一个`RandomForestClassifier`对象，然后将其传递给`mlxtend`的顺序特征选择器:

```
rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)
```

```
sfs = SequentialFeatureSelector(rfc, k_features=5,
```

```
  forward=False, floating=False, verbose=2,
```

```
  scoring='accuracy', cv=5)
```

```
sfs.fit(X_train_enc, y_train.values.ravel())
```

```
selcols = X_train_enc.columns[list(sfs.k_feature_idx_)]
```

```
selcols
```

```
Index(['satverbal', 'gpascience', 'gpaenglish',
```

```
 'gpaoverall', 'gender_Female'], dtype='object')
```

也许不出所料，我们的特征选择会得到不同的结果。`satmath`和`parentincome`不再被选中，而`gpascience`和`gpaenglish`被选中。

向后特征选择的缺点与向前特征选择相反。*一旦一个特征被移除，它就不会被重新评估，即使它的重要性可能随着不同的特征组合而改变*。让我们尝试彻底的特征选择。

# 使用穷举特征选择

如果向前和向后选择的结果没有说服力，并且您不介意在出去喝咖啡或吃午餐时运行模型，则可以尝试穷举特征选择。**详尽的特征选择**在所有可能的特征组合上训练给定的模型，并选择最佳的特征子集。但这是有代价的。顾名思义，这个过程可能会耗尽系统资源和您的耐心。

让我们对学士学位完成模型使用详尽的特征选择:

1.  我们首先加载所需的库，包括来自`scikit-learn`的`RandomForestClassifier`和`LogisticRegression`模块以及来自`mlxtend`的`ExhaustiveFeatureSelector`模块。我们还导入了`accuracy_score`模块，这样我们就可以评估具有所选特性的模型:

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from mlxtend.feature_selection import ExhaustiveFeatureSelector from sklearn.metrics import accuracy_score
    ```

2.  接下来，我们加载 NLS 教育程度数据，并创建培训和测试数据框架:

    ```
    nls97compba = pd.read_csv("data/nls97compba.csv") feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall','gender',   'motherhighgrade','fatherhighgrade','parentincome'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97compba[feature_cols],\   nls97compba[['completedba']], test_size=0.3, random_state=0)
    ```

3.  然后，我们对训练和测试数据进行编码和缩放:

    ```
    ohe = OneHotEncoder(drop_last=True, variables=['gender']) ohe.fit(X_train) X_train_enc, X_test_enc = \   ohe.transform(X_train), ohe.transform(X_test) scaler = StandardScaler() standcols = X_train_enc.iloc[:,:-1].columns scaler.fit(X_train_enc[standcols]) X_train_enc = \   pd.DataFrame(scaler.transform(X_train_enc[standcols]),   columns=standcols, index=X_train_enc.index).\   join(X_train_enc[['gender_Female']]) X_test_enc = \   pd.DataFrame(scaler.transform(X_test_enc[standcols]),   columns=standcols, index=X_test_enc.index).\   join(X_test_enc[['gender_Female']])
    ```

4.  我们创建一个随机森林分类器对象，并将其传递给`mlxtend`的`ExhaustiveFeatureSelector`。我们告诉特征选择器评估一到五个特征的所有组合，并返回在预测学位实现方面具有最高准确度的组合。运行`fit`后，我们可以使用`best_feature_names_`属性来获取选中的特性:

    ```
    rfc = RandomForestClassifier(n_estimators=100, max_depth=2,n_jobs=-1, random_state=0) efs = ExhaustiveFeatureSelector(rfc, max_features=5,   min_features=1, scoring='accuracy',    print_progress=True, cv=5) efs.fit(X_train_enc, y_train.values.ravel()) efs.best_feature_names_ ('satverbal', 'gpascience', 'gpamath', 'gender_Female')
    ```

5.  我们来评价一下这个模型的准确性。我们首先需要转换训练和测试数据，使其只包含四个选定的特性。然后，我们可以使用这些特征再次拟合随机森林分类器，并生成学士学位完成的预测值。然后，我们可以计算正确预测目标的时间百分比，即 67%

    ```
    X_train_efs = efs.transform(X_train) X_test_efs = efs.transform(X_test) rfc.fit(X_train_efs, y_train.values.ravel()) y_pred = rfc.predict(X_test_efs) confusion = pd.DataFrame(y_pred, columns=['pred'],   index=y_test.index).\   join(y_test) confusion.loc[confusion.pred==confusion.completedba].shape[0]\   /confusion.shape[0] 0.6703296703296703
    ```

6.  We get the same answer if we just use scikit-learn's `accuracy score` instead. (We calculate it in the previous step because it is pretty straightforward and it gives us a better sense of what is meant by accuracy in this case.)

    ```
    accuracy_score(y_test, y_pred)
    0.6703296703296703
    ```

    注意

    准确度分数通常用于评估分类模型的性能。在这一章中，我们将依靠它，但是根据您的模型的目的，其他度量可能同样重要或者更重要。例如，我们有时更关心敏感度，即我们正确的阳性预测与实际阳性数量的比率。我们在第 6 章 、*准备模型评估中详细检查分类模型的评估。*

7.  现在让我们用逻辑模型

    ```
    lr = LogisticRegression(solver='liblinear') efs = ExhaustiveFeatureSelector(lr, max_features=5,   min_features=1, scoring='accuracy',    print_progress=True, cv=5) efs.fit(X_train_enc, y_train.values.ravel()) efs.best_feature_names_ ('satmath', 'gpascience', 'gpaenglish', 'motherhighgrade', 'gender_Female')
    ```

    尝试详尽的特征选择
8.  让我们看看逻辑模型的准确性。我们得到一个相当相似的准确度分数:

    ```
    X_train_efs = efs.transform(X_train_enc) X_test_efs = efs.transform(X_test_enc) lr.fit(X_train_efs, y_train.values.ravel()) y_pred = lr.predict(X_test_efs) accuracy_score(y_test, y_pred) 0.6923076923076923
    ```

9.  逻辑模型的一个关键优势是训练速度快得多，这与详尽的特征选择确实有所不同。如果我们对每个模型的训练进行计时(在您的计算机上进行可能不是一个好主意，除非它是一台非常高端的机器或者您不介意离开您的计算机一段时间)，我们会看到平均训练时间的巨大差异——从随机森林的惊人的 5 分钟到逻辑回归的 4 秒钟。(当然，绝对数字是机器相关的。)

    ```
    rfc = RandomForestClassifier(n_estimators=100, max_depth=2,    n_jobs=-1, random_state=0) efs = ExhaustiveFeatureSelector(rfc, max_features=5,   min_features=1, scoring='accuracy',    print_progress=True, cv=5) %timeit efs.fit(X_train_enc, y_train.values.ravel()) 5min 8s ± 3 s per loop (mean ± std. dev. of 7 runs, 1 loop each) lr = LogisticRegression(solver='liblinear') efs = ExhaustiveFeatureSelector(lr, max_features=5,   min_features=1, scoring='accuracy',    print_progress=True, cv=5) %timeit efs.fit(X_train_enc, y_train.values.ravel()) 4.29 s ± 45.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    ```

详尽的特性选择可以为选择特性提供非常清晰的指导，正如我提到的，但是对于许多项目来说，这可能代价太高。它实际上可能更适合于*诊断工作*而不是在机器学习管道中使用。如果线性模型是合适的，它可以大大降低计算成本。

包装器方法，例如向前、向后和穷举特征选择，占用税收系统资源，因为它们需要在每次迭代中被训练，并且所选择的算法越难以实现，这就越是一个问题。**递归特征消除** ( **RFE** )是过滤器方法的简单性和包装器方法提供的更好信息之间的折衷。它类似于向后特征选择，只是它基于模型的整体性能而不是重新评估每个特征，从而简化了每次迭代时的特征移除。我们将在接下来的两节中探讨递归特性选择。

# 递归消除回归模型中的特征

一种流行的包装方法是 RFE。该方法从所有特征开始，移除权重最低的特征(基于系数或特征重要性度量)，并重复该过程，直到识别出最佳拟合模型。当一个特性被删除时，它被赋予一个反映它被删除的点的等级。

RFE 可用于回归模型和分类模型。我们将从在回归模型中使用它开始:

1.  我们导入必要的库，其中三个我们还没有使用:来自`scikit-learn` :

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import RFE from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression
    ```

    的`RFE`、`RandomForestRegressor`和`LinearRegression`模块
2.  接下来，我们加载关于工资的 NLS 数据并创建培训和测试数据框架:

    ```
    nls97wages = pd.read_csv("data/nls97wages.csv") feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall','motherhighgrade',   'fatherhighgrade','parentincome','gender','completedba'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97wages[feature_cols],\   nls97wages[['weeklywage']], test_size=0.3, random_state=0)
    ```

3.  我们需要对`gender`特征进行编码，并标准化其他特征和目标(`wageincome`)。我们不对`completedba`做任何编码或缩放，它是一个二进制特征:

    ```
    ohe = OneHotEncoder(drop_last=True, variables=['gender']) ohe.fit(X_train) X_train_enc, X_test_enc = \   ohe.transform(X_train), ohe.transform(X_test) scaler = StandardScaler() standcols = feature_cols[:-2] scaler.fit(X_train_enc[standcols]) X_train_enc = \   pd.DataFrame(scaler.transform(X_train_enc[standcols]),   columns=standcols, index=X_train_enc.index).\   join(X_train_enc[['gender_Male','completedba']]) X_test_enc = \   pd.DataFrame(scaler.transform(X_test_enc[standcols]),   columns=standcols, index=X_test_enc.index).\   join(X_test_enc[['gender_Male','completedba']]) scaler.fit(y_train) y_train, y_test = \   pd.DataFrame(scaler.transform(y_train),   columns=['weeklywage'], index=y_train.index),\   pd.DataFrame(scaler.transform(y_test),   columns=['weeklywage'], index=y_test.index)
    ```

现在，我们准备做一些递归特征选择。因为 RFE 是一个包装器方法，我们需要选择一个算法，选择将围绕这个算法被包装。在这种情况下，用于回归的随机森林是有意义的。我们正在模拟一个连续的目标，并且不希望假设特征和目标之间的线性关系。

1.  用`scikit-learn`实现 RFE 是相当容易的。我们实例化一个 RFE 对象，告诉它我们在这个过程中需要什么样的估计器。我们表示`RandomForestRegressor`。然后，我们拟合模型，并使用`get_support`获得所选的特征。我们将`max_depth`限制为`2`以避免过度拟合:

    ```
    rfr = RandomForestRegressor(max_depth=2) treesel = RFE(estimator=rfr, n_features_to_select=5) treesel.fit(X_train_enc, y_train.values.ravel()) selcols = X_train_enc.columns[treesel.get_support()] selcols  Index(['satmath', 'gpaoverall', 'parentincome', 'gender_Male', 'completedba'], dtype='object')
    ```

请注意，这给了我们一个与使用过滤方法(带 f 检验)的工资收入目标略有不同的特征列表。这里选择的是`gpaoverall`和`motherhighgrade`，而不是`gender`标志或`gpascience`。

1.  我们可以使用`ranking_`属性来查看每个被删除的特性何时被删除:

    ```
    pd.DataFrame({'ranking': treesel.ranking_,   'feature': X_train_enc.columns},    columns=['feature','ranking']).\    sort_values(['ranking'], ascending=True)            feature                ranking 1          satmath 1 5          gpaoverall             1 8          parentincome           1 9          gender_Male            1 10         completedba            1 6          motherhighgrade 2 2          gpascience             3 0          satverbal              4 3          gpaenglish             5 4          gpamath                6 7          fatherhighgrade 7
    ```

第一次交互后删除`fatherhighgrade`，第二次交互后删除`gpamath`。

1.  让我们运行一些测试统计。我们只在随机森林回归模型上拟合选定的特征。RFE 选择器的`transform`方法通过`treesel.transform(X_train_enc)`只给我们选择的特征。我们可以用`score`方法得到 r 平方值，也就是所谓的决定系数。R-squared 是我们的模型所解释的总变化百分比的度量。我们得到一个很低的分数，表明我们的模型只解释了一点点变化。(请注意，这是一个随机过程，因此我们每次拟合模型时可能会得到不同的结果。)

    ```
    rfr.fit(treesel.transform(X_train_enc), y_train.values.ravel()) rfr.score(treesel.transform(X_test_enc), y_test) 0.13612629794428466
    ```

2.  让我们看看使用 RFE 和线性回归模型是否能得到更好的结果。该模型返回与随机森林回归器相同的特征:

    ```
    lr = LinearRegression() lrsel = RFE(estimator=lr, n_features_to_select=5) lrsel.fit(X_train_enc, y_train) selcols = X_train_enc.columns[lrsel.get_support()] selcols Index(['satmath', 'gpaoverall', 'parentincome', 'gender_Male', 'completedba'], dtype='object')
    ```

3.  我们来评价一下线性模型:

    ```
    lr.fit(lrsel.transform(X_train_enc), y_train) lr.score(lrsel.transform(X_test_enc), y_test) 0.17773742846314056
    ```

线性模型并不比随机森林模型好多少。这很可能是一个迹象，总体来说，我们可用的功能只捕捉到了每周工资变化的一小部分。这是一个重要的提醒，我们可以确定几个重要的特征，但仍然有一个解释能力有限的模型。(也许这也是一个好消息，我们在标准化考试中的分数，甚至我们获得的学位都很重要，但不是多年后我们工资的决定因素。)

让我们试试 RFE 的分类模型。

# 在分类模型中递归地消除特征

对于分类问题，RFE 也是一个很好的选择。我们可以使用 RFE 来选择学士学位完成模型的特征。您可能还记得，我们在本章前面使用了详尽的特性选择来为该模型选择特性。让我们看看我们是否得到了更好的准确性或更容易训练的 RFE 模型:

1.  我们导入到目前为止在本章中使用过的相同库:

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFE from sklearn.metrics import accuracy_score
    ```

2.  接下来，我们从 NLS 教育程度数据

    ```
    nls97compba = pd.read_csv("data/nls97compba.csv") feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall','gender',   'motherhighgrade','fatherhighgrade','parentincome'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97compba[feature_cols],\   nls97compba[['completedba']], test_size=0.3,    random_state=0)
    ```

    中创建培训数据和测试数据
3.  然后，我们对训练和测试数据进行编码和缩放:

    ```
    ohe = OneHotEncoder(drop_last=True, variables=['gender']) ohe.fit(X_train) X_train_enc, X_test_enc = \   ohe.transform(X_train), ohe.transform(X_test) scaler = StandardScaler() standcols = X_train_enc.iloc[:,:-1].columns scaler.fit(X_train_enc[standcols]) X_train_enc = \   pd.DataFrame(scaler.transform(X_train_enc[standcols]),   columns=standcols, index=X_train_enc.index).\   join(X_train_enc[['gender_Female']]) X_test_enc = \   pd.DataFrame(scaler.transform(X_test_enc[standcols]),   columns=standcols, index=X_test_enc.index).\   join(X_test_enc[['gender_Female']])
    ```

4.  我们实例化一个随机森林分类器，并将其传递给RFE 选择方法。然后，我们可以拟合模型并获得选定的特征。

    ```
    rfc = RandomForestClassifier(n_estimators=100, max_depth=2,    n_jobs=-1, random_state=0) treesel = RFE(estimator=rfc, n_features_to_select=5) treesel.fit(X_train_enc, y_train.values.ravel()) selcols = X_train_enc.columns[treesel.get_support()] selcols Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')
    ```

5.  我们还可以通过使用 RFE `ranking_`属性:

    ```
    pd.DataFrame({'ranking': treesel.ranking_,   'feature': X_train_enc.columns},    columns=['feature','ranking']).\    sort_values(['ranking'], ascending=True)              feature                 ranking 0 satverbal               1 1          satmath                 1 2          gpascience              1 3          gpaenglish              1 5          gpaoverall              1 4 gpamath                 2 8          parentincome            3 7          fatherhighgrade         4 6          motherhighgrade         5 9          gender_Female           6
    ```

    来展示特性是如何排序的
6.  让我们来看看模型的准确性，使用我们用于基线模型的相同随机森林分类器来选择特征:

    ```
    rfc.fit(treesel.transform(X_train_enc), y_train.values.ravel()) y_pred = rfc.predict(treesel.transform(X_test_enc)) accuracy_score(y_test, y_pred) 0.684981684981685
    ```

回想一下，我们在详尽的特征选择中有 67%的准确率。我们在这里得到了同样的精度。RFE 的好处是它比穷举特征选择更容易训练。

包装器和类似包装器的特征选择方法中的另一个选择是`scikit-learn`集成方法。在下一节中，我们将它与`scikit-learn`的随机森林分类器一起使用。

# 使用 Boruta 进行特征选择

Boruta 包采用了一种独特的方法来进行特性选择，尽管它与包装器方法有一些相似之处。对于每个特征，Boruta 创建一个阴影特征，它具有与原始特征相同的值范围，但是具有混合的值。然后，它会评估原始要素是否比阴影要素提供更多信息，逐渐移除提供最少信息的要素。在每次迭代中，Boruta 输出确认的、试验的和拒绝的特征。

让我们使用 Boruta 包为学士学位完成的分类模型选择特性(如果您还没有安装，您可以使用`pip`安装 Boruta 包):

1.  我们首先加载必要的库:

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from boruta import BorutaPy from sklearn.metrics import accuracy_score
    ```

2.  我们再次加载 NLS 教育成绩数据，并创建培训和测试数据框架:

    ```
    nls97compba = pd.read_csv("data/nls97compba.csv") feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall','gender',   'motherhighgrade','fatherhighgrade','parentincome'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97compba[feature_cols],\   nls97compba[['completedba']], test_size=0.3, random_state=0)
    ```

3.  接下来，我们对训练和测试数据进行编码和缩放:

    ```
    ohe = OneHotEncoder(drop_last=True, variables=['gender']) ohe.fit(X_train) X_train_enc, X_test_enc = \   ohe.transform(X_train), ohe.transform(X_test) scaler = StandardScaler() standcols = X_train_enc.iloc[:,:-1].columns scaler.fit(X_train_enc[standcols]) X_train_enc = \   pd.DataFrame(scaler.transform(X_train_enc[standcols]),   columns=standcols, index=X_train_enc.index).\   join(X_train_enc[['gender_Female']]) X_test_enc = \   pd.DataFrame(scaler.transform(X_test_enc[standcols]),   columns=standcols, index=X_test_enc.index).\   join(X_test_enc[['gender_Female']])
    ```

4.  我们运行博鲁塔特征选择的方式与运行 RFE 特征选择的方式大致相同。我们再次使用随机森林作为基线方法。我们实例化一个随机森林分类器，并将其传递给 Boruta 的特征选择器。然后我们拟合模型，模型在`100`迭代中停止，识别提供信息的`9`特征:

    ```
    rfc = RandomForestClassifier(n_estimators=100,    max_depth=2, n_jobs=-1, random_state=0) borsel = BorutaPy(rfc, random_state=0, verbose=2) borsel.fit(X_train_enc.values, y_train.values.ravel()) BorutaPy finished running. Iteration: 100 / 100 Confirmed:            9 Tentative:            1 Rejected: 0 selcols = X_train_enc.columns[borsel.support_] selcols Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpamath', 'gpaoverall', 'motherhighgrade', 'fatherhighgrade', 'parentincome', 'gender_Female'], dtype='object')
    ```

5.  我们可以使用`ranking_`属性来查看特性的排名:

    ```
    pd.DataFrame({'ranking': borsel.ranking_,   'feature': X_train_enc.columns},    columns=['feature','ranking']).\    sort_values(['ranking'], ascending=True)            feature               ranking 0          satverbal             1 1          satmath               1 2          gpascience            1 3          gpaenglish            1 4 gpamath               1 5          gpaoverall            1 6          motherhighgrade       1 7          fatherhighgrade       1 8          parentincome          1 9 gender_Female         2
    ```

6.  为了评估模型的准确性，我们仅用选择的特征来拟合随机森林分类器模型。然后我们可以对测试数据进行预测并计算准确度:

    ```
    rfc.fit(borsel.transform(X_train_enc.values), y_train.values.ravel()) y_pred = rfc.predict(borsel.transform(X_test_enc.values)) accuracy_score(y_test, y_pred) 0.684981684981685
    ```

Boruta 的部分吸引力在于其对每个功能的选择具有说服力。如果已经选择了一个特征，那么它很可能确实提供了未被排除它的特征组合所捕获的信息。然而，与穷举特征选择不同，这在计算上相当昂贵。它可以帮助我们找出哪些特性是重要的，但它可能并不总是适合训练速度很重要的管道。

最后几节已经展示了包装器特性选择方法的一些优点和一些缺点。我们将在下一节探讨嵌入式选择方法。这些方法提供了比过滤方法更多的信息，但没有包装方法的计算成本。他们通过将特征选择嵌入到训练过程中来做到这一点。我们将使用到目前为止已经处理过的相同数据来探索嵌入式方法。

# 使用正则化和其他嵌入方法

**正则化**方法是嵌入式方法。像包装器方法一样，嵌入式方法评估与给定算法相关的特性。但是它们在计算上并不昂贵。这是因为特征选择已经内置于算法中，并且在模型被训练时发生。

嵌入式模型使用以下过程:

1.  训练一个模特。
2.  估计每个特征对模型预测的重要性。
3.  移除不重要的功能。

正则化通过向任何模型添加惩罚来约束参数，从而实现这一点。 **L1 正则化**，也被称为**套索正则化**，将回归模型中的一些系数缩小到 0，有效地消除了那些特征。

## 使用 L1 正则化

1.  我们将使用 L1 正则化和逻辑回归来选择学士学位获得模型的特征:我们需要首先导入所需的库，包括我们将第一次使用的模块，来自`scikit-learn` :

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import SelectFromModel from sklearn.metrics import accuracy_score
    ```

    的`SelectFromModel`
2.  接下来，我们加载 NLS 的教育程度数据:

    ```
    nls97compba = pd.read_csv("data/nls97compba.csv") feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall','gender',   'motherhighgrade','fatherhighgrade','parentincome'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97compba[feature_cols],\   nls97compba[['completedba']], test_size=0.3,    random_state=0)
    ```

3.  然后，我们对进行编码，并对训练和测试数据进行缩放:

    ```
    ohe = OneHotEncoder(drop_last=True,                      variables=['gender']) ohe.fit(X_train) X_train_enc, X_test_enc = \   ohe.transform(X_train), ohe.transform(X_test) scaler = StandardScaler() standcols = X_train_enc.iloc[:,:-1].columns scaler.fit(X_train_enc[standcols]) X_train_enc = \   pd.DataFrame(scaler.transform(X_train_enc[standcols]),   columns=standcols, index=X_train_enc.index).\   join(X_train_enc[['gender_Female']]) X_test_enc = \   pd.DataFrame(scaler.transform(X_test_enc[standcols]),   columns=standcols, index=X_test_enc.index).\   join(X_test_enc[['gender_Female']])
    ```

4.  现在，我们准备基于具有 L1 惩罚的逻辑回归进行特征选择:

    ```
    lr = LogisticRegression(C=1, penalty="l1",                          solver='liblinear') regsel = SelectFromModel(lr, max_features=5) regsel.fit(X_train_enc, y_train.values.ravel()) selcols = X_train_enc.columns[regsel.get_support()] selcols Index(['satmath', 'gpascience', 'gpaoverall',  'fatherhighgrade', 'gender_Female'], dtype='object')
    ```

5.  我们来评价一下模型的准确性。我们得到了`0.68` :

    ```
    lr.fit(regsel.transform(X_train_enc),         y_train.values.ravel()) y_pred = lr.predict(regsel.transform(X_test_enc)) accuracy_score(y_test, y_pred) 0.684981684981685
    ```

    的准确度分数

这为我们提供了与学士学位完成前向特征选择非常相似的结果。在那个例子中，我们使用随机森林分类器作为包装器方法。

在这种情况下，套索正则化对于特征选择是一个很好的选择，特别是当性能是一个关键问题时。然而，它确实假设了特征和目标之间的线性关系，这可能是不合适的。幸运的是，有一些嵌入式特征选择方法不会做出这种假设。对于嵌入式模型，逻辑回归的一个很好的替代方案是随机森林分类器。我们用同样数据尝试下一步。

## 使用随机森林分类器

在本节中，让我们使用一个随机森林分类器:

1.  我们可以使用`SelectFromModel`来使用随机森林分类器，而不是逻辑回归:

    ```
    rfc = RandomForestClassifier(n_estimators=100,    max_depth=2, n_jobs=-1, random_state=0) rfcsel = SelectFromModel(rfc, max_features=5) rfcsel.fit(X_train_enc, y_train.values.ravel()) selcols = X_train_enc.columns[rfcsel.get_support()] selcols Index(['satverbal', 'gpascience', 'gpaenglish',    'gpaoverall'], dtype='object')
    ```

这实际上从套索回归中选择了非常不同的特征。`satmath`、`fatherhighgrade`和`gender_Female`不再被选中，而`satverbal`和`gpaenglish`被选中。这可能部分是由于线性假设的放宽。

1.  让我们评估随机森林分类器模型的准确性。我们得到了 0.67 的准确度分数。这和我们用套索回归得到的分数差不多:

    ```
    rfc.fit(rfcsel.transform(X_train_enc),          y_train.values.ravel()) y_pred = rfc.predict(rfcsel.transform(X_test_enc)) accuracy_score(y_test, y_pred) 0.673992673992674
    ```

嵌入式方法通常比包装方法更少占用 CPU/GPU 资源，但仍然可以产生良好的结果。使用本节中的学士学位完成模型，我们可以获得与基于详尽特征选择的模型相同的准确性。

正如我们所讨论的，到目前为止，我们讨论的每种方法都有重要的用例。然而，我们还没有真正讨论一个非常具有挑战性的特征选择问题。如果你有太多的特性，其中许多独立地解释了你的模型中的一些重要的东西，你会怎么做？这里的“太多”是指有太多的特征使得模型无法有效运行，无论是训练还是预测目标值。我们如何在不牺牲模型预测能力的情况下减少特征集？在这种情况下，**主成分分析** ( **PCA** )可能是一个不错的方法。我们将在下一节讨论 PCA。

# 使用主成分分析

迄今为止，与我们讨论的任何方法相比，一种非常不同的特征选择方法是 PCA。PCA 允许我们用有限数量的组件替换现有的特性集，每个组件都解释了一个重要的差异量。它通过查找获取最大差异量的组件，然后是获取最大剩余差异量的第二个组件，然后是第三个组件，依此类推。这种方法的一个主要优点是，这些被称为**主成分**的成分是不相关的。我们在 [*第十五章*](B17978_15_ePub.xhtml#_idTextAnchor170) 、*主成分分析*中详细讨论 PCA。

虽然我在这里将 PCA 作为一种特征选择方法，但更好的做法可能是将其视为一种降维工具。当我们需要在不牺牲太多解释能力的情况下限制维数时，我们使用它来进行特征选择。

让我们再次使用 NLS 的数据，并使用主成分分析来选择学士学位完成模型的特征:

1.  我们从加载必要的库开始。本章中我们唯一没有用到的模块是`scikit-learn`的`PCA` :

    ```
    import pandas as pd from feature_engine.encoding import OneHotEncoder from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score
    ```

2.  接下来，我们再次创建训练和测试数据帧:

    ```
    nls97compba = pd.read_csv("data/nls97compba.csv") feature_cols = ['satverbal','satmath','gpascience',   'gpaenglish','gpamath','gpaoverall','gender',   'motherhighgrade', 'fatherhighgrade','parentincome'] X_train, X_test, y_train, y_test =  \   train_test_split(nls97compba[feature_cols],\   nls97compba[['completedba']], test_size=0.3,   random_state=0)
    ```

3.  我们需要对数据进行缩放和编码。缩放比例对 PCA 特别重要:

    ```
    ohe = OneHotEncoder(drop_last=True,                      variables=['gender']) ohe.fit(X_train) X_train_enc, X_test_enc = \   ohe.transform(X_train), ohe.transform(X_test) scaler = StandardScaler() standcols = X_train_enc.iloc[:,:-1].columns scaler.fit(X_train_enc[standcols]) X_train_enc = \   pd.DataFrame(scaler.transform(X_train_enc[standcols]),   columns=standcols, index=X_train_enc.index).\   join(X_train_enc[['gender_Female']]) X_test_enc = \   pd.DataFrame(scaler.transform(X_test_enc[standcols]),   columns=standcols, index=X_test_enc.index).\   join(X_test_enc[['gender_Female']])
    ```

4.  现在，我们实例化一个`PCA`对象并拟合一个模型:

    ```
    pca = PCA(n_components=5) pca.fit(X_train_enc)
    ```

5.  `PCA`对象的`components_`属性返回 5 个组件中每一个的所有 10 个特性的分数。最能驱动第一部分的特征是那些具有最高绝对值的分数的特征。在这种情况下，就是`gpaoverall`、`gpaenglish`和`gpascience`。对于第二个组件，最重要的特征是`motherhighgrade`、`fatherhighgrade`和`parentincome`。`satverbal`和`satmath`驱动第三组件。

在以下输出中，列 **0** 到 **4** 是五个主要组件:

```
pd.DataFrame(pca.components_,
  columns=X_train_enc.columns).T
 0 1      2       3       4
satverbal         -0.34   -0.16  -0.61   -0.02   -0.19
satmath           -0.37 -0.13  -0.56    0.10    0.11
gpascience        -0.40    0.21   0.18    0.03    0.02
gpaenglish        -0.40 0.22   0.18    0.08   -0.19
gpamath           -0.38    0.24   0.12    0.08    0.23
gpaoverall        -0.43 0.25   0.23   -0.04   -0.03
motherhighgrade   -0.19   -0.51   0.24   -0.43   -0.59
fatherhighgrade   -0.20 -0.51   0.18   -0.35    0.70
parentincome      -0.16   -0.46   0.28    0.82   -0.08
gender_Female     -0.02 0.08   0.12   -0.04   -0.11
```

理解这些分数的另一种方式是，它们表明每个特性对组件的贡献有多大。(实际上，如果你对每个部分的 10 个分数进行平方，然后对平方求和，你总共得到 1 分。)

1.  让我们看看每个组件解释了多少特性差异。仅第一个因素就占了方差的 46%,其次是第二个因素的 19%。我们可以使用 NumPy 的`cumsum`方法来查看这五个组成部分累计解释了多少特征方差。我们可以用 5 个部分解释 10 个特征中 87%的变化:

    ```
    pca.explained_variance_ratio_ array([0.46073387, 0.19036089, 0.09295703, 0.07163009, 0.05328056]) np.cumsum(pca.explained_variance_ratio_) array([0.46073387, 0.65109476, 0.74405179, 0.81568188, 0.86896244])
    ```

2.  让我们根据这五个主要成分来转换测试数据中的特征。这将返回一个只有五个主要成分的 NumPy 数组。我们看前几行。我们还需要转换测试数据帧:

    ```
    X_train_pca = pca.transform(X_train_enc) X_train_pca.shape (634, 5) np.round(X_train_pca[0:6],2) array([[ 2.79, -0.34, 0.41,  1.42, -0.11],        [-1.29,  0.79,  1.79, -0.49, -0.01],        [-1.04, -0.72, -0.62, -0.91,  0.27],        [-0.22, -0.8 , -0.83, -0.75,  0.59], [ 0.11, -0.56, 1.4 ,  0.2 , -0.71],        [ 0.93,  0.42, -0.68, -0.45, -0.89]]) X_test_pca = pca.transform(X_test_enc)
    ```

我们现在可以用这些主要成分来拟合一个学士学位完成模型。让我们运行一个随机森林分类。

1.  我们首先创建一个随机森林分类器对象。然后，我们将带有主成分和目标值的训练数据传递给它的`fit`方法。我们将带有组件的测试数据传递给分类器的`predict`方法，然后得到一个准确度分数:

    ```
    rfc = RandomForestClassifier(n_estimators=100,    max_depth=2, n_jobs=-1, random_state=0) rfc.fit(X_train_pca, y_train.values.ravel()) y_pred = rfc.predict(X_test_pca) accuracy_score(y_test, y_pred) 0.7032967032967034
    ```

当特征选择的挑战是我们具有高度相关的特征，并且我们希望在不显著减少解释的方差的情况下减少维数时，诸如 PCA 之类的降维技术可能是一个好的选择。在这个例子中，高中 GPA 特征一起移动，父母教育和收入水平以及 SAT 特征也一起移动。它们成为我们前三个组件的关键特性。(可以认为我们的模型可能只有这三个组成部分，因为它们共同解释了特征方差的 74%。)

根据您的数据和建模目标，PCA 有几个可能有用的修改。这包括处理异常值和正则化的策略。通过使用核，PCA 还可以扩展到分量不是线性可分的情况。我们在 [*第十五章*](B17978_15_ePub.xhtml#_idTextAnchor170) *、* *主成分分析*中详细讨论 PCA。

让我们总结一下本章所学的内容。

# 总结

在这一章中，我们讨论了一系列的特性选择方法，从过滤器到包装器再到嵌入式方法。我们还看到了它们是如何处理分类目标和连续目标的。对于包装器和嵌入式方法，我们考虑了它们如何与不同的算法一起工作。

过滤方法非常容易运行和解释，并且不占用系统资源。但是，在评估每个特性时，他们不会考虑其他特性。他们也没有告诉我们这种评估会因所使用的算法而有所不同。包装方法没有这些限制，但是它们的计算量很大。嵌入式方法通常是一个很好的折衷方案，它基于多元关系和给定的算法来选择特性，而不会像包装方法那样耗费太多的系统资源。我们还探索了降维方法 PCA 如何改进我们的特征选择。

你可能也注意到了，我在这一章中加入了一点模型验证。我们将在下一章更详细地讨论模型验证。