<title>Chapter 7: Linear Regression Models</title>

# *第七章*:线性回归模型

线性回归可能是最著名的机器学习算法，起源于至少 200 年前的统计学习。如果你在大学选修了统计学、计量经济学或心理计量学课程，你很可能会接触到线性回归，即使你在本科课程中教授机器学习之前很久就选修了这门课程。事实证明，许多社会和物理现象可以成功地建模为预测变量的线性组合的函数。这对于机器学习来说，就像这些年来对于统计学习一样有用，不过，对于机器学习来说，我们对参数值的关心远不如对预测的关心。

假设我们的特征和目标具有一定的质量，线性回归是建模连续目标的非常好的选择。在本章中，我们将回顾线性回归模型的假设，并使用与这些假设基本一致的数据构建一个模型。然而，我们也将探索替代方法，如非线性回归，当这些假设不成立时，我们使用。我们将通过研究解决过度拟合可能性的技术(如套索回归)来结束本章。

在本章中，我们将讨论以下主题:

*   关键概念
*   线性回归和梯度下降
*   使用经典线性回归
*   使用套索回归
*   使用非线性回归
*   梯度下降回归

# 技术要求

在这一章中，我们将坚持使用 Python 大多数科学发行版中可用的库——NumPy、pandas 和 scikit-learn。本章的代码可以在本书的 GitHub 知识库中找到，网址是[https://GitHub . com/packt publishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning)。

# 关键概念

典型的分析师已经做了一段时间的预测建模，这些年来已经构建了几十个，也许几百个线性回归模型。如果你在 20 世纪 80 年代末为一家大型会计公司工作，就像我一样，你正在做预测，你可能会花一整天的时间，每天都在指定线性模型。您将运行所有可能的自变量排列和因变量转换，并努力寻找异方差(残差中的非恒定方差)或多重共线性(高度相关的特征)的证据。但最重要的是，你努力识别关键预测变量，并解决参数估计中的任何偏差(你的系数或权重)。

## 线性回归模型的关键假设

这种努力的大部分今天仍然适用，尽管现在更强调预测的准确性而不是参数估计。我们现在担心过度拟合，这是 30 年前所没有的。当线性回归模型的假设被违反时，我们也更有可能寻求替代方案。这些假设如下:

*   特征(自变量)和目标(因变量)之间存在线性关系
*   残差(实际值和预测值之差)是正态分布的
*   残差在观测值之间是独立的
*   残差的方差是常数

这些假设中的一个或多个被现实世界的数据违反是很常见的。特征和目标之间的关系通常不是线性的。该特征的影响可以在该特征的范围内变化。任何熟悉“厨房里有太多厨师”这一说法的人都可能意识到，第五个厨师的边际生产率增长可能没有第二个或第三个厨师的那么大。

我们的残差有时不是正态分布的。这可能表明我们的模型在目标的某些范围内不太准确。例如，在目标范围的中间有较小的残差，比如 25 到 75 个百分点，而在极端情况下有较高的残差，这并不罕见。当与目标的关系是非线性时，这可能发生。

残差可能不独立有几个原因。时间序列数据通常就是这种情况。对于每日股票价格的模型，相邻几天的残差可能是相关的。这被称为自相关。这也可能是纵向或重复测量数据的问题。例如，我们可能有 20 个不同教室的 600 名学生的考试成绩或 100 人的年收入。如果我们的模型不能解释一个群体中的某些特征没有变化，我们的残差就不会是独立的，在这些例子中，这些特征是教室决定的和个人决定的。

最后，我们的残差在一个特征的不同范围内具有更大的可变性是很常见的。如果我们预测世界各地气象站的温度，并且纬度是我们正在使用的特征之一，那么纬度值越高，残差就越大。这就是所谓的异方差。这也可能是我们的模型遗漏了重要预测因子的一个指标。

除了这四个关键假设，线性回归的另一个常见挑战是特性之间的高度相关性。这就是所谓的多重共线性。正如我们在 [*第 5 章*](B17978_05_ePub.xhtml#_idTextAnchor058) 、*特征选择*中所讨论的，当我们的模型难以隔离特定特征的独立影响时，我们很可能会增加过度拟合的风险，因为它会随着另一个特征移动太多。这对于那些花了几周时间构建模型的人来说应该是熟悉的，在模型中，系数会随着每个新的规格而显著变化。

当这些假设中的一个或多个被违反时，我们仍然可以使用传统的回归模型。然而，我们可能需要以某种方式转换数据。我们将在本章中讨论识别违反这些假设的技术，这些违反对模型性能的影响,以及解决这些问题的可能方法。

## 线性回归和普通最小二乘法

线性回归最常用的估计技术是**普通最小二乘法** ( **OLS** )。OLS 选择使实际目标值和预测值之间的距离平方和最小的系数。更准确地说，OLS 最大限度地减少了以下内容:

![](img/B17978_07_001.jpg)

这里，![](img/B17978_07_002.png)是第 I 次观察时的实际值，![](img/B17978_07_003.png)是预测值。正如我们所讨论的，实际目标值和预测目标值之间的差值![](img/B17978_07_004.png)被称为残差。

从图形上看，OLS 拟合了一条穿过我们的数据的线，该线使数据点与该线的垂直距离最小化。下图说明了具有一个特征的模型，称为简单线性回归，具有虚构的数据点。每个数据点和回归线之间的垂直距离是残差，可以是正的，也可以是负的:

![Figure 7.1 – Ordinary least squares regression line

](img/B17978_07_0011.jpg)

图 7.1–普通最小二乘回归线

这条线![](img/B17978_07_005.png)给出了 *x* 的每个值的 *y* 的预测值。它等于估计截距![](img/B17978_07_006.png)，加上特征的估计系数乘以特征值![](img/B17978_07_007.png)。这是 OLS 线。穿过数据的任何其他直线将导致更高的残差平方和。这可以扩展到多元线性回归模型——即具有多个特征的模型:

![](img/B17978_07_008.jpg)

这里， *y* 是目标，每个 *x* 是特征，每个![](img/B17978_07_009.png)是系数(或截距)， *n* 是特征数， *ɛ* 是误差项。每个系数是从相关特征的 1 个单位变化估计的目标变化。这是注意到系数在每个特征的整个范围内是恒定的好地方；也就是说，假设特征从 0 到 1 的增加对目标具有与从 999 到 1000 相同的影响。然而，这并不总是有意义的。在本章的后面，我们将讨论当一个特征和目标之间的关系不是线性时，如何使用变换。

线性回归的一个重要优势是它不像其他监督回归算法那样计算量大。当线性回归表现良好时，基于我们在前一章中讨论的那些度量，它是一个好的选择。当您有大量数据需要训练，或者您的业务流程不允许大量时间用于模型训练时，这一点尤为重要。该算法的效率还可以使使用更多资源密集型特征选择技术变得可行，例如我们在 [*第 5 章*](B17978_05_ePub.xhtml#_idTextAnchor058) 、*特征选择*中讨论的包装器方法。正如我们在那里看到的，您可能不希望使用决策树回归器进行详尽的特征选择。然而，用线性回归模型可能完全没问题。

# 线性回归和梯度下降

我们可以使用梯度下降，而不是普通的最小二乘法，来估计我们的线性回归参数。梯度下降在可能的系数值上迭代，以找到最小化残差平方和的那些系数值。它从随机的系数值开始，计算该迭代的误差平方和。然后，它为系数生成新值，这些新值产生的残差小于上一步产生的残差。当使用梯度下降时，我们指定一个学习率。学习率决定了每一步残差的改善程度。

当处理非常大的数据集时，梯度下降通常是一个不错的选择。如果整个数据集不适合您的计算机内存，这可能是唯一的选择。在下一节中，我们将使用 OLS 和梯度下降来估计我们的参数。

# 使用经典线性回归

在本节中，我们将指定一个相当简单的线性模型。我们将根据几个国家的经济和政治措施，用它来预测一个国家隐含的汽油税。但是在我们指定我们的模型之前，我们需要完成我们在本书前几章中讨论过的预处理任务。

## 为我们的回归模型预处理数据

在本章以及本书的其余部分，我们将使用管道来预处理数据。我们需要估算缺失值，识别和处理异常值，并对数据进行编码和缩放。我们还需要以一种避免数据泄漏和清理训练数据而不提前查看测试数据的方式来做到这一点。正如我们在 [*第 6 章*](B17978_06_ePub.xhtml#_idTextAnchor078) 、*准备模型评估*中看到的，scikit-learn 的管道可以帮助完成这些任务。

我们将使用的数据集包含每个国家的隐含汽油税和一些可能的预测因素，包括人均国民收入、政府债务、燃料收入依赖、汽车使用程度以及民主进程和政府效率的衡量标准。

注意

这个按国家划分的隐含汽油税数据集可以在位于 https://dataverse.harvard.edu/dataset.xhtml?的哈佛数据库上公开使用 persistent id = doi:10.7910/DVN/rx 4 jgk。由*帕莎·马赫达维*、*塞萨尔·b·马丁内斯-阿尔瓦雷斯*、*迈克尔·罗斯*整理而成。隐含的汽油税是根据一升汽油的世界基准价格和当地价格之间的差额计算的。高于基准价格的当地价格代表一种税。当基准价格较高时，可以认为是一种补贴。我们将使用每个国家 2014 年的数据进行分析。

让我们从预处理数据开始:

1.  首先，我们加载了许多我们在前几章中使用过的库。然而，我们还需要两个新的库来为我们的数据构建管道—`ColumnTransformer`和`TransformedTargetRegressor`。这些库允许我们建立一个管道，对数字和分类特征进行不同的预处理，并且转换我们的目标:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.impute import SimpleImputer from sklearn.pipeline import make_pipeline from sklearn.compose import ColumnTransformer from sklearn.compose import TransformedTargetRegressor from sklearn.feature_selection import RFE from sklearn.impute import KNNImputer from sklearn.model_selection import cross_validate, KFold import sklearn.metrics as skmet import matplotlib.pyplot as plt
    ```

2.  我们可以通过添加自己的类来扩展 scikit-learn 管道的功能。让我们添加一个名为`OutlierTrans`的类来处理极值。

要在管道中包含这个类，它必须从`BaseEstimator`类继承。我们也必须继承`TransformerMixin`，尽管还有其他的可能性。我们的类需要`fit`和`transform`方法。我们可以在`transform`方法中添加指定缺失极值的代码。

但是在使用我们的类之前，我们需要导入它。为了导入它，我们需要添加`helperfunctions`子文件夹，因为在那里我们放置了包含我们的类的`preprocfunc`模块:

```
import os
import sys
sys.path.append(os.getcwd() + “/helperfunctions”)
from preprocfunc import OutlierTrans
```

这个导入了`OutlierTrans`类，我们可以将它添加到我们创建的管道中:

```
class OutlierTrans(BaseEstimator,TransformerMixin):
  def __init__(self,threshold=1.5):
    self.threshold = threshold

  def fit(self,X,y=None):
    return self

  def transform(self,X,y=None):
    Xnew = X.copy()
    for col in Xnew.columns:
      thirdq, firstq = Xnew[col].quantile(0.75),\
        Xnew[col].quantile(0.25)
      interquartilerange = self.threshold*(thirdq-firstq)
      outlierhigh, outlierlow = interquartilerange+thirdq,\
        firstq-interquartilerange
      Xnew.loc[(Xnew[col]>outlierhigh) | \
        (Xnew[col]<outlierlow),col] = np.nan
    return Xnew.values
```

`OutlierTrans`类使用相当标准的单变量方法来识别异常值。它计算每个特征的四分位数间距 ( **IQR** )，然后将第三个四分位数以上或第一个四分位数以下大于 IQR 1.5 倍的任何值设置为缺失。如果我们想更保守一些，我们可以将阈值改为 1.5 以外的值，例如 2.0。(我们在 [*第 1 章*](B17978_01_ePub.xhtml#_idTextAnchor014) 、*检查特征和目标的分布*中讨论了识别异常值的技术。)

1.  接下来，我们加载 2014 年的汽油税数据。数据框中有 154 行，每个国家一行。一些特性有一些缺失值，但是只有一个特性`motorization_rate`有两位数的缺失值。`motorization_rate`是人均汽车数量:

    ```
    fftaxrate14 = pd.read_csv(“data/fossilfueltaxrate14.csv”) fftaxrate14.set_index(‘countrycode’, inplace=True) fftaxrate14.info() <class ‘pandas.core.frame.DataFrame’> Index: 154 entries, AFG to ZWE Data columns (total 19 columns): #   Column                     Non-Null Count  Dtype                                --------------  -----   0   country                    154 non-null    object 1   region                     154 non-null    object 2   region_wb                  154 non-null    object 3   year                       154 non-null    int64 4   gas_tax_imp                154 non-null    float64 5   bmgap_diesel_spotprice_la  146 non-null    float64 6   fuel_income_dependence     152 non-null    float64 7   national_income_per_cap    152 non-null    float64 8   VAT_Rate                   151 non-null    float64 9   gov_debt_per_gdp           139 non-null    float64 10  polity                     151 non-null    float64 11  democracy_polity           151 non-null    float64 12  autocracy_polity           151 non-null    float64 13  goveffect                  154 non-null    float64 14  democracy_index            152 non-null    float64 15  democracy                  154 non-null    int64 16  nat_oil_comp               152 non-null    float64 17  nat_oil_comp_state         152 non-null    float64 18  motorization_rate          127 non-null    float64 dtypes: float64(14), int64(2), object(3) memory usage: 24.1+ KB
    ```

2.  让我们把特征分成数字特征和二进制特征。我们将把`motorization_rate`放在一个特殊的类别中，因为我们预计它会比其他特性做得更多一些:

    ```
    num_cols = [‘fuel_income_dependence’,    ’national_income_per_cap’, ‘VAT_Rate’,     ‘gov_debt_per_gdp’, ’polity’, ’goveffect’,   ‘democracy_index’] dummy_cols = ‘democracy_polity’,’autocracy_polity’,   ‘democracy’,’nat_oil_comp’,’nat_oil_comp_state’] spec_cols = [‘motorization_rate’]
    ```

3.  我们应该看一些数字特征和目标的描述符。我们的目标`gas_tax_imp`的中值为 0.52。请注意，有些功能的范围非常不同。超过一半的国家的`polity`得分为 7 分或更高；10 分是最高的`polity`分，意味着最民主。大多数国家的政府效率都是负值。`democracy_index`与`polity`非常相似，尽管有更多的变化:

    ```
    fftaxrate14[[‘gas_tax_imp’] + num_cols + spec_cols].\   agg([‘count’,’min’,’median’,’max’]).T                       count min    median   max gas_tax_imp             154 -0.80  0.52     1.73 fuel_income_dependence  152 0.00   0.14     34.43 national_income_per_cap 152 260.00 6,050.00 104,540.00 VAT_Rate                151 0.00   16.50    27.00 gov_debt_per_gdp        139 0.55   39.30    194.76 polity                  151 -10.00 7.00     10.00 goveffect               154 -2.04  -0.15    2.18 democracy_index         152 0.03   0.57     0.93 motorization_rate       127 0.00   0.20     0.81
    ```

4.  让我们也看看二进制特征的分布。我们必须将`normalize`设置为`True`来生成比率而不是计数。`democracy_polity`和`autocracy_polity`特征只是`polity`特征的二进制版本；非常高的`polity`分数得到的`democracy_polity`值为 1，而非常低的`polity`分数得到的`autocracy_polity`值为 1。同样，`democracy`是那些具有高`democracy_index`值的国家的虚拟特征。有趣的是，将近一半的国家(0.46)有国家石油公司，几乎四分之一的国家(0.23)有国有国家石油公司:

    ```
    fftaxrate14[dummy_cols].apply(pd.value_counts, normalize=True).T                                0               1 democracy_polity               0.41            0.59 autocracy_polity               0.89            0.11 democracy                      0.42            0.58 nat_oil_comp                   0.54            0.46 nat_oil_comp_state             0.77            0.23
    ```

这一切看起来都相当不错。但是，我们需要对几个特性的缺失值做一些工作。我们还需要做一些缩放，但是不需要做任何编码，因为我们可以使用二进制特性。有些特征是相关的，所以我们需要做一些特征消除。

1.  我们从创建训练和测试数据帧开始我们的预处理。我们将只保留 20%用于测试:

    ```
    target = fftaxrate14[[‘gas_tax_imp’]] features = fftaxrate14[num_cols + dummy_cols + spec_cols] X_train, X_test, y_train, y_test =  \   train_test_split(features,\   target, test_size=0.2, random_state=0)
    ```

2.  我们需要用列转换构建一个管道，这样我们就可以对数字和分类数据进行不同的预处理。我们将为`num_cols`中的所有数字列构建一个管道`standtrans`。首先，我们希望将离群值设置为 missing。对于该特征，我们将异常值定义为高于第三四分位数或低于第一四分位数的四分位数间距的两倍以上。我们将使用`SimpleImputer`将缺失值设置为中值。

我们不想缩放`dummy_cols`中的二进制特征，但是我们想使用`SimpleImputer`将缺失值设置为每个分类特征的最频繁值。

我们不会用`SimpleImputer`换`motorization_rate`。记住`motorization_rate`不在`num_cols`列表中——它在`spec_cols`列表中。我们为`spec_cols`中的特性设置了一个特殊的管道`spectrans`。我们将使用`motorization_rate`值:

```
standtrans = make_pipeline(OutlierTrans(2), 
  SimpleImputer(strategy=”median”), StandardScaler())
cattrans = make_pipeline(
  SimpleImputer(strategy=”most_frequent”))
spectrans = make_pipeline(OutlierTrans(2), 
  StandardScaler())
coltrans = ColumnTransformer(
  transformers=[
    (“stand”, standtrans, num_cols),
    (“cat”, cattrans, dummy_cols),
    (“spec”, spectrans, spec_cols)
  ]
)
```

这设置了我们想要对汽油税数据进行的所有预处理。为了进行转换，我们需要做的就是调用列转换器的`fit`方法。然而，我们还不会这样做，因为我们还想将特征选择添加到管道中，并让它运行线性回归。我们将在接下来的几个步骤中做到这一点。

## 运行和评估我们的线性模型

我们将使用**递归特征消除** ( **RFE** )来为我们的模型选择特征。RFE 具有包装器特征选择方法的优势——它基于选定的算法评估特征，并在评估中考虑多元关系。然而，它也可能计算量很大。因为我们没有太多的特征或观察值，所以在这种情况下这不是什么大问题。

选择特征后，我们运行一个线性回归模型并查看我们的预测。让我们开始吧:

1.  首先，我们创建线性回归和递归特征消除实例，并将它们添加到管道中。我们还创建了一个`TransformedTargetRegressor`对象，因为我们仍然需要转换目标。我们将管道传递给回归变量参数`TransformedTargetRegressor`。

现在，我们可以称之为目标回归量的`fit`方法。之后，管道的`rfe`步骤的`support_`属性将为我们提供所选择的特性。类似地，我们可以通过获得`linearregression`步骤的`coef_`值来获得系数。这里的关键是引用`ttr.regressor`让我们进入管道:

```
lr = LinearRegression()
rfe = RFE(estimator=lr, n_features_to_select=7)
pipe1 = make_pipeline(coltrans, 
  KNNImputer(n_neighbors=5), rfe, lr)
ttr=TransformedTargetRegressor(regressor=pipe1,
  transformer=StandardScaler())
ttr.fit(X_train, y_train)
selcols = X_train.columns[ttr.regressor_.named_steps[‘rfe’].support_]
coefs = ttr.regressor_.named_steps[‘linearregression’].coef_
np.column_stack((coefs.ravel(),selcols))
array([[0.44753064726665703, ‘VAT_Rate’],
 [0.12368913577287821, ‘gov_debt_per_gdp’],
 [0.17926454403985687, ‘goveffect’],
 [-0.22100930246392841, ‘autocracy_polity’],
 [-0.15726572731003752, ‘nat_oil_comp’],
 [-0.7013454686632653, ‘nat_oil_comp_state’],
 [0.13855012574945422, ‘motorization_rate’]], dtype=object)
```

我们的特征选择确定了增值税率、政府债务、衡量政府效率的指标(`goveffect`)、该国是否属于专制国家、是否有一家国家石油公司和一家国有石油公司，以及机动化率作为前七大特征。所示特征的数量是超参数的一个例子，我们在这里选择七个特征是相当随意的。我们将在下一节讨论超参数调优技术。

请注意，在数据集中的几个专制/民主衡量指标中，似乎最重要的一个是专制虚拟指标，对于`polity`得分非常低的国家，该指标的值为 1。据估计，汽油税有负面影响；也就是说，减少它们。

1.  让我们看看预测和残差。我们可以将测试数据的特征传递给变压器/管道的`predict`方法，以生成预测。有一点正偏和一些总体偏；残差总体为负:

    ```
    pred = ttr.predict(X_test) preddf = pd.DataFrame(pred, columns=[‘prediction’],   index=X_test.index).join(X_test).join(y_test) preddf[‘resid’] = preddf.gas_tax_imp-preddf.prediction preddf.resid.agg([‘mean’,’median’,’skew’,’kurtosis’]) mean                -0.09 median              -0.13 skew                 0.61 kurtosis             0.04 Name: resid, dtype: float64
    ```

2.  让我们也生成一些整体的模型评估统计数据。我们得到平均绝对误差`0.23`。考虑到汽油税价格的中间值是`0.52`，这不是一个很大的平均误差。不过，r 平方还不错:

    ```
    print(“Mean Absolute Error: %.2f, R-squared: %.2f” %    (skmet.mean_absolute_error(y_test, pred),   skmet.r2_score(y_test, pred))) Mean Absolute Error: 0.23, R-squared: 0.75
    ```

3.  看一看残差图通常是有帮助的。让我们也在残差的平均值处画一条红色虚线:

    ```
    plt.hist(preddf.resid, color=”blue”, bins=np.arange(-0.5,1.0,0.25)) plt.axvline(preddf.resid.mean(), color=’red’, linestyle=’dashed’, linewidth=1) plt.title(“Histogram of Residuals for Gax Tax Model”) plt.xlabel(“Residuals”) plt.ylabel(“Frequency”) plt.xlim() plt.show()
    ```

这会产生以下情节:

![Figure 7.2 – Gas tax model residuals

](img/B17978_07_002.jpg)

图 7.2-天然气税模型残差

该图显示了正偏斜。此外，我们的模型更有可能高估汽油税，而不是低估它。(当预测值大于实际目标值时，残差为负。)

1.  让我们也看看预测相对于残差的散点图。让我们在 *Y* 轴

    ```
    plt.scatter(preddf.prediction, preddf.resid, color=”blue”) plt.axhline(0, color=’red’, linestyle=’dashed’, linewidth=1) plt.title(“Scatterplot of Predictions and Residuals”) plt.xlabel(“Predicted Gax Tax”) plt.ylabel(“Residuals”) plt.show()
    ```

    的 0 处画一条红色虚线

这会产生以下情节:

![Figure 7.3 – Scatter plot of predictions against residuals

](img/B17978_07_003.jpg)

图 7.3–预测与残差的散点图

这里，过度预测发生在预测值的整个范围内，但是没有预测值低于 0 或高于 1 的预测值(正残差)。这应该使我们对我们的线性假设产生一些怀疑。

## 改进我们的模型评估

到目前为止，我们评估模型的一个问题是我们没有充分利用数据。我们只对大约 80%的数据进行训练。我们的度量标准也非常依赖于代表我们想要预测的真实世界的测试数据。然而，也可能不是。正如我们在前一章讨论的那样，我们可以通过 k 倍交叉验证来提高我们的胜算。

由于我们一直在使用管道进行分析，我们已经完成了 k-fold 交叉验证所需的大量工作。回想一下上一章，k 倍模型评估将我们的数据分成 k 等份。其中一个折叠被指定用于测试，其余的用于训练。这被重复 k 次，每次使用不同的折叠进行测试。

让我们用我们的线性回归模型尝试一下 k 倍交叉验证:

1.  我们将从创建新的训练和测试数据框架开始，只留下 10%用于以后的验证。我们不需要保留那么多数据来验证，尽管总是保留一点数据是个好主意
2.  我们还需要实例化`KFold`和`LinearRegression`对象:

    ```
    kf = KFold(n_splits=3, shuffle=True, random_state=0)
    ```

3.  现在，我们准备运行我们的 k 倍交叉验证。我们指出，我们希望每个分裂都有 r 平方误差和平均绝对误差。`cross_validate`自动给我们每次折叠的健身和得分时间:

    ```
    scores = cross_validate(ttr, X=X_train, y=y_train,   cv=kf, scoring=(‘r2’, ‘neg_mean_absolute_error’), n_jobs=1) print(“Mean Absolute Error: %.2f, R-squared: %.2f” %    (scores[‘test_neg_mean_absolute_error’].mean(),   scores[‘test_r2’].mean())) Mean Absolute Error: -0.25, R-squared: 0.62
    ```

这些分数不是很可观。我们并没有像我们希望的那样解释那么多的差异。三个层次的 r 平方平均得分约为 0.62。这部分是因为每个褶皱的测试数据框架非常小，每个褶皱只有大约 40 个观测值。尽管如此，我们应该探索经典线性回归方法的修改，如正则化和非线性回归。

正则化的一个优点是，我们可以得到类似的结果，而不需要通过计算昂贵的特征选择过程。正规化也可以帮助我们避免过度拟合。我们将在下一节用同样的数据探索 lasso 回归。我们还将研究非线性回归策略。

# 使用套索回归

OLS 的一个关键特性是它产生的参数估计偏差最小。然而，OLS 估计的方差可能比我们想要的要高。当我们使用经典的线性回归模型时，我们需要小心过度拟合。降低过度拟合可能性的一个策略是使用正则化。正则化也可以允许我们将特征选择和模型训练结合起来。这对于包含大量要素或观测值的数据集来说可能很重要。

OLS 最小化均方误差，而正则化技术寻求最小误差和减少特征数量。我们在本节中探讨的 Lasso 回归使用 L1 正则化，其对系数的绝对值不利。岭回归也类似。它使用 L2 正则化，这惩罚系数的平方值。弹性网络回归使用 L1 和 L2 正则化。

同样，我们将使用上一节中的汽油税数据:

1.  我们将从导入与上一节相同的库开始，除了我们将导入`Lasso`模块而不是`linearregression`模块:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Lasso from sklearn.impute import SimpleImputer from sklearn.pipeline import make_pipeline from sklearn.compose import ColumnTransformer from sklearn.compose import TransformedTargetRegressor from sklearn.model_selection import cross_validate, KFold import sklearn.metrics as skmet import matplotlib.pyplot as plt
    ```

2.  我们将还需要我们创建的`OutlierTrans`类:

    ```
    import os import sys sys.path.append(os.getcwd() + “/helperfunctions”) from preprocfunc import OutlierTrans
    ```

3.  现在，让我们加载汽油税数据并创建测试和训练数据框架:

    ```
    fftaxrate14 = pd.read_csv(“data/fossilfueltaxrate14.csv”) fftaxrate14.set_index(‘countrycode’, inplace=True) num_cols = [‘fuel_income_dependence’,’national_income_per_cap’,   ‘VAT_Rate’,  ‘gov_debt_per_gdp’,’polity’,’goveffect’,   ‘democracy_index’] dummy_cols = [‘democracy_polity’,’autocracy_polity’, ‘democracy’,’nat_oil_comp’,’nat_oil_comp_state’] spec_cols = [‘motorization_rate’] target = fftaxrate14[[‘gas_tax_imp’]] features = fftaxrate14[num_cols + dummy_cols + spec_cols] X_train, X_test, y_train, y_test =  \   train_test_split(features,\   target, test_size=0.2, random_state=0)
    ```

4.  我们还需要设置列转换:

    ```
    standtrans = make_pipeline(   OutlierTrans(2), SimpleImputer(strategy=”median”),   StandardScaler()) cattrans = make_pipeline(SimpleImputer(strategy=”most_frequent”)) spectrans = make_pipeline(OutlierTrans(2), StandardScaler()) coltrans = ColumnTransformer(   transformers=[     (“stand”, standtrans, num_cols),     (“cat”, cattrans, dummy_cols),     (“spec”, spectrans, spec_cols)   ] )
    ```

5.  现在，我们已经准备好适应我们的模型了。我们将从一个相当保守的α值 0.1 开始。阿尔法值越高，我们系数的惩罚就越大。在 0 处，我们得到与线性回归相同的结果。除了列变换和套索回归，我们的管道使用 KNN 填补缺失值。我们还将使用目标转换器来缩放汽油税目标。在拟合之前，我们将把刚刚创建的管道传递给目标转换器的回归参数:

    ```
    lasso = Lasso(alpha=0.1,fit_intercept=False) pipe1 = make_pipeline(coltrans, KNNImputer(n_neighbors=5), lasso) ttr=TransformedTargetRegressor(regressor=pipe1,transformer=StandardScaler()) ttr.fit(X_train, y_train)
    ```

6.  让我们来看看拉索回归的系数。如果我们将它们与上一节中的线性回归系数进行比较，我们会注意到我们最终选择了相同的特征。用递归特征选择消除的那些特征与用 lasso 回归得到接近零值的特征基本相同:

    ```
    coefs = ttr.regressor_[‘lasso’].coef_ np.column_stack((coefs.ravel(), num_cols + dummy_cols + spec_cols)) array([[‘-0.0026505240129231175’, ‘fuel_income_dependence’],        [‘0.0’, ‘national_income_per_cap’],        [‘0.43472262042825915’, ‘VAT_Rate’],        [‘0.10927136643326674’, ‘gov_debt_per_gdp’],        [‘0.006825858127837494’, ‘polity’],        [‘0.15823493727828816’, ‘goveffect’],        [‘0.09622123660935211’, ‘democracy_index’],        [‘0.0’, ‘democracy_polity’],        [‘-0.0’, ‘autocracy_polity’],        [‘0.0’, ‘democracy’],        [‘-0.0’, ‘nat_oil_comp’],        [‘-0.2199638245781246’, ‘nat_oil_comp_state’],        [‘0.016680304258453165’, ‘motorization_rate’]], dtype=’<U32’)
    ```

7.  让我们看看这个模型的预测和残差。残差看起来不错，几乎没有偏差，也没有太多偏斜:

    ```
    pred = ttr.predict(X_test) preddf = pd.DataFrame(pred, columns=[‘prediction’],   index=X_test.index).join(X_test).join(y_test) preddf[‘resid’] = preddf.gas_tax_imp-preddf.prediction preddf.resid.agg([‘mean’,’median’,’skew’,’kurtosis’]) mean                 -0.06 median               -0.07 skew                  0.33 kurtosis              0.10 Name: resid, dtype: float64
    ```

8.  让我们也生成平均绝对误差和 r 平方。这些分数并不令人印象深刻。r 平方低于线性回归，但平均绝对误差大致相同:

    ```
    print(“Mean Absolute Error: %.2f, R-squared: %.2f” %    (skmet.mean_absolute_error(y_test, pred),   skmet.r2_score(y_test, pred))) Mean Absolute Error: 0.24, R-squared: 0.68
    ```

9.  我们应该看看残差的直方图。残差的分布非常类似于线性回归模型:

    ```
    plt.hist(preddf.resid, color=”blue”, bins=np.arange(-0.5,1.0,0.25)) plt.axvline(preddf.resid.mean(), color=’red’, linestyle=’dashed’, linewidth=1) plt.title(“Histogram of Residuals for Gax Tax Model”) plt.xlabel(“Residuals”) plt.ylabel(“Frequency”) plt.show()
    ```

这会产生以下情节:

![Figure 7.4 – Gas tax model residuals

](img/B17978_07_004.jpg)

图 7.4-天然气税模型残差

1.  让我们再来看一下残差预测值的散点图。我们的模型很可能在较低范围内预测过高，而在较高范围内预测过低。这是对线性模型的一个改变，在线性模型中，我们在两个极端情况下都一直过度预测:

    ```
    plt.scatter(preddf.prediction, preddf.resid, color=”blue”) plt.axhline(0, color=’red’, linestyle=’dashed’, linewidth=1) plt.title(“Scatterplot of Predictions and Residuals”) plt.xlabel(“Predicted Gax Tax”) plt.ylabel(“Residuals”) plt.show()
    ```

这会产生以下情节:

![Figure 7.5 – Scatter plot of predictions against residuals

](img/B17978_07_005.jpg)

图 7.5–预测与残差的散点图

1.  我们将通过对模型执行 k-fold 交叉验证得出结论。得分低于但接近线性回归模型:

    ```
    X_train, X_test, y_train, y_test =  \   train_test_split(features,\   target, test_size=0.1, random_state=22) kf = KFold(n_splits=4, shuffle=True, random_state=0) scores = cross_validate(ttr, X=X_train, y=y_train,   cv=kf, scoring=(‘r2’, ‘neg_mean_absolute_error’), n_jobs=1) print(“Mean Absolute Error: %.2f, R-squared: %.2f” %    (scores[‘test_neg_mean_absolute_error’].mean(),   scores[‘test_r2’].mean())) Mean Absolute Error: -0.27, R-squared: 0.57
    ```

这给了我们一个并不比原始模型更好的模型，但它至少更有效地处理了特征选择过程。如果我们尝试α超参数的不同值，我们也可能得到更好的结果。为什么不是 0.05 或者 1.0 来代替？我们将在接下来的两步中尝试回答这个问题。

## 使用网格搜索调整超参数

计算出超参数的最佳值，例如前面例子中的 alpha 值的，被称为超参数调整。scikit-learn 中用于超参数调整的一个工具是`GridSearchCV`。CV 后缀用于交叉验证。

使用`GridSearchCV`非常简单。如果我们已经有了一个管道，就像我们在本例中所做的那样，我们将它传递给一个`GridSearchCV`对象，以及一个参数字典。`GridSearchCV`将尝试所有参数组合并返回最佳组合。让我们在我们的 lasso 回归模型上尝试一下:

1.  首先，我们将实例化一个`lasso`对象，并用要调优的超参数创建一个字典。字典`lasso_params`表示我们希望以 0.5 的间隔尝试 0.05 到 0.9 之间的所有 alpha 值。我们不能为字典键选择任何我们想要的名称。`regressor__lasso__alpha`基于管道中步骤的名称。另外，请注意我们使用了双下划线。单下划线将返回错误:

    ```
    lasso = Lasso() lasso_params = {‘regressor__lasso__alpha’: np.arange(0.05, 1, 0.05)}
    ```

2.  现在，我们可以进行网格搜索了。我们将传递管道，在本例中是一个`TransformedTargetRegressor`，并将字典传递给`GridSearchCV`。`best_params_`属性表示最佳 alpha 是`0.05`。当我们使用这个值时，我们得到了一个 r 平方的`0.60` :

    ```
    gs = GridSearchCV(ttr,param_grid=lasso_params, cv=5) gs.fit(X_train, y_train) gs.best_params_ {‘regressor__lasso__alpha’: 0.05} gs.best_score_ 0.6028804486340877
    ```

就平均绝对误差和 r 平方而言，lasso 回归模型接近但不如线性模型。lasso 回归的一个好处是，我们不需要在训练模型之前进行单独的特征选择步骤。(回想一下，对于包装器特征选择方法，模型需要在特征选择期间以及之后进行训练，正如我们在第 5 章 、*特征选择*中所讨论的。)

# 使用非线性回归

线性回归假设特征与目标的关系在特征范围内保持不变。您可能还记得，我们在本章开始时讨论的简单线性回归方程对每个特征都有一个斜率估计:

![](img/B17978_07_010.jpg)

这里， *y* 是目标，每个 *x* 是特征，每个β是系数(或者截距)。如果特征和目标之间的真实关系是非线性的，我们的模型可能表现不佳。

幸运的是，当我们不能假设特征和目标之间的线性关系时，我们仍然可以很好地利用 OLS。我们可以使用与上一节中相同的线性回归算法，但是对特征进行多项式变换。这被称为多项式回归。

我们给特征增加一个幂来运行多项式回归。这给了我们以下等式:

![](img/B17978_07_011.jpg)

下图比较了线性回归和多项式回归的预测值。多项式曲线似乎比线性回归线更适合虚构的数据点:

![Figure 7.6 – Illustration of the polynomial equation curve and linear equation line

](img/B17978_07_006.jpg)

图 7.6–多项式方程曲线和线性方程线的图示

在本节中，我们将对世界各地气象站的年平均气温的线性模型和非线性模型进行实验。我们将使用纬度和海拔作为特征。首先，我们将使用多元线性回归预测温度，然后尝试使用多项式变换的模型。请遵循以下步骤:

1.  我们将从导入必要的库开始。如果你已经阅读了这一章，你会对这些库很熟悉
2.  我们还需要来导入包含用于识别异常值的类的模块:

    ```
    import os import sys sys.path.append(os.getcwd() + “/helperfunctions”) from preprocfunc import OutlierTrans
    ```

3.  我们加载土地温度数据，识别我们想要的特征，并生成一些描述性统计数据。海拔有一些缺失值，年平均气温有一些极端负值。目标和特征的值的范围是非常不同的，所以我们可能想要缩放我们的数据:

    ```
    landtemps = pd.read_csv(“data/landtempsb2019avgs.csv”) landtemps.set_index(‘locationid’, inplace=True) feature_cols = [‘latabs’,’elevation’] landtemps[[‘avgtemp’] + feature_cols].\   agg([‘count’,’min’,’median’,’max’]).T              count      min      median    max avgtemp      12,095    -60.82    10.45     33.93 latabs       12,095     0.02     40.67     90.00 elevation    12,088    -350.00   271.30    4,701.00
    ```

4.  接下来，我们创建训练和测试数据帧:

    ```
    X_train, X_test, y_train, y_test =  \   train_test_split(landtemps[feature_cols],\   landtemps[[‘avgtemp’]], test_size=0.1, random_state=0)
    ```

5.  现在，我们构建一个管道来处理我们的预处理——将异常值设置为缺失，对所有缺失值进行 KNN 插补，并缩放特征——然后运行一个线性模型。我们用 10 倍做 k 倍交叉验证，得到平均 r 平方为 0.79，平均绝对误差为-2.8:

    ```
    lr = LinearRegression() knnimp = KNNImputer(n_neighbors=45) pipe1 = make_pipeline(OutlierTrans(3),knnimp,   StandardScaler(), lr) ttr=TransformedTargetRegressor(regressor=pipe1,   transformer=StandardScaler()) kf = KFold(n_splits=10, shuffle=True, random_state=0) scores = cross_validate(ttr, X=X_train, y=y_train,   cv=kf, scoring=(‘r2’, ‘neg_mean_absolute_error’), n_jobs=1) scores[‘test_r2’].mean(), scores[‘test_neg_mean_absolute_error’].mean() (0.7933471824738406, -2.8047627785750913)
    ```

请注意，我们对异常值的识别非常保守。我们通过一个阈值 3，这意味着一个值需要是该范围上下四分位数范围的三倍以上。显然，我们通常会对异常值的识别给予更多的考虑。我们在这里只演示如何处理管道中的异常值，只要我们认为这是有意义的。

1.  让我们看看预测和残差。总体上几乎没有偏差(残差的平均值为 0)，但是有一些负偏差:

    ```
    ttr.fit(X_train, y_train) pred = ttr.predict(X_test) preddf = pd.DataFrame(pred, columns=[‘prediction’],   index=X_test.index).join(X_test).join(y_test) preddf.resid.agg([‘mean’,’median’,’skew’,’kurtosis’]) mean                 0.00 median               0.50 skew                -1.13 kurtosis             3.48 Name: resid, dtype: float64
    ```

2.  如果我们创建一个残差直方图，就很容易看到这种偏斜。有一些极端的负残差——也就是我们过度预测平均温度的时候:

    ```
    plt.hist(preddf.resid, color=”blue”) plt.axvline(preddf.resid.mean(), color=’red’,    linestyle=’dashed’, linewidth=1) plt.title(“Histogram of Residuals for Linear Model of Temperature”) plt.xlabel(“Residuals”) plt.ylabel(“Frequency”) plt.show()
    ```

这会产生以下情节:

![Figure 7.7 – Temperature model residuals

](img/B17978_07_007.jpg)

图 7.7–温度模型残差

1.  根据残差绘制预测值也很有帮助:

    ```
    plt.scatter(preddf.prediction, preddf.resid, color=”blue”) plt.axhline(0, color=’red’, linestyle=’dashed’, linewidth=1) plt.title(“Scatterplot of Predictions and Residuals”) plt.xlabel(“Predicted Temperature”) plt.ylabel(“Residuals”) plt.xlim(-20,40) plt.ylim(-27,10) plt.show()
    ```

此产生以下情节:

![Figure 7.8 – Scatter plot of predictions against residuals

](img/B17978_07_0081.jpg)

图 7.8–预测与残差的散点图

我们的模型超额预测了大约 28 摄氏度以上的所有预测。当它预测值在 18 到 28 之间时，也可能会低估。

让我们看看多项式回归是否能得到更好的结果:

1.  首先，我们将创建一个`degree`为`4`的`PolynomialFeatures`对象，并对其进行拟合。我们可以将原始的特性名称传递给`get_feature_names`方法，以获得转换后将返回的列的名称。创建每个特征的二次方值、三次方值、四次方值，以及变量之间的交互效果(如`latabs` * `elevation`)。我们不需要在这里运行 fit，因为这将在管道中发生。我们在这里这样做只是为了感受一下它是如何工作的:

    ```
    polytrans = PolynomialFeatures(degree=4, include_bias=False) polytrans.fit(X_train.dropna()) featurenames = polytrans.get_feature_names(feature_cols) featurenames [‘latabs’,  ‘elevation’,  ‘latabs^2’,  ‘latabs elevation’,  ‘elevation^2’,  ‘latabs^3’,  ‘latabs^2 elevation’,  ‘latabs elevation^2’,  ‘elevation^3’,  ‘latabs^4’,  ‘latabs^3 elevation’,  ‘latabs^2 elevation^2’,  ‘latabs elevation^3’,  ‘elevation^4’]
    ```

2.  接下来，我们将为我们的多项式回归创建一个管道。流水线与线性回归非常相似，除了我们在 KNN 插补后插入了多项式变换步骤:

    ```
    pipe2 = make_pipeline(OutlierTrans(3), knnimp,   polytrans, StandardScaler(), lr) ttr2 = TransformedTargetRegressor(regressor=pipe2,\   transformer=StandardScaler())
    ```

3.  现在，让我们基于多项式模型创建预测和残差。残差中的偏差比线性模型中的偏差小一点:

    ```
    ttr2.fit(X_train, y_train) pred = ttr2.predict(X_test) preddf = pd.DataFrame(pred, columns=[‘prediction’],   index=X_test.index).join(X_test).join(y_test) preddf[‘resid’] = preddf.avgtemp-preddf.prediction preddf.resid.agg([‘mean’,’median’,’skew’,’kurtosis’]) mean                 0.01 median               0.20 skew                -0.98 kurtosis             3.34 Name: resid, dtype: float64
    ```

4.  我们应该看看残差的直方图:

    ```
    plt.hist(preddf.resid, color=”blue”) plt.axvline(preddf.resid.mean(), color=’red’, linestyle=’dashed’, linewidth=1) plt.title(“Histogram of Residuals for Temperature Model”) plt.xlabel(“Residuals”) plt.ylabel(“Frequency”) plt.show()
    ```

这会产生以下情节:

![Figure 7.9 – Temperature model residuals

](img/B17978_07_009.jpg)

图 7.9–温度模型残差

1.  让我们再做另一个预测值与残差的散点图。这些看起来比线性模型的残差好一点，特别是在预测的上限:

    ```
    plt.scatter(preddf.prediction, preddf.resid, color=”blue”) plt.axhline(0, color=’red’, linestyle=’dashed’, linewidth=1) plt.title(“Scatterplot of Predictions and Residuals”) plt.xlabel(“Predicted Temperature”) plt.ylabel(“Residuals”) plt.xlim(-20,40) plt.ylim(-27,10) plt.show()
    ```

这会产生以下情节:

![Figure 7.10 – Scatter plot of predictions against residuals

](img/B17978_07_0101.jpg)

图 7.10–预测与残差的散点图

1.  让我们再次进行 k-fold 交叉验证，并取所有折叠的平均 r 平方值。与线性模型相比，r 平方误差和平均绝对误差都有所改善:

    ```
    scores = cross_validate(ttr2, X=X_train, y=y_train,   cv=kf, scoring=(‘r2’, ‘neg_mean_absolute_error’),   n_jobs=1) scores[‘test_r2’].mean(), scores[‘test_neg_mean_absolute_error’].mean() (0.8323274036342788, -2.4035803290965507)
    ```

多项式变换改善了我们的总体结果，特别是在我们预测因子的某些范围内。高温下的残留物明显较低。当我们的残差表明我们的特征和目标之间可能存在非线性关系时，尝试多项式变换通常是个好主意。

# 梯度下降回归

对于优化线性模型的损失函数，梯度下降可以是普通最小二乘法的一个很好的替代方案。在处理非常大的数据集时尤其如此。在本节中，我们将对陆地温度数据集使用梯度下降，主要是为了演示如何使用它，并给我们另一个机会来探索彻底的网格搜索。让我们开始吧:

1.  首先，我们将加载到目前为止一直在使用的相同库，加上来自 scikit-learn 的随机梯度下降回归器:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDRegressor from sklearn.compose import TransformedTargetRegressor from sklearn.pipeline import make_pipeline from sklearn.impute import KNNImputer from sklearn.model_selection import GridSearchCV import os import sys sys.path.append(os.getcwd() + “/helperfunctions”) from preprocfunc import OutlierTrans
    ```

2.  然后，我们将再次加载陆地温度数据集，并创建训练和测试数据帧:

    ```
    landtemps = pd.read_csv(“data/landtempsb2019avgs.csv”) landtemps.set_index(‘locationid’, inplace=True) feature_cols = [‘latabs’,’elevation’] X_train, X_test, y_train, y_test =  \   train_test_split(landtemps[feature_cols],\   landtemps[[‘avgtemp’]], test_size=0.1, random_state=0)
    ```

3.  接下来，我们将建立一个管道来处理异常值，估算缺失的值，并在运行梯度下降之前缩放数据:

    ```
    knnimp = KNNImputer(n_neighbors=45) sgdr = SGDRegressor() pipe1 = make_pipeline(OutlierTrans(3),knnimp,StandardScaler(), sgdr) ttr=TransformedTargetRegressor(regressor=pipe1,transformer=StandardScaler())
    ```

4.  现在，我们需要创建一个字典来指明我们想要调优的超参数和要尝试的值。我们想尝试α、损失函数、ε和惩罚的值。我们将给字典中的每个关键字一个前缀`regressor__sgdregressor__`,因为在管道中可以找到随机梯度下降回归器。

`alpha`参数决定惩罚的大小。默认为`0.0001`。我们可以选择 L1、L2 或弹性网正规化。我们将选择`huber`和`epsilon_insensitive`作为包含在搜索中的损失函数。默认的损失函数是`squared_error`，但这只会再次给我们普通的最小二乘法。

与 OLS 相比，`huber`损失函数对异常值不太敏感。它有多敏感，取决于我们指定的ε值。使用`epsilon_insensitive`损失函数，给定范围(ε)内的误差不会受到惩罚(我们将在下一章使用ε不敏感管构建模型，在下一章中，我们将检查支持向量回归):

```
sgdr_params = {
 ‘regressor__sgdregressor__alpha’: 10.0 ** -np.arange(1, 7),
 ‘regressor__sgdregressor__loss’: [‘huber’,’epsilon_insensitive’],
 ‘regressor__sgdregressor__penalty’: [‘l2’, ‘l1’, ‘elasticnet’],
 ‘regressor__sgdregressor__epsilon’: np.arange(0.1, 1.6, 0.1)
}
```

1.  现在，我们准备进行彻底的网格搜索。最佳参数是α的`0.001`、ε的`1.3`、损失函数的`huber`以及弹性网络正则化的

    ```
    gs = GridSearchCV(ttr,param_grid=sgdr_params, cv=5, scoring=”r2”) gs.fit(X_train, y_train) gs.best_params_ {‘regressor__sgdregressor__alpha’: 0.001,  ‘regressor__sgdregressor__epsilon’: 1.3000000000000003,  ‘regressor__sgdregressor__loss’: ‘huber’,  ‘regressor__sgdregressor__penalty’: ‘elasticnet’} gs.best_score_ 0.7941051735846133
    ```

2.  我通常发现查看一些其他网格搜索迭代的超参数很有帮助。使用弹性网或 L2 正则化的 Huber 损失模型表现最佳:

    ```
    Results = \   pd.DataFrame(gs.cv_results_[‘mean_test_score’], \     columns=[‘meanscore’]).\   join(pd.DataFrame(gs.cv_results_[‘params’])).\   sort_values([‘meanscore’], ascending=False) results.head(3).T                                          254      252      534 meanscore                           0.794105 0.794011 0.794009 regressor__sgdregressor__alpha      0.001000 0.001000 0.000001 regressor__sgdregressor__epsilon    1.300000 1.300000 1.500000 regressor__sgdregressor__loss          huber    huber    huber regressor__sgdregressor__penalty  elasticnet       l2       l2
    ```

随机梯度下降是一种通用的优化方法，可以应用于许多机器学习问题。正如我们在这里看到的，它通常非常高效。我们能够相当快地对惩罚、惩罚大小、ε和损失函数进行彻底的网格搜索。

# 总结

这一章让我们探索了一个非常著名的机器学习算法:线性回归。我们检查了使其成为线性模型的良好候选的特征空间的质量。我们还探讨了如何在必要时通过正则化和变换来改进线性模型。然后，我们将随机梯度下降作为 OLS 优化的替代方案。我们还学习了如何将我们自己的类添加到管道中，以及如何进行超参数调优。

在下一章，我们将探讨支持向量回归。