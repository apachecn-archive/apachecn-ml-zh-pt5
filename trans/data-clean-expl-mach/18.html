<html><head/><body>









<title>Chapter 14: Naïve Bayes Classification</title>







<div><div><h1 id="_idParaDest-161"><em class="italic"> <a id="_idTextAnchor162"/>第十四章</em>:朴素贝叶斯分类</h1>

<p>在这一章中，我们将考察朴素贝叶斯可能是比我们目前所考察的更有效的分类器的情况。朴素贝叶斯是一个非常直观且易于实现的分类器。假设我们的特征是独立的，我们甚至可以获得比逻辑正则化更好的性能，特别是如果我们没有对后者使用正则化。</p>

<p>在本章中，我们将讨论朴素贝叶斯的基本假设，以及如何使用该算法来解决我们已经探索过的一些建模挑战，以及一些新的挑战，如文本分类。我们将考虑什么时候朴素贝叶斯是一个好的选择，什么时候不是。我们也将检验朴素贝叶斯模型的解释。</p>

<p>我们将在本章中讨论以下主题:</p>

<ul>

<li>关键概念</li>

<li>朴素贝叶斯分类模型</li>

<li>用于文本分类的朴素贝叶斯</li>

</ul>

<h1 id="_idParaDest-162"><a id="_idTextAnchor163"/>技术要求</h1>

<p>在本章中，我们将主要讨论 pandas、NumPy 和 scikit-learn 库。唯一的例外是不平衡学习库，可以用<code>pip install imbalanced-learn</code>安装。本章中的所有代码都是用 scikit-learn 版本 0.24.2 和 1.0.2 测试的。</p>

<h1 id="_idParaDest-163"><a id="_idTextAnchor165"/>关键概念</h1>

<p><a id="_idIndexMarker1069"/>朴素贝叶斯分类器使用贝叶斯定理来预测类成员。贝叶斯定理描述了事件的概率和给定新的相关数据的事件的概率之间的关系。给定新<a id="_idIndexMarker1070"/>数据的事件概率称为<strong class="bold">后验概率</strong>。事件在新数据之前发生的概率被恰当地称为<a id="_idIndexMarker1071"/>先验概率<strong class="bold"/>。</p>

<p>贝叶斯定理给出了下面的等式:</p>

<div><div><img alt="" height="107" src="img/B17978_14_001.jpg" width="1435"/>

</div>

</div>

<p><a id="_idIndexMarker1072"/>后验概率(给定新数据的事件概率)等于给定事件的数据概率乘以事件的先验概率，再除以新数据的概率。</p>

<p>不太通俗地说，这通常是这样写的:</p>

<div><div><img alt="" height="117" src="img/B17978_14_002.jpg" width="471"/>

</div>

</div>

<p>这里，<em class="italic"> A </em>是事件，比如类成员关系，<em class="italic"> B </em>是新信息。当应用于分类时，我们得到以下等式:</p>

<div><div><img alt="" height="129" src="img/B17978_14_003.jpg" width="674"/>

</div>

</div>

<p>这里，<img alt="" height="54" src="img/B17978_14_004.png" width="288"/>是给定实例的特征时实例的类成员的概率，而<img alt="" height="52" src="img/B17978_14_005.png" width="288"/>是给定类成员时特征的概率。<em class="italic"> P(y) </em>是类成员的概率，而<img alt="" height="51" src="img/B17978_14_006.png" width="227"/>是特征值的概率。因此，后验概率<img alt="" height="53" src="img/B17978_14_007.png" width="288"/>等于给定类别成员的特征值的概率乘以类别成员的概率，除以特征值的概率。</p>

<p>这里的假设是这些特性是相互独立的。这就是给这个方法一个天真的形容词的原因。然而，作为一个实际问题，特征独立性并不是朴素贝叶斯获得好结果的必要条件。</p>

<p>朴素贝叶斯可以处理数字或分类特征。当我们主要有数字特征时，我们通常使用高斯朴素贝叶斯。顾名思义，高斯朴素贝叶斯假设特征值的条件概率<img alt="" height="53" src="img/B17978_14_008.png" width="288"/>遵循正态分布。<img alt="" height="52" src="img/B17978_14_009.png" width="288"/>然后可以使用标准偏差和每类特征的平均值相对直接地计算出来。</p>

<p>当我们的特征是离散的或是计数时，我们可以使用多项式朴素贝叶斯来代替。更一般地，当特征值的条件概率遵循多项式分布时，它工作得很好。多项式朴素贝叶斯的一个常见应用是使用<strong class="bold">单词袋</strong>方法进行文本分类。对于单词袋，<a id="_idIndexMarker1074"/>功能是文档中每个单词的计数。我们可以应用贝叶斯定理来估计类成员的概率:</p>

<div><div><img alt="" height="142" src="img/B17978_14_010.jpg" width="648"/>

</div>

</div>

<p>这里，<img alt="" height="47" src="img/B17978_14_011.png" width="175"/>是属于<em class="italic"> k </em>类的概率，给定一个字数向量，<em class="italic"> W </em>。我们将在本章的最后一节充分利用这一点。</p>

<p>朴素贝叶斯适用于相当广泛的文本分类任务。它被用于情感分析、垃圾邮件检测和新闻故事分类，仅举几个例子。</p>

<p>朴素贝叶斯对于训练和预测都是有效的算法，并且通常表现得非常好。它非常具有可伸缩性，可以很好地处理大量的实例和特性。也很容易解读。当模型的复杂性对于<a id="_idIndexMarker1075"/>好的预测来说不是必需的时候，该算法做得最好。即使朴素贝叶斯不太可能是产生最小偏差的方法，它对于诊断目的或检查不同算法的结果通常也是有用的。</p>

<p>我们在上一章中使用的 NBA 数据可能是使用朴素贝叶斯建模的一个很好的候选。我们将在下一节探讨这一点。</p>

<h1 id="_idParaDest-164"><a id="_idTextAnchor166"/>朴素贝叶斯分类模型</h1>

<p>朴素贝叶斯的吸引力之一是，即使你有很多数据，你也能很快得到不错的结果。拟合和预测对系统资源来说都相当容易。另一个优点是，可以捕获相对复杂的关系，而不必转换特征空间或进行大量超参数调整。我们可以用上一章处理过的 NBA 数据来证明这一点。</p>

<p>我们将在本节中使用关于国家篮球协会比赛的数据。该数据集包含从 2017/2018 赛季到 2020/2021 赛季的每场 NBA 比赛的统计数据。这包括主队；主队是否获胜；客队；客队和主队的投篮命中率；两队失误、篮板、助攻；和其他几项措施。</p>

<p class="callout-heading">注意</p>

<p class="callout">公众可以在<a href="https://www.kaggle.com/datasets/wyattowalsh/basketball">https://www.kaggle.com/datasets/wyattowalsh/basketball</a>下载 NBA 比赛数据<a id="_idIndexMarker1078"/>。该数据集包含从 1946/1947 NBA 赛季开始的比赛数据。它使用<code>nba_api</code>从<code>nba.com</code>获取统计数据。该 API 可在 https://github.com/swar/nba_api 获得。</p>

<p>让我们使用朴素贝叶斯建立一个<a id="_idIndexMarker1079"/>分类模型:</p>

<ol>

<li>我们将加载我们在最后几章中使用的相同的库:<pre>import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.impute import SimpleImputer from sklearn.pipeline import make_pipeline from sklearn.compose import ColumnTransformer from sklearn.feature_selection import RFE from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_validate, \   RandomizedSearchCV, RepeatedStratifiedKFold import sklearn.metrics as skmet import os import sys sys.path.append(os.getcwd() + "/helperfunctions") from preprocfunc import OutlierTrans</pre></li>

<li>接下来，我们<a id="_idIndexMarker1080"/>将加载 NBA 比赛数据。我们需要在这里做一些数据清理。对于主队是否获胜，少数观察值有缺失值，<code>WL_HOME</code>。我们将移除那些成为我们目标的人。我们还将把<code>WL_HOME</code>转换成一个整数。请注意，并没有太多的阶级不平衡，我们需要采取积极的措施来处理它:<pre>nbagames = pd.read_csv("data/nbagames2017plus.csv", parse_dates=['GAME_DATE']) nbagames = nbagames.loc[nbagames.WL_HOME.isin(['W','L'])] nbagames.shape <strong class="bold">(4568, 149)</strong> nbagames['WL_HOME'] = \   np.where(nbagames.WL_HOME=='L',0,1).astype('int') nbagames.WL_HOME.value_counts(dropna=False) <strong class="bold">1    2586</strong> <strong class="bold">0    1982</strong> <strong class="bold">Name: WL_HOME, dtype: int64</strong></pre></li>

<li>现在，让我们<a id="_idIndexMarker1081"/>创建训练和测试数据框架，按照数字和分类特征组织它们。我们还应该生成一些描述性统计数据。因为我们在前一章已经做了，所以我们在这里不再重复；然而，回过头来看看这些数字，为建模阶段做好准备可能会有所帮助:<pre>num_cols = ['FG_PCT_HOME','FTA_HOME','FG3_PCT_HOME',   'FTM_HOME','FT_PCT_HOME','OREB_HOME','DREB_HOME',   'REB_HOME','AST_HOME','STL_HOME','BLK_HOME',   'TOV_HOME', 'FG_PCT_AWAY','FTA_AWAY','FG3_PCT_AWAY',   'FT_PCT_AWAY','OREB_AWAY','DREB_AWAY','REB_AWAY',   'AST_AWAY','STL_AWAY','BLK_AWAY','TOV_AWAY'] cat_cols = ['TEAM_ABBREVIATION_HOME','SEASON'] X_train, X_test, y_train, y_test =  \   train_test_split(nbagames[num_cols + cat_cols],\   nbagames[['WL_HOME']], test_size=0.2,random_state=0)</pre></li>

<li>现在，我们需要设置列转换。我们将处理数值特征的一些异常值，将这些值和任何缺失值分配给中值。然后，我们将使用标准定标器。我们将为分类特征设置一个热编码:<pre>ohe = OneHotEncoder(drop='first', sparse=False) cattrans = make_pipeline(ohe) standtrans = make_pipeline(OutlierTrans(2),   SimpleImputer(strategy="median"), StandardScaler()) coltrans = ColumnTransformer(   transformers=[     ("cat", cattrans, cat_cols),     ("stand", standtrans, num_cols)   ] )</pre></li>

<li>我们现在准备运行一个朴素贝叶斯分类器。我们将把一个高斯朴素贝叶斯实例添加到一个管道中，该管道将在列转换和一些递归特征消除之后运行:<pre>nb = GaussianNB() rfe = RFE(estimator=LogisticRegression(),   n_features_to_select=15) pipe1 = make_pipeline(coltrans, rfe, nb)</pre></li>

<li>让我们用 K-fold 交叉验证来评估这个模型。我们得到了不错的分数，虽然没有上一章中支持向量分类的分数好:<pre>kf = RepeatedStratifiedKFold(n_splits=7,n_repeats=10,\   random_state=0) scores = cross_validate(pipe1, X_train, \   y_train.values.ravel(), \   scoring=['accuracy','precision','recall','f1'], \   cv=kf, n_jobs=-1) print("accuracy: %.2f, precision: %.2f,    sensitivity: %.2f, f1: %.2f"  %   (np.mean(scores['test_accuracy']),\    np.mean(scores['test_precision']),\    np.mean(scores['test_recall']),\    np.mean(scores['test_f1']))) <strong class="bold">accuracy: 0.81, precision: 0.84, sensitivity: 0.83, f1: 0.83</strong></pre></li>

<li>使用<a id="_idIndexMarker1083"/>高斯朴素贝叶斯，只有一个超参数我们必须担心调整。我们可以确定使用<code>var_smoothing</code>超参数进行多少平滑处理。我们可以进行随机网格搜索，找出最佳值。</li>

</ol>

<p><code>var_smoothing</code>超参数决定方差的增加量，这将使模型更少依赖于接近平均值的实例:</p>

<pre>nb_params = {
    'gaussiannb__var_smoothing': np.logspace(0,-9, num=100)
}
rs = RandomizedSearchCV(pipe1, nb_params, cv=kf, \
  scoring='accuracy')
rs.fit(X_train, y_train.values.ravel())</pre>

<ol>

<li value="8">我们<a id="_idIndexMarker1084"/>得到了更好的精度:<pre>rs.best_params_ <strong class="bold">{'gaussiannb__var_smoothing': 0.657933224657568}</strong> rs.best_score_ <strong class="bold">0.8608648056923919</strong></pre></li>

<li>我们还应该看看不同迭代的结果。正如我们所见，平滑值越大越好:<pre>results = \   pd.DataFrame(rs.cv_results_['mean_test_score'], \     columns=['meanscore']).\   join(pd.DataFrame(rs.cv_results_['params'])).\   sort_values(['meanscore'], ascending=False) results <strong class="bold">         meanscore     gaussiannb__var_smoothing</strong> <strong class="bold">2        0.86086        0.65793</strong> <strong class="bold">1        0.85118        0.03511</strong> <strong class="bold">9        0.81341        0.00152</strong> <strong class="bold">5        0.81212        0.00043</strong> <strong class="bold">7        0.81180        0.00019</strong> <strong class="bold">8        0.81169        0.00002</strong> <strong class="bold">3        0.81152        0.00000</strong> <strong class="bold">6        0.81152        0.00000</strong> <strong class="bold">0        0.81149        0.00000</strong> <strong class="bold">4        0.81149        0.00000</strong></pre></li>

<li>我们还可以<a id="_idIndexMarker1085"/>查看每次迭代的平均拟合和得分时间:<pre>print("fit time: %.3f, score time: %.3f"  %   (np.mean(rs.cv_results_['mean_fit_time']),\   np.mean(rs.cv_results_['mean_score_time']))) <strong class="bold">fit time: 0.660, score time: 0.029</strong></pre></li>

<li>让我们来看看最佳模型的预测。除了提高精度，灵敏度也有所提高，从<code>0.83</code>到<code>0.92</code> : <pre>pred = rs.predict(X_test) print("accuracy: %.2f, sensitivity: %.2f, \   specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred, \      pos_label=0),   skmet.precision_score(y_test.values.ravel(), pred))) <strong class="bold">accuracy: 0.86, sensitivity: 0.92, specificity: 0.79, precision: 0.83</strong></pre></li>

<li>查看混淆矩阵来更好地理解模型是一个好主意:<pre>cm = skmet.confusion_matrix(y_test, pred) cmplot = skmet.ConfusionMatrixDisplay(   confusion_matrix=cm, display_labels=['Loss', 'Won']) cmplot.plot() cmplot.ax_.set(title='Home Team Win Confusion Matrix',    xlabel='Predicted Value', ylabel='Actual Value')</pre></li>

</ol>

<p>此<a id="_idIndexMarker1086"/>产生以下情节:</p>

<div><div><img alt="Figure 14.1 – Confusion matrix for home team wins based on the Gaussian naïve Bayes model&#10;&#10;" height="444" src="img/B17978_14_0011.jpg" width="523"/>

</div>

</div>

<p class="figure-caption">图 14.1-基于高斯朴素贝叶斯模型的主队获胜的混淆矩阵</p>

<p>这并不坏，虽然仍然不如前一章中我们的支持向量模型。特别是，我们希望在预测损失方面做得更好一点。这也反映在我们在上一步中看到的相对较低的特异性分数<strong class="bold"> 0.79 </strong>。回想一下<a id="_idIndexMarker1087"/>特异性是我们从实际的否定中正确预测出否定值的比率。</p>

<p>另一方面，拟合和评分进行得相当快。我们也不需要做太多的超参数调整。在对二进制或多类目标建模时，朴素贝叶斯通常是一个很好的起点。</p>

<p>朴素贝叶斯已经被证明是一种更受欢迎的文本分类方法。我们将在下一节中使用它。</p>

<h1 id="_idParaDest-165"><a id="_idTextAnchor167"/>用于文本分类的朴素贝叶斯</h1>

<p>也许令人惊讶的是，一种基于计算条件概率的算法对文本分类很有用。但是这遵循了一个非常简单的假设。让我们假设我们的文档可以由文档中每个单词的计数来很好地表示，而不考虑词序或语法。这就是所谓的词汇袋。一个词袋与一个分类目标的关系——比如，垃圾邮件/非垃圾邮件或肯定/否定——可以用多项朴素贝叶斯成功建模。</p>

<p>在这一节中，我们将使用文本消息数据。我们将使用的数据集包含垃圾邮件标签，而不是垃圾邮件。</p>

<p class="callout-heading">注意</p>

<p class="callout">公众可以在<a href="https://www.kaggle.com/datasets/team-ai/spam-text-message-classification">https://www . ka ggle . com/datasets/team-ai/spam-text-message-classification</a>下载这个关于短信的数据集。它包含两列:文本消息和垃圾邮件或非垃圾邮件(ham)标签。</p>

<p>让我们用朴素贝叶斯做一些文本分类:</p>

<ol>

<li value="1">We will <a id="_idIndexMarker1089"/>need a couple of modules that we have not used so far in this book. We will import <code>MultinomialNB</code>, which we will need to construct a multinomial naïve Bayes model. We will also need <code>CountVectorizer</code> to create a bag-of-words. We will import<a id="_idIndexMarker1090"/> the <code>SMOTE</code> module to handle class imbalance. Note that we will use an <em class="italic">imbalanced-learn</em> pipeline<a id="_idIndexMarker1091"/> rather than a <em class="italic">scikit-learn</em> one. This is because we will be using <code>SMOTE</code> in our pipeline:<pre>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from imblearn.pipeline import make_pipeline
from imblearn.over_sampling import SMOTE
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
import sklearn.metrics as skmet</pre><p class="callout-heading">注意</p><p class="callout">我们在本节中使用<code>SMOTE</code>进行过采样；也就是说，我们将在代表性不足的班级中复制实例。当我们担心我们的模型在捕获一个类中的变化方面做得不好时，过采样可能是一个好的选择，因为相对于一个或多个其他类，我们对该类的实例太少了。过采样复制该类的实例。</p></li>

<li>接下来，我们将加载文本消息数据集。我们将把我们的目标转换成一个整数变量，并确认它按预期工作。注意明显的阶级不平衡。让我们看看前几行，以便更好地理解这些数据:<pre>spamtext = pd.read_csv("data/spamtext.csv") spamtext['spam'] = np.where(spamtext.category=='spam',1,0) spamtext.groupby(['spam','category']).size() <strong class="bold">spam  category</strong> <strong class="bold">0     ham         4824</strong> <strong class="bold">1     spam         747</strong> <strong class="bold">dtype: int64</strong> spamtext.head()  <strong class="bold">category  message                                spam</strong> <strong class="bold">0  ham     Go until jurong point, crazy..         0</strong> <strong class="bold">1  ham     Ok lar... Joking wif u oni...          0</strong> <strong class="bold">2  spam    Free entry in 2 a wkly comp to win...  1</strong> <strong class="bold">3  ham     U dun say so early hor... U c already..0</strong> <strong class="bold">4  ham     Nah I don't think he goes to usf, ..   0</strong></pre></li>

<li>现在，我们<a id="_idIndexMarker1092"/>创建训练<a id="_idIndexMarker1093"/>和测试数据框架。我们将使用<code>stratify</code>参数来确保目标值在训练和测试数据中的平均分布。</li>

</ol>

<p>稍后我们还将实例化一个<code>CountVectorizer</code>对象来创建我们的单词包。我们表示希望忽略一些单词，因为它们不提供有用的信息。我们本来可以创建一个停用词列表，但在这里，我们将利用 scikit-learn 的英文停用词列表:</p>

<pre>X_train, X_test, y_train, y_test =  \
  train_test_split(spamtext[['message']],\
  spamtext[['spam']], test_size=0.2,\
  stratify=spamtext[['spam']], random_state=0)
countvectorizer = CountVectorizer(analyzer='word', \
  stop_words='english')</pre>

<ol>

<li value="4">让我们看看矢量器如何处理我们数据中的一些观察结果。为了便于查看，我们将只从包含少于 50 个字符的邮件中提取。</li>

</ol>

<p>使用矢量器<a id="_idIndexMarker1094"/>,我们得到每个观察使用的所有非停止词的计数<a id="_idIndexMarker1095"/>。例如，<code>like</code>在第一条消息中使用了一次，而在第二条消息中根本没有使用。这给了<code>like</code>转换数据中第一次观察的值<code>1</code>和第二次观察的值<code>0</code>。</p>

<p>我们不会在模型中使用这一步中的任何东西。我们这样做只是为了说明:</p>

<pre>smallsample = \
  X_train.loc[X_train.message.str.len()&lt;50].\
    sample(2, random_state=35)
smallsample
<strong class="bold">                                        message</strong>
<strong class="bold">2079                I can take you at like noon</strong>
<strong class="bold">5393  I dont know exactly could you ask chechi.</strong>
ourvec = \
    pd.DataFrame(countvectorizer.\
    fit_transform(smallsample.values.ravel()).\
    toarray(),\
    columns=countvectorizer.get_feature_names())
ourvec
<strong class="bold">    ask   chechi  dont   exactly  know  like  noon</strong>
<strong class="bold">0    0    0       0      0        0     1     1</strong>
<strong class="bold">1    1    1       1      1        1     0     0</strong></pre>

<ol>

<li value="5">现在，让我们<a id="_idIndexMarker1096"/>实例化一个<code>MultinomialNB</code>对象<a id="_idIndexMarker1097"/>并将其添加到管道中。我们将使用<code>SMOTE</code>添加过采样来处理类别不平衡:<pre>nb = MultinomialNB() smote = SMOTE(random_state=0) pipe1 = make_pipeline(countvectorizer, smote, nb) pipe1.fit(X_train.values.ravel(),    y_train.values.ravel())</pre></li>

<li>现在，让我们来看看一些预测。我们得到了令人印象深刻的 0.97 的准确率和同样好的特异性。这种极好的特异性表明我们没有很多假阳性。稍低的敏感度表明我们没有捕捉到一些积极的信息(垃圾邮件)，尽管我们仍然做得很好:<pre>pred = pipe1.predict(X_test.values.ravel()) print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),   skmet.precision_score(y_test.values.ravel(), pred))) <strong class="bold">accuracy: 0.97, sensitivity: 0.87, specificity: 0.98, precision: 0.87</strong></pre></li>

<li>用一个混淆矩阵<pre>cm = skmet.confusion_matrix(y_test, pred) cmplot = skmet.ConfusionMatrixDisplay(   confusion_matrix=cm, \   display_labels=['Not Spam', 'Spam']) cmplot.plot() cmplot.ax_.set(   title='Spam Prediction Confusion Matrix',    xlabel='Predicted Value', ylabel='Actual Value')</pre>来形象化我们模型的<a id="_idIndexMarker1099"/>表现是很有帮助的<a id="_idIndexMarker1098"/></li>

</ol>

<p>这会产生以下情节:</p>

<div><div><img alt="Figure 14.2 – Spam prediction using multinomial naïve Bayes&#10;&#10;" height="439" src="img/B17978_14_0021.jpg" width="564"/>

</div>

</div>

<p class="figure-caption">图 14.2–使用多项式朴素贝叶斯的垃圾邮件预测</p>

<p>当构建文本分类模型时，朴素贝叶斯<a id="_idIndexMarker1100"/>可以产生极好的结果<a id="_idIndexMarker1101"/>。这些指标通常很好，也很有效。这是一个非常简单的二元分类问题。然而，朴素贝叶斯也可以有效地处理多类文本分类问题。该算法的应用方式与我们在这里对多类目标的应用方式非常相似。</p>

<h1 id="_idParaDest-166"><a id="_idTextAnchor168"/>总结</h1>

<p>朴素贝叶斯是一个很好的算法，可以添加到我们解决分类问题的常规工具包中。这种方法通常不会产生偏差最小的预测。然而，另一方面也是如此。过度拟合的风险较小，尤其是在处理连续特征时。它也非常有效，可以很好地扩展到大量的观察值和大的特征空间。</p>

<p>本书接下来的两章将探索无监督学习算法——那些我们没有预测目标的算法。在下一章中，我们将研究主成分分析，然后在下一章中研究 K-均值聚类。</p>

</div>

</div>



</body></html>