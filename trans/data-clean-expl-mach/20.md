<title>Chapter 15: Principal Component Analysis</title>

# *第十五章*:主成分分析

降维是机器学习中比较重要的概念/策略之一。它有时等同于特征选择，但这是一种过于狭隘的降维观点。我们的模型经常不得不处理过多的特征，其中一些特征捕捉相同的信息。不解决这个问题会大大增加过度拟合或结果不稳定的风险。但是删除一些特性并不是我们工具箱中的唯一工具。特征提取策略，如**主成分分析** ( **PCA** )，往往能产生好的结果。

我们可以使用 PCA 来降低数据集的维度(特征的数量),而不会失去显著的预测能力。捕捉数据中大部分方差所需的主成分的数量通常少于特征的数量，通常少得多。

这些组件可用于我们的回归或分类模型，而不是初始特征。这不仅可以加快我们的模型学习的速度，还可以减少我们估计的方差。这种特征提取策略的主要缺点是新特征通常更难解释。当我们有分类特征时，PCA 也不是一个好的选择。

我们将通过首先检查每个组件是如何构造的来发展我们对 PCA 如何工作的理解。我们将构建一个主成分分析，解释结果，然后在分类模型中使用这些结果。最后，当我们的组件可能不是线性可分时，我们将使用内核来改进 PCA。

具体来说，我们将在本章中探讨以下主题:

*   PCA 的关键概念
*   基于主成分分析的特征提取
*   使用带 PCA 的内核

# 技术要求

在本章中，我们将主要关注 pandas、NumPy 和 scikit-learn 库。所有代码都用 scikit-learn 版本 0.24.2 和 1.0.2 进行了测试。

本章的代码可以从 GitHub 资源库下载:[https://GitHub . com/packt publishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning)。

# PCA 的关键概念

PCA 产生特征的多个线性组合，每个线性组合是一个组件。它确定一个获取最大方差的组件，第二个获取最大剩余方差的组件，然后是第三个组件，依此类推，直到到达我们指定的停止点。停止点可以基于组件的数量、解释的变化百分比或领域知识。

主成分的一个非常有用的特征是它们相互正交。这意味着它们是不相关的，这对建模来说是个好消息。*图 15.1* 显示了由特征 x1 和 x2 构成的两个组件。用 *PC1* 捕捉最大方差，用 *PC2* 捕捉最大剩余方差。(图中数据点是瞎编的。)注意，这两个向量是正交的(垂直的)。

![Figure 15.1 – An illustration of PCA with two features

](img/B17978_15_001.jpg)

图 15.1–具有两个特征的 PCA 示意图

你们中那些做过很多因子分析的人可能已经明白了大概的意思，即使这是你们第一次探索 PCA。主成分与因子没有太大的不同，尽管有一些概念和数学上的差异。使用主成分分析，可以分析所有的方差。只有变量之间的共享方差用因子分析来分析。在因子分析中，未观察到的因子被理解为*导致了*观察到的变量。PCA 不需要对潜在的、未观察到的力进行假设。

那么，这一点计算魔法是如何实现的呢？可通过以下步骤计算主成分:

1.  标准化您的数据。
2.  计算变量的协方差矩阵。
3.  计算协方差矩阵的特征向量和特征值。
4.  按特征值降序排列特征向量。第一个特征向量是主成分 1，第二个是主成分 2，以此类推。

没有必要完全理解这些步骤来理解本章其余部分的讨论。我们将得到 scikit-learn 来为我们做这项工作。不过，如果您计算一个非常小的数据子集(两列或三列，只有几行)的协方差矩阵，然后对该矩阵进行特征分解，这可能会提高您的直觉。一种更简单的尝试构造组件的方法是使用 NumPy 线性代数函数(`numpy.linalg`)，同时仍然是说明性的。这里的关键点是推导主分量的计算有多简单。

PCA 用于许多机器学习任务。它可以用来调整图像大小，分析财务数据，或者用于推荐系统。从本质上讲，对于任何有大量特性并且其中许多特性相互关联的应用程序来说，这都是一个不错的选择。

毫无疑问，你们中的一些人已经注意到，我无意中提到 PCA 构建了特征的线性组合。当线性可分性不可行时，我们该怎么办，就像我们在支持向量机中遇到的那样？事实证明，我们依赖于支持向量机的核心技巧也适用于 PCA。我们将在本章探讨如何实现内核 PCA。然而，我们将从一个相对简单的 PCA 例子开始。

# 利用主成分分析进行特征提取

PCA 可用于降维，为我们随后运行的模型做准备。虽然严格来说，PCA 不是一个特征选择工具，但我们可以用与运行 [*第 5 章*](B17978_05_ePub.xhtml#_idTextAnchor058) 、*特征选择*中的包装器特征选择方法几乎相同的方式来运行它。在一些预处理(比如处理异常值)之后，我们生成组件，然后我们可以将这些组件用作我们的新特性。有时我们实际上并不在模型中使用这些组件。相反，我们生成它们主要是为了帮助我们更好地可视化我们的数据。

为了说明 PCA 的使用，我们将使用国家篮球协会的比赛数据。该数据集包含从 2017/2018 赛季到 2020/2021 赛季的每场 NBA 比赛的统计数据。这包括主队；主队是否获胜；客队；客队和主队的投篮命中率；两队失误、篮板、助攻；以及其他一些措施。

注意

公众可以在 https://www.kaggle.com/datasets/wyattowalsh/basketball[下载 NBA 比赛数据。该数据集包含从 1946/1947 NBA 赛季开始的比赛数据。它使用`nba_api`从 nba.com 获取统计数据。该 API 在 https://github.com/swar/nba_api 有售。](https://www.kaggle.com/datasets/wyattowalsh/basketball)

让我们在模型中使用 PCA:

1.  我们从加载所需的库开始。除了 scikit-learn 的`PCA`模块:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.impute import SimpleImputer from sklearn.linear_model import LogisticRegression from sklearn.decomposition import PCA from sklearn.model_selection import RandomizedSearchCV from scipy.stats import uniform from scipy.stats import randint import os import sys sys.path.append(os.getcwd() + “/helperfunctions”) from preprocfunc import OutlierTrans
    ```

    之外，你已经在前面的章节中看到了所有这些
2.  接下来，我们加载 NBA 数据并做一些清理。一些实例没有主队赢或输的值，`WL_HOME`，所以我们删除它们。`WL_HOME`将是我们的目标。在我们构建好组件之后，我们将尝试稍后对其进行建模。注意，主队大部分时间都赢了，但是阶级不平衡还不错:

    ```
    nbagames = pd.read_csv(“data/nbagames2017plus.csv”, parse_dates=[‘GAME_DATE’]) nbagames = nbagames.loc[nbagames.WL_HOME.isin([‘W’,’L’])] nbagames.shape (4568, 149) nbagames[‘WL_HOME’] = \   np.where(nbagames.WL_HOME==’L’,0,1).astype(‘int’) nbagames.WL_HOME.value_counts(dropna=False) 1    2586 0    1982 Name: WL_HOME, dtype: int64
    ```

3.  我们应该看一些描述性统计:

    ```
    num_cols = [‘FG_PCT_HOME’,’FTA_HOME’,’FG3_PCT_HOME’,   ‘FTM_HOME’,’FT_PCT_HOME’,’OREB_HOME’,’DREB_HOME’,   ‘REB_HOME’,’AST_HOME’,’STL_HOME’,’BLK_HOME’,   ‘TOV_HOME’, ‘FG_PCT_AWAY’,’FTA_AWAY’,’FG3_PCT_AWAY’,   ‘FT_PCT_AWAY’,’OREB_AWAY’,’DREB_AWAY’,’REB_AWAY’,   ‘AST_AWAY’,’STL_AWAY’,’BLK_AWAY’,’TOV_AWAY’] nbagames[[‘WL_HOME’] + num_cols].agg([‘count’,’min’,’median’,’max’]).T
    ```

这产生了以下输出。没有丢失值，但是我们的特征具有非常不同的范围。我们需要做一些扩展:

```
 count       min     median    max
WL_HOME         4,568.00    0.00    1.00      1.00
FG_PCT_HOME     4,568.00    0.27    0.47      0.65
FTA_HOME        4,568.00    1.00    22.00     64.00
FG3_PCT_HOME    4,568.00    0.06    0.36      0.84
FTM_HOME        4,568.00    1.00    17.00     44.00
FT_PCT_HOME     4,568.00    0.14    0.78      1.00
OREB_HOME       4,568.00    1.00    10.00     25.00
DREB_HOME 4,568.00    18.00   35.00     55.00
REB_HOME        4,568.00    22.00   45.00     70.00
AST_HOME        4,568.00    10.00 24.00     50.00
STL_HOME        4,568.00    0.00    7.00      22.00
BLK_HOME        4,568.00    0.00    5.00      20.00
TOV_HOME 4,568.00    1.00    14.00     29.00
FG_PCT_AWAY     4,568.00    0.28    0.46      0.67
FTA_AWAY        4,568.00    3.00 22.00     54.00
FG3_PCT_AWAY    4,568.00    0.08    0.36      0.78
FT_PCT_AWAY     4,568.00    0.26    0.78      1.00
OREB_AWAY 4,568.00    0.00    10.00     26.00
DREB_AWAY       4,568.00    18.00   34.00     56.00
REB_AWAY        4,568.00    22.00 44.00     71.00
AST_AWAY        4,568.00    9.00    24.00     46.00
STL_AWAY        4,568.00    0.00    8.00      19.00
BLK_AWAY 4,568.00    0.00    5.00      15.00
TOV_AWAY        4,568.00    3.00    14.00     30.00
```

1.  让我们也检查一下我们的特征是如何关联的:

    ```
    corrmatrix = nbagames[[‘WL_HOME’] + num_cols].\   corr(method=”pearson”) sns.heatmap(corrmatrix, xticklabels=corrmatrix.columns,   yticklabels=corrmatrix.columns, cmap=”coolwarm”) plt.title(‘Heat Map of Correlation Matrix’) plt.tight_layout() plt.show()
    ```

这会产生以下情节:

![Figure 15.2 – Heat map of NBA features](img/B17978_15_002.jpg)

图 15.2-NBA 特征热图

许多特征是显著正相关或负相关的。比如主队(`FG_PCT_HOME`)的投篮命中率和主队(`FG3_PCT_HOME`)的三分球命中率是正相关的，并不奇怪。此外，主队的篮板(`REB_HOME`)和主队的防守篮板(`DREB_HOME`)可能过于密切相关，任何模型都无法理清它们的影响。

这个数据集可能是 PCA 的一个很好的候选。虽然有些特征是高度相关的，但我们仍然会丢失一些信息。PCA 至少提供了在不丢失信息的情况下处理相关性的可能性。

1.  现在我们创建训练和测试数据帧:

    ```
    X_train, X_test, y_train, y_test =  \   train_test_split(nbagames[num_cols],\   nbagames[[‘WL_HOME’]],test_size=0.2, random_state=0)
    ```

2.  我们现在已经准备好创建组件了。我们有点武断地指出，我们想要七个组件。(稍后，我们将使用超参数调谐来选择组件的数量。)在运行 PCA 之前，我们设置了管道来做一些预处理:

    ```
    pca = PCA(n_components=7) pipe1 = make_pipeline(OutlierTrans(2),       SimpleImputer(strategy=”median”),       StandardScaler(), pca) pipe1.fit(X_train)
    ```

3.  我们现在可以使用`pca`对象的`components_`属性。这将返回所有 23 个特征在七个组件中的每一个上的分数:

    ```
    components = pd.DataFrame(pipe1[‘pca’].components_,   columns=num_cols) components.T.to_excel(‘views/components.xlsx’)
    ```

这将生成以下电子表格:

![Figure 15.3 – Principal components of NBA features

](img/B17978_15_003.jpg)

图 15.3-NBA 特征的主要组成部分

每个特征说明了每个组件差异的一部分。(如果对每个部分，你对 23 个分数中的每一个进行平方，然后对平方求和，你得到的总数是 1。)如果您想了解哪些特性真正驱动了一个组件，请寻找绝对值最大的特性。对于组件 1，主队的投篮命中率(`FG_PCT_HOME`)最为重要，其次是客场队的篮板数(`REB_AWAY`)。

回想一下我们在本章开始时的讨论，每个组件都试图捕捉前一个或多个组件之后剩余的差异。

1.  让我们展示前三个组件的五个最重要的特性。第一个组成部分似乎主要是关于主队的投篮命中率和每个队的篮板。第二个组成部分似乎没有太大的不同，但第三个组成部分是由罚球和尝试(`FTM_HOME`和`FTA_HOME`)以及失误(`TOV_HOME`和`TOV_AWAY` ):

    ```
    components.pc1.abs().nlargest(5) FG_PCT_HOME    0.38 REB_AWAY       0.37 DREB_AWAY      0.34 REB_HOME       0.33 FG_PCT_AWAY    0.30 Name: pc1, dtype: float64 components.pc2.abs().nlargest(5) DREB_HOME      0.38 FG_PCT_AWAY    0.37 DREB_AWAY      0.32 REB_HOME       0.32 FG_PCT_HOME    0.29 Name: pc2, dtype: float64 components.pc3.abs().nlargest(5) FTM_HOME    0.55 FTA_HOME    0.53 TOV_HOME    0.30 STL_AWAY    0.27 TOV_AWAY    0.26 Name: pc3, dtype: float64
    ```

    所驱动的
2.  我们可以使用`pca`对象的`explained_variance_ratio_`属性来检查每个组件捕获了多少差异。第一个成分解释了特征变化的 14.5%。第二个成分解释了另外 13.4%。如果我们使用 NumPy 的`cumsum`方法，我们可以看到这七个因素总共解释了大约 65%的方差。

所以,仍然存在相当大的差异。我们可能希望为我们构建的任何模型使用更多组件:

```
np.set_printoptions(precision=3)
pipe1[‘pca’].explained_variance_ratio_
array([0.145, 0.134, 0.095, 0.086, 0.079, 0.059, 0.054])
np.cumsum(pipe1[‘pca’].explained_variance_ratio_)
array([0.145, 0.279, 0.374, 0.46 , 0.539, 0.598, 0.652])
```

1.  我们可以绘制前两个主成分图，看看它们能在多大程度上区分主队的输赢。我们可以使用管道的`transform`方法创建一个包含主要组件的数据帧，并将其与目标的数据帧连接起来。

我们使用 Seaborn 的`scatterplot`的便利的`hue`属性来显示输赢。前两个主成分很好地区分了输赢，尽管它们加起来只占我们特征变化的 28%:

```
X_train_pca = pd.DataFrame(pipe1.transform(X_train),
  columns=components.columns, index=X_train.index).join(y_train)
sns.scatterplot(x=X_train_pca.pc1, y=X_train_pca.pc2, hue=X_train_pca.WL_HOME)
plt.title(“Scatterplot of First and Second Components”)
plt.xlabel(“Principal Component 1”)
plt.ylabel(“Principal Component 2”)
plt.show()
```

此产生以下图形:

![Figure 15.4 – Scatterplot of wins and losses by first and second principal components

](img/B17978_15_004.jpg)

图 15.4–第一和第二主成分的赢输散点图

1.  让我们用主成分来预测主队是否会赢。我们只需在管道中添加一个逻辑回归就可以做到这一点。我们还进行网格搜索以找到最佳超参数值:

    ```
    lr = LogisticRegression() pipe2 = make_pipeline(OutlierTrans(2),   SimpleImputer(strategy=”median”), StandardScaler(),   pca, lr) lr_params = {   “pca__n_components”: randint(3, 20),   “logisticregression__C”: uniform(loc=0, scale=10) } rs = RandomizedSearchCV(pipe2, lr_params, cv=4,    n_iter=40, scoring=’accuracy’, random_state=1) rs.fit(X_train, y_train.values.ravel())
    ```

2.  我们现在可以查看最佳参数并进行评分。正如我们在前面的步骤中所怀疑的那样，网格搜索表明，我们的逻辑回归模型在包含更多组件的情况下表现更好。我们得到了很高的分数。

我们在 [*第十章*](B17978_10_ePub.xhtml#_idTextAnchor126) *、逻辑回归*中详细讨论超参数 *C* :

```
rs.best_params_
{‘logisticregression__C’: 6.865009276815837, ‘pca__n_components’: 19}
rs.best_score_
0.9258345296842831
```

这个部分展示了我们如何从数据集生成主成分，以及如何解释这些成分。我们还研究了如何在模型中使用主要组件，而不是初始特征。但是我们假设主成分可以很好地描述为特征的线性组合。事实往往并非如此。在下一节中，我们将使用核 PCA 来处理非线性关系。

# 使用带 PCA 的内核

对于某些数据，不可能构建线性可分的主成分。在我们建模之前，这可能不太容易想象。幸运的是，我们可以使用一些工具来确定产生最佳结果的内核，包括线性内核。具有线性核的核 PCA 应该与标准 PCA 类似地执行。

在本节中，我们将使用 kernel PCA 对国家一级的劳动力参与率、教育程度、青少年生育频率以及按性别划分的参政情况进行特征提取。

注意

这份关于教育和劳动力结果性别差异的数据集由联合国开发计划署在 https://www.kaggle.com/datasets/undp/human-development 公开使用。2015 年，每个国家都有一份按性别分列的就业、收入和教育数据记录。

让我们开始构建模型:

1.  我们将导入我们一直在使用的相同库加上 scikit-learn 的`KernelPCA`模块。我们还将导入`RandomForestRegressor`模块:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import make_pipeline from sklearn.impute import SimpleImputer from sklearn.decomposition import KernelPCA from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import RandomizedSearchCV import seaborn as sns import matplotlib.pyplot as plt import os import sys sys.path.append(os.getcwd() + “/helperfunctions”) from preprocfunc import OutlierTrans
    ```

2.  我们按性别加载教育和劳动力成果的数据。我们构建了女性与男性收入比率、受教育年限比率、劳动力参与率和人类发展指数比率的序列:

    ```
    un_income_gap = pd.read_csv(“data/un_income_gap.csv”) un_income_gap.set_index(‘country’, inplace=True) un_income_gap[‘incomeratio’] = \   un_income_gap.femaleincomepercapita / \     un_income_gap.maleincomepercapita un_income_gap[‘educratio’] = \   un_income_gap.femaleyearseducation / \      un_income_gap.maleyearseducation un_income_gap[‘laborforcepartratio’] = \   un_income_gap.femalelaborforceparticipation / \      un_income_gap.malelaborforceparticipation un_income_gap[‘humandevratio’] = \   un_income_gap.femalehumandevelopment / \      un_income_gap.malehumandevelopment un_income_gap.dropna(subset=[‘incomeratio’], inplace=True)
    ```

3.  让我们来看一些描述性的统计数据。有一些丢失的值，特别是`genderinequality`和`humandevratio`。有些功能的范围比其他功能大得多:

    ```
    num_cols = [‘educratio’,’laborforcepartratio’,   ‘humandevratio’,’genderinequality’,   ‘maternalmortality’,’adolescentbirthrate’,   ‘femaleperparliament’,’incomepercapita’] gap_sub = un_income_gap[[‘incomeratio’] + num_cols] gap_sub.\   agg([‘count’,’min’,’median’,’max’]).T                        count    min    median  max incomeratio            177.00   0.16   0.60    0.93 educratio              169.00   0.24   0.93    1.35 laborforcepartratio    177.00   0.19   0.75    1.04 humandevratio          161.00   0.60   0.95    1.03 genderinequality       155.00   0.02   0.39    0.74 maternalmortality     174.00   1.00   60.00  1,100.00 adolescentbirthrate    177.00   0.60   40.90  204.80 femaleperparliament    174.00   0.00   19.35    57.50 incomepercapita        177.00   581.00 10,512.00  123,124.00
    ```

4.  我们还应该看看一些相关性:

    ```
    corrmatrix = gap_sub.corr(method=”pearson”) sns.heatmap(corrmatrix,    xticklabels=corrmatrix.columns,   yticklabels=corrmatrix.columns, cmap=”coolwarm”) plt.title(‘Heat Map of Correlation Matrix’) plt.tight_layout() plt.show()
    ```

这会产生以下情节:

![Figure 15.5 – Correlation matrix of NBA games data

](img/B17978_15_005.jpg)

图 15.5-NBA 比赛数据的相关矩阵

`humandevratio`和`educratio`高度相关，`genderinequality`和`adolescentbirthrate`也是如此。我们可以看到`educratio`和`maternalmortality`是高度负相关的。考虑到所有这些特征的高度相关性，很难建立一个性能良好的模型。然而，我们可以用核主成分分析来降低维数。

1.  我们创建训练和测试数据框架:

    ```
    X_train, X_test, y_train, y_test =  \   train_test_split(gap_sub[num_cols],\   gap_sub[[‘incomeratio’]], test_size=0.2,   random_state=0)
    ```

2.  我们现在准备实例化`KernelPCA`和`RandomForestRegressor`对象。我们将两者都添加到管道中。我们还用我们的核 PCA 和随机森林回归子的超参数创建了一个字典。

字典有一系列超参数值，用于组件数量、伽玛和内核 PCA 使用的内核。对于那些不使用 gamma 的内核，这些值将被忽略。注意，内核的一个选项是线性内核。

我们在 [*第 8 章*](B17978_08_ePub.xhtml#_idTextAnchor106) *、支持向量回归*和 [*第 13 章*](B17978_13_ePub.xhtml#_idTextAnchor152) *、支持向量机分类*中更详细地讨论伽马:

```
rfreg = RandomForestRegressor()
kpca = KernelPCA()
pipe1 = make_pipeline(OutlierTrans(2),
  SimpleImputer(strategy=”median”), MinMaxScaler(),
  kpca, rfreg)
rfreg_params = {
 ‘kernelpca__n_components’:
    randint(2, 9),
 ‘kernelpca__gamma’:
     np.linspace(0.03, 0.3, 10),
 ‘kernelpca__kernel’:
     [‘linear’, ‘poly’, ‘rbf’, 
      ‘sigmoid’, ‘cosine’],
 ‘randomforestregressor__max_depth’:
     randint(2, 20),
 ‘randomforestregressor__min_samples_leaf’:
     randint(5, 11)
}
```

1.  现在让我们用这些超参数值进行随机网格搜索。为提供最佳随机森林回归性能的主成分分析的核心是多项式。我们得到一个很好的均方误差的平方，大约是平均值的 10%:

    ```
    rs = RandomizedSearchCV(pipe1, rfreg_params,   cv=4, n_iter=40,   scoring=’neg_mean_absolute_error’,   random_state=1) rs.fit(X_train, y_train.values.ravel()) rs.best_params_ {‘kernelpca__gamma’: 0.12000000000000001,  ‘kernelpca__kernel’: ‘poly’,  ‘kernelpca__n_components’: 4,  ‘randomforestregressor__max_depth’: 18,  ‘randomforestregressor__min_samples_leaf’: 5} rs.best_score_ -0.06630618838886537
    ```

2.  再来看看其他表现优秀的车型。内核为`rbf`的模型和内核为 sigmoid 的模型表现几乎一样好。第二和第三个表现最好的模型比表现最好的模型有更多的主成分:

    ```
    results = \   pd.DataFrame(rs.cv_results_[‘mean_test_score’], \     columns=[‘meanscore’]).\   join(pd.DataFrame(rs.cv_results_[‘params’])).\   sort_values([‘meanscore’], ascending=False) results.iloc[1:3].T                                         39     0  meanscore                              -0.067 -0.070 kernelpca__gamma                        0.240  0.180 kernelpca__kernel                       rbf    sigmoid kernelpca__n_components                 6      6 randomforestregressor__max_depth        12     10 randomforestregressor__min_samples_leaf 5      6
    ```

核 PCA 是一个相对容易实现的降维选项。当我们有许多高度相关的特征，而这些特征可能不是线性可分的，并且对预测的解释并不重要时，这是最有用的。

# 总结

本章探讨了主成分分析，包括它是如何工作的，以及我们什么时候可能需要使用它。我们学习了如何检查从 PCA 创建的组件，包括每个特征如何贡献给每个组件，以及解释了多少差异。我们讨论了如何可视化组件，以及如何在后续分析中使用组件。我们还研究了如何将内核用于 PCA，以及什么时候这可能会给我们带来更好的结果。

我们在下一章探索另一种无监督学习技术，k-means 聚类。