<title>Chapter 11: Decision Trees and Random Forest Classification</title>

# *第十一章*:决策树和随机森林分类

决策树和随机森林是非常流行的分类模型。这部分是因为它们易于训练和解释。它们也相当灵活。我们可以模拟复杂性，而不必增加特征空间或转换特征。我们甚至不需要做任何特殊的事情来将算法应用于多类问题，这是我们在逻辑回归中必须做的事情。

另一方面，决策树可能不如其他分类模型稳定，对训练数据中的微小变化相当敏感。当存在显著的类别不平衡时(当一个类别中的观察值比另一个类别中的多得多时)，决策树也会有偏差。幸运的是，这些问题可以通过 bagging 等技术来解决，以减少差异，并通过过采样来处理不平衡。我们将在本章中研究这些技术。

在本章中，我们将讨论以下主题:

*   关键概念
*   决策树模型
*   实现随机森林
*   实现梯度增强

# 技术要求

除了我们到目前为止一直在使用的 scikit-learn 模块之外，我们将使用不平衡学习中的 SMOTENC。我们将使用 SMOTENC 来解决阶级不平衡问题。不平衡学习库可以安装`pip install -U imbalanced-learn`。本章中的所有代码都是用 scikit-learn 版本 0.24.2 和 1.0.2 测试的。

# 关键概念

决策树是一种非常有用的机器学习工具。它们是非参数的，易于解释，并且可以处理大范围的数据。没有关于特征和目标之间关系的线性和误差项的正态性的假设。甚至没有必要缩放数据。决策树通常也能很好地捕捉预测者和目标之间的复杂关系。

决策树算法的灵活性及其对数据中复杂且不可预测的关系进行建模的能力归功于用于分割数据的递归分区**过程**。决策树根据其特征值对观察值进行分组。这是通过一系列二元决策来完成的，从根节点的初始分裂开始，到每个分组的叶子结束。每一次分割都基于提供关于目标的最多信息的特征和特征值。更准确地说，一个分裂的选择是基于它是否产生最低的基尼系数。我们将在后面更详细地讨论基尼系数。

沿着从根节点到叶子的分支，具有相同值或相同值范围的所有新观察值都获得目标的相同预测值。当目标是分类的时，这是在该叶的训练观察中目标的最频繁值。

下图提供了一个相当直观的决策树示例，其中包含上过大学的人的大学毕业模型的虚构数据和结果。对于这个决策树，我们发现，与其他可用特征以及其他阈值相比，将高中 GPA 初始分为 3.0 或更低和 GPA 大于 3.0 的高中 GPA 会导致最低的杂质。所以高中 GPA 是我们的根节点，也就是深度 0:

![ Figure 11.1 – A decision tree for college completion

](img/B17978_11_001.jpg)

图 11.1-完成大学学业的决策树

根节点处的二进制分裂导致 45%的观察值在树的左侧，55%在右侧。在深度 1，双方基于父母收入进行二元分割，尽管门槛不同；左侧和右侧分别为$80k 和$45k。对于父母收入在 80k 美元以上，高中平均绩点大于 3 的家庭，不再进行分割。在这里，我们得到一个**毕业**的预测。这是一个叶节点。

我们可以从每一片叶子开始沿着树向上导航，描述树是如何分割数据的，就像我们对父母收入超过 8 万美元、高中 GPA 超过 3 的个人所做的那样。例如，决策树预测那些从学校获得低水平支持、父母收入超过 45，000 美元、高中 GPA 低于或等于 3 的人不会毕业。

那么，决策树算法是如何发挥这种魔力的呢？它如何选择特征和阈值量或类值？为什么父母收入大于 8 万美元或 4.5 万美元？为什么父母收入少于或等于 45，000 美元的学生在深度 2(分割 3)获得补助，但其他假期的学生补助水平为深度 2？一片叶子在深度 2 处甚至没有进一步的分裂。

衡量二元分裂提供的类信息的一种方法是看它在多大程度上帮助我们区分类内和类外成员。我们经常使用基尼系数计算进行评估，尽管有时也使用熵。基尼不纯统计告诉我们在每个节点上类成员被分割的有多好。这可以从下面的公式中看出:

![](img/B17978_11_0011.jpg)

这里，![](img/B17978_11_002.png)是属于 *k* 类的概率， *m* 是类的数量。如果类成员在一个节点上是相等的，那么基尼系数是 0.5。完全纯的时候是 0。

尝试手工计算基尼系数可能会有所帮助，以便更好地理解它是如何工作的。我们可以为下图所示的非常简单的决策树这样做。只有两个叶节点，一个用于高中 GPA 大于 3 的个人，另一个用于高中 GPA 小于或等于 3 的个人。(同样，这些计数是为了说明的目的而编造的。我们假设*图 11.1* 中使用的百分比是 100 人中的计数。)看一下下图:

![Figure 11.2 - A decision tree with one split and Gini Impurity calculations

](img/B17978_11_002.jpg)

图 11.2 -具有一次分割和基尼系数杂质计算的决策树

基于这个模型，GPA 高的人会被预测为**毕业**，因为这些人中的大多数，45 人中的 40 人，确实毕业了。基尼系数相当低，这很好。我们可以使用前面公式计算该节点的 Gini 杂质:

![](img/B17978_11_003.jpg)

我们的模型会预测高中 GPA 低于或等于 3 的个人**没有毕业**，因为这些个人中的大多数没有从大学毕业。然而，这里的纯度要低得多。该节点的 Gini 杂质值如下:

![](img/B17978_11_004.jpg)

决策树算法计算来自给定点的所有可能分裂的基尼系数杂质值的加权和，并选择分数最低的分裂。如果使用熵而不是基尼系数，该算法将遵循类似的过程。本章我们将使用 scikit-learn 的**分类和回归树** ( **CART** )算法来构建决策树。这个工具默认使用基尼系数，尽管我们可以让它使用熵来代替。

决策树就是我们所说的贪婪学习者。该算法选择一个分裂，在当前水平上给我们最好的基尼系数或熵值。它不检查该选择如何影响随后可用的拆分，而是基于该信息在当前级别重新考虑该选择。这使得该算法比其他情况更有效，但是它可能不会提供全局最优解。

决策树的主要缺点是方差大。他们可能会过度拟合训练数据中的异常观察值，因此无法很好地处理新数据。根据我们数据的特征，每次我们拟合决策树时，我们可以得到一个非常不同的模型。我们可以使用集合方法，如 bagging 或随机森林，来解决这个问题。

## 使用随机森林进行分类

随机森林，也许不奇怪，是决策树的集合。但这并不能区分随机森林和引导聚集，通常称为装袋。Bagging 通常用于减少机器学习算法的方差，例如具有高方差的决策树。通过 bagging，我们从数据集生成随机样本，比如 100 个。然后，我们对每个样本运行我们的模型，如决策树分类器，对预测进行平均。

然而，用 bagging 生成的样本可以相互关联，并且生成的决策树可能有许多相似之处。当只有几个特征可以解释大部分差异时，这种情况更有可能发生。随机森林通过限制每次分割可选择的要素数量来解决这一问题。决策树分类模型的一个好的经验法则是用可用特征数量的平方根来确定要使用的特征数量。例如，如果有 25 个特征，我们将为每个分割使用 5 个。

让我们更精确地描述一下构建随机森林的步骤:

1.  从训练数据中随机抽取替换实例。样本与原始数据集具有相同数量的观察值。
2.  从样本中随机选择并替换特征。(每次要选择的功能数量是可用功能总数的平方根。)
3.  从在*步骤 2* 中随机选择的特征中确定一个特征，在该特征中，分裂将导致具有最大纯度的节点。
4.  **内循环**:重复*步骤 2* 和 *3* ，直到构建好一个决策树。
5.  **外循环**:重复所有步骤，包括内循环，直到创建出所需数量的树。所有树的结果由投票决定；也就是说，对于给定的特征值，基于所有树中最频繁的类别标签来预测类别。

这个过程的一个很酷的副作用是，可以说它为我们生成了测试数据。bootstrapping 过程——即替换抽样——将导致一个或多个树的许多实例被排除在样本之外，通常多达三分之一。这些实例被称为**随机样本**，可用于评估模型。

将我们的类预测建立在许多不相关的决策树上对方差有积极的影响(降低方差！)这是你所期望的。随机森林模型通常比决策树模型更一般化。它们不太容易过度拟合，也不太可能被异常数据左右。但这是有代价的。构建一百或更多的决策树比仅仅构建一个决策树消耗更多的系统资源。我们也失去了解释决策树的便利性；很难解释每个特性的重要性。

## 使用梯度增强决策树

从概念上讲，梯度增强决策树类似于随机森林。他们依靠多个决策树来提高模型性能。但是它们是按顺序执行的，每棵树都从前面的树中学习。每一棵新树都是从上一次迭代的残差开始工作的。

梯度增强决策树的学习速率由*超参数决定。你可能想知道为什么我们不希望我们的模型尽可能快地学习。学习速度越快，效率越高，占用的系统资源越少。但是，我们可以用较低的学习率建立一个更通用的模型。过度拟合的风险较小。最优学习率最终是一个经验问题。我们需要做一些超参数调整来找到它。我们将在本章的最后一节做这件事。*

正如随机森林的情况一样，我们可以通过完成二元分类问题的步骤来提高我们对梯度增强的直觉:

1.  基于样本中目标的平均值对目标进行初始预测。对于二元目标，这是一个比例。将此预测分配给所有观察值。(这里我们用的是类成员几率的对数。)
2.  计算每个实例的残差，对于类内实例，残差为 1 减去初始预测，对于类外实例，残差为 0 减去预测或-预测。
3.  构建一个决策树来预测残差。
4.  基于决策树模型生成新的预测。
5.  基于由学习率缩放的新预测，调整每个实例的先前预测。如前所述，我们使用学习率是因为我们不希望预测过快地向正确的方向发展。
6.  循环回到*步骤 3* ，除非已经达到树的最大数量或者残差非常小。

虽然这是对梯度增强如何工作的简化解释，但它确实为我们提供了该算法正在做什么的良好感觉。希望它也能帮助你理解为什么梯度推进变得如此流行。该算法重复工作以调整先前的错误，但这样做相对有效，并且比单独的决策树具有更小的过拟合风险。

在本章的剩余部分，我们将学习决策树、随机森林和梯度推进的例子。我们将讨论如何调整超参数以及如何评估这些模型。我们还将讨论每种方法的优缺点。

# 决策树模型

我们将在本章中再次处理心脏疾病数据。这将是一个伟大的方式来比较我们的结果从逻辑回归模型的非参数模型，如决策树。请遵循以下步骤:

1.  首先，我们加载到目前为止一直在使用的相同库。新模块是来自 scikit-learn 的`DecisionTreeClassifier`和来自不平衡学习的`SMOTENC`，它们将帮助我们处理不平衡数据:

    ```
    import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder from imblearn.pipeline import make_pipeline from sklearn.compose import ColumnTransformer from sklearn.model_selection import RandomizedSearchCV from imblearn.over_sampling import SMOTENC from sklearn.tree import DecisionTreeClassifier, plot_tree from scipy.stats import randint import sklearn.metrics as skmet import os import sys sys.path.append(os.getcwd() + "/helperfunctions") from preprocfunc import MakeOrdinal,\   ReplaceVals
    ```

2.  让我们加载心脏病数据。我们将把我们的目标`heartdisease`转换成一个`0`和`1`整数。我们将不再重复我们根据上一章的数据生成的频率和描述性统计数据。如果你没有完成第 10 章 、*逻辑回归* :

    ```
    healthinfo = pd.read_csv("data/healthinfosample.csv") healthinfo.set_index("personid", inplace=True) healthinfo.heartdisease.value_counts() No     27467 Yes     2533 Name: heartdisease, dtype: int64 healthinfo['heartdisease'] = \   np.where(healthinfo.heartdisease=='No',0,1).\   astype('int') healthinfo.heartdisease.value_counts() 0    27467 1     2533 Name: heartdisease, dtype: int64
    ```

    ，快速浏览一下它们可能会有帮助

注意阶级的不平衡。不到 10%的观察对象有心脏病。我们需要在我们的模型中处理这个问题。

1.  让我们也看看年龄类别特性的值，因为它有点不寻常。它包含年龄范围的字符数据。我们将使用加载的`MakeOrdinal`类将它转换成一个序数特征。例如，`18-24`的值将会给我们一个`0`的值，而`50-54`的值在我们稍后进行转换后将会是`6`:

    ```
    healthinfo.agecategory.value_counts().\   sort_index().reset_index()           index  agecategory 0         18-24         1973 1         25-29         1637 2         30-34         1688 3         35-39         1938 4         40-44         2007 5         45-49         2109 6         50-54         2402 7         55-59         2789 8         60-64         3122 9         65-69         3191 10        70-74         2953 11        75-79         2004 12  80 or older         2187
    ```

2.  我们应该按照数据类型来组织我们的特征，因为这将使以后的一些任务更容易。我们还将建立一个字典来记录`genhealth`和`diabetic`特性:

    ```
    num_cols = ['bmi','physicalhealthbaddays',    'mentalhealthbaddays','sleeptimenightly'] binary_cols = ['smoking','alcoholdrinkingheavy',   'stroke','walkingdifficult','physicalactivity',   'asthma','kidneydisease','skincancer'] cat_cols = ['gender','ethnicity'] spec_cols1 = ['agecategory'] spec_cols2 = ['genhealth','diabetic'] rep_dict = {   'genhealth': {'Poor':0,'Fair':1,'Good':2,     'Very good':3,'Excellent':4},   'diabetic': {'No':0,     'No, borderline diabetes':0,'Yes':1,     'Yes (during pregnancy)':1}            }
    ```

3.  现在，让我们创建训练和测试数据框架:

    ```
    X_train, X_test, y_train, y_test =  \   train_test_split(healthinfo[num_cols +      binary_cols + cat_cols + spec_cols1 +     spec_cols2],\   healthinfo[['heartdisease']], test_size=0.2,     random_state=0)
    ```

4.  接下来，我们将设置列转换。我们将使用我们的自定义类将`agecategory`特性编码为序数，并将`genhealth`和`diabetic`的字符值替换为数字值。

我们将不转换数字列，因为在使用决策树时通常没有必要缩放这些特性。我们也不会担心离群值,因为决策树对它们的敏感度低于逻辑回归。我们将把`remainder`设置为`passthrough`,让转换器按原样传递剩余的列(数字列):

```
ohe = OneHotEncoder(drop='first', sparse=False)
spectrans1 = make_pipeline(MakeOrdinal())
spectrans2 = make_pipeline(ReplaceVals(rep_dict))
bintrans = make_pipeline(ohe)
cattrans = make_pipeline(ohe)
coltrans = ColumnTransformer(
  transformers=[
    ("bin", bintrans, binary_cols),
    ("cat", cattrans, cat_cols),
    ("spec1", spectrans1, spec_cols1),
    ("spec2", spectrans2, spec_cols2),
  ],
    remainder = 'passthrough'
)
```

1.  在运行我们的模型之前，我们需要做一些工作。正如您将在下一步中看到的，我们需要知道有多少特性将从一键编码器返回。我们应该在做这件事的时候也抓住新特性的名字。我们以后还会需要它们。(为此，我们只需在数据的一个小的随机样本上安装列转换器。)看看下面的代码:

    ```
    coltrans.fit(X_train.sample(1000)) new_binary_cols = \   coltrans.\   named_transformers_['bin'].\   named_steps['onehotencoder'].\   get_feature_names(binary_cols) new_cat_cols = \   coltrans.\   named_transformers_['cat'].\   named_steps['onehotencoder'].\   get_feature_names(cat_cols)
    ```

2.  让我们来看看功能名称:

    ```
    new_cols = np.concatenate((new_binary_cols,    new_cat_cols, np.array(spec_cols1 + spec_cols2 +   num_cols))) new_cols array(['smoking_Yes', 'alcoholdrinkingheavy_Yes',        'stroke_Yes', 'walkingdifficult_Yes',        'physicalactivity_Yes', 'asthma_Yes',        'kidneydisease_Yes', 'skincancer_Yes',        'gender_Male', 'ethnicity_Asian',        'ethnicity_Black', 'ethnicity_Hispanic',        'ethnicity_Other', 'ethnicity_White',        'agecategory', 'genhealth', 'diabetic', 'bmi',        'physicalhealthbaddays', 'mentalhealthbaddays',        'sleeptimenightly'], dtype=object)
    ```

3.  在拟合决策树之前，我们需要处理不平衡的数据集。我们可以使用不平衡学习中的`SMOTENC`模块对心脏病类进行过采样。这个将产生足够的心脏病类的代表性实例来平衡类成员。

接下来，我们必须实例化一个决策树分类器，并指出叶节点至少需要五个观察值，并且树的深度不能超过两个。然后，我们将使用列转换器来转换定型数据并拟合模型。

我们将在本节的后面做一些超参数调整。目前，我们只想生成一个易于解释和可视化的决策树。

`SMOTENC`需要知道哪些列是分类的。当我们设置列转换器时，我们首先编码二进制列，然后编码分类列。因此，二进制列的数量加上分类列的数量就给出了这些列索引的端点。然后，我们必须向`SMOTENC`的`categorical_features`参数传递一个范围，从 0 开始到分类列数结束。

现在，我们可以创建一个包含列转换、过采样和决策树分类器的管道，并对其进行调整:

```
catcolscnt = new_binary_cols.shape[0] + \
  new_cat_cols.shape[0]
smotenc = \
  SMOTENC(categorical_features=np.arange(0,catcolscnt),
  random_state=0)
dtc_example = DecisionTreeClassifier(
  min_samples_leaf=5, max_depth=2)
pipe0 = make_pipeline(coltrans, smotenc, dtc_example)
pipe0.fit(X_train, y_train.values.ravel())
```

注意

当我们担心我们的模型在捕获一个类中的变化方面做得不好时，过采样可能是一个好的选择，因为相对于一个或多个其他类，我们对该类的实例太少了。过采样复制该类的实例。

**合成少数过采样技术** ( **SMOTE** )是一种使用 KNN 复制实例的算法。本章中 SMOTE 的实现来自不平衡学习，特别是 SMOTENC，它可以处理分类数据。

当类别不平衡甚至比该数据集更严重时，例如 100 比 1，通常会进行过采样。尽管如此，我认为在本章中演示如何使用 SMOTE 和类似的工具是有帮助的。

1.  运行拟合后，我们可以看看哪些特征被认为是重要的。`agecategory`、`genhealth`和`diabetic`是这个简单模型

    ```
    feature_imp = \   pipe0.named_steps['decisiontreeclassifier'].\   tree_.compute_feature_importances(normalize=False) feature_impgt0 = feature_imp>0 feature_implabs = np.column_stack((feature_imp.\   ravel(), new_cols)) feature_implabs[feature_impgt0] array([[0.10241844433036575, 'agecategory'],        [0.04956947743193013, 'genhealth'],        [0.012777650193266089, 'diabetic']],       dtype=object)
    ```

    中的重要特征
2.  接下来，我们可以生成一个决策树的图形:

    ```
    plot_tree(pipe0.named_steps['decisiontreeclassifier'],   feature_names=new_cols,    class_names=['No Disease','Disease'], fontsize=10)
    ```

这会产生以下情节:

![Figure 11.3 – Decision tree example with a heart disease target

](img/B17978_11_0031.jpg)

图 11.3–以心脏病为目标的决策树示例

在根节点(也称为深度 0)的初始二进制分裂是基于`agecategory`是否小于或等于`6`。(回想一下`agecategory`原本是人物特征。在我们编码后，`50-54`的初始值得到一个值`6`。)如果根节点语句为真，它将导致节点向下一级并向左。如果语句为假，它将导致节点向右下一级。样本数是到达该节点的观测值的数量。因此，`diabetic<=0.001`(即非糖尿病)节点处的`12576`的样本值反映了来自父节点的语句为真的实例的数量；也就是说，`12576`实例的年龄类别值小于或等于`6`。

每个节点中的`value`列表为我们提供了训练数据中每个类的实例。在这种情况下，第一个值用于无疾病观察的计数。第二个值是对心脏病的观察计数。例如，`diabetic<=0.001`节点有`10781`无疾病观察和`1795`疾病观察。

决策树预测叶子模式下最频繁的类。因此，该模型将预测小于或等于 54 岁(`agecategory<=6`)且非糖尿病(`diabetic<=0.001`)的个体没有疾病。在训练数据中没有该组的`10142`疾病观察，和`990`疾病观察。这给了我们一个具有非常好的 Gini 杂质`0.162`的叶节点。

如果这个人有糖尿病，即使他们是 54 岁或更年轻，我们的模型预测心脏病。然而，它对此的预测却不那么明确。基尼杂质是`0.493`。相比之下，对 54 岁以上(`agecategory<=6.001`为假)健康状况不太好(`genhealth<=3.0`)的个人的疾病预测，基尼系数明显较低。

我们的模型预测，54 岁以上、总体健康水平相当于`4`的人没有疾病。(当我们进行列转换时，我们将*优秀*的一般健康值编码为`4`。)然而，糟糕的基尼系数表明，我们的模型并没有信心十足地做出这样的预测。

1.  让我们来看看这个模型的一些指标。该模型具有不错的灵敏度，虽然不是很高，但预测约 70%的时间有心脏病。精确度，即我们做出正面预测时的正确率，是相当低的。我们做出正面预测的时候，只有 19%是正确的:

    ```
    pred = pipe0.predict(X_test) print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred,     pos_label=0),   skmet.precision_score(y_test.values.ravel(), pred))) accuracy: 0.72, sensitivity: 0.70, specificity: 0.73, precision: 0.19
    ```

该模型在超参数方面做出了一些相当随意的决定，将最大深度设置在`2`处，将一片叶子的最小样本数设置在`5`处。我们应该探索这些超参数的替代值，这将产生一个性能更好的模型。让我们做一个随机网格搜索来找到这些值。

1.  让我们用列转换、过采样和决策树分类器建立一个管道。我们还将创建一个字典，其中包含最小叶子大小和最大树深度超参数的范围。注意，对于每个字典键，`decisiontreeclassifier`后面有两个下划线:

    ```
    dtc = DecisionTreeClassifier(random_state=0) pipe1 = make_pipeline(coltrans, smotenc, dtc) dtc_params = {  'decisiontreeclassifier__min_samples_leaf': randint(100, 1200),  'decisiontreeclassifier__max_depth': randint(2, 11) }
    ```

2.  现在，我们可以进行随机网格搜索。我们将运行 20 次迭代来测试超参数的大量值:

    ```
    rs = RandomizedSearchCV(pipe1, dtc_params, cv=5,   n_iter=20, scoring="roc_auc") rs.fit(X_train, y_train.values.ravel()) rs.best_params_ {'decisiontreeclassifier__max_depth': 9,  'decisiontreeclassifier__min_samples_leaf': 954} rs.best_score_ 0.7964540832005679
    ```

3.  让我们看看每次迭代的分数:

    ```
    results = \   pd.DataFrame(rs.cv_results_['mean_test_score'], \     columns=['meanscore']).\   join(pd.DataFrame(rs.cv_results_['params'])).\   sort_values(['meanscore'], ascending=False).\   rename(columns=\     {'decisiontreeclassifier__max_depth':'maxdepth',      'decisiontreeclassifier__min_samples_leaf':\      'samples'})
    ```

这会产生以下输出。表现最好的模型比我们之前构建的模型有更大的`max_depth`。我们的模型在每个叶的最小实例数高得多的情况下也做得最好:

```
 meanscore  maxdepth  samples
15      0.796         9      954
13      0.796         8      988
4       0.795         7      439
19      0.795         9      919
12      0.794         9      856
3       0.794         9      510
2       0.794         9     1038
5       0.793         8      575
0       0.793        10     1152
10      0.793         7     1080
6       0.793         6     1013
8       0.793        10      431
17      0.793         6      896
14      0.792         6      545
16      0.784         5      180
1       0.778         4      366
11      0.775         4      286
9       0.773         4      138
18      0.768         3      358
7       0.765         3      907 
```

1.  让我们生成一个混淆矩阵:

    ```
    pred2 = rs.predict(X_test) cm = skmet.confusion_matrix(y_test, pred2) cmplot = \   skmet.ConfusionMatrixDisplay(confusion_matrix=cm,   display_labels=['Negative', 'Positive']) cmplot.plot() cmplot.ax_.\   set(title='Heart Disease Prediction Confusion Matrix',    xlabel='Predicted Value', ylabel='Actual Value') 
    ```

这产生了下面的情节:

![Figure 11.4 – Heart disease confusion matrix from the decision tree model

](img/B17978_11_0041.jpg)

图 11.4-决策树模型中的心脏病混淆矩阵

关于这个图，你可能已经注意到了，我们的精度有多低。绝大多数时候我们预测积极，我们是错误的。

1.  让我们看看准确性、敏感性、特异性和精确度分数，看看在没有超参数调整的情况下，模型的指标是否有很大的改善。我们在敏感度方面明显更差，现在是 63%,而以前是 70%。然而，我们在特异性方面做得更好。现在，我们在 79%的时间里对测试数据做出了否定的正确预测，相比之下，早期的模型为 73%:

    ```
    print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred2),    skmet.recall_score(y_test.values.ravel(), pred2),    skmet.recall_score(y_test.values.ravel(), pred2,      pos_label=0),    skmet.precision_score(y_test.values.ravel(),       pred2))) accuracy: 0.77, sensitivity: 0.63, specificity: 0.79, precision: 0.21
    ```

决策树是分类模型的良好起点。它们对底层数据做很少的假设，并且不需要太多的预处理。请注意，在这个例子中，我们没有进行任何缩放或离群点检测，因为这对于决策树来说通常并不重要。我们还得到了一个相当容易理解或解释的模型，只要我们限制深度的数量。

我们通常可以通过随机森林来提高决策树模型的性能，原因我们在本章开始时已经讨论过。一个关键原因是当使用随机森林而不是决策树时，方差减少了。

在下一节中，我们将看看随机森林。

# 实现随机森林

让我们尝试用一个随机森林来改进我们的心脏病模型:

1.  首先，让我们加载我们在上一节中使用的相同的库，除了我们这次将导入随机森林分类器:

    ```
    import pandas as pd import numpy as np from imblearn.pipeline import make_pipeline from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import RandomizedSearchCV from scipy.stats import randint import sklearn.metrics as skmet import os import sys sys.path.append(os.getcwd() + "/helperfunctions") import healthinfo as hi
    ```

我们还加载了`healthinfo`模块；它加载健康信息数据并进行预处理。这里没有什么花哨的东西。我们之前逐步执行的预处理代码只是被复制到当前工作目录的`helperfunctions`子文件夹中。

1.  现在，让我们获取已经由`healthinfo`模块处理的数据，以便我们可以将它用于我们的随机森林分类器:

    ```
    X_train = hi.X_train X_test = hi.X_test y_train = hi.y_train y_test = hi.y_test
    ```

2.  让我们实例化一个随机森林分类器，并为网格搜索创建一个管道。我们还将为要搜索的超参数创建一个字典。除了我们用于决策树的`max_depth`和`min_samples_leaf`超参数，随机森林的一个重要超参数是`n_estimators`。这表示用于该搜索迭代的树的数量。除了`gini`之外，我们还将添加`entropy`作为标准，到目前为止一直在使用:

    ```
    rfc = RandomForestClassifier(random_state=0) pipe1 = make_pipeline(hi.coltrans, hi.smotenc, rfc) rfc_params = {  'randomforestclassifier__min_samples_leaf':     randint(100, 1200),  'randomforestclassifier__max_depth':      randint(2, 11),  'randomforestclassifier__n_estimators':      randint(100, 3000),  'randomforestclassifier__criterion':      ['gini','entropy'] } rs = RandomizedSearchCV(pipe1, rfc_params, cv=5,    n_iter=20, scoring="roc_auc") rs.fit(X_train, y_train.values.ravel())
    ```

3.  我们可以使用随机网格搜索对象的`best_params_`和`best_score_`属性来分别找到最佳参数和相关分数。最好的模型有`1023`棵树，最大深度为`9`。

在这里，我们可以看到`roc_auc`分数相对于上一节决策树模型的一些改进:

```
rs.best_params_
{'randomforestclassifier__criterion': 'gini',
 'randomforestclassifier__max_depth': 9,
 'randomforestclassifier__min_samples_leaf': 667,
 'randomforestclassifier__n_estimators': 1023}
rs.best_score_
0.8210934290375318
```

1.  随机森林的结果比单个决策树的结果更难解释，但是从特征重要性开始是一个好的起点。前三个特征与我们在决策树中看到的特征相同，即`agecategory`、`genhealth`和`diabetic` :

    ```
    feature_imp = \   rs.best_estimator_['randomforestclassifier'].\   feature_importances_ feature_implabs = np.column_stack((feature_imp.\   ravel(), hi.new_cols)) pd.DataFrame(feature_implabs,   columns=['importance','feature']).\   sort_values(['importance'], ascending=False)          importance       feature 14       0.321            agecategory 15       0.269            genhealth 16       0.159            diabetic 13       0.058            ethnicity_White 0        0.053            smoking_Yes 18       0.033            physicalhealthbaddays 8        0.027            gender_Male 3        0.024            walkingdifficult_Yes 20       0.019            sleeptimenightly 11       0.010            ethnicity_Hispanic 17       0.007            bmi 19       0.007            mentalhealthbaddays 1        0.007            alcoholdrinkingheavy_Yes 5        0.003            asthma_Yes 10       0.002            ethnicity_Black 4        0.001            physicalactivity_Yes 7        0.001            skincancer_Yes 2        0.000            stroke_Yes 6        0.000            kidneydisease_Yes 9        0.000            ethnicity_Asian 12       0.000            ethnicity_Other
    ```

2.  让我们来看一些指标。与以前的模型相比，灵敏度有所提高，尽管在其他方面没有太大的提高。我们保持相同的相对体面的特异性分数。总的来说，这是我们目前为止最好的模型:

    ```
    print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred,     pos_label=0),   skmet.precision_score(y_test.values.ravel(), pred))) accuracy: 0.77, sensitivity: 0.69, specificity: 0.78, precision: 0.22
    ```

我们仍然可以改进我们模型的性能指标。我们至少应该尝试一些梯度推进。正如我们在本章的*使用梯度增强决策树*一节中所讨论的，梯度增强决策树有时会产生比随机森林更好的模型。这是因为每棵树都从以前的错误中学习。

# 实施梯度推进

在这一节中，我们将尝试使用梯度推进来改进我们的随机森林模型。我们必须注意的一件事是过度拟合，这可能是梯度推进决策树比随机森林更大的问题。这是因为随机森林的树不从其他树学习，而使用梯度增强，每棵树都建立在先前树的学习上。这里我们对超参数的选择是关键。让我们开始吧:

1.  我们将从导入必要的库开始。我们将使用与随机森林相同的模块，除了我们将从`ensemble`而不是`RandomForestClassifier` :

    ```
    import pandas as pd import numpy as np from imblearn.pipeline import make_pipeline from sklearn.model_selection import RandomizedSearchCV from sklearn.ensemble import GradientBoostingClassifier import sklearn.metrics as skmet from scipy.stats import uniform from scipy.stats import randint import matplotlib.pyplot as plt import os import sys sys.path.append(os.getcwd() + "/helperfunctions") import healthinfo as hi
    ```

    导入`GradientBoostingClassifier`
2.  现在，让我们为我们的随机森林分类器

    ```
    X_train = hi.X_train X_test = hi.X_test y_train = hi.y_train y_test = hi.y_test
    ```

    获取已经由`healthinfo`模块处理的数据
3.  接下来，我们将实例化一个梯度增强分类器实例，并将其添加到一个管道中，以及我们预处理健康数据的步骤(在我们导入的模块中)。

我们将创建一个包含梯度增强分类器的超参数的字典。这些包括熟悉的每叶最小样本数、最大深度和估计数超参数。我们还将添加值来检查学习率:

```
gbc = GradientBoostingClassifier(random_state=0)
pipe1 = make_pipeline(hi.coltrans, hi.smotenc, gbc)
gbc_params = {
 'gradientboostingclassifier__min_samples_leaf':
     randint(100, 1200),
 'gradientboostingclassifier__max_depth':
     randint(2, 20),
 'gradientboostingclassifier__learning_rate':
     uniform(loc=0.02, scale=0.25),
 'gradientboostingclassifier__n_estimators':
     randint(100, 1200)
}
```

1.  现在，我们准备好进行网格搜索。(请注意，这可能需要一些时间才能在您的计算机上运行。)我们运行的七次迭代中的最佳模型具有`0.25`和`308`估计器或树的学习率。它有一个体面的`roc_auc`分数`0.82` :

    ```
    rs = RandomizedSearchCV(pipe1, gbc_params, cv=5,    n_iter=7, scoring="roc_auc") rs.fit(X_train, y_train.values.ravel()) rs.best_params_ {'gradientboostingclassifier__learning_rate': 0.2528,  'gradientboostingclassifier__max_depth': 3,  'gradientboostingclassifier__min_samples_leaf': 565,  'gradientboostingclassifier__n_estimators': 308} rs.best_score_ 0.8162378796382679
    ```

2.  让我们看看特性的重要性:

    ```
    feature_imp = pd.Series(rs.\   best_estimator_['gradientboostingclassifier'].\   feature_importances_, index=hi.new_cols) feature_imp.loc[feature_imp>0.01].\     plot(kind='barh') plt.tight_layout()     plt.title('Gradient Boosting Feature Importance')
    ```

这会产生以下情节:

![Figure 11.5 – Gradient boosting feature importance

](img/B17978_11_005.jpg)

图 11.5–梯度增强特征的重要性

1.  让我们来看一些指标。有趣的是，我们获得了出色的准确性和特异性，但却有着极其糟糕的灵敏度。这可能是由于过度拟合:

    ```
    pred = rs.predict(X_test) print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %   (skmet.accuracy_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred),   skmet.recall_score(y_test.values.ravel(), pred,     pos_label=0),   skmet.precision_score(y_test.values.ravel(), pred))) accuracy: 0.91, sensitivity: 0.19, specificity: 0.97, precision: 0.40
    ```

虽然我们的结果是混合的，梯度提升决策树通常是分类模型的一个很好的选择。如果我们正在对特征和目标之间的复杂关系进行建模，这一点尤其正确。当决策树是一个合适的选择，但我们担心决策树模型的高方差时，梯度推进决策树至少是与随机森林一样好的选择。

现在，让我们总结一下本章所学的内容。

# 总结

本章探讨了如何使用决策树解决分类问题。尽管本章中的例子都涉及到一个二元目标，我们使用的算法也可以处理多类问题。与从逻辑回归到多项式逻辑回归的转换不同，当我们的目标有两个以上的值时，很少需要改变就可以很好地使用算法。

我们研究了两种处理决策树高方差的方法。一种方法是使用随机森林，这是一种装袋的形式。这将减少我们预测的差异。另一种方法是使用梯度推进决策树。Boosting 可以帮助我们捕捉数据中非常复杂的关系，但也存在不小的过度拟合风险。牢记这一点来调整我们的超参数尤其重要。

在下一章中，我们将探索另一个著名的分类算法:K-最近邻。